MIT OpenCourseWare 
http://ocw.mit.edu
 
 
 
18.02 Multivariable Calculus
Fall 2007
 
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.  
 

M.  Matrices and Linear Algebra 

1.  Matrix algebra. 
In  section D we  calculated  t h e  de te rm inan ts  of  squa re  a r rays  of  numbers.  Such a r r ay s  a r e  
impo r tan t   in  ma thema t ics   and   i t s   applications;  th ey   a r e   called  matrices.  I n   general,  they  
need  not  be   squa re ,  only  rectangular. 

A rectangular  a r r ay  of  numbers having m  rows and  n  columns is called  a n  m  x n  ma t r ix .  
T h e  number  in  t h e   i - th   row  and   j - t h   column  (where  1 5 i 5 m ,   1 5 j  5 n )   is  called  t h e  
ij-entry,  and   denoted  a i j ;  t h e  ma t r ix   itself  is  denoted  by  A,  o r   sometimes  by  ( a i j ) .  

Two matrices  of  t h e  same  size  a r e   equal if  corresponding entries a r e  equa l .  
Two  special  kinds  of  matrices  a r e  t h e   row-vectors:  t h e   1 x  n  matrices  ( a l ,  a z ,  . . . ,a,); 
and  t h e  column  vectors:  t h e  m  x  1 matrices  consisting  of  a  column  of  m  numbers. 

From now on ,  row-vectors  o r  column-vectors will b e  indicated by boldface small 
letters;  when  writing  th em   by  h and ,  p u t   a n  arrow over  t h e  symbol. 

Matrix  operations 
Th e r e  a r e  four  basic  operations which  produce  new  matrices  from old. 
1.  Scalar  multiplication:  Multiply  each en t ry  by  c  : cA = (ca i j )  
2.  M a t r i x   addition:  Add  t h e   corresponding  entries:  A + B  =  ( a i j  + bij); 
matrices mus t   have  t h e  same  number  of  rows  and   t h e  same number  of  columns. 
3.  Transposition:  T h e  transpose of t h e  m  x n  ma t r ix  A is t h e  n  x m  m a t r ix  obtained  by 
making  t h e  rows  of  A t h e  columns of  t h e  new  ma t r ix .   Common no ta t ions   for  t h e  t ranspose  
a r e  AT  and  A'; using  t h e  first  we  can  write  i t s  definition  as AT = ( a j i ) .  

t h e   two 

If  t h e  m a t r ix  A  is  squa re ,  you  can  th ink   of  AT  a s  t h e  ma t r ix   obtained  by  flipping A over 
around  i t s  main  diagonal. 

Example  1 .1   Let  A = 

2 

-3 

1 5  

.  F i n d A + B ,   AT,  2 A - 3 B .  

2 

18.02 NOTES  

4.  Matrix  multiplication  This is th e  most  impo r tan t  operation.  Schematically, we  have 

m x n  

n x p  

m x P  

T h e  essential points  a re :  

1.  For  t h e  multiplication  t o  be  defined, A must  have  a s  many  columns a s  B  has   rows; 

2 .   Th e  i j - t h  en t ry   of  t h e  product  ma t r ix   C  is  t h e  do t   product  of  t h e   i - t h  row  of A with 
t h e  j - t h   column  of  B .  

T h e  two most  impo r tan t   types of  multiplication,  for multivariable  calculus and  
differential  equations,  are: 
1.  AB ,  where A  and  B  a re  two  square matrices of  t h e  same size - these  can 
always be multiplied; 
2 .   A b ,  where A  is  a  square n  x  n  ma t r ix ,  and  b is  a  column  n-vector. 

Laws  and   p rope r t ies   of  ma t r ix   multiplication 
(A + B ) C  = A C  + B C  
M-1.  A ( B  + C )  = A B  + AC, 
M-2.  ( A B ) C  = A ( B C ) ;  
(cA) B  = c (AB ) .  

In  bo th   cases, t h e  matrices must  have  compatible  dimensions. 

distributive  laws 

associative  laws 

M-3.  Let  I3= 

; 

then  A I  = A  and   I A  = A  for  any  3 x  3 ma t r ix .  

I is  called  t h e   iden t i ty   ma t r ix   of  order  3.  The re  is  a n   analogously defined  squa re  identity 
ma t r ix   Inof  any  order n ,  obeying  t h e  same multiplication  laws. 
M-4.  In  general, for two square n x n  matrices A and  B ,   A B  # BA :   ma t r ix  multiplication 
is no t   commutative.  (The re  a r e  a few  impo r tan t  exceptions, bu t   they  a r e  very  special - for 
example,  t h e  equality  A I  = I A   where  I is  t h e  identity matrix.) 
M-5.  For  two  squa re  n  x  n  matrices A  and  B ,  we  have  t h e   determinant  law: 

lABl  = IAIJBI, 

also written 

d e t (AB )  = d e t (A ) d e t (B )  

For  2  x  2  matrices,  th is   can  be  verified  by  direct  calculation,  bu t   th is   naive  me thod   is 
unsuitable  for  larger  matrices;  i t 's   be t te r  t o  use  some theory.  We  will  simply  assume i t   in  
these  notes; we  will  also  assume t h e  other  results  above  (of  which  only  t h e   associative  law 
M-2 offers any  difficulty  in   t h e  proof). 

M .   MATRICES  AND   LINEAR   ALGEBRA  

3 

M-6.  A useful fact is this:  matrix multiplication  can be  used  t o  pick  ou t  a row or column 
of  a given matrix:  you multiply  by  a  simple row  or column vector  t o  do this.  Two examples 
should give  the  idea: 
( i   %  g ) (8)  =  ( i )
th e  second column 
( 1   0  0 )  (: 4  :5  6 :) = ( 1   2  3 )  
Exercises:  Section  1F 

t h e f i r s t r ow

2 .   Solving square  systems  of  linear  equations; inverse matrices. 
Linear  algebra  is  essentially  about  solving  systems  of  linear  equations,  an   important 
application  of  mathematics  t o   real-world  problems  in  engineering,  business,  and  science, 
especially th e  social sciences. Here we  will  just  stick  t o  th e  most  impo r tan t  case, where  th e  
system  is  square, i.e.,  there a re  as  many  variables as  there a re  equations.  In   low  dimensions 
such  systems  look  as  follows  (we give  a  2  x  2  system  and  a  3 x  3 system): 

In  these  systems, th e  a i j  and  bi a re  given, and we  want  t o  solve for  th e  x i .  

As  a  simple mathematical  example,  consider  th e   linear  change  of  coordinates  given  by 
th e  equations 

If  we  know  th e   y-coordinates  of  a  point,  then  these  equations  tell  us  its  x-coordinates 
immediately.  Bu t   if  instead  we  a re  given  th e   x-coordinates,  t o   find  th e   y-coordinates  we 
must  solve a  system  of  equations  like  (7) above, with  th e  yi  as  th e  unknowns. 

Using matrix multiplication, we  can  abbreviate  the   system on  th e  right  in  (7) by 

where A  is  th e  square matrix  of  coefficients ( a i j  ) .   (The  2  x  2  system  and  th e  n  x n  system 
would  be  written  analogously;  all  of  them  a re   abbreviated  by  th e   same equation  Ax   = b, 
notice.) 

You  have had  experience with  solving small  systems  like  (7) by  elimination:  multiplying 
th e   equations  by  constants  and  subtracting  them  from  each  other,  th e   purpose  being  t o  

4 

18.02  NOTES  

eliminate all t h e  variables bu t  one.  When elimination is done systematically, i t  is a n  efficient 
method.  Here however  we  want  t o  ta lk   abou t   ano the r  method  more  compatible  with  hand- 
held  calculators and  Ma tLab ,  and  which  leads more rapidly  t o  certain  key  ideas and  results 
in  linear  algebra. 

Inverse matrices. 

Referring t o  t h e  system  (8 ) ,  suppose we  can find  a  squa re  ma t r ix  M ,  t h e  same size as A ,  
such  t h a t  

(9) 

M A  = I 

( th e  identity ma t r ix ) .  

We  can then   solve  (8) by  ma t r ix  multiplication,  using  t h e  successive steps, 

where  t h e  s t ep   M ( A x )  = x  is justified  by 

M ( A x )  = (M A ) x ,  
= I x ,  
= x ,  

by  M-2; 
by  (9 ) ;  
by  M-3  . 

Moreover, t h e  solution  is  unique,  since  (10) gives  a n  explicit  formula  for  i t .  

T h e  same  procedure  solves t h e  problem  of  determining  t h e  inverse  t o   t h e   linear  change 
of  coordinates  x  = Ay,  as t h e  next  example  illustrates. 

E x a m p l e   2.1  Let  A  = 

1 2
2, 
( 2   3 )   and   M  =  ( -32  -1 ' 
above, and  use  i t  t o  solve t h e  first  system  below  for xi and  t h e  second for  t h e  yi  in te rms  of 
t h e  xi: 

Verify  t h a t   M  satisfies  (9) 

S o l u t i o n .   We  have  ( 2   3 )   ( -32  -12,  =  ( i   :), by  ma t r ix   multiplication.  To 
2
1
solve  t h e   first  system,  we  have  by  ( l o ) ,   ( )  =  (  2  -1  (3=  (A:),so  t h e  
solution is x1 = 11,x2 = -6.  By  reasoning similar t o  t h a t  used  above in going from Ax  = b 
t o  x  = M b ,  t h e  solution  t o  x  = Ay   is  y  = M x ,  so t h a t  we  get 

as t h e  expression for  t h e  yi  in   te rms  of  t h e  xi. 

Ou r   problem  now  is:  how  do we  get  t h e  ma t r ix   M ?   In  practice,  you  mostly  press  a  key 
on  t h e  calculator, or  type   a  Ma t lab   command.  Bu t   we  need  t o  be  able  t o  work  abs t rac t ly  

M.  MATR ICES   AND   L INEAR   ALGEBRA  

5 

with t h e  ma t r ix  - i.e.,  with  symbols, no t   just  numbers,  and  for  th is  some theoretical  ideas 
a r e  impo r tan t .   T h e  first  is  t h a t  M  doesn't  always exist. 

IAI  # 0. 
M  exists 
Th e  implication  + follows immediately  from t h e  law M-5, since 

Th e  implication  in  t h e  o the r  direction  requires more; for  t h e  low-dimensional  cases, we  will 
produce  a  formula  for  M .   Let's go  t o   t h e   formal  definition  first,  and   give  M  i t s   proper 
name, A-l: 

Definition.  Let  A  be  an  n  x  n  ma t r ix ,  with  IAl  #  0.  Then   t h e   inverse of  A  is  a n  n  x  n 
ma t r ix ,  w r i t ten  A-',  such t h a t  

( I t  is actually enough t o  verify either equation; t h e  other follows automatically - see th e  
exercises.) 

Using  th e   above no ta t ion ,  our  previous  reasoning  (9)  - (10) shows t h a t  

(12) 
(12) 

IAl  # 0  +  th e  unique  solution of  A x  = b  is  x  = A-'b; 
JAl # 0  +  t h e  solution  of  x  = A y   for  t h e  yi  is  y  = A-'x. 

Calculating  the   inverse o f  a  3 x  3  matrix 

Let  A be  t h e  matrix.  T h e  formulas for  i t s  inverse A-'  and  for a n  auxiliary ma t r ix   ad j  A 
called  t h e  adjoint  of  A  (or in  some books  th e  adjugate of  A)  a re  

(13) 

T 

1  A l l   A12  A13 
t::)  ' 
a d j  A  =  - Azl  Azz
1  ( A   A 
1
In  t h e  formula,  Aij  is  t h e   cofactor  of  t h e  element  a i j   in  t h e  ma t r ix ,   i.e.,  i t s  minor  with  i ts  
sign changed  by  t h e  checkerboard  rule  (see section  1 on  determinants). 

1 
A-'  =  -I 
A1 

Formula  (13) shows  t h a t   t h e  s teps  in  calculating  t h e  inverse ma t r ix   a re :  

1 .  Calculate t h e  ma t r ix   of  minors. 

2.  Change  t h e  signs  of  t h e  entries according to  t h e  checkerboard rule. 

3.  3 -anspose  t h e  resulting  ma t r ix ;  th is  gives  ad j  A. 

4.  Divide  every  en t ry  by  IAl. 
(If  inconvenient, for  example  if  i t  would  produce  a  ma t r ix   having fractions  for  every  entry, 
you  can just  leave t h e   1 / IA (  factor outside, as  in  t h e  formula.  Note  t h a t   s t ep  4 can only  be 
taken   if  IAl  # 0, so  if  you  haven't  checked this  before,  you'll be  reminded  of  i t  now.) 
T h e  no ta t ion  Aij  for a cofactor makes i t  look like a ma t r ix ,  ra the r  t h a n  a signed 
determinant;  th is   isn't  good, bu t   we  can  live with  i t .  

6 

18.02 NOTES  

Example 2 .2   Find  t h e  inverse  t o  A  = 

Solution.  We  calculate t h a t   IAl  = 2.  Then  t h e  steps  a r e  (T means  t ranspose ) :  

1  0  -1 

1  -1 

1 0  

ma t r ix   A 

cofactor ma t r ix  

T 

ad j  A 

inverse  of  A 

To  get  practice in ma t r ix  multiplication,  check  t h a t   A .  A-l  = I ,  or  t o  avoid  t h e  fractions, 
check  t h a t  A .  ad j   (A) = 21. 
Th e   same procedure  works  for  calculating  t h e  inverse  of  a  2 x  2 ma t r ix   A.  We  do  i t   for 
a  general  ma t r ix ,   since i t   will  save  you  time  in  differential  equations  if  you  can  learn  t h e  
resulting  formula. 

ma t r ix  A 

cofactors 

T 

ad j  A 

inverse  of  A 

Example 2.3  F ind   t h e  inverses to :  

3  2 
Solution.  a )  Use  t h e  formula:  IAI  = 2,  so A-'  = -2  ( - 3   1 )  = ( 4  $ )   . 
2 0  
1 
0
1
b )   Follow  t h e  previous  scheme: 

2  2 

Bo th   solutions  should be  checked by multiplying  th e  answer by  t h e  respective  A. 

Proof  of  formula  (13)  for  t h e   inverse matrix. 

We  want  t o  show A .  A-l  = I ,  or equivalently, A .  ad j  A = IA J I ;  when  th is   last  is written 
out  using  (13)  (remembering t o  transpose  t h e  ma t r ix  on  t h e  right  th e r e ) ,  it  becomes 

To prove  (14), i t  will be  enough  t o  look  a t  two  typical  entries  in  t h e  ma t r ix  on  t h e  right -
say  t h e  first  two  in  t h e  top   row.  According  t o  t h e  rule  for multiplying  t h e  two matrices on 
t h e  left, wha t   we  have  t o  show is  t h a t  

M .   MATRICES  A N D   LINEAR  ALGEBRA  

7 

These  two  equations  a re   bo th   evaluating  determinants  by  Laplace  expansions:  th e   first 
equation  (15)  evaluates  t h e  determinant  on  t h e  left  below  by  t h e  cofactors of  t h e  first row; 
th e   second  equation  (16)  evaluates  t h e  determinant  on  t h e   right  below  by  t h e   cofactors of 
th e   second  row  (notice  t h a t   t h e   cofactors  of  t h e   second  row  don't  care wha t 's  actually  in 
th e  second  row,  since  t o  calculate  them   you  only  need  t o  know  t h e  other  two  rows). 

Th e   two  equations  (15)  and   (16)  now  follow, since  t h e  de te rm inan t   on  t h e   left  is  just  J A l ,  
while t h e  determinant  on  t h e  right  is  0,  since two  of  i ts   rows  a r e  t h e  same. 

T h e  procedure we  have given for calculating  an  inverse works for n x n matrices, bu t  gets 
t o   be  too   cumbersome  if  n  > 3 ,  a n d  other  methods  a r e  used.  T h e   calculation  of  A-'  for 
reasonable-sized  A  is a  s t and a rd  package in computer  algebra programs and  Ma tLab .   Unfor-
tunately,  social scientists often want  t h e  inverses of  very  large matrices,  a n d  for  this  special 
techniques have had   t o  be  devised,  which  produce  approximate bu t   acceptable results. 
Exercises:  Section  1G  

3.  Cramer's rule  (some  18.02 classes  omit  this) 
T h e  general  square system  and   i ts  solution may  be written 

When  this  solution  is  written  ou t   and   simplified,  i t   becomes  a  rule  for  solving  t h e  system 
A x  = b  known  as Cramer's  rule.  We  illustrate with  t h e  2  x  2  case  first; t h e  system  is 

Th e  solution  is, according t o   (17), 

If  we  write ou t   t h e  answer  using  determinants,  i t  becomes  Crarner's  rule: 

Th e  formulas in  t h e  3 x  3  case a r e  similar, and  may  be  expressed  th is  way: 
Crarner's rule.  If  IAI  # 0,  th e  solution  of  A x = b  i s  given  by 
l Ail
x i   =  -
1A.I  ' 
where  lAil  is  th e  determinant  obtained  by  replacing  the   i - t h   column  of  IA(  by  th e   column 
vector  b. 

8 

18.02 NOTES  

Cramer's rule is particularly useful if  one only wants one of  t h e  x i ,  a s  in t h e  next example. 
Example  3.1.  Solve for  x ,  using  Cramer's rule  (19): 

Solution.  We  rewrite  th e  system  on  t h e  left  below,  then   use  Cramer's  rule (19): 
::I 
I-: 

-7 

Proof  of  ( 1 9 ) .   Since  t h e   solution  t o   t h e   system  is  x  = A - 'b ,   when  we  write  i t   ou t  
explicitly,  i t   becomes 

A l l   A21  A31 

A13  A23  A33 

We  show  t h a t   th is   gives  formula  (19)  for  X I ;   t h e   arguments  for  t h e   other  xi  go  similarly. 
From  t h e  definition  of  ma t r ix  multiplication,  we  get  from t h e  above 

according  t o   t h e   Laplace  expansion  of  t h e   determinant  by  i t s   first  column.  Bu t   th is   last 
equation  is  exactly  Cramer's rule  for  finding  x l .  

Cramer's rule  is  also  valid  for  n  x  n  systems;  i t   is  not  normally  used  for  systems  larger 
th an  3 x 3 however.  You would  use A - l ,   or systematic elimination of  variables.  Nonetheless, 
t h e  formula  (19) is  impo r tan t   as  a  theoretical  tool  in  proofs  and   derivations. 
Exercises:  Section 1H  

4.  Theorems about  homogeneous and  inhomogeneous  systems. 
On  t h e   basis  of  our  work  so  fa r ,   we  can  formulate  a  few  general  results  abou t   square 
systems  of  linear  equations.  They  a r e   t h e   theorems  most  frequently  referred  t o   in  t h e  
applications. 

Definition.  Th e   linear  system  Ax  = b  is  called  homogeneous if  b = 0 ;  otherwise,  i t   is 
called  inhomogeneous. 

M .  MATRICES  AND   LINEAR  ALGEBRA 

Theorem 1.  Let  A  be  an n x  n ma t r ix .  

(20) 
(21) 

IAl  # 0 
IAl  # 0 

A x  = b  h a s   th e  unique solution,  x = A - l b   . 
A x  = 0  h a s  only  th e  trivial solution,  x = 0. 

Notice  t h a t   (21) is  t h e  special case of  (20) where b = 0 .  Often  i t   is  s t a t ed  a n d  used  in  t h e  
contrapositive form: 

(21') 

A x  = 0  has  a  non-zero  solution  +-
(The   contrapositive  of  a  s ta temen t   P +- Q  is  not-Q  +- n o t - P ;   t h e  two  s ta temen ts   say 
t h e  same  thing.) 

IAl  = 0. 

Theorem 2 .   L e t  A  be  an  n  x  n ma t r ix .  

(22) 

IAl  = 0  +- A X  = 0  has  non-trivial  (i.e.,  non-zero)  solutions. 

(23) 

IAl  = 0  +- A x  = b  usually h a s  no  solutions,  but  h a s  solutions for some b .  

In  (23), we  call  t h e  system consistent  if  i t   has  solutions,  inconsistent  otherwise. 

This  probably  seems  like  a  maze  of  similar-sounding and   confusing  theorems.  Let's  get 
ano the r  perspective on  these ideas by  seeing how they  apply separately t o  homogeneous  a n d  
inhomogeneous  systems. 
Homogeneous systems:  A x  = 0  h a s  non-trivial  solutions w  IAl  = 0. 
Inhomogeneous  systems:  A x  = b h a s   the   unique  solution x = A - l b ,   if  (A1 # 0. 
If  IA( = 0,  then A x  = b  usually has  n o  solutions,  bu t   does have solutions for some b .  

T h e  s ta temen ts   (20) and   (21) a r e  proved, since we  have  a  formula for  t h e  solution, and  i t  
easy  t o  see by  multiplying  A x  = b by  A - I   t h a t   if  x is  a  solution,  i t  must  b e  of  t h e  form 
= A - 'b .  
We  prove  (22)  just  for  t h e   3  x  3  case,  by  interpreting  i t   geometrically.  We  will  give  a 
pa r t ia l   argument  for  (23), based  on bo th   algebra and  geometry. 
Proof of  ( 2 2 ) .  
We  represent  t h e   th ree   rows  of  A  by  th e   row  vectors  a ,  b ,  c  and   we  let  x  =  ( x ,y,z); 
th ink  of  all these a s  origin  vectors, i.e.,  place  their  tails  a t  th e  origin.  Th en ,  considering t h e  
homogeneous  system first, 

(24) 

A x  = 0 

is  t h e  same a s  t h e  system 

a .  x = 0,  b . x = 0 ,   c  . x = 0. 

In other words, we  a r e  looking for a  row vector  x which  is orthogonal t o  th ree  given vectors, 
namely  th e  th ree   rows of  A .   By  Section  1, we  have 

IAl  = a . b x  c  =  volume of  parallelepiped  spanned by  a ,  b ,  c .  

I t  follows t h a t  if  IAl  = 0, t h e  parallelepiped has zero volume, and  therefore t h e  origin vectors 
a ,  b ,  c  lie  in  a  plane.  Any  non-zero  vector  x which  is  orthogonal t o  th is   plane  will  th en   be 
orthogonal t o  a,b, c ,  and  therefore will  be  a  solution  t o  t h e  system  (24).  Th is  proves  (22): 
if  JAl= 0, then   A x  = 0  has  a  nontrivial  solution. 

10  

18 .02   NOTES 

P a r t i a l  p r o o f   o f   (23 ) .   We  write  the  system as  A x  = d ,   where d  is  th e  column vector 
d = ( d l ,  d2, d3)T  . 
Writing  this out  as we  did  in  (24), i t   becomes  th e  system 

If  IAl  = 0 ,  th e   three  origin  vectors  a ,  b, c  lie  in  a  plane,  which  means  we  can  write  one  of 
them, say  c ,  as  a  linear  combination  of  a and  b :  
c = r a  + s b ,  
Then  if  x  is  any vector,  it  follows th a t  

r . s   real  numbers. 

(26) 

Now  if  x  is  also a  solution  t o   (25), we  see from  (25) and  (27) t h a t  

this  shows  t h a t   unless  th e   components  of  d  satisfy  th e   relation  (28),  there  cannot  be  a 
solution  t o  (25); thus  in  general  there a re  no  solutions. 

If  however,  d  does  satisfy  th e   relation  (28), then  th e   last  equation  in  (25)  is  a  conse- 
quence of  th e  first  two and  can be  discarded, and we  get  a  system of  two  equations  in  three 
unknowns, which will  in  general have  a  non-zero solution, unless  they   represent  two planes 
which  a re  parallel. 

S i n g u l a r  m a t r i c e s ;   c o m p u t a t i o n a l  d i f f icu l t ies .  

Because  so much  depends  on whether  IAl  is  zero  or  not,  th is  property  is  given  a  name. 
We  say  th e  square matrix A is  singular if  IAl  = 0 ,  and  nonsingular or  invertible if  IAl  # 0. 
Indeed, we  know th a t  A-I  exists if  and only if  IAl  # 0, which explains th e  term 
"invertible";  th e  use  of  "singular"  will  be  familiar  t o  Sherlock Holmes  fans:  i t   is 
the   19th century  version of  "peculiar"  or  th e  late  20th  century  word  "special". 
Even  if  A  is  nonsingular,  the   solution  of  Ax  = b  is  likely  t o  run  in to  trouble  if  IAl  z 0, 
or  as  one  says, A  is  almost-singular.  Namely,  in  th e   formulas  given  in  Cramer's rule  (19), 
th e   IAI  occurs  in  th e   denominator,  so  t h a t   unless  there  is  some  sort  of  compensation  for 
this  in  th e  numerator,  th e   solutions  are  likely  t o  be  very  sensitive  t o  small  changes  in  th e  
coefficients of  A,  i.e.,  t o   th e   coefficients  of  th e   equations.  Systems  (of  any  kind)  whose 
solutions  behave  this way  are said  t o  be  ill-conditioned;  i t   is  difficult  t o  solve such  systems 
numerically  and  special methods must  be  used. 
To see the  difficulty geometrically, think of  a  2 x  2 system Ax  = b as  representing a pair 
of  lines;  th e   solution  is  th e   point  in  which  they  intersect.  If  IAl  E 0,  bu t   its  entries  are 
not  small,  then   its  two  rows  must  be  vectors  which  a re   almost  parallel  (since  they  span  a 
parallelogram  of  small  a rea ) .   The  two  lines are therefore  almost parallel; their  intersection 
point  exists, bu t   its  position  is  highly  sensitive  t o  th e  exact  positions  of  th e  two  lines,  i.e., 
t o  th e  values  of  th e  coefficients of  th e  system of  equations. 

Exercises:  Section 1H 

