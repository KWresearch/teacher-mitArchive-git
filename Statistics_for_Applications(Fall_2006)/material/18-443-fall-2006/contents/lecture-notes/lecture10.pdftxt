Section  9 

Most  powerful  tests  for  two  simple 
hypotheses. 

Now  that  we  learned  how  to  construct  the  decision  rule  that  minimizes  the  Bayes  error  we 
will  turn to our next goal - constructing  the decision  rule with controlled error of type 1 that 
minimizes  error  of  type  2. Given  � 
[0, 1]  we  consider  the  class  of  decision  rules 
⇒ 
K�  = {ξ  : P1 (ξ =∅ H1 ) � �} 
and  we  will  try  to  ﬁnd  ξ K�  that  makes  the  type  2  error  �2  =  P2 (ξ  =  H2 )  as  small  as 
⇒
possible. 
Theorem.  Assume  that  there  exists  a  constant  c,  such  that 
 
⎨ f1 (X ) 
⎩
< c  = �. 
P1 
f2 (X ) 
 
�
f1 (X ) 
H1  : 
f2 (X )  � c 
f1 (X )  < c 
H2  : 
f2 (X ) 

Then  the  decision  rule 

(9.0.1)

(9.0.2)

ξ = 

is  the  most  powerful  in  class  K� .

Proof.  Take  � (1)  and  � (2)  such  that


� (1) + � (2) = 1, 

� (2)
� (1) 

= c,

i.e. 

� (1) = 

and  � (2) = 

c 
1 
. 
1 + c 
1 + c 
Then  the  decision  rule  ξ  in  (9.0.2)  is  the  Bayes  decision  rule  corresponding  to  weights  � (1) 
and  � (2) which  can  be  seen  by  comparing  it with  (8.0.1),  only  here we  break  the  tie  in  favor 
of  H1 .  Therefore,  this  decision  rule  ξ  minimizes  the  Bayes  error  which  means  that  for  any 
other  decision  rule  ξ � , 
� (1)P1(ξ  ∅= H1 ) + � (2)P2(ξ  ∅= H2 ) � � (1)P1 (ξ �  ∅= H1 ) + � (2)P2(ξ �  ∅= H2 ). 
57 


(9.0.3) 

∅
By  assumption  (9.0.1)  we  have 

 
⎨ f1 (X ) 
⎩
P1 (ξ = H1 ) = P1 
< c  = �, 
f2 (X ) 
which  means  that  ξ  comes  from  the  class K� .  If  ξ �  K�  then
⇒ 
P1 (ξ �  =∅ H1 ) � � 

and  equation  (9.0.3)  gives  us  that 
� (1)� + � (2)P2 (ξ =∅ H2 ) � � (1)� + � (2)P2(ξ �  =∅ H2 ) 

and,  therefore, 

P2 (ξ =∅ H2 ) � P2 (ξ �  =∅ H2 ). 
This  exactly  means  that  ξ  is more  powerful  than  any  other  decision  rule  in  class  K� . 

or


Example.  Suppose  we  have  a  sample  X  = (X1 , . . . , Xn)  and  two  simple  hypotheses 
H1  : P = N (0, 1)  and H2  : P = N (1, 1).  Let  us  ﬁnd most  powerful  ξ  with  the  error  of  type  1 
�1  � � = 0.05. 
According  to  the  above  Theorem  if  we  can  ﬁnd  c  such  that 
 
⎨ f1 (X ) 
⎩
< c  = � = 0.05 
P1 
f2 (X ) 
then  we  know  how  to  ﬁnd  ξ.  Simplifying  this  equation  gives 
 
 
n 
⎨�
⎩
2  − log c  = � = 0.05 
Xi  > 
P1 
  1  �
 
 
1  n 
⎩
⎨
Xi  > c�  =  ∞n 
2  − log c) = � = 0.05. 
P1  ∞n
(
But  under  the  hypothesis  H1  the  sample  comes  from  standard  normal  distribution  P1  = 
N (0, 1)  which  implies  that  the  random  variable 
1  �
∞n 
is  standard  normal. We  can  use  the  table  of  normal  distribution  to  ﬁnd 
P(Y  > c� ) = � = 0.05 ≤ c�  = 1.64. 
Therefore,  the most  powerful  test  with  level  of  signiﬁcance  � = 0.05  is: 
 
 
�
1  �
Xi  � 1.64
H1  : 
�n
 
n �
Xi  > 1.64. 
H2  :  �1
58 

 
Xi

Y  = 

ξ = 

∅
What  will  the  error  of  type  2  be  for  this  test? 

 

  1  �
⎨
⎩
Xi  � 1.64 
�2  =  P2 (ξ =∅ H2 ) = P2  ∞n
 
 
n 
 
1 
(Xi  − 1) � 1.64 − ∞n ⎩
=  P2 ⎨
n �
. 
∞
i=1 
The  reason  we  subtracted  1  from  each  Xi  is  because  under  the  second  hypothesis  X ’s  have 
distribution  N (1, 1)  and  random  variable 
 
n
1  �
(Xi  − 1) 
Y  =  ∞n 
i=1 
will  be  standard  normal.  Therefore,  the  error  of  type  2  for  this  test  will  be  equal  to 
P(Y < 1.64 − ∞n) = N (0, 1)(−∼, 1.64 − ∞n). 
For  example,  when  the  sample  size  n = 10, �2  = P(Y < 1.64 − ∞10) = 0.087. 
Randomized  most  powerful  test. 
Next  we  will  show  how  to  get  rid  of  the  assumption  (9.0.1)  which,  unfortunately,  does 
not  always  hold  as  will  become  clear  from  examples  below. 
If  we  examine  carefully  the  proof  of  Theorem  we  notice  that  condition  (9.0.1)  was  nec­
essary  to  ensure  that  the  likelihood  ratio  test  has  error  of  type  1  exactly!  equal  to  �.  Also, 
the  test  was  designed  to  be  a  Bayes  test  and  in  Bayes  tests  we  have  a  freedom  of  breaking 
a  tie  in  an  arbitrary way.  In  the  following  version  of previous  theorem  we  will  show  that  the 
most  powerful  test  in  class  K�  can  always  be  constructed  by  breaking  a  tie  randomly  in  a 
way  that makes  error  of  type  1  exactly  equal  to  �. 
[0, ∼)  and  p 
Theorem.  Given  any  � 
[0, 1]  we  can  always  ﬁnd  c 
⇒ 
⇒
 
⎨ f1 (X ) 
⎨ f1 (X ) 
⎩
⎩
= c  = �. 
< c  + (1 − p)P1 
P1 
f2 (X ) 
f2 (X ) 
In  this  case,  the  most  powerful  test  ξ K�  is  given  by 
⇒ 
 
�
f1 (X )
> c 
H1  : 
 
⎧
f2 (X ) 
�
f1 (X )
f2 (X )  < c 
ξ =  H2  : 
 
⎧
f1 (X )
pH1  + (1 − p)H2  :
= c,
�
f2 (X ) 
where  in  the  case  of  equality  we  break  the  tie  randomly  by  picking  H1  with  probability  p  and 
H2  with probability 1 − p. This test  ξ  is cal led a randomized most powerful  test  for  two  simple 
hypotheses  at  the  level  of  signiﬁcance  �. 
Proof.  Let  us  ﬁrst  assume  that  we  can  ﬁnd  c  and  p  such  that  (9.0.4)  holds.  Then  the 
error  of  type  1  for  the  randomized  test  ξ  is 
⎨ f1 (X ) 
�1  = P1 (ξ =∅ H1 ) = P1 
f2 (X ) 

 
⎩
= c  = �, 

(9.0.5) 

[0, 1]  such  that 

⇒ 

(9.0.4)

⎨ f1 (X ) 
⎩
< c  + (1 − p)P1 
f2 (X )
59 


since  ξ  does  not  pick  H1  exactly  when  the  likelihood  ratio  is  less  than  c  or  when  it  is  equal 
to  c  in  which  case H1  is  not  picked  with  probability  1 − p.  This means  that  the  randomized 
test  ξ K� .  The  rest  of  the  proof  repeats  the  proof  of  the  previous  theorem. We  only  need 
⇒
to  point  out  that  our  randomized  test  will  still  be  a  Bayes  test  since  in  the  case  of  equality 

f1 (X ) 
f2 (X ) 

= c 

 
⎩
< c  = � 

the  Bayes  test  allows  us  to  break  the  tie  arbitrarily  and  we  choose  to  break  it  randomly  in 
a  way  that  ensures  that  the  error  of  type  one  will  be  equal  to  �,  as  in  (9.0.5). 
The  only question  left  is why we  can always  choose  c and p  such  that  (9.0.4)  is  satisﬁed. 
If  we  look  at  the  function 
 
⎨ f1 (X ) 
⎩
F (t) = P 
< t 
f2 (X ) 
as a  function of  t,  it will  increase  from 0 to 1 as  t  increases  from 0 to ∼. Let us keep  in mind 
that,  in  general,  F (t)  might  have  jumps.  We  can  have  two  possibilities:  either  (a)  at  some 
point  t = c  the  function  F (c)  will  be  equal  to  �,  i.e. 
⎨ f1 (X ) 
F (c) = P 
f2 (X ) 
or  (b)  at  some  point  t = c  it  will  jump  over  �,  i.e. 
 
⎨ f1 (X ) 
⎩
< c  < � 
F (c) = P 
f2 (X ) 
 
⎨ f1 (X ) 
⎨ f1 (X ) 
⎩
⎩
= c  � �. 
f2 (X )  � c  = F (c) + P 
P 
f2 (X )
In  both  cases  we  ﬁnd  p  by  solving  (9.0.4)  with  this  value  of  c.  In  the  case  (a)  we  get  p = 1 
and  for  (b)  we  get 
 
� ⎨ f1 (X ) 
⎩
= c . 
1 − p = (� − F (c))  P 
f2 (X )
Example. Suppose  that we have one observation X  � B (p) from Bernoulli distribution 
with  probability  of  success  p 
(0, 1).  Consider  two  simple  hypotheses 
⇒ 
H1  : p = 0.2,  i.e.  f1 (x) = 0.2x0.81−x ,

H2  : p = 0.4,  i.e.  f2 (x) = 0.4x0.61−x .


but 

Let  us  take  the  level  of  signiﬁcance  � = 0.05  and  ﬁnd  the most  powerful  ξ K0.05 .  In  ﬁgure 
⇒
9.1  we  show  the  graph  of  the  function 

⎨ f1 (X ) 
F (c) = P1 
f2 (X ) 
60 

 
⎩
< c  . 

PSfrag replacements

1 

0.2 

1
2 

c 

4 
3 

Figure  9.1: Graph  of  F (c). 

1/2,  if  X  = 1 
4/3,  if  X  = 0. 

Let  us  explain  how  this  graph  is  obtained.  First  of  all,  the  likelihood  ratio  can  take  only 
two  values: 
 
f1 (X )  �
= 
f2 (X )
If  c � 1/2  then  the  set 
 
⎪ f1 (X ) 
�
< c  = ≥  is  empty  and  F (c) = P1 (≥) = 0. 
f2 (X ) 
If  1/2 < c � 4/3  then  the  set 
 
⎪ f1 (X ) 
�
< c  = {X  = 1}  and  F (c) = P1 (X  = 1) = 0.2. 
f2 (X ) 
Finally,  if  4/3 < c  then  the  set 
 
⎪ f1 (X ) 
�
< c  = {X  = 0  or  1}  and  F (c) = P1 (X  = 0  or  1) = 1. 
f2 (X ) 
The  function  F (c)  jumps  over  the  level  �  = 0.05  at  the  point  c  = 1/2.  To  determine  p  we 
have  to make  sure  that  the  error  of  type  one  is  equal  to  0.05,  i.e. 
 
⎨ f1 (X ) 
⎨ f1 (X ) 
⎩
⎩
< c  + (1 − p)P1 
= c  = 0 + (1 − p)0.2 = 0.05 
P1 
f2 (X ) 
f2 (X )
which  gives  that  p = 3/4.  Therefore,  the most  powerful  test  of  size  � = 0.05  is 
 
�
f1 (X )
>  or X  = 0 
H1  : 
1
 
⎧
2 
f2 (X )
�
1  or  never 
f1 (X )
f2 (X )  <  2 
ξ =  H2  : 
  3 H1  +  1 H2  : 
⎧�
f1 (X )  =  1  or X  = 1.
2 
4 
4
f2 (X )
In the example above one could also consider two or more observations. Another example 
would be to consider,  let’s  say,  two observations  from Poisson distribution �(�) and test  two 
simple  hypotheses  H1  : � = 0.1  vs.  H2  : � = 0.3. 

61 

