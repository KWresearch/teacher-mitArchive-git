Lecture  6 

Gamma  distribution,  �2-distribution, 

Student  t-distribution,

Fisher  F -distribution. 


1 = 

0

Gamma  distribution.  Let  us  take  two  parameters  � >  0  and  � >  0.  Gamma  function 
�(�)  is  deﬁned  by 
 
�

x �−1 e−xdx. 

�(�) = 

� 

0 

y �−1 e−�y dy 

�  � � 
�(�)

f (x|�, � ) = 

e−�x 
, x ∼ 0 
x < 0 

�  1 
x �−1 e−xdx = 
�(�) 

If  we  divide  both  sides  by  �(�)  we  get 
 
�
�
0 
where  we made  a  change  of  variables  x = � y .  Therefore,  if  we  deﬁne 
  �� 
�
�(�) x�−1
0, 
then  f (x �, � ) will be  a probability  density  function  since  it  is  nonnegative  and  it  integrates 
|
to  one. 
Deﬁnition.  The  distribution  with  p.d.f.  f (x �, � )  is  called  Gamma  distribution  with 
|
parameters  �  and  �  and  it  is  denoted  as  �(�, � ). 
Next,  let us  recall  some properties  of gamma  function �(�).  If we  take � > 1  then using 
integration  by  parts  we  can  write: 
 
�
�
x �−1 e−xdx = 
x �−1 d(−e−x ) 
0 
 
0
 
�
� 
�
 � 
=  x �−1 (−e−x ) 
(−e−x )(� − 1)x �−2 dx 
0  − 
�
 
�
0 
 
� 
�
x(�−1)−1 e−xdx = (� − 1)�(� − 1). 
= (� − 1) 
0 

�(�) = 

� 

 

� 

 

35 

Since  for  � = 1  we  have 

we  can  write 

�(1) = 

 

� 

0 

�

e−xdx = 1 

� 

k

x

= 

EX k 

· 
· 
· 
· 
�(2) = 1  1,  �(3) = 2  1,  �(4) = 3  2  1,  �(5) = 4  3  2  1
·
·
·
and  proceeding  by  induction  we  get  that  �(n) = (n − 1)! 
Let  us  compute  the  kth moment  of  gamma  distribution. We  have, 
 
 
� �  �
�
� � 
�−1 e−�xdx = 
x 
�(�) 
�(�) 
0
 
�  � �+k 
�
� �  �(� + k) 
x �+k−1 e−�xdx 
�(�)  � �+k 
0  �(� + k) 
 
 
 
�
�
��
p.d.f.  of  �(� + k , � )  integrates  to  1 
� �  �(� + k) 
(� + k − 1)�(� + k − 1) 
�(� + k)
= 
=
�(�)  � �+k 
�(�)� k 
�(�)� k 
(� + k − 1)(� + k − 2) . . . ��(�)
(� + k − 1) · · · �
� k 
�(�)� k 

(�+k)−1 e−�xdx

= 

= 

� 

x

0 

=

=

. 

Therefore,  the mean  is 

the  second  moment  is 

and  the  variance 

EX  = 

� 
� 

EX 2 

= 

(� + 1)� 
� 2 

(� + 1)�  � � �2 
Var(X ) = EX 2  − (EX )2  = 
− 
� 
� 2 
Below  we  will  need  the  following  property  of Gamma  distribution. 
Lemma.  If  we  have  a  sequence  of  independent  random  variables 

= 

� 
� 2 . 

X1  � �(�1 , � ), . . . , Xn  � �(�n , � ) 
then  X1  + . . . + Xn  has  distribution  �(�1  + . . . + �n , � )

Proof.  If X  � �(�, � )  then  a moment  generating  function  (m.g.f.)  of X  is

 
 
�
�
� 
�  � � 
� � 
Ee tX  = 
e tx 
x �−1 e−�xdx = 
x �−1 e−(�−t)x dx 
�(�) 
�(�) 
0 
 
�  (� − t)� 
�
� � 
x �−1 e−(�−t)x dx . 
(� − t)� 
�(�) 
0 
 
 
 
��
�
�
36 

= 

0

The  function  in  the  last  (underbraced)  integral  is  a  p.d.f.  of  gamma  distribution  �(�, � − t) 
and,  therefore,  it  integrates  to  1. We  get, 
  �  �� 
�
Ee tX  = 
. 
� − t 
Moment  generating  function  of  the  sum �n
i=1 Xi  is 
  �  ��i 
 
 
n 
n 
n
tXi  ��
�
tXi  �
�
t Pn 
i=1 Xi  = E 
Ee 
e  = 
Ee  =
� − t
i=1 
i=1 
i=1 
and  this  is  again  a m.g.f.  of Gamma  distibution,  which  means  that 
 
 
 
 
n 
n
�
�
�
�
Xi  � � 
�i , �  . 
i=1 
i=1 

  �  � �i 
P
� − t 

= 

2 
t
e− 
2

dt. 

n -distribution.  In  the  previous  lecture  we  deﬁned  a  ∂2 
∂2 
n -distribution  with  n  degrees 
2 , where Xi s  are  i.i.d.  standard  normal. 
2 + . . . + Xn
of  freedom  as  a distribution  of  the  sum X1
n -distribution  coincides  with  a  gamma  distribution  �( n 
1 ), 
We  will  now  show  that  which  ∂2 
2 , 2
i.e. 
 
� n  1 �
∂2 
n  = � 
. 
, 
2 2 
Consider  a  standard  normal  random  variable X  � N (0, 1).  Let  us  compute  the  distribution 
of X 2 .  The  c.d.f.  of X 2  is  given  by 
  �x 
P(X 2  � x) = P(−≥x � X  � ≥x) = �
1
≥
−�x 
2α
The  p.d.f.  can  be  computed  by  taking  a  derivative  d  P(X  � x)  and  as  a  result  the  p.d.f.  of 
dx 
X 2  is 
  �x 
�
d 
(≥x)�  − ≥
(−≥x)� 
1
1
1
dt =  ≥
−�x  ≥
dx 
2α
2α
2α
1 
1 
1
2 −1 e− x 
e− x 
x 1
2  = 
≥2α 
≥2α ≥x 
2  .
1 ),  i.e.  we  proved  that  X 2  � �( 2
1 , 2
1 ). 
We  see  that  this  is  p.d.f.  of  Gamma  Distribution  �( 1
2 , 2
2  � �( n 
1 ). 
2  + . . . + Xn 
2 , 2
Using  Lemma  above  proves  that X1
Fisher  F -distribution.  Let  us  consider  two  independent  random  variables, 
 
� m  1 �
� k  1 �
X  � ∂2 
2 
Y  � ∂m 
= � 
, 
= � 
, 
.
k 
2 2 
2 2 
Deﬁnition:  Distribution  of  the  random  variable 

fX 2 (x) = 

x)2 
e− (�
2 

(−
e− 

x)2 
�
2 

2 
t
e−  2

= 

and 

Z  = 

X/k
Y /m 

37 

is  cal led  a  Fisher  distribution  with  degrees  of  freedom  k  and  m,  is  denoted  by  Fk,m . 
2 + . . . + X 2  for  i.i.d. 
First of all,  let us notice  that  since X  � ∂2  can be  represented  as X1
k 
k
standard  normal X1 , . . . , Xk ,  by  law  of  large  numbers, 
1 
2 ) � EX 2  = 1 
2  + . . . + Xk 
(X1
1 
k 
when  k � →.  This  means  that  when  k  is  large,  the  numerator  X/k  will  ’concentrate’  near 
1.  Similarly,  when  m  gets  large,  the  denominator  Y /m  will  concentrate  near  1.  This  means 
that  when  both  k  and m  get  large,  the  distribution  Fk,m  will  concentrate  near  1. 
Another  property  that  is  sometimes  useful  when  using  the  tables  of  F -distribution  is 
that 
  1 �
 
�
) = Fm,k  0, 
.
c 
  1 �
 X/k 
� Y /m 
�
1 �
�
�
Y /m  ∼ c  = P 
Fk,m(c, →) = P 
X/k  � 
= Fm,k  0,
c 
c 
Next  we  will  compute  the  p.d.f.  of  Z  � Fk,m .  Let  us  ﬁrst  compute  the  p.d.f.  of 
X 
k
Z  = 
. 
Y 
m

This  is  because 

Fk,m (c, 
→

 
. 

The  p.d.f.  of X  and  Y  are 

y

1
2

x


f (x) =


y
 m 
2 −1 e−

k 
1
−1 e−  x  and  g (y ) =

2
2

)
 m 
k 
( 1 
( 1 
)

2
2
2 
2
�( m )2
�( k 
)

2 
correspondingly, where  x ∼ 0  and  y ∼ 0. To ﬁnd  the p.d.f  of  the  ratio X/Y ,  let us  ﬁrst write 
its  c.d.f.  Since  X  and  Y  are  always  positive,  their  ratio  is  also  positive  and,  therefore,  for 
t ∼ 0  we  can  write: 

 
�
���
� X 
�
�
f (x)g (y )dx  dy 
P 
Y  �  t  = P(X  � tY ) = 
0 
0
since f (x)g (y ) is the  joint density of X, Y . Since we  integrate over  the set {x � ty} the  limits 
of  integration  for  x  vary  from  0  to  ty . 
Since  p.d.f.  is  the  derivative  of  c.d.f.,  the  p.d.f.  of  the  ratio  X/Y  can  be  computed  as 
follows: 

  ty 

 

 
d  � X 
�
Y  � t  = 
P 
dt 

=


=


  ty 

 

� �
d  �
dt  0
0 
 
k 
�  ( 1 
�
)

2
2 
�( k 
)

0 
2 
k+m 
( 1
2 ) 2
�( k )�( m )
2 
2

f (x)g (y )dxdy = 

 

� 

�
m 
−1
y

2

0 

f (ty )g (y )ydy 

1
e−  y ydy 
2

ty 

)
 m 
( 1 
2
2
�( m )2 

(ty )


k 
2 −1 

t


 

� 

k 
1
−1 e− 
2
2
�
0 
 
�
38 

y


1
2

( k+m 
)−1 
e−
2
 
��

 

(t+1)y dy
�

d 
dt

The  function  in  the  underbraced  integral  almost  looks  like  a  p.d.f.  of  gamma  distribution 
�(�, � )  with  parameters  � = (k + m)/2  and  �  = 1/2,  only  the  constant  in  front  is  missing. 
If  we miltiply  and  divide  by  this  constant,  we  will  get  that, 
 
 
�
P � X
�
Y  � t  = 

( 1  k+m 
m ) 
k+
2 )
�(
2 
t k 
2 −1 
2 
)�( m ) 
�( k 
k+m 
( 1 (
t + 1)) 
2 
2
2
2 
m ) 
�( k+
2 −1 (1 + t) k+m 
t k 
2 
2
)�( m )
�( k 
2 
2
since  the  p.d.f.  integrates  to  1.  To  summarize,  we  proved  that  the  p.d.f.  of  (k/m)Z  = X/Y 
is  given  by 

k+m 
�  ( 1 
t + 1)) 
2 (
2 
k+m ) 
�(
2 

m )−1 e− 1 (t+1)y dy 
y ( k+
2 
2 

= 

, 

0

fX/Y (t) = 

2 −1 (1 + t)− k+m 
t k 
2

. 

m ) 
�( k+
2 
�( k )�( m )
2 
2

 

Since 

� 
P(Z  � t) = fX/Y 
� t 

kt �
� X 
=≤  fZ (t) = 
P(Z  � t) = P 
Y  � 
m 
this  proves  that  the  p.d.f.  of  Fk,m -distribution  is 
�( k+m )  k � kt � k 
m 
kt �− k+
2 −1�
2 
2 
�( k )�( m ) m m 
m 
2 
2
m ) 
�( k+
kk/2 mm/2 t 2 −1 (m + kt)− k+m 
k 
2 
2
�( k )�( m )
2 
2

fk,m (t) = 

 
1 + 

= 

. 

. 

 k 
� kt �
m  m

, 

Student tn -distribution. Let us recall that we deﬁned tn -distibution as the distribution 
of  a  random  variable 
X1
 
T  =  �
1 (Y1
2  +
· · · 
+ Yn 
2 )
n 
if  X1 , Y1 , . . . , Yn  are  i.i.d.  standard  normal.  Let  us  compute  the  p.d.f.  of  T .  First,  we  can 
write, 
 
X 2 
�
�
1 
P(−t � T  � t) = P(T 2  �  t2 ) = P 
+ Y 2 )/n  � t2 . 
(Y 2  + 
· · · 
1 
n
If  fT (x)  denotes  the  p.d.f.  of  T  then  the  left  hand  side  can  be  written  as 
  t 
�

P(−t � T  � t) = 
On  the  other  hand,  by  deﬁnition, 

fT (x)dx. 

−t 

 

X 2 
1 
2  + . . . + Yn 
2 )/n 
(Y1
has  Fisher  F1,n -distribution  and,  therefore,  the  right  hand  side  can  be  written  as 
  t2 
�

f1,n (x)dx. 

0 

39 


We  get  that,


  t 

  t2 

�
�
−t 
0 
Taking  derivative  of  both  side  with  respect  to  t  gives 

fT (x)dx = 

f1,n (x)dx. 

fT (t) + fT (−t) = f1,n (t2 )2t. 
But fT (t) = fT (−t) since the distribution of T  is obviously symmetric, because the numerator 
X  has  symmetric  distribution  N (0, 1).  This,  ﬁnally,  proves  that 

fT (t) = f1,n(t2 )t = 

�( n+1 )
1 
2 
(1 + 
2 ) ≥n 
1 )�( n 
�( 2

t2 
n+1 
)−  2  . 
n 

40


