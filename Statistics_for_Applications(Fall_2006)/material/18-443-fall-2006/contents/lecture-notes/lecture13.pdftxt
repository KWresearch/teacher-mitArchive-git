Section  12 

Tests  of  independence  and 
homogeneity. 

In this lecture we will consider a situation when our observations are classiﬁed by two diﬀerent 
features and we would  like  to  test  if these  features are  independent. For example, we  can ask 
if  the  number  of  children  in  a  family  and  family  income  are  independent.  Our  sample  space 
X  will  consist  of  a × b  pairs 
X  = {(i, j ) : i = 1, . . . , a,  j  = 1, . . . , b} 
where  the ﬁrst  coordinate  represents  the ﬁrst  feature  that belongs  to one of a  categories  and 
the  second  coordinate  represents  the  second  feature  that  belongs  to  one  of  b  categories.  An 
i.i.d.  sample  X1 , . . . , Xn  can  be  represented  by  a  contingency  table  below  where  Nij  is  the 
number  all  observations  in  a  cell  (i, j ). 

Table  12.1:  Contingency  table. 
Feature  2

· · · 
2 
1
· · · 
N11  N12 
· · · 
N21  N22 
. 
.
.
. 
.
.
. 
.
.
· · · 
Na1  Na2 

Feature  1

1

2

. . . 
a 

b

N1b
N2b
. 
. 
. 
Nab

We  would  like  to  test  the  independence  of  two  features  which  means  that 

P(X  = (i, j )) = P(X 1  = i)P(X 2  = j ). 

If  we  introduce  the  notations 

P(X  = (i, j )) = αij ,  P(X 1  = i) = pi  and  P(X 2  = j ) = qj , 

77 

then  we  want  to  test  that  for  all  i  and  j  we  have  αij  =  pi qj .  Therefore,  our  hypotheses  can 
be  formulated  as  follows: 

H0  :  αij  = piqj  for  all  (i, j )  for  some  (p1 , . . . , pa )  and  (q1 , . . . , qb ) 
H1  :  otherwise. 

We  can  see  that  this  null  hypothesis  H0  is  a  special  case  of  the  composite  hypotheses  from 
previous  lecture  and  it  can  be  tested  using  the  chi-squared  goodness-of-ﬁt  test.  The  total 
number  of  groups  is  r = a × b.  Since  pi s  and  qj s  should  add  up  to  one 
p1  + . . . + pa  = 1  and  q1  + . . . + qb  = 1 

one  parameter  in  each  sequence,  for  example  pa  and  qb ,  can  be  computed  in  terms  of  other 
probabilities and we can take (p1 , . . . , pa−1 ) and (q1 , . . . , qb−1 ) as free parameters of the model. 
This  means  that  the  dimension  of  the  parameter  set  is 

2 
� �r
−s−1 

= �2 
ab−(a−1)−(b−1)−1

= �2 
(a−1)(b−1) 

s = (a − 1) + (b − 1). 
Therefore,  if  we  ﬁnd  the  maximum  likelihood  estimates  for  the  parameters  of  this  model 
then  the  chi-squared  statistic: 
 (Nij  − np�i qj� )2 
T  = �
np�i qj� 
i,j 
converges  in distribution  to �2
(a−1)(b−1)  distribution with  (a − 1)(b − 1) degrees of freedom. To 
formulate  the  test  it  remains  to  ﬁnd  the  maximum  likelihood  estimates  of  the  parameters. 
We  need  to maximize  the  likelihood  function 
 
 
j  Nij  �
(piqj )Nij  = �
�
p P
i 
j 
i,j 
i
where  we  introduced  the  notations 
�
�
i 
j
for  the  total  number  of  observations  in  the  ith  row  and  j th  column.  Since  p i s  and  qj s  are 
not  related  to  each  other,  maximizing  the  likelihood  function  above  is  equivalent  to  maxi-
 
 
mizing �
separately. Let us maximize �a
and �
N+j 
Ni+ 
Ni+
or,  taking  the  logarithm, 
i pi 
j  qj 
i=1 pi 
maximize 
a−1 
 
 
a
�
�
Ni+  log pi  = 
Ni+  log pi  + Na+  log(1 − p1  − . . . − pa ), 
i=1 
i=1 
since  the  probabilities  add  up  to  one.  Setting  derivative  in  pi  equal  to  zero,  we  get 

 
Pi Nij  = �
qj 
i

 
Nij  and  N+j  = 

 
Ni+ �
pi
j 

 
N+j 
qj

Ni+  = 

 
Nij 

Ni+ 
pi  − 

Na+ 
1 − p1  − . . . − pa−1 

= 

Ni+  Na+ 
pi  − 
pa 

= 0 

78 

or Ni+pa  = Na+ pi .  Adding  up  these  equations  for  all  i � a  gives 
Ni+ 
Na+ 
=≤ pi  = 
npa  = Na+  =≤ pa  =
. 
n 
n
Therefore,  we  get  that  the MLE  for  pi : 

p�i  = 

Ni+ 
. 
n 

Similarly,  the MLE  for  qj  is: 

N+j 
qj�  = 
n 
Therefore,  chi-square  statistic  T  in  this  case  can  be  written  as 
 (Nij  − Ni+N+j /n)2 
�
Ni+N+j /n
i,j 

T  = 

. 

and  the  decision  rule  is  given  by 

� = �
where  the  threshold  is  determined  from  the  condition 

H1  :  T  � c 
H2  :  T  > c 

 

�2
(a−1)(b−1) (c, + ) = �. 
→
Example.  In 1992 poll 189 Montana  residents were  asked whether  their personal ﬁnan­
cial  status  was  worse,  the  same  or  better  than  one  year  ago. The  opinions  were  divided  into 
three  groups  by  income  range:  under  20K,  between  20K  and  35K,  and  over  35K. We  would 
like  to  test  if  opinions  were  independent  of  income. 

Table  12.2: Montana  outlook  poll. 
b = 3

Worse  Same  Better

12 

15 
20
32 

27 
24
22 
23

14
67 

64 
58

a = 3

�  20K

(20K,  35K)

�  35K


47 
83 
59 
189 

The  chi-squared  statistic  is 
(20 − 47 × 58/189)2 
47 × 58/189 

T  =

+ . . . + 

(23 − 67 × 59/189)2 
67 × 59/189 

= 5.21. 

79 

If  we  take  level  of  signiﬁcance  � = 0.05  then  the  threshold  c  is: 
�2
2 (c, →) = � = 0.05 ≤ c = 9.488. 
(a−1)(b−1) (c, +→) = �4
Since  T  = 5.21 < c = 9.488  we  accept  the  null  hypothesis  that  opinions  are  independent  of 
income. 

Test  of  homogeneity. 
Suppose  that  the  population  is  divided  into  R  groups  and  each  group  (or  the  entire 
population)  is  divided  into  C  categories.  We  would  like  to  test  whether  the  distribution  of 
categories  in  each  group  is  the  same.


Table  12.3:  Test  of  homogeneity

· · ·  Category  C 
Category  1 
· · · 
N11 
N1C 
. 
. 
. 
. 
. 
. 
. 
. 
. 
· · · 
NRC 
NR1 
· · · 
N+1 
N+C 

Group  1 
. . .

Group R

 
�

 

�
N1+ 
. . . 
NR+ 
n 

If  we  denote 

P(Categoryj |Groupi ) = pij 
so  that  for  each  group  i � R  we  have 

 
pij  = 1 

C 
�
j=1 
then  we  want  to  test  the  following  hypotheses: 
H0  :  pij  = pj  for  all  groups  i � R 
H1  :  otherwise 
If  observations  X1 , . . . , Xn  are  sampled  independently  from  the  entire  population  then  ho­
mogeneity  over  groups  is  the  same  as  independence  of groups  and  categories.  Indeed,  if have 
homogeneity 

P(Categoryj |Groupi ) = P(Categoryj ) 

then  we  have 
P(Groupi , Categoryj ) = P(Categoryj |Groupi )P(Groupi ) = P(Categoryj )P(Groupi ) 
which  means  the  groups  and  categories  are  independent.  Another  way  around,  if  we  have 
independence  then 

P(Categoryj |Groupi )  = 
= 

P(Groupi , Categoryj ) 
P(Groupi ) 
P(Categoryj )P(Groupi ) 
P(Groupi ) 

80


= P(Categoryj ) 

which  is  homogeneity.  This means  that  to  test  homogeneity  we  can  use  the  test  of  indepen­
dence  above. 
Interestingly,  the  same  test  can  be used  in  the  case  when  the  sampling  is done  not  from 
the  entire  population  but  from  each  group  separately  which  means  that  we  decide  a  priori 
about  the  sample  size  in  each  group  - N1+ , . . . , NR+ . When  we  sample  from  the  entire  pop­
ulation  these  numbers  are  random  and  by  the  LLN  Ni+/n  will  approximate  the  probability 
P(Groupi ),  i.e. Ni+  reﬂects  the proportion  of  group  i  in  the population. When we  pick  these 
numbers  a priori one can  simply  think  that we artiﬁcially  renormalize  the proportion of each 
group  in  the  population  and  test  for  homogeneity  among  groups  as  independence  in  this 
new  artiﬁcial  population.  Another  way  to  argue  that  the  test  will  be  the  same  is  as  follows. 
Assume  that 

2 
� �C

−1 

P(Categoryj |Groupi ) = pj 
where  the probabilities  pj  are all given. Then by Pearson’s  theorem we have  the convergence 
in  distribution 
C 
(Nij  − Ni+pj )2 
�
Ni+pj 
j=1 
for  each  group  i � R  which  implies  that 
C 
R
(Nij  − Ni+pj )2 
�
�
Ni+pj
i=1  j=1 
since  the  samples  in  diﬀerent  groups  are  independent.  If  now  we  assume  that  probabilities 
p1 , . . . , pC  are  unknown  and  plug  in  the maximum  likelihood  estimates  p�j  = N+j /n  then 
C 
R
�
�
i=1  j=1 
because we have C − 1  free parameters p1 , . . . , pC−1  and estimating each unknown parameter 
results  in  losing  one  degree  of  freedom. 
Example  (Textbook,  page  560).  In  this  example,  100  people  were  asked  whether  the 
service  provided  by  the ﬁre department  in  the city was  satisfactory. Shortly after  the  survey, 
a  large  ﬁre  occured  in  the  city.  Suppose  that  the  same  100  people  were  asked  whether  they 
thought  that  the  service  provided  by  the  ﬁre  department  was  satisfactory.  The  result  are  in 
the  following  table: 

(Nij  − Ni+N+j /n)2 
Ni+N+j /n 

� �2 
R(C−1) 

� �2 
R(C−1)−(C−1) 

= �2 
(R−1)(C−1) 

Satisfactory  Unsatisfactory 
20 
80 
Before  ﬁre 
72 
28 
After  ﬁre 
Suppose  that  we  would  like  to  test  whether  the  opinions  changed  after  the  ﬁre  by  using  a 
chi-squared  test.  However,  the  i.i.d.  sample  consisted  of  pairs  of  opinions  of  100  people 

(X 1 , X 2 ), . . . , (X 1  , X 2  )
1 
1 
100
100

81 











where  the  ﬁrst  coordinate/feature  is  a  person’s  opinion  before  the  ﬁre  and  it  belongs  to  one 
of  two  categories 

{“Satisfactory“,  ”Unsatisfactory”}, 
and  the  second  coordinate/feature  is  a  person’s  opinion  after  the  ﬁre  and  it  also  belongs  to 
one  of  two  categories 

{“Satisfactory“,  ”Unsatisfactory”}. 
So the correct contingency  table corresponding  to the above data and satisfying  the assump­
tion  of  the  chi-squared  test  would  be  the  following: 

Sat.  after 
Uns.  after 

Sat.  before  Uns.  before

10

70 
2 
18 


In  order  to  use  the  ﬁrst  contingency  table,  we  would  have  to  poll  100  people  after  the  ﬁre 
independently  of  the  100  people  polled  before  the  ﬁre. 

82


