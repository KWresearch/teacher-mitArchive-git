Lecture  3 

Properties  of  MLE:  consistency, 
asymptotic  normality. 
Fisher  information. 

In  this  section  we  will  try  to  understand  why  MLEs  are  ’good’. 
Let  us  recall  two  facts  from  probability  that  we  be  used  often  throughout  this  course. 
•  Law  of  Large  Numbers  (LLN): 
If the distribution of the i.i.d. sample X1 , . . . , Xn  is such that X1  has ﬁnite expectation, 
i.e.  EX1 < ,  then  the  sample  average 
|
| →
X1  + . . . + Xn
� EX1 
n 
converges  to  its  expectation  in  probability  , which means  that  for  any  arbitrarily  small 
α > 0, 

¯
Xn  = 

¯P(|Xn  − EX1 | > θ) � 0  as  n � →. 
Note.  Whenever  we  will  use  the  LLN  below  we  will  simply  say  that  the  average 
converges  to  its  expectation  and will  not mention  in what  sense. More mathematically 
inclined  students are welcome  to carry out these steps more rigorously, especially when 
we  use  LLN  in  combination  with  the  Central  Limit  Theorem. 
•  Central  Limit  Theorem  (CLT): 
If the distribution of the  i.i.d.  sample X1 , . . . , Xn  is such  that X1  has ﬁnite expectation 
and  variance,  i.e.  |EX1 | < →  and  π 2  = Var(X ) < →,  then 
≥n(X¯n  − EX1 ) �d  N (0, π 2 ) 
converges  in distribution to normal distribution with zero mean and variance π 2 , which 
means  that  for  any  interval  [a, b], 
P �≥n(X¯n  − EX1 ) ∞  [a, b] �
16 

1 
2 x 
e− 
2�2  dx. 
≥2
∂π 

� �

 

 

b 

a

In other words,  the  random variable ≥n(X¯n − EX1 ) will behave  like  a random variable 
from  normal  distribution  when  n  gets  large. 
Exercise.  Illustrate  CLT  by  generating  100  Bernoulli  random  varibles  B (p)  (or  one 
Binomial  r.v. B (100, p))  and  then  computing ≥n(X¯n − EX1 ). Repeat  this many  times 
and use ’dﬁttool’ to see that this random quantity will be well approximated by normal 
distribution. 

We will prove that MLE satisﬁes (usually) the following two properties called consistency 
and  asymptotic  normality. 
1.	 Consistency.  We  say  that  an  estimate  ϕˆ is  consistent  if  ϕˆ �  ϕ0  in  probability  as 
n � →,  where  ϕ0  is  the  ’true’  unknown  parameter  of  the  distribution  of  the  sample. 
2.	 Asymptotic  Normality. We  say  that  ϕˆ is  asymptotically  normal  if 
≥n(ϕˆ − ϕ0 ) �d  N (0, π�
2 
0 ) 
0  is  called  the  asymptotic  variance  of  the  estimate  ϕˆ.  Asymptotic  normality 
2 
where  π�
says  that the estimator not only converges  to the unknown parameter, but it converges 
fast  enough,  at  a  rate  1/≥n. 
Consistency  of  MLE. 
To  make  our  discussion  as  simple  as  possible,  let  us  assume  that  a  likelihood  function 
is  smooth  and  behaves  in  a  nice  way  like  shown  in  ﬁgure  3.1,  i.e.  its  maximum  is  achieved 
at  a  unique  point  ϕ . ˆ

)
ϕ
(
�

1 

0.8 

0.6 

0.4 

0.2 

0 

−0.2 

−0.4 

−0.6 

−0.8 

−1 

�(ϕ) 

PSfrag replacements

0 

0.5 

1 

1.5 

2 
ϕˆ	

2.5 

3 

3.5 

4 

ϕ 

Figure  3.1: Maximum  Likelihood  Estimator  (MLE) 

Suppose  that  the  data  X1 , . . . , Xn  is  generated  from  a  distribution  with  unknown  pa­
rameter  ϕ0  and  ϕˆ is  a  MLE.  Why  ϕˆ converges  to  the  unknown  parameter  ϕ0 ?  This  is  not 
immediately  obvious  and  in  this  section  we  will  give  a  sketch  of  why  this  happens. 

17 



Ln (ϕ) = 

 
log f (Xi |ϕ) 

First  of  all, MLE  ϕˆ is  the maximizer  of

n1 �
n 
i=1 
1  (of  course,  this  does  not  aﬀect  maxi­
which  is  a  log-likelihood  function  normalized  by  n 
mization). Notice  that  function  Ln (ϕ)  depends  on  data. Let  us  consider  a  function  l(X ϕ) = 
|
log f (X ϕ)  and  deﬁne 
|

L(ϕ) = E�0 l(X ϕ),
|
where  E�0  denotes  the  expectation  with  respect  to  the  true  uknown  parameter  ϕ0  of  the 
sample  X1 , . . . , Xn .  If  we  deal  with  continuous  distributions  then 
 
�

(log f (x ϕ))f (x ϕ0 )dx.
|
|

L(ϕ) = 

By  law  of  large  numbers,  for  any  ϕ , 

Ln (ϕ) � E�0 l(X |ϕ) = L(ϕ). 
Note  that  L(ϕ)  does  not  depend  on  the  sample,  it  only  depends  on  ϕ .  We  will  need  the 
following 
Lemma. We  have  that  for  any  ϕ , 

L(ϕ) ≡ L(ϕ0 ). 
Moreover,  the  inequality  is  strict,  L(ϕ) < L(ϕ0 ),  unless 

P�0 (f (X ϕ) = f (X ϕ0 )) = 1.
|
|

which  means  that  P�  = P�0 . 
Proof.  Let  us  consider  the  diﬀerence 

X |
(
f
ϕ
|
(
f
X
ϕ

0

)
. 
) 

L(ϕ) − L(ϕ0 ) = E�0 (log f (X |ϕ) − log f (X |ϕ0 )) = E�0  log 
Since  log t ≡ t − 1,  we  can  write 
  � �
|
|
)  ≡  E�0 �
) − 1 �
) − 1 �
)
(
f
x
)
(
f
X
)
ϕ
ϕ
= 
|
|
(
f
x
ϕ
(
f
X
ϕ
0
0
 
 
�
�
f (x|ϕ0 )dx = 1 − 1 = 0. 
f (x|ϕ)dx − 
= 
Both  integrals  are  equal  to  1  because  we  are  integrating  the  probability  density  functions. 
This  proves  that  L(ϕ) − L(ϕ0 ) ≡ 0.  The  second  statement  of  Lemma  is  also  clear. 

 
|
f (x ϕ0 )dx

|
(
f
X
ϕ
|
(
f
X
ϕ

log 

E�0 

 

 

0

18


We  will  use  this  Lemma  to  sketch  the  consistency  of  the MLE. 
Theorem:  Under  some  regularity  conditions  on  the  family  of  distributions,  MLE  ϕˆ is 
consistent,  i.e.  ϕˆ � ϕ0  as  n � →. 
The statement of this Theorem is not very precise but but rather than proving a rigorous 
mathematical statement our goal here  is to illustrate  the main idea. Mathematically  inclined 
students  are  welcome  to  come  up  with  some  precise  statement. 

Ln (ϕ) 

L(ϕ) 

PSfrag replacements

ˆϕ 

ϕ0 

ϕ

Figure  3.2:  Illustration  to  Theorem. 

Proof. We  have  the  following  facts: 
1.  ϕˆ is  the maximizer  of  Ln (ϕ)  (by  deﬁnition). 
2.  ϕ0  is  the maximizer  of  L(ϕ)  (by  Lemma). 
3.  �ϕ  we  have  Ln (ϕ) � L(ϕ)  by  LLN.

This  situation  is  illustrated  in  ﬁgure  3.2.  Therefore,  since  two  functions  Ln  and  L  are

getting  closer,  the  points  of  maximum  should  also  get  closer  which  exactly  means  that 
ϕˆ � ϕ0 . 
Asymptotic  normality  of  MLE.  Fisher  information. 
We  want  to  show  the  asymptotic  normality  of MLE,  i.e.  to  show  that 
≥n(ϕˆ − ϕ0 ) �d  N (0, π 2 
)  for  some  π 2 
M LE 
M LE 
and  compute  π 2 
M LE .  This  asymptotic  variance  in  some  sense  measures  the  quality  of  MLE. 
First,  we  need  to  introduce  the  notion  called  Fisher  Information. 
Let  us  recall  that  above  we  deﬁned  the  function  l(X ϕ)  =  log f (X ϕ).  To  simplify  the 
|
|
notations  we  will  denote  by  l � (X ϕ), l �� (X ϕ),  etc.  the  derivatives  of  l(X ϕ) with  respect  to 
|
|
|
ϕ . 
Deﬁnition.  (Fisher  information.)  Fisher  information  of  a  random  variable  X  with 
distribution  P�0  from  the  family  {P�  : ϕ ∞ �}  is  deﬁned  by 
  � 
�
�
I (ϕ0 ) = E�0 (l � (X |ϕ0 ))2  � E�0  � ϕ 
log f (X |ϕ)��
19


�2 
 
. 
�=�0 

Remark. Let us give a very informal interpretation of Fisher information. The derivative


0

) 
0
)

�
|
X
(
f
ϕ
l � (X |ϕ0 ) = (log f (X |ϕ0 ))�  = 
|
(
f
X
ϕ
can  be  interpreted  as  a  measure  of  how  quickly  the  distribution  density  or  p.f.  will  change 
when we slightly change the parameter ϕ near ϕ0 . When we square  this and take expectation, 
i.e.  average  over  X,  we  get  an  averaged  version  of  this  measure.  So  if  Fisher  information  is 
large,  this means  that  the  distribution  will  change  quickly  when  we move  the  parameter,  so 
the  distribution  with  parameter  ϕ0  is  ’quite  diﬀerent’  and  ’can  be  well  distinguished’  from 
the  distributions  with  parameters  not  so  close  to  ϕ0 .  This  means  that  we  should  be  able  to 
estimate  ϕ0  well  based  on  the  data.  On  the  other  hand,  if  Fisher  information  is  small,  this 
means  that  the  distribution  is  ’very  similar’  to  distributions  with  parameter  not  so  close 
to  ϕ0  and,  thus,  more  diﬃcult  to  distinguish,  so  our  estimation  will  be  worse.  We  will  see 
precisely  this  behavior  in  Theorem  below. 
Next  lemma  gives  another  often  convenient  way  to  compute  Fisher  information. 
Lemma. We  have, 

� 2 
E�0 l �� (X |ϕ0 ) � E�0  � ϕ2  log f (X |ϕ0 ) = −I (ϕ0 ). 

Proof.  First  of  all,  we  have 

l � (X |ϕ) = (log f (X |ϕ))�  = 

) 
�
X |
ϕ
(
f
|
)
(
f
X
ϕ

and 

) 
��
|
ϕ
X
(
f
|
)
(
f
X
ϕ

2 
. 

(f
f

�
|ϕ
))
(
X
) 
2
|ϕ
X
(

− 

(log f (X |ϕ))��  = 
Also,  since  p.d.f.  integrates  to  1, 
 
�
f (x ϕ)dx = 1,
|
if  we  take  derivatives  of  this  equation  with  respect  to  ϕ  (and  interchange  derivative  and 
integral,  which  can  usually  be  done)  we  will  get, 
 
 
�
�
� 2 
� 
� ϕ2 f (x|ϕ)dx = 
f (x|ϕ)dx = 0  and 
� ϕ 
To  ﬁnish  the  proof  we  write  the  following  computation 
 
�
� 2 
(log f (x ϕ0 ))��f (x ϕ0 )dx
E�0 l �� (X ϕ0 ) =  E�0 
log f (X ϕ0 ) = 
|
|
|
|
� ϕ2 
 
=  �
) 
) 
��
�
|
|
�2�
� f (x
  � f
x
(
ϕ
ϕ
0
0
− 
f (x|ϕ0 )dx 
(x|
|
(
f
x
ϕ
ϕ
)
)
f
0
0
�
f �� (x|ϕ0 )dx − E�0 (l � (X |ϕ0 ))2  = 0 − I (ϕ0  = −I (ϕ0 ). 
= 

f �� (x|ϕ)dx = 0. 

�

 

20 

We  are  now  ready  to  prove  the main  result  of  this  section. 
Theorem.  (Asymptotic  normality  of  MLE.) We  have, 
 
 
1  �
≥n(ϕˆ − ϕ0 ) � N �
. 
0, 
I (ϕ0 ) 
As we can see, the asymptotic variance/dispersion of the estimate around true parameter 
will  be  smaller  when  Fisher  information  is  larger. 
n 
1  �i
Proof.  Since  MLE  ϕˆ is maximizer  of  Ln (ϕ) =  n 
=1  log f (Xi |ϕ),  we  have 
L� (ϕˆ) = 0.
n

Let  us  use  the Mean  Value  Theorem 
f (a) − f (b) 
= f � (c)  or  f (a) = f (b) + f � (c)(a − b)  for  c ∞  [a, b] 
a − b 
with  f (ϕ) = L�n (ϕ), a = ϕˆ and  b = ϕ0 .  Then  we  can  write, 
0 = L� (ϕˆ) = L�n (ϕ0 ) + L�� (ϕˆ
1 )(ϕˆ − ϕ0 )
n
n
for  some  ˆ
ϕ1  ∞  [ ˆ
ϕ , ϕ0 ].  From  here  we  get  that 
≥nL
(ϕ0 ) 
0 ) 
and ≥n(ϕˆ − ϕ0 ) = −
�
L�
(ϕ
ˆ
n
n
. 
ϕ − ϕ0  = − 
ˆ
ˆ
Ln�� ϕ1 ) 
L��n ϕ1 ) 
(
(
Since  by  Lemma  in  the  previous  section  we  know  that  ϕ0  is  the maximizer  of  L(ϕ),  we  have 

(3.0.1) 

L� (ϕ0 ) = E�0 l � (X ϕ0 ) = 0. 
|

(3.0.2)

 

(3.0.3) 

 

Therefore,  the  numerator  in  (3.0.1) 
 
n 
 
1 �
≥nLn� (ϕ0 ) =  ≥n �
l � (Xi |ϕ0 ) − 0 �
n 
i=1 
 1 �
 
n
=  ≥n
�
�
�
�
l � (Xi |ϕ0 ) − E�0 l � (X1 |ϕ0 )  � N  0, Var�0 (l � (X1 |ϕ0 )) 
n 
i=1 
converges  in  distribution  by  Central  Limit  Theorem. 
Next,  let  us  consider  the  denominator  in  (3.0.1).  First  of  all,  we  have  that  for  all  ϕ , 
 
1 �
l �� (Xi |ϕ) � E�0 l �� (X1 |ϕ)  by  LLN. 
n 
Also, since ϕˆ
ϕ � ϕ0 , we have  ˆ
1  ∞  [ ˆ
ϕ , ϕ0 ] and by consistency  result of previous section,  ˆ
ϕ1  � ϕ0 . 
Using  this  together  with  (10.0.3)  we  get 
L��n (ϕˆ
1 ) � E�0 l �� (X1 |ϕ0 ) = −I (ϕ0 )  by  Lemma  above. 
21 

L��n (ϕ) = 

(3.0.4) 

Combining  this  with  (3.0.3)  we  get 
≥nL�n (ϕ0 ) 
− 
L��n (ϕˆ1 )  �

Var�0 (l � (X1 |ϕ0 )) �
d  N �
(I (ϕ0 ))2 
Finally,  the  variance, 
Var�0 (l � (X1 |ϕ0 )) = E�0 (l � (X |ϕ0 ))2  − (E�0 l � (x|ϕ0 ))2  = I (ϕ0 ) − 0 
where  in  the  last  equality  we  used  the  deﬁnition  of  Fisher  information  and  (3.0.2). 

 
0, 

 
.

Let  us  compute  Fisher  information  for  some  particular  distributions.

Example  1.  The  family  of  Bernoulli  distributions  B (p)  has  p.f.

f (x|p) = p x (1 − p)1−x 

and  taking  the  logarithm 

1 − x
. 
(1 − p)2 

=

1 
. 
p(1 − p) 

+

log f (x|p) = x log p + (1 − x) log(1 − p). 
The  second  derivative  with  respect  to  parameter  p  is 
� 2 
� 
1 − x
x 
x 
p  − 
log f (x p) = − 
log f (x p) = 
, 
p − 
|
|
� p 
1 − p  � p2 
2
Then  the  Fisher  information  can  be  computed  as 
� 2 
1 − p 
1 − EX 
EX 
log f (X p) = 
I (p) = −E 
|
(1 − p)2 
(1 − p)2 
� p2 
p2 
¯
The  MLE  of  p  is  ˆp = X  and  the  asymptotic  normality  result  states  that 
≥n( ˆp − p0 ) � N (0, p0 (1 − p0 )) 
which,  of  course,  also  follows  directly  from  the  CLT. 
Example.  The  family  of  exponential  distributions  E (�)  has  p.d.f. 
 
f (x �) = �
�e−�x , x ∀ 0 
|
0, 
x < 0 
log f (x|�) =  log � − �x ≤ 
This  does  not  depend  on X  and  we  get 

� 2 
1 
��2  log f (x|�) = − �2 . 

and,  therefore, 

p 
p2 

= 

+

� 2 
1 
. 
log f (X |�) = 
I (�) = −E 
�2 
��2 
Therefore,  the MLE  ˆ� = 1/X¯ is  asymptotically  normal  and 
≥n( ˆ� − �0 ) � N (0, �2 ).0

22 

