Lecture  5 

Conﬁdence  intervals  for  parameters  of 
normal  distribution. 

Let  us  consider  a Matlab  example  based  on  the  dataset  of  body  temperature  measurements 
of  130  individuals  from  the  article  [1].  The  dataset  can  be  downloaded  from  the  journal’s 
website.  This  dataset  was  derived  from  the  article  [2]. First  of  all,  if we  use  ’dﬁttool’  to  ﬁt  a 
normal  distribution  to  this  data  we  get  a  pretty  good  approximation,  see  ﬁgure  5.1. 

0.6 

0.5 

0.4 

0.3 

0.2 

0.1 

y
t
i
s
n
e
D

body temperature 
normal fit 

1 

0.9 

0.8

body temperature 
normal fit 

0.6 

y 0.7 
t
i
l
i
b
a
b
o
r
p
 
e
v
i
t
a
l
u
m
u
C

0.5 

0.4 

0.3 

0 
96 

96.5 

97 

97.5 

98 

98.5 
Data 

99 

99.5 

100  100.5 

0.2 

0.1 

0 
96 

96.5 

97 

97.5 

98 

98.5 
Data 

99 

99.5 

100  100.5 

Figure 5.1: Fitting a body temperature dataset. (a) Histogram of the data and p.d.f. of ﬁtted 
normal  distribution;  (b)  Empirical  c.d.f.  and  c.d.f.  of  ﬁtted  normal  distribution. 

The  tool  also  outputs  the  following  MLEstimates  ˆµ  and  ˆα  of  parameters  µ, α  of  normal 
distribution: 

Parameter  Estimate  Std.  Err. 
mu 
98.2492  0.0643044 
0.733183  0.0457347. 
sigma 

27 

Also,  if  our  dataset  vector  name  is  ’normtemp’  then  using  the  matlab  function  ’normﬁt’  by 
typing  ’[mu,sigma,muint,sigmaint]=normﬁt(normtemp)’  outputs  the  following: 

mu  =  98.2492,  sigma  =  0.7332,

muint  =  [98.122,  98.376],  sigmaint  =  [0.654,  0.835].


The  last  two  intervals  here  are  95% conﬁdence  intervals  for parameters µ and α. This means 
that  not  only  we  are  able  to  estimate  the  parameters  of  normal  distribution  using MLE  but 
also  to garantee with conﬁdence  95%  that the  ’true’ unknown parameters  of the distribution 
belong  to  these  conﬁdence  intervals.  How  this  is  done  is  the  topic  of  this  lecture.  Notice 
that conventional  ’normal’  temperature  98.6 does not  fall  into  the  estimated  95% conﬁdence 
interval  [98.122, 98.376]. 
Distribution  of  the  estimates  of  parameters  of  normal  distribution. 
Let  us  consider  a  sample


X1 , . . . , Xn  � N (µ, α 2 )

from normal distribution with mean µ and variance α 2 . MLE gave us  the  following estimates 
of  µ  and  α 2  - µˆ = X¯ and  ˆα 2  = X¯2  − (X¯ )2 .  The  question  is:  how  close  are  these  estimates  to 
actual  values  of  the  unknown  parameters  µ  and  α 2 ?  By  LLN  we  know  that  these  estimates 
converge  to  µ  and  α 2 , 

X¯ � µ, X¯2  − (X¯ )2  � α 2 , n � →, 
but  we  will  try  to  describe  precisely  how  close  X¯ and  X¯2  − (X¯ )2  are  to  µ  and  α 2 .  We  will 
start  by  studying  the  following  question: 
What  is  the  joint  distribution  of  ( ¯ X¯2  − ( ¯
X , 
X )2 )  when  X1 , . . . , Xn 
A  similar  question  for  a  sample  from  a  general  normal  distribution N (µ, α 2 )  can  be  reduced 
to  this  one  by  renormalizing  Zi  = (Xi  − µ)/α. We  will  need  the  following  deﬁnition. 
Deﬁnition.  If  X1 , . . . , Xn  are  i.i.d.  standard  normal  then  the  distribution  of 

are  i.i.d  from  N (0, 1)?

2  + . . . + Xn 
2 
X1

is  cal led  the  �2 
n -distribution  (chi-squared  distribution)  with  n  degrees  of  freedom. 
We will ﬁnd the p.d.f. of this distribution  in the  following lectures. At this point we only 
need  to  note  that  this  distribution  does  not  depend  on  any  parameters  besides  degrees  of 
freedom  n  and,  therefore,  could  be  tabulated  even  if  we  were  not  able  to  ﬁnd  the  explicit 
formula for its p.d.f. Here is the main result that will allow us to construct conﬁdence intervals 
for  parameters  of  normal  distribution  as  in  the Matlab  example  above. 
¯
Theorem.  If  X1 , . . . , Xn  are  i.d.d.  standard  normal,  then  sample  mean  X  and  sample 
variance  X¯2  − (X¯ )2  are  independent, 
≥nX  � N (0, 1)  and  n( ¯ − (X¯ )2 ) � �n
2 
¯
−1 , 
X 2
i.e.  ≥nX  has  standard  normal  distribution  and  n(X 2  −  (X¯ )2 )  has  �2 
¯
¯
n−1 
(n − 1)  degrees  of  freedom. 

distribution  with 

28 


Y
 =


 

Y1 
.
.
.

Yn 

.


X1
.
.
.

Xn


 = V X
 =


Proof.  Consider  a  vector  Y  given  by  a  speciﬁc  orthogonal  transformation  of X :

 
�
�
�
⎞
⎞
⎞
· · · 
1
1
�n 
�n
 
 
.

.
. 
⎜⎝
�
�
�
⎜⎝
⎜⎝
.
? 
.

.
�
�
�
· · · 
· · · 
· · · 
Here  we  choose  a  ﬁrst  row  of  the matrix  V  to  be  equal  to 

 1
1
 ⎛
⎟
, . . . , 
≥n 
≥n 
and  let  the  remaining  rows  be  any  vectors  such  that  the matrix  V  deﬁnes  orthogonal  trans­
formation.  This  can  be  done  since  the  length  of  the  ﬁrst  row  vector  v1 = 1,  and  we  can 
|
|
simply  choose  the  rows  v2 , . . . , vn  to  be  any  orthogonal  basis  in  the  hyperplane  orthogonal 
to  vector  v1 . 
Let  us  discuss  some  properties  of  this  particular  transformation. First  of  all, we  showed 
above  that  Y1 , . . . , Yn  are  also  i.i.d.  standard  normal. Because  of  the  particular  choice  of  the 
ﬁrst  row  v1  in  V ,  the  ﬁrst  r.v. 

v1  = 

1 
Y1  =  ≥
n

1 
X1  + . . . + ≥
n

Xn  = ≥nX 
¯

and,  therefore, 

1
Y1 . 
≥n 
Next,  n  times  sample  variance  can  be  written  as 

¯X  = 

(5.0.1)


 1 
⎛2 
⎟
n(X¯2  − (X¯ )2 ) =  X1
2  −  ≥n 
2  + . . . + Xn 
(X1  + . . . + Xn ) 
2  + . . . + Xn 
2  − Y1
2 . 
=  X1
The  orthogonal  transformation  V  preserves  the  length  of X,  i.e.  Y  =  V X =  X or
| 
|
|
|
|
|

2  +
Y1

2  = X1
2  +
+ Yn 
· · · 

2 
+ Xn 
· · · 

and,  therefore,  we  get 
n(X¯2  − (X¯ )2 ) = Y 2  + . . . + Y 2  − Y 2  = Y 2  + . . . + Y 2 . 
1 
n 
2 
n 
1
Equations  (5.0.1)  and  (5.0.2)  show  that  sample  mean  and  sample  variance  are  independent 
since  Y1  and  (Y2 , . . . , Yn)  are  independent,  ≥nX  = Y1  has  standard  normal  distribution  and 
¯
n(X¯2  − (X¯ )2 )  has  �2 
n−1  distribution  since  Y2 , . . . , Yn  are  independent  standard  normal. 
Let  us  write  down  the  implications  of  this  result  for  a  general  normal  distribution:


(5.0.2)

X1 , . . . , Xn  � N (µ, α 2 ). 

29 





















In  this  case,  we  know  that 

Z1  = 

X1  − µ
α 

Xn  − µ 
α 

 

= 

and 

� N (0, 1) 

· · · 
, 
, Zn  = 
� N (0, 1)
are  independent  standard  normal.  Theorem  applied  to  Z1 , . . . , Zn  gives  that 
≥n(X¯ − µ) 
n 
 Xi  − µ 
1 �
≥nZ¯ = ≥n 
α 
α 
n
i=1 
 
 
 Xi 
1 �
1 �⎟ Xi 
− µ ⎛2 
− µ ⎛2⎛
n(Z¯2  − (Z¯ )2 ) =  n ⎟
− ⎟
n 
α 
n 
α 
 Xi  − µ ⎛2 
n1 �⎟ Xi  − µ 
1 �
α  − 
=  n 
α 
n
n
i=1 
X¯2  − (X¯ )2 
2 
� �n
−1 . 
α 2 
We  proved  that MLE  ˆµ = X¯ and  ˆα 2  = X¯2  − (X¯ )2  are  independent  and 
�n( ˆµ−µ)  � N (0, 1),  n�ˆ 2 
� �2 
n−1 . 
�2 
�

=  n

Conﬁdence  intervals  for  parameters  of  normal  distribution. 
We  know  that  by  LLN  a  sample  mean  µˆ and  sample  variance  ˆα 2  converge  to  mean  µ 
and  variance  α 2 : 

µˆ = X¯ � µ, αˆ 2  = X¯2  − (X¯ )2  � α 2 . 
In  other  words,  these  estimates  are  consistent.  Based  on  the  above  description  of  the  joint 
distribution  of  the  estimates,  we  will  give  a  precise  quantitative  description  of  how  close  ˆµ 
and  ˆα 2  are  to  the  unknown  parameters  µ  and  α 2 . 
Let  us  start  by  giving  a  deﬁnition  of  a  conﬁdence  interval  in  our  usual  setting  when  we 
observe  a  sample  X1 , . . . , Xn  with  distribution  P�0  from  a  parametric  family  {P�  :  �  ∞  �}, 
and  �0  is  unknown. 
Deﬁnition:  Given  a  conﬁdence  level  parameter  � ∞  [0, 1],  if  there  exist  two  statistics 
S1  = S1 (X1 , . . . , Xn )  and  S2  = S2 (X1 , . . . , Xn ) 

such  that  probability 

P�0 (S1  � �0  � S2 ) = �  (  or  ∼ �) 
then  we  will  call  [S1 , S2 ] a  conﬁdence  interval  for  the  unknown  parameter  �0  with  the  conﬁ­
dence  level  �. 

30 

This  deﬁnition  means  that  we  can  garantee  with  probability/conﬁdence  �  that  our 
unknown  parameter  lies  within  the  interval  [S1 , S2 ]. We  will  now  show  how  in  the  case  of  a 
normal  distribution  N (µ, α 2 )  we  can  construct  conﬁdence  intervals  for  unknown  µ  and  α 2 . 
Let  us  recall  that  in  the  last  lecture  we  proved  that  if 

X1 , . . . , Xn  are  i.d.d.  with  distribution  N (µ, α 2 ) 

then 

≥n( ˆµ − µ) 
αˆ
n
� �2 
� N (0, 1)  and  B = 
n−1
α 
2
α
and  the  random  variables  A  and  B  are  independent.  If  we  recall  the  deﬁnition  of  �2 ­
distribution,  this means  that  we  can  represent  A  and B  as 

A = 

2 

2  + . . . + Yn 
2 
A = Y1  and  B  = Y2

for  some  Y1 , . . . , Yn  - i.d.d.  standard  normal. 

0.4 

0.35 

0.3 

0.25 

0.2 

0.15 

0.1 

1−� 
2 

0.05 

Tails  of  �2 
n−1 -distribution. 

1−� 
2 

0 
0 

5 

15 

20 

10 

c1 

c2 
Figure  5.2:  p.d.f.  of  �2 
n−1 -distribution  and  �-conﬁdence  interval. 
First,  let us  consider p.d.f.  of �2 
n−1  distribution  (see  ﬁgure 5.2) and choose points  c1  and 
c2  so  that  the  area  in  each  tail  is  (1 − �)/2.  Then  the  area  between  c1  and  c2  is  �  which 
means  that 
P(c1  � B  � c2 ) = �. 
Therefore,  we  can  ’garantee’  with  probability  �  that 

25 

c1  � 

nαˆ 2 
α 2  � c2 . 

31 

PSfrag replacements

Solving  this  for  α 2  gives 

nαˆ 2 
c2  � α 2  � 
This  precisely  means  that  the  interval 

nαˆ 2 
. 
c1 

⎠ nαˆ 2  nαˆ 2 �
,

c1

c2 
is  the  �-conﬁdence  interval  for  the  unknown  variance  α 2 . 
Next,  let us construct  the conﬁdence  interval  for the mean µ. We will need  the  following 
deﬁnition. 

 

Deﬁnition.  If  Y0 , Y1 , . . . , Yn  are  i.i.d.  standard  normal  then  the  distribution  of  the  ran­
dom  variable 

Y0 
 
�
1 (Y 2  + . . . + Y 2 )
1 
n 
n 
is  called  (Student)  tn -distribution  with  n  degrees  of  freedom. 
We will ﬁnd the p.d.f. of this distribution  in the  following lectures  together with p.d.f. of 
�2 -distribution  and  some  others.  At  this  point  we  only  note  that  this  distribution  does  not 
depend  on  any  parameters  besides  degrees  of  freedom  n  and,  therefore,  it  can  be  tabulated. 
Consider  the  following  expression: 

= 

� tn−1 

A
Y1 
 
 
�
�
1 
1 
2  + . . . + Yn 
n−1 B 
2 ) 
n−1 (Y2
which, by deﬁnition, has  tn−1 -distribution with n − 1 degrees  of  freedom. On the other hand, 
 
≥n
µ) ��
( ˆµ − 
A 
− 1
2 
= ≥n 
αˆ
n
1
 
= 
( ˆµ − µ). 
�
n − 
1 
2
α
α
αˆ
1 
B 
n−1 
If  we  now  look  at  the  p.d.f.  of  tn−1  distribution  (see  ﬁgure  5.3)  and  choose  the  constants 
−c  and  c  so  that  the  area  in  each  tail  is  (1 − �)/2,  (the  constant  is  the  same  on  each  side 
because  the  distribution  is  symmetric)  we  get  that  with  probability  �, 
≥n − 1
−c � 
( ˆµ − µ) � c
αˆ
and  solving  this  for  µ,  we  get  the  conﬁdence  interval 

αˆ
αˆ
µˆ − c ≥n − 1  � µ � µˆ + c ≥n − 1 
. 
Example.  (Textbook,  Section  7.5,  p.  411))  Consider  a  sample  of  size  n  =  10  from 
normal  distribution  with  unknown  parameters: 

0.86, 1.53, 1.57, 1.81, 0.99, 1.09, 1.29, 1.78, 1.29, 1.58. 

32 

0.4 

0.3 

0.2 

0.1 

0 

PSfrag replacements

Tails  of  t2 
n−1 -distribution. 

1−� 
2 

1−� 
2 

−4 

−6 

−c 
Figure  5.3:  p.d.f.  of  tn−1  distribution  and  conﬁdence  interval  for  µ. 

0 

4 

c 

6 

We  compute  the  estimates 
µˆ = X¯ = 1.379  and  ˆα 2  = X¯2  − (X¯ )2  = 0.0966. 
Let  us  choose  conﬁdence  level  �  =  95%  =  0.95.  We  have  to  ﬁnd  c1 , c2  and  c  as  explained 
above.  Using  the  table  for  t9 -distribution  we  need  to  ﬁnd  c  such  that 
t9 (−→, c) = 0.975 
which  gives  us  c = 2.262.  To  ﬁnd  c1  and  c2  we  have  to  use  the  �2
9 -distribution  table  so  that 
�2 ([0, c1 ]) = 0.025 ≤ c1  = 2.7
9
�2 ([0, c2 ]) = 0.975 ≤ c2  = 19.02.
9
Plugging  these  into  the  formulas  above,  with  probability  95%  we  can  garantee  that 
�
�
(X¯2  − (X¯ )2 ) �  µ  � X¯ + c 
X¯ − c 
1.1446 �  µ  � 1.6134 
and  with  probability  95%  we  can  garantee  that 
n(X¯2  − (X¯ )2 ) 
c2 

n(X¯2  − (X¯ )2 ) 
c1 

(X¯2  − (X¯ )2 ) 

� α 2  � 

1
9

1
9

or 

0.0508 � α 2  � 0.3579. 
33 

These  conﬁdence  intervals  may  not  look  impressive  but  the  sample  size  is  very  small  here, 
n = 10. 
References. 
[1]  Allen  L  .Shoemaker  (1996),  ”What’s  Normal?  - Temperature,  Gender,  and  Heart 
Rate”.  Journal  of  Statistics  Education,  v.4,  n.2. 
[2] Mackowiak, P. A., Wasserman, S. S., and Levine, M. M. (1992), ”A Critical Appraisal 
of  98.6  Degrees  F,  the  Upper  Limit  of  the  Normal  Body  Temperature,  and  Other  Legacies 
of  Carl  Reinhold  August  Wunderlich”.  Journal  of  the  American  Medical  Association,  268, 
1578-1580. 

34


