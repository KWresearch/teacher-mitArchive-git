Section  10 

Chi-squared  goodness-of-ﬁt  test. 

Example.  Let  us  start  with  a  Matlab  example.  Let  us  generate  a  vector  X  of  100  i.i.d. 
uniform  random  variables  on  [0, 1] : 

X=rand(100,1). 

Parameters (100, 1) here mean that we generate a 100×1 matrix or uniform random variables. 
Let  us  test  if  the  vector X  comes  from  distribution  U [0, 1]  using  �2  goodness-of-ﬁt  test: 

[H,P,STATS]=chi2gof(X,’cdf’,@(z)unifcdf(z,0,1),’edges’,0:0.2:1) 

The  output  is 

H  =  0,  P  =  0.0953, 
STATS  =  chi2stat:  7.9000 
df:  4 
edges:  [0  0.2  0.4  0.6  0.8  1] 
O:  [17  16  24  29  14] 
E:  [20  20  20  20  20] 

We  accept  null  hypothesis  H0  : P = U [0, 1]  at  the  default  level  of  signiﬁcance  � = 0.05  since 
the  p-value  0.0953  is  greater  than  �.  The  meaning  of  other  parameters  will  become  clear 
when we  explain  how  this  test works. Parameter  ’cdf ’  takes  the handle @  to a  fully  speciﬁed 
c.d.f. For example,  to test  if the data comes  from N (3, 5) we would use  ’@(z)normcdf(z,3,5)’, 
or  to  test  Poisson  distribution  �(4)  we  would  use  ’@(z)poisscdf(z,4).’ 
It  is  important  to  note  that  when  we  use  chi-squared  test  to  test,  for  example,  the 
null  hypothesis  H0  :  P  =  N (1, 2),  the  alternative  hypothesis  is  H0  :  P  =  N (1, 2).  This  is 
diﬀerent  from the setting of t-tests where we would assume  that the data comes  from normal 
distribution  and  test  H0  : µ = 1  vs.  H0  : µ = 1.

62


∞
∞
Pearson’s  theorem. 
PSfrag replacements
Chi-squared  goodness-of-ﬁt  test  is  based  on  a  probabilistic  result  that  we  will  prove  in 
this  section. 

�1 

�2 

B1 
p1 

B2 
p2 

... 

Figure  10.1: 

�r 

Br 
pr 

Let  us  consider  r  boxes  B1 , . . . , Br  and  throw  n  balls X1 , . . . , Xn  into  these  boxes  inde­
pendently  of  each  other  with  probabilities 
P(Xi  ≥ B1 ) = p1 , . . . , P(Xi  ≥ Br ) = pr , 

so  that 

p1  + . . . + pr  = 1. 
Let  �j  be  a  number  of  balls  in  the  j th  box: 

I (Xl  ≥ Bj ). 

�j  = #{balls X1 , . . . , Xn  in  the  box  Bj } = 

n 
�
l=1 
On  average,  the  number  of  balls  in  the  j th  box  will  be  npj  since 
n 
n
�
�
l=1 
l=1 
We  can  expect  that  a  random  variable  �j  should  be  close  to  npj .  For  example,  we  can  use 
a  Central  Limit  Theorem  to  describe  precisely  how  close  �j  is  to  npj .  The  next  result  tells 
us  how  we  can  describe  the  closeness  of  �j  to  npj  simultaneously  for  all  boxes  j  ≈  r.  The 
main  diﬃculty  in  this  Thorem  comes  from  the  fact  that  random  variables  �j  for  j  ≈  r  are 
not  independent  because  the  total  number  of  balls  is  ﬁxed 

E�j  = 

 
EI (Xl  ≥ Bj ) = 

 
P(Xl  ≥ Bj ) = npj . 

�1  + . . . + �r  = n. 
If  we  know  the  counts  in  n − 1  boxes  we  automatically  know  the  count  in  the  last  box. 
Theorem.(Pearson) We  have  that  the  random  variable 
r 
(�j  − npj )2 
�
d  �2 
npj  � r−1 
j=1 
converges  in  distribution  to  �2 
r−1 -distribution  with  (r − 1)  degrees  of  freedom. 
63






Proof.  Let  us  ﬁx  a  box  Bj .  The  random  variables 
I (X1  ≥ Bj ), . . . , I (Xn  ≥ Bj ) 
that  indicate  whether  each  observation  Xi  is  in  the  box  Bj  or  not  are  i.i.d.  with  Bernoulli 
distribution  B (pj )  with  probability  of  success 
EI (X1  ≥ Bj ) = P(X1  ≥ Bj ) = pj 

and  variance 

= 

= 

d  N (0, 1)

Var(I (X1  ≥ Bj )) = pj (1 − pj ). 
Therefore,  by  Central  Limit  Theorem  the  random  variable 
�n  I (Xl  ≥ Bj ) − npj
�j  − npj 
l=1 
 
 
�
�
npj (1 − pj ) 
npj (1 − pj ) 
�n 
l=1  I (Xl  ≥ Bj ) − nE 
≤nVar 
�
converges  in  distribution  to N (0, 1).  Therefore,  the  random  variable 
�j  − npj 
d  �
1 − pj N (0, 1) = N (0, 1 − pj )
≤npj  �
converges  to normal  distribution with  variance  1 − pj . Let  us  be  a  little  informal  and  simply 
say  that 
�j  − npj 
≤npj  � Zj 
where  random  variable  Zj  � N (0, 1 − pj ). 
We know  that each Zj  has distribution N (0, 1 − pj ) but, unfortunately, this does not tell 
 
us  what  the  distribution  of  the  sum  �
2  will  be,  because  as  we  mentioned  above  r.v.s  �j 
Zj 
are not  independent  and  their  correlation  structure  will play  an  important  role. To  compute 
the  covariance  between  Zi  and  Zj  let  us  ﬁrst  compute  the  covariance  between 
�j  − npj 
�i  − npi 
and  ≤npj 
≤npi 

which  is  equal  to 
1 
�i  − npj  �j  − npj 
(E�i �j  − E�inpj  − E�j npi  + n 2 pipj ) 
E  ≤npi  ≤npj 
n≤pipj 
1 
1 
2 
2 
= 
(E�i�j  − n pipj ). 
(E�i �j  − npinpj  − npj npi  + n pipj ) = 
n≤pipj 
n≤pipj 
To  compute  E�i �j  we  will  use  the  fact  that  one  ball  cannot  be  inside  two  diﬀerent  boxes 
simultaneously  which  means  that 

= 

I (Xl  ≥ Bi )I (Xl  ≥ Bj ) = 0. 
64 

(10.0.1) 

Therefore, 

 
I (Xl  ≥ Bi )I (Xl�  ≥ Bj ) 

 
 
 
n
n
� �
��
��
�
I (Xl�  ≥ Bj ) = E 
I (Xl  ≥ Bi ) 
E�i�j  =  E 
l,l� 
l=1 
l�=1 
 
 
�
�
I (Xl  ≥ Bi )I (Xl�  ≥ Bj ) 
I (Xl  ≥ Bi )I (Xl�  ≥ Bj ) +E 
=  E 
l=l� 
l=l� 
 
 
 
��
�
�
this  equals  to  0  by  (10.0.1) 
=  n(n − 1)EI (Xl  ≥ Bj )EI (Xl�  ≥ Bj ) = n(n − 1)pipj . 
Therefore,  the  covariance  above  is  equal  to 
1  �
�
n(n − 1)pipj  − n pipj  = −≤pipj . 
2 
n≤pipj 
To  summarize,  we  showed  that  the  random  variable 
r 
 (�j  − npj )2  �
r
�
npj  � 
j=1 
j=1 
where  normal  random  variables  Z1 , . . . , Zn  satisfy 
EZ 2  = 1 − pi  and  covariance  EZiZj  = −≤pipj .
i 
To  prove  the  Theorem  it  remains  to  show  that  this  covariance  structure  of  the  sequence  of 
(Zi )  implies  that  their  sum  of  squares  has  �2 
r−1 -distribution.  To  show  this  we  will  ﬁnd  a 
 
diﬀerent  representation  for �
2 . 
Zi 
Let  g1 , . . . , gr  be  i.i.d.  standard  normal  random  variables.  Consider  two  vectors 
g = (g1 , . . . , gr )T  and  p = (≤p1 , . . . , ≤pr )T 
and  consider  a  vector  g − (g p)p,  where  g p = g1≤p1  + . . . + gr≤pr  is  a  scalar  product  of 
· 
·
g  and  p. We  will  ﬁrst  prove  that 
g − (g p)p  has  the  same  joint  distribution  as  (Z1 , . . . , Zr ). 
· 
To  show  this  let  us  consider  two  coordinates  of  the  vector  g − (g p)p  :
· 
r 
 
 
r
gl≤pl≤pi  and  j th  : gj  − �
gl≤pl≤pj 
ith  : gi  − �
l=1 
l=1 
and  compute  their  covariance: 
 
 
r

 
 
r
gl≤pl≤pj �
gl≤pl≤pi ��
E �
gj  − �
gi  − �
l=1 
l=1

n

 
= −≤pi≤pj  − ≤pj ≤pi  + �
pl≤pi≤pj  = −2≤pipj  + ≤pipj  = −≤pipj . 
l=1 

(10.0.2)

 
Z 2 
j . 

65


�


Similarly,  it  is  easy  t o  compu te   t h a t  

This  proves  (10 .0 .2) ,  which  provides  us  w i th   ano the r   way  t o   formulate  the   convergence, 
namely,  we  have 

B u t   th is   vector  has  a simple  geometric  in te rp re ta t ion .  Since  vector  p  is  a unit  vector: 

vector  Vl  = (p . g ) p  is  t h e  pro jec t ion   of  vector  g   on   the   line  along p  a n d ,  therefore,  vector 
Vz  = g  - (p . g ) p  will  be  the   pro jec t ion   of  g   on to   the   plane  or thogona l   t o  p, a s   shown  in 
figure  10 .2 .  

Figure  10.2: New  coord ina te   sy s tem ,  

Let  us  consider  a  new  o r thono rma l  coord ina te   sys tem   w i th   the   first  basis  vector  (first 
axis)  equal  t o p .  In  th is   new  coord ina te   sys tem  vector  g   will  have  coord ina tes  

obtained  from  g  by  orthogonal  transformation 

V  = (p, p2 , . . . , pr ) 

that  maps  canonical  basis  into  this  new  basis.  But  we  proved  in  Lecure  4  that  in  that 
case  g1� , . . . , gr� will  also  be  i.i.d.  standard  normal.  From  ﬁgure  10.2  it  is  obvious  that  vector 
V2  = g − (p g )p  in  the  new  coordinate  system  has  coordinates 
· 
(0, g2� , . . . , gr� )T 

and,  therefore, 

|V2 |2  =  |g − (p · g )p|2  = (g � )2  + . . . + (g � )2 .
2
r
But  this  last  sum,  by  deﬁnition,  has  �2 
, g � are  i.i.d.  standard 
· · · 
r−1  distribution  since  g2� , 
r 
normal.  This  ﬁnishes  the  proof  of  Theorem. 

Chi-squared  goodness-of-ﬁt  test  for  simple  hypothesis. 
Suppose  that  we  observe  an  i.i.d.  sample  X1 , . . . , Xn  of  random  variables  that  take  a 
ﬁnite number of values B1 , . . . , Br  with unknown probabilities p1 , . . . , pr . Consider hypotheses 

H0  :  pi  = p�i  for  all  i = 1, . . . , r, 
for  some  i, pi  = p�i .
H1  : 

T  = 

If  the  null  hypothesis  H0  is  true  then  by  Pearson’s  theorem 
r 
 (�i  − np� )2 
�
i  �d  �2 
r−1 
np�
i 
i=1 
where  �i  = #{Xj  : Xj  = Bi}  are  the  observed  counts  in  each  category.  On  the  other  hand, 
if H1  holds  then  for  some  index  i,  pi  = p�i  and  the  statistics  T  will  behave  diﬀerently.  If  pi  is
the  true  probability  P(X1  = Bi )  then  by  CLT 
≤− 
�i 
np
i  �d  N (0, 1 − pi ). 
np

i

If  we  rewrite 

= 

 
= �
pi  − pi�
pi  �i  − npi  + ≤n
�i  − npi  + n(pi  − pi� )
�i  − npi�
≤pi�
≤npi�
≤npi�
pi� ≤npi 
then  the  ﬁrst  term  converges  to N (0, (1 − pi )pi/p�i )  and  the  second  term  diverges  to  plus  or 
because  pi  = pi� .  Therefore,
minus 
→
(�i  − np� )2 
i 
np�i  � +→ 
.  Therefore,  as  sample  size  n  increases  the  distri­
�  +→
which,  obviously,  implies  that  T
bution  of  T  under  null  hypothesis  H0  will  approach  �2 
r−1 -distribution  and  under  alternative 
hypothesis  H1  it  will  shift  to  + ,  as  shown  in  ﬁgure  10.3. 
→

67


∞
∞
∞
0.1 

0.09 

0.08 

0.07 

0.06 

0.05 

0.04 

0.03 

0.02 

0.01 

H0  : T  � �2 
r−1 

H1  : T  � +→ 

� 

PSfrag replacements

0 
0 

10 

c 

20 

30 

40 

50 

60 

Figure  10.3:  Behavior  of  T  under H0  and H1 . 

Therefore,  we  deﬁne  the  decision  rule 
 
�
α =  H1  :  T  ≈ c 
H2  :  T  > c. 
We  choose  the  threshold  c  from  the  condition  that  the  error  of  type  1  is  equal  to  the  level  of 
signiﬁcance  �  : 

� = P1 (α =∞ H1 ) = P1 (T  > c) � �2 
r−1 (c, →) 
since  under  the  null  hypothesis  the  distribution  of  T  is  approximated  by  �2 
r−1  distribution. 
Therefore, we  take c such  that � = �2 
r−1 (c, 
). This  test α  is called the chi-squared goodness­
→
of-ﬁt  test. 

Example.  (Montana  outlook  pol l.)  In  a  1992  poll  189  Montana  residents  were  asked 
(among  other  things)  whether  their  personal  ﬁnancial  status  was  worse,  the  same  or  better 
than  a  year  ago. 

Worse  Same  Better  Total 
189 
64 
58 
67
We want to test the hypothesis H0  that the underlying distribution  is uniform,  i.e. p1  = p2  = 
p3  = 1/3. Let  us  take  level  of  signiﬁcance  � = 0.05.  Then  the  threshold  c  in  the  chi-squared 

68


test 

 
�
α =  H0  :  T  ≈ c 
H1  :  T  > c 
3−1=2 (c, 
) = 0.05  which  gives  c  = 5.9.  We  compute
is  found  from  the  condition  that  �2
→
chi-squared  statistic 
(58 − 189/3)2 
189/3 

(64 − 189/3)2 
189/3 

(67 − 189/3)2 
189/3 

= 0.666 < 5.9 

+ 

T  = 

+

which  means  that  we  accept H0  at  the  level  of  signiﬁcance  0.05. 

Goodness-of-ﬁt  for  continuous  distribution. 
Let  X1 , . . . , Xn  be  an  i.i.d.  sample  from  unknown  distribution  P  and  consider  the  fol­
lowing  hypotheses: 
 
�
H0  :  P = P0

H1  :  P = P0

for some particular, possibly continuous distribution P0 . To apply  the chi-squared  test above 
we  will  group  the  values  of X s  into  a  ﬁnite  number  of  subsets.  To  do  this,  we  will  split  a  set 
of all possible  outcomes X  into a ﬁnite number  of  intervals  I1 , . . . , Ir  as  shown  in ﬁgure 10.4. 

0.4 

0.35 

0.3 

0.25 

0.2 

p.d.f.  of  P0 

PSfrag replacements

0.15 

p0 
2 

0.1 

0.05 


0


p0 
1 

p0 
r 

I2 

I1 

· · · 
Figure  10.4: Discretizing  continuous  distribution. 

Ir

x

69 

∞
The  null  hypothesis  H0 ,  of  course,  implies  that  for  all  intervals 
0 . 
P(X  ≥ Ij ) = P0 (X  ≥ Ij ) = pj 
Therefore,  we  can  do  chi-squared  test  for 
0  for  all  j  ≈ r 
H0� :  P(X  ≥ Ij ) = pj 
H1� :  otherwise. 
Asking whether H0� holds  is, of course, a weaker question  that asking  if H0  holds, because H0 
implies H0� but not the other way around. There are many distributions diﬀerent  from P that 
have  the  same  probabilities  of  the  intervals  I1 , . . . , Ir  as  P.  On  the  other  hand,  if  we  group 
into more and more  intervals, our discrete  approximation of P will get closer and closer  to P, 
so  in  some  sense H0� will get  ’closer’  to H0 . However, we can not split  into too many  intervals 
either,  because  the  �2 
r−1 -distribution  approximation  for  statistic  T  in  Pearson’s  theorem  is 
asymptotic.  The  rule  of  thumb  is  to  group  the  data  in  such  a  way  that  the  expected  count 
in  each  interval 

0  = nP0 (X  ≥ Ii ) ∼ 5 
npi 
is  at  least  5.  (Matlab,  for  example,  will  give  a  warning  if  this  expected  number  will  be  less 
than ﬁve  in any  interval.) One approach could be to  split  into  intervals of equal probabilities 
0  = 1/r  and  choose  their  number  r  so  that 
pi 
n 
0  =  ∼ 5. 
npi 
r 
Example.  Let  us  go  back  to  the  example  from Lecture  2. Let  us  generate  100  observa­
tions  from  Beta  distribution  B (5, 2). 

X=betarnd(5,2,100,1); 

Let  us  ﬁt  normal  distribution  N (µ, ν 2 )  to  this  data.  The  MLE  ˆµ  and  ˆν  are 

mean(X)  =  0.7421,  std(X,1)=0.1392. 
Note that ’std(X)’  in Matlab will produce the square root of unbiased estimator (n/n − 1) ˆν 2 . 
Let  us  test  the  hypothesis  that  the  sample  has  this  ﬁtted  normal  distribution. 

[H,P,STATS]=  chi2gof(X,’cdf’,@(z)normcdf(z,0.7421,0.1392)) 

outputs 

H  =  1,  P  =  0.0041, 
STATS  =  chi2stat:  20.7589 
df:  7 
edges:  [1x9  double] 
O:  [14  4  11  14  14  16  21  6] 
E:  [1x8  double] 

Our hypothesis was  rejected with p-value of 0.0041. Matlab split  the  real  line  into 8  intervals 
of  equal  probabilities.  Notice  ’df:  7’  - the  degrees  of  freedom  r − 1 = 8 − 1 = 7. 

70


