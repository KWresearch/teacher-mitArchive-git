Section  15 

Multiple  linear  regression. 

Let  us  consider  a model 

Yi  = �1Xi1  + . . . + �pXip  + χi 
where  random  noise  variables  χ1 , . . . , χn  are  i.i.d.  N (0, π 2 ).  We  can  write  this  in  a  matrix 
form 

Y  = X � + χ, 
where  Y  and  χ  are  n × 1  vectors,  �  is  p × 1  vector  and  X  is  n × p  matrix.  We  will  denote 
the  columns  of matrix X  by  X1 , . . . Xp ,  i.e. 

X  = (X1 , . . . , Xp ) 

and  we  will  assume  that  these  columns  are  linearly  independent.  If  they  are  not  linearly 
independent,  we  can  not  reconstruct  parameters  �  from  X  and  Y  even  if  there  is  no  noise 
χ.  In  simple  linear  regression  this  would  correspond  to  all  X s  being  equal  and  we  can  not 
estimate  a  line  from  observations  only  at  one  point.  So  from  now  on  we  will  assume  that 
n > p  and  the  rank  of matrix X  is  equal  to  p. To  estimate  unknown  parameters  �  and  π  we 
will  use  maximum  likelihood  estimators. 
Lemma  1.  The  MLE  of  �  and  π 2  are  given  by: 

�ˆ = (X T X )−1X T Y  and  πˆ 2  = 

1 
|Y  − X �ˆ|2  = 
n

1 
|Y  − X (X T X )−1X T Y |2 . 
n

Proof.  The  p.d.f.  of  Yi  is

 
1 
1 
⎟
exp  − 2π 2 (x − �1Xi1  − . . . − �pXip)2
fi (x) =  ∼2απ 
and,  therefore,  the  likelihood  function  is

  1  ⎠n 
n
⎟
�
fi (Yi ) =  ∼2απ 
i=1 
  1  ⎠n 
⎟
=  ∼2απ 

 
n 
1  �
⎟
(Yi  − �1Xi1  − . . . − �pXip)2
exp  − 2π 2 
i=1 
 
 
1 
⎟
2⎠
exp  −
|Y  − X � |
. 
2π 2 

⎠

 

⎠

102 



To maximize  the  likelihood  function, ﬁrst, we need  to minimize  |Y  − X � |2 .  If we  rewrite  the 
norm  squared  using  scalar  product: 

 
�iXi )

 
p
p
�
�
Y  − X � |2  = (Y  − 
�iXi , Y  − 
|

i=1 
i=1 
p	
 
p
�
�
�i (Y , Xi) + 
�i�j (Xi , Xj ). 
= (Y , Y ) − 2 
i,j=1 
i=1 
Then  setting  the  derivatives  in  each  �i  equal  to  zero 
p
�
−2(Y , Xi ) + 2 
j=1 

 
�j (Xi , Xj ) = 0 

 

we get 

 
p
�
�j (Xi , Xj )  for  all  i � p. 
i=1 
In  matrix  notations  this  can  be  written  as  X T Y  = X T X � . Matrix  X T X  is  a  p × p  matrix. 
Is  is  invertible  since  by  assumption  X  has  rank  p.  So  we  can  solve  for  �  to  get  the MLE 

(Y , Xi ) = 

It  is  now  easy  to minimize  over  π  to  get 

�ˆ = (X T X )−1X T Y . 

πˆ 2  = 

1 
|Y  − X �ˆ|2  = 
n

1 
|Y  − X (X T X )−1X T Y |2 . 
n

To  do  statistical  inference  we  need  to  compute  the  joint  distribution  of  these  estimates. 
We  will  prove  the  following. 
Theorem. We  have 

⎠
⎟
�ˆ � N
 � , π 2 (X T X )−1
and  estimates  �ˆ and  πˆ 2  are  independent. 
Proof.  First  of  all,  let  us  rewrite  the  estimates  in  terms  of  random  noise  χ  using  Y  = 
X � + χ. We  have 

� �2 
n−p

,


nπˆ 2
π 2 

�ˆ = (X T X )−1X T Y  = (X T X )−1X T (X � + χ) 
= (X T X )−1 (X T X )� + (X T X )−1X T χ = � + (X T X )−1X T χ 

and  since 
Y  − X (X T X )−1X T Y	 =  X � + χ − X (X T X )−1X T (X � + χ) 
=  X � + χ − X � − X (X T X )−1X T χ = (I − X (X T X )−1X T )χ 
103 






we  have 

1 
|(I − X (X T X )−1X T )χ|2 . 
n
Since  �ˆ is  a  linear  transformation  of  a  normal  vector  χ  it  will  also  be  normal  with mean 

πˆ 2  = 

E�ˆ = E(� + (X T X )−1X T χ) = � 

and  covariance matrix 
E(�ˆ − � )(�ˆ − � )T	 =  E(X T X )−1X T χχT X (X T X )−1 
= (X T X )−1X T EχχT X (X T X )−1 
= (X T X )−1X T (π 2I )X (X T X )−1 
=  π 2 (X T X )−1 (X T X )(X T X )−1  = π 2 (X T X )−1 . 
This proves  that �ˆ � N (� , π 2 (X T X )−1 ). To prove that �ˆ and  ˆπ 2  are independent and to ﬁnd 
the  distribution  of  nπˆ 2 /π 2  we  will  use  the  following  trick.  This  trick  can  also  be  very  useful 
computationally since it will relate all quantities of interest expressed in terms of n× p matrix 
X  to quantities  expressed  in  terms  of  a  certain  p × p matrix R  which  can  be  helpful  when  n 
is  very  large  compared  to  p. We  would  like  to manipulate  the  columns  of matrix X  to make 
them  orthogonal  to  each  other,  which  can  be  done  by  Gram-Schmidt  orthogonalization.  In 
other  words,  we  want  to  represent  matrix X  as 

X  = X0R 
where  X0  is  n × p  matrix  with  columns  X 1 , . . . , X p  that  are  orthogonal  to  each  other  and, 
0 
0
moreover,  form an orthonormal basis, and matrix R  is p × p invertible  (and upper triangular) 
matrix.  In Matlab  this  can  be  done  using  economy  size  QR  factorization 

[X0,  R]=qr(X,0). 

The  fact  that  columns  of X0  are  orthonormal  implies  that 

T X0  = I 
X0 
- a  p × p  identity  matrix.  Let  us  replace  X  by X0R  everywhere  in  the  estimates. We  have 
(X T X )−1X T  = (RT X0 
T X0R)−1RT X0 
T  = (RT R)−1RT X0 
T  = R−1 (RT )−1RT  = R−1X0 
T , 

T  = X0X0 
T  = X0RR−1 (RT )−1RT X0 
2X0R)−1RT X0 
X (X T X )−1X T  = X0R(RT X0
T . 
As  a  result 

�ˆ − �  = R−1X0 
T χ  and  nπˆ 2  = |(I − X0X0 
T )χ|2 . 
(15.0.1) 
T , are orthonormal. Therefore, 
By construction p columns of X0 , which are also the rows of X0 
we  can  choose  the  last  n − p  rows  of  a  n × n matrix 
 
 
⎛
X T  �
0 
A = 
· · · 
104 


=


.


X T 
0 
· · ·


 

g1 
g2 
.
.
.

gn 

χ1
χ2 
.
.
.

χn 

g = Aχ,  i.e.


to  make  A  an  orthogonal  matrix,  we  just  need  to  choose  them  to  complete,  together  with 
T ,  the  orthonormal  basis  in  Rn .  Let  us  deﬁne  a  vector 
rows  of X0 
 
�
�
⎞
⎞
�
⎛
���
���
⎜⎜⎜⎝
⎜⎜⎜⎝
 
�
�
Since  χ  is  a  vector  of  i.i.d.  standard  normal,  we  proved  before  that  its  orthogonal  transfor­
mation  g  will  also  be  a  vector  of  independent  N (0, π 2 )  random  variables  g1 , . . . , gn .  First  of 
all,  since

 

 �
�
⎞
⎞
χ1
g1 
 
 
.
.
. 
.
.
.

⎜⎝
⎜⎝
�
�
�
�
χn 
gp 

 �
⎞
g1 
 
.
.
. 
�ˆ − �  = R−1X0 
= R−1 ˆg . 
T χ = R−1 
�
gp  ⎜⎝
�
+1  + . . . + g 2 . 
|(I − X0X T )χ|2  = gp
2
(15.0.3)
0 
n
First of all, orthogonal transformation preserves  lengths,  so  g 2  =  Aχ 2  =  χ 2 . On  the  other 
| |
|
|
| |
hand,  let  us  write  χ = χT χ  and  break  χ  into  a  sum  of  two  terms 
2 
| |
χ = X0X T χ + (I − X0X T )χ.
0 
0

Next,  we  will  prove  that 

gˆ :=


T
= X0 

,


we  have 

(15.0.2)


Then  we  get 

2

⎠⎟
⎟
⎠
=  χ = χT χ =  χT X0X0 
T χ + (I − X0X0 
T )  X0X0 
2 
T  + χT (I − X0X0 
T )χ
|g |

|
 |

T X0  =  I  since  the  matrix  X0 
T X0 
When  we  multiply  all  the  terms  out  we  will  use  that  X0 
consists  of  scalar  products  of  columns  of X0  which  are  orthonormal.  This  also  implies  that 
T  − X0IX0 
T (I − X0X0 
T ) = X0X0 
T  = 0. 
X0X0 

.


Using  this  we  get 
g 2  =  χ 2  =  χT X0X T χ + χT (I − X0X T )(I − X0X T )χ
| |
| |
0 
0
0
|X T χ|2  + |(I − X0X T )χ|2  =  |gˆ|2  + |(I − X0X T )χ|2 
= 
0 
0 
0
T χ  so  we  ﬁnally  proved  that 
because  ˆg = X0 
2  + . . . + gn 
+1  + . . . + g 2 
2  − . . . − g 2  = gp
2  − g1
T )χ|2  =  |g |2  − |gˆ|2  = g1
2
|(I − X0X0 
n 
p
n−p  and  it  is  also  independent  of  �ˆ which 
which  is  (15.0.3).  This  proves  that  nπˆ 2 /π 2  �  �2 
depends  only  on  g1 , . . . , gp  by  (15.0.2). 

105 




































Let  us  for  convenience  write  down  equation  (15.0.2)  as  a  separate  result. 
Lemma  2.  Given  a  decomposition  X  =  X0R  with  n × p  matrix  X0  with  orthonormal 
columns  and  invertible  (upper  triangular)  p × p  matrix  R  we  can  represent 
 
⎞ �
g1 
  ..  ⎜
 
.
�ˆ − �  = R−1 gˆ = R−1 �
 
� ⎝
gp 
for  independent  N (0, π 2 )  random  variables  g1 , . . . , gp . 
Conﬁdence  intervals  and  t-tests  for  linear  combination  of  parameters  � .  Let 
us  consider  a  linear  combination 

c1�1  + . . . + cp�p  = c T � 

where  c = (c1 , . . . , cp)T . To construct  conﬁdence  intervals  and t-tests  for this  linear combina-
T  ˆ
tion  we  need  to  write  down  a  distribution  of  c � .  Clearly,  it  has  a  normal  distribution  with 
mean  EcT �ˆ = cT �  and  variance 
E(c T (�ˆ − � ))2  = Ec T (�ˆ − � )(�ˆ − � )T c = c T Cov( �ˆ)c = π 2 c T (X T X )−1 c. 
Therefore, 

c  � N (0, 1) 

 
n − p

nπˆ 2cT (X T X )−1c  � tn−p .


cT (�ˆ − � ) 
�
π 2 cT (X T X )−1
and  using  that  nπˆ 2 /π 2  � �2 
n−p  we  get 
 
cT (�ˆ − � )  ��
�
1  nπˆ 2 
n − p π 2  = c T (�ˆ − � ) 
�π 2cT (X T X )−1c
To obtain the distribution of one parameter �ˆi  we need to choose a vector c that has all zeros 
and  1  in  the  ith  coordinate.  Then  we  get 
 
�
(�ˆi  − �i ) 
� tn−p . 
Here  ((X T X )−1 )ii  is  the  ith  diagonal  element  of  the  matrix  (X T X )−1 .  This  is  a  good  time 
to mention  how  the  quality  of  estimation  of  �  depends  on  the  choice  of X.  For  example,  we 
mentioned  before  that  the  columns  of  X  should  be  linearly  independent.  What  happens  if 
some  of  them  are nearly  collinear? Then  some  eigenvalues  of  (X T X ) will be  ’small’  (in  some 
sense)  and  some  eigenvalues  of  (X T X )−1  will  be  ’large’.  (Small  and  large  here  are  relative 
terms because  the  size  of the matrix  also  grows with n.) As  a  result,  the conﬁdence  intervals 
for  some  parameters  will  get  very  large  too  which  means  that  their  estimates  are  not  very 
accurate.  To  improve  the  quality  of  estimation  we  need  to  avoid  using  collinear  predictors. 
We  will  see  this  in  the  example  below. 

n − p 
nπˆ 2 ((X T X )−1 )ii 

106 

Joint  conﬁdence  set  for  �  and  F -test.  By  Lemma  2, R(�ˆ − � ) = ˆg  and,  therefore, 
2  = |gˆ|2  = ˆg T gˆ = ( �ˆ − � )T RT R(�ˆ − � ) = ( �ˆ − � )T X T X (�ˆ − � ). 
2  + . . . + gp 
g1
Since  gi  � N (0, π 2 )  this  proves  that 
(�ˆ − � )T X T X (�ˆ − � ) 
π 2 

� �2 . 
p

(n − p) 
(�ˆ − � )T X T X (�ˆ − � ) � Fp,n−p . 
π 2 
np ˆ

Using  that  nπˆ 2/π 2  � �2 
n−p  gives 
  nπˆ 2 
(�ˆ − � )T X T X (�ˆ − � ) �
(n − p)π 2  = 
pπ 2 
If  we  take  c  such  that  Fp,n−p(0, c� ) = �  then 
(n − p)
(�ˆ − � )T X T X (�ˆ − � ) � c� 
npπˆ 2 
deﬁnes  a  joint  conﬁdence  set  for  all  parameters  �  simultaneously  with  conﬁdence  level  �. 
Suppose  that  we  want  to  test  a  hypothesis  about  all  parameters  simultaneously,  for 
example, 

(15.0.4) 

Then  we  consider  a  statistic 

H0  : �  = �0 . 

F  =

− 
(n
p)
(�ˆ − �0 )T X T X (�ˆ − �0 ), 
πˆ 2 
np
which  under  null  hypothesis  has  Fp,n−p  distribution,  and  deﬁne  a  decision  rule  by 
 
�
β =  H0  :  F  � c 
H1  :  F  > c, 
where  a  threshold  c  is  determined  by  Fp,n−p(c, �) =  �  - a  level  of  signiﬁcance.  Of  course, 
this  test  is  equivalent  to  checking  if  vector  �0  belongs  to  a  conﬁdence  set  (15.0.4)!  (We  just 
need  to  remember  that  conﬁdence  level  =  1  - level  of  signiﬁcance.) 

(15.0.5) 

Simultaneous  conﬁdence  set  and  F -test  for  subsets  of  � .  Let 
s = {i1 , . . . , ik } ≤ {1, . . . , p} 
be  s  subset  of  size  k  �  p  of  indices  {1, . . . , p}  and  let  �s  = (�i1 , . . . , �ik )T  be  a  vector  that 
consists  of  the  corresponding  subset  of parameters � . Suppose  that we would  like  to  test  the 
hypothesis 

0 
H0  : �s  = �s 
0  = 0. Let �ˆs  be a corresponding vector of estimates. 
0 ,  for example, �s 
for some given vector �s 
Let 
 
⎟
⎠
= (X T X )−1
i,j 

i,j�s 

�s 

107 


be  a  k × k  submatrix  of  (X T X )−1  with  row  and  column  indices  in  the  set  s.  By  the  above 
Theorem,  the  joint  distribution  of  �ˆ
s  is 
�ˆs  � N (�s , π 2�s ). 
1/2 ,  i.e. A  is a symmetric k × k matrix such  that �s  = AAT . As a result, a centered 
Let A = �s 
vector  of  estimates  can  be  represented  as 
�ˆs  − �s  = Ag , 
where  g = (g1 , . . . , gk )T  are  independent  N (0, π 2 ).  Therefore,  g = A−1 (�ˆs  − �s )  and  the  rest 
is  similar  to  the  above  argument.  Namely, 
|g |2  = g T g = ( �ˆs  − �s )T (A−1 )T A−1 (�ˆs  − �s ) 
2	 = 
2  + . . . + gk 
g1
s  − �s)T �s−1 (�ˆ
= (�ˆ
s  − �s )T (AAT )−1 (�ˆ
s  − �s ) = ( �ˆ
s  − �s ) � π 2�2 
k . 

As  before  we  get 

(n − p)
s  (�ˆs  − �s ) � Fk,n−p
π 2  (�ˆs  − �s
)T �−1
nk ˆ
and  we  can  now  construct  a  simultaneous  conﬁdence  set  and  F -tests. 

F  = 

Remark. Matlab  regression  function  ’regress’  assumes  that  a matrix X  of  explanatory 
variables will contain a ﬁrst column of ones that corresponds to an ”intercept” parameter �1 . 
The F -statistic output by ’regress’ corresponds to F -test about all other ”slope” parameters: 

H0  : �2  = . . . = �p  = 0. 

In  this  case  s = {2, 3, . . . , p}, k = p − 1  and 
(n − p) 
�ˆT �−1 ˆ
F  =
s  �s  � Fp−1,n−p . 
n(p − 1) ˆπ 2  s

Example.  Let  us  take  a  look  at  the  ’cigarette’  dataset  from  previous  lecture.  We  saw 
that  tar,  nicotine  and  carbon  monoxide  content  are  positively  correlated  and  any  pair  is 
well  described  by  a  simple  linear  regression.  Suppose  that  we  would  like  to  predict  carbon 
monoxide  as  a  linear  function  of  both  tar  and  nicotine  content.  We  create  a  25 × 3  matrix 
X : 

X=[ones(25,1),tar,nic]; 

We  introduce a ﬁrst column of ones  to allow an  intercept parameter �1  in our multiple  linear 
regression  model: 

COi  = �1  + �2Tari  + �3Nicotini  + χi . 
If  we  perform  a multiple  linear  regression: 

108 

[b,bint,r,rint,stats]  =  regress(carb,X); 

We  get  the  estimates  of  parameters  and  95%  conﬁdence  intervals  for  each  parameter 

b  =  3.0896 
0.9625 
-2.6463 

bint  =  1.3397 
0.4717 
-10.5004 

4.8395 
1.4533 
5.2079 

and,  in  order, R2 -statistic,  F -statistic  from  (15.0.5),  p-value  for  this  statistic 

Fp,n−p(F , +�) = F3,25−3 (F , +�) 
and  the  estimate  of  variance  ˆπ 2  : 

stats  =  0.9186  124.1102  0.000  1.9952. 

First  of  all,  we  see  that  high  R2  means  that  linear  model  explain  most  of  the  variability 
in  the  data  and  small  p-value  means  that  we  reject  the  hypothesis  that  all  parameters  are 
equal  to zero. On  the other hand,  simple  linear regression  showed  that carbon monoxide had 
a  positive  correlation  with  nicotine  and  now  we  got  �ˆ3  =  −2.6463.  Also,  notice  that  the 
conﬁdence  interval  for �3  is very  poor. The  reason  for  this  is  that tar and nicotine  are nearly 
collinear.  Because  of  this  the matrix 
�
⎞
0.3568 
0.0416  −0.9408 
 
  0.0416 
(X T X )−1  = �
0.0281  −0.4387  ⎝
−0.9408  −0.4387  7.1886 
has  relatively  large  last  diagonal  diagonal  value.  We  recall  that  Theorem  gives  that  the 
variance  of  estimate  �ˆ3  is  7.1886π 2  and  we  also  see  that  the  estimate  of  π 2  is  ˆπ 2  = 1.9952. 
As  a  result  the  conﬁdence  interval  for  �3  is  rather  poor. 
Of course,  looking at  linear  combinations  of  tar and nicotine  as new predictors  does not 
make  sense  because  they  lose  their meaning,  but  for  the  sake  of  illustrations  let  us  see  what 
would  happen  if  our  predictors  were  not  nearly  collinear  but,  in  fact,  orthonormal.  Let  us 
use  economic  QR  decomposition 

[X0,R]=qr(X,0) 

a  new  matrix  of  predictor  X0  with  orthonormal  columns  that  are  some  linear  combinations 
of  tar  and  nicotine.  Then  regressing  carbon monoxide  on  these  new  predictors 

[b,bint,r,rint,stats]  =  regress(carb,X0); 

we  would  get 

b  =  -62.6400 
-22.2324 
0.9870 

bint  =  -65.5694  -59.7106 
-25.1618  -19.3030 
3.9164 
-1.9424 

109 

all  conﬁdence  intervals  of  the  same  relatively  better  size.


Example. The  following data presents per capita  income of 20 countries  for 1960s. Also 
presented  are  the  percentages  of  labor  force  employed  in  agriculture,  industry  and  service 
for  each  country.  (Data  source:  lib.stat.cmu.edu/DASL/Dataﬁles/oecdat.html) 

COUNTRY 

PCINC  AGR  IND  SER 

1536  13  43  45 
CANADA 
SWEEDEN 
1644  14  53  33 
SWITZERLAND  1361  11  56  33 
LUXEMBOURG  1242  15  51  34 
U.  KINGDOM  1105  4 
56  40 
DENMARK 
1049  18  45  37 
W.  GERMANY  1035  15  60  25 
FRANCE 
1013  20  44  36 
1005  6 
BELGUIM 
52  42 
20  49  32 
977 
NORWAY 
ICELAND 
839 
25  47  29 
11  49  40 
NETHERLANDS  810 
23  47  30 
681 
AUSTRIA 
36  30  34 
529 
IRELAND 
ITALY 
504 
27  46  28 
33  35  32 
344 
JAPAN 
56  24  20 
324 
GREECE 
SPAIN 
290 
42  37  21 
44  33  23 
238 
PORTUGAL 
TURKEY 
177 
79  12  9 

We can perform simple  linear regression of income on each of the other explanatory variables 
or  multiple  linear  regression  on  any  pair  of  the  explanatory  variables.  Fitting  simple  linear 
regression  of  income  vs.  percent  of  labor  force  in  agriculture,  industry  and  service: 

polytool(agr,income,1), 

etc.,  produces  ﬁgure  15.1.  Next,  we  perform  statistical  inference  using  ’regress’  function. 
Statistical  analysis  of  linear  regression  ﬁt  of  income  vs.  percent  of  labor  force  in  agriculture: 

[b,bint,r,rint,stats]=regress(income,[ones(20,1),agr]) 

b  =  1317.9 
-18.9 

bint  =  1094.7 
-26.0 

1541.1 
-11.7 

stats  = 

0.6315  30.8472  2.8e-005  74596. 

110 

2500 

2000

1500 

1000 

500 

0 

−500 

−1000 

15 

20 

25 

30 

35 

40 

45 

50 

55 

60 

10 

20 

30 

40 

50 

60 

70 

80 

10 

15 

20 

25 

30 

35 

40 

45 

1500 

1000 

500 

0 

−500 

2500


2000


1500


1000


500


0


−500


−1000


Figure  15.1:  Linear  regression  of  income  on  percent  of  labor  force  in  agriculture,  industry  and 
service. 

For  income  vs.  percent  of  labor  force  in  industry 

[b,bint,r,rint,stats]=regress(income,[ones(20,1),ind]); 

b  =  -359.3115 
27.4905 

bint  =  -907.1807  188.5577 
15.3058 
39.6751 

stats  =  0.5552  22.4677  0.0002  90042 

and  for  income  vs.  labor  force  in  service 

[b,bint,r,rint,stats]=regress(income,[ones(20,1),serv]); 

b  =  -264.5199 
35.3024 

bint  =  -858.0257  328.9858 
53.7093 
16.8955 

111 

stats  = 

0.4742  16.2355 

0.0008 

106430. 

We  see  that  in  all  three  cases,  the  hypotheses  that  parameters  of  least-squares  line  are  both 
zero  can be  rejected  at conventional  level  of  signiﬁcance � = 0.05. Looking at  the  conﬁdence 
intervals  for  the  estimates  of  slopes  we  observe  that  the  correlation  of  income  with  percent 
of  labor  force  in  agriculture  is  negative,  and  other  two  correlations  are  positive. 
We  can  also  perform  a  multiple  regression  on  any  two  explanatory  variables.  We  can 
not perform multiple  linear  regression with  all  three  explanatory  variables  because  they  add 
up  to  100%,  i.e.  they  are  linearly  dependent.  If  we  create  a  predictor  matrix 

X=[ones(20,1),agr,ind]; 

and  perform multiple  linear  regression 

[b,bint,r,rint,stats]=regress(income,X); 

we  get 

b  =  1272.1 
-18.4 
0.8 

bint  =  -632.6 
-39.1 
-31.4 

3176.9 
2.3 
32.9 

stats  =  0.6316  14.5703  0.0002  78972 

Of  course,  one  can  ﬁnd  many  shortcomings  of  this  model.  For  example,  having  the  entire 
population  in  agriculture  results  in  prediction  of  1272.1 − 1840  <  0  negative  income  per 
capita. 

112 


