Massachusetts Institute of Technology 
6.042J/18.062J, Fall ’05: Mathematics for Computer Science 
Prof. Albert R. Meyer and Prof. Ronitt Rubinfeld 

Course Notes, Week 8 
October 24 
revised October 25, 2005, 695 minutes 

Sums, Products & Asymptotics 

1  Closed Forms and Approximations 

Sums and products arise regularly in the analysis of algorithms and in other technical areas such 
n� 
as ﬁnance and probabilistic systems. We’ve already seen that 
i  = 
i=1 

n(n   +  1) 
2 

. 

Having a simple closed form expression such as n(n+ 1)/2 makes the sum a lot easier to understand 
and  evaluate.  We  proved  by  induction  that  this  formula  is  correct,  but  not where  it  came  from. 
In Section 4, we’ll discuss ways  to ﬁnd  such  closed  forms.  Even when  there are no closed  forms 
exactly  equal  to  a  sum,  we may  still  be  able  to  ﬁnd  a  closed  form  that  approximates  a  sum  with 
useful accuracy. 
� 
The product we focus on in these notes is the familiar factorial: 
n
i. 
i=1 

n!  ::=  1 2 · · · (n  − 1)  · n  = 
· 

We’ll describe a closed form approximation for it called Stirling’s Formula. 
Finally, when there isn’t a good closed form approximation for some expression, there may still be 
a  closed  form  that  characterizes  its growth  rate.  We’ll  introduce  asymptotic notation,  such as “big 
Oh”, to describe growth rates. 

2  The Value of an Annuity 

Would  you  prefer  a million  dollars  today  or  $50,000  a  year  for  the  rest  of  your  life?  On  the  one 
hand, instant gratiﬁcation is nice. On the other hand, the total dollars received at $50K per year is 
much larger if you live long enough. 
Formally, this is a question about the value of an annuity. An annuity is a ﬁnancial instrument that 
pays  out  a  ﬁxed  amount  of money  at  the  beginning  of  every  year  for  some  speciﬁed  number  of 
years.  In particular,  an n­year, m­payment annuity pays m  dollars at  the  start of  each year  for n 
years.  In some cases, n  is ﬁnite, but not always.  Examples include lottery payouts, student loans, 
and home mortgages. There are even Wall Street people who specialize in trading annuities. 

Copyright © 2005, Prof. Albert R. Meyer and Prof. Ronitt Rubinfeld. 

2 

Course Notes, Week 8:  Sums, Products & Asymptotics 

A  key  question  is what  an  annuity  is worth.  For  example,  lotteries  often  pay  out  jackpots  over 
many years.  Intuitively, $50,   000  a year for 20 years ought  to be worth  less  than a million dollars 
right now.  If you had all the cash right away, you could invest it and begin collecting interest. But 
what if the choice were between $50,  000  a year for 20 years and a half million dollars today? Now 
it is not clear which option is better. 
In order to answer such questions, we need to know what a dollar paid out in the future is worth 
today.  To model  this,  let’s  assume  that money  can  be  invested  at  a  ﬁxed  annual  interest  rate  p. 
We’ll assume an 8% rate1  for the rest of the discussion. 
Here  is why  the  interest  rate p  matters.  Ten dollars  invested  today at  interest  rate p  will become 
(1 + p) 10  = 10.80  dollars in a year, (1 + p)2  10  ≈ 11.66  dollars in two years, and so forth. Looked 
·
· 
at  another way,  ten dollars paid out a year  from now are only  really worth 1/(1  +  p) 10  ≈  9.26
· 
dollars  today.  The  reason  is  that  if  we  had  the  $9.26  today,  we  could  invest  it  and  would  have 
$10.00   in a year anyway. Therefore, p  determines the value of money paid out in the future. 

2.1  The Future Value of Money 

V  = 

Our  goal  is  to  determine  the  value  of  an  n­year,  m­payment  annuity.  The  ﬁrst  payment  of  m 
dollars  is  truly  worth  m  dollars.  But  the  second  payment  a  year  later  is  worth  only  m/(1  +  p) 
dollars.  Similarly,  the  third  payment  is  worth  m/(1  +  p)2 ,  and  the  n­th  payment  is  worth  only 
m/(1  +  p)n−1  .  The  total value V  of  the  annuity  is  equal  to  the  sum  of  the payment values.  This 
n�  m 
gives: 
(1  +  p)i−1  . 
i=1 
To  compute  the  real  value  of  the  annuity,  we  need  to  evaluate  this  sum.  One  way  is  to  plug 
in  m,  n,  and  p,  compute  each  term  explicitly,  and  then  add  them  up.  However,  this  sum  has  a 
special closed form that makes the job easier.  (The phrase “closed form” refers to a mathematical 
expression without any summation or product notation.)  First, lets make the summation prettier 
n�  m 
with some substitutions. 
(1  +  p)i−1 
n−1�  m 
i=1  
n−1� 
(1  +  p)j 
j=0 
xj 
j=0 

(substitute j  =  i  − 1) 

(substitute x  =

1 
1 +  p

).

V  = 

= 

=  m

The goal of these substitutions is to put the summation into a special form so that we can bash it 
with a theorem given in the next section. 

1U.S.  interest rates have dropped steadily  for several years, and ordinary bank deposits now earn around 3%.  But 
just a few years ago the rate was 8%; this rate makes some of our examples a little more dramatic.  The rate has been as 
high as 17% in the past twenty years. 
In Japan, the standard interest rate is near zero%, and on a few ocasions in the past few years has even been slightly 
negative.  It’s a mystery to U.S. economists why the Japanese populace keeps any money in their banks. 

Course Notes, Week 8:  Sums, Products & Asymptotics 

3 

2.2  Geometric Sums 
Theorem 2.1.  For all n ≥ 1 and all x = 1, � 
n−1
x  =
i 
i=0 

1 − x
n
1 − x

. 

The summation in this theorem is a geometric sum.  The distinguishing feature of a geometric sum 
is that each of the terms 
1,  x,   x ,  x ,  . . . ,  x n−1  . 
3
2
in  the  sum  is a constant  times  the one before;  in  this case,  the constant  is x.  The  theorem gives a 
closed form for a geometric sum that starts with 1. 
We  already  saw  one proof  of  this  theorem  in  our  lectures  on  induction.  As  is  often  the  case,  the 
proof  by  induction  gives  no  hint  about  how  the  formula was  found  in  the  ﬁrst  place.  Here  is  a 
more  insightful  derivation.  The  trick  is  to  let  S  be  the  value  of  the  sum  and  then  observe what 
−xS  is: 
· · · 
+xn−1 
−xn−1  − x .
· · · 
n 

S  = 1  +x  +x2  +x3 
+ 
−x  −x −x − 
−xS  = 
2
3 
Adding these two equations gives: 

S − xS  = 1 − x ,  
n

so 

1 − x
n
1 − x
We’ll say more about ﬁnding (as opposed to just proving) summation formulas later. 

S =

. 

2.3  Return of the Annuity Problem 

V  =  m

Now we  can  solve  the  annuity  pricing  problem.  The  value  of  an  annuity  that  pays m  dollars  at 
the start of each year for n years is computed as follows: 
� 
n−1
xj 
j=0 
1 − x
n
=  m  1 − x 
1 − ( 1+p )
1 
n
1 −  1+p 
1
1 + p − (
p 

=  m

=  m 

)n−1 

1
1+p

. 

The  ﬁrst  line  is  a  restatement  of  the  summation we  obtained  earlier  for  the  value  of  an  annuity.

The  second  line  uses  the  closed  form  formula  for  a  geometric  sum.  In  the  third  line,  we  undo


�
4	

Course Notes, Week 8:  Sums, Products & Asymptotics 

the earlier substitution x  = 1/(1  +  p).  In  the ﬁnal step, both  the numerator and denominator are 
multiplied by 1 +  p  to simplify the expression. 
The resulting formula is much easier to use than a summation with dozens of terms. For example, 
what is the real value of a winning lottery ticket that pays $50,   000  per year for 20 years? Plugging 
in m  =  $50,  000,  n  =  20,  and  p  = 0.08  gives V  ≈  $530,   180.  Because payments  are deferred,  the 
million dollar lottery is really only worth about a half million dollars!  This is a good trick for the 
lottery advertisers! 

2.4  Inﬁnite Geometric Series 

The  question  at  the  beginning  of  this  section  was  whether  you  would  prefer  a  million  dollars 
today or $50,  000  a year for the rest of your life.  Of course, this depends on how long you live, so 
optimistically assume that the second option is to receive $50,   000  a year forever.  This sounds like 
inﬁnite money! 
We can compute the value of an annuity with an inﬁnite number of payments by taking the limit 
of our geometric sum in Theorem 2.1 as n   tends to inﬁnity. This one is worth remembering! 
� 
Theorem 2.2.  If |x| <  1, then 
∞
i x  =
i=0 

1
1 − x
� 
n−1
lim 
i 
x 
n→∞ 
i=0 
1 − x
n 
lim 
n→∞  1 − x 
1 
1 − x 
.
The  ﬁrst  equality  follows  from  the  deﬁnition  of  an  inﬁnite  summation.  In  the  second  line,  we 
apply  the  formula  for  the  sum  of  an  n­term  geometric  sum  given  in Theorem  2.1.  The ﬁnal  line 
follows by evaluating the limit; the xn  term vanishes since we assumed that |x| <  1. 

� 
∞
i	
x  = 
i=0 

Proof. 

. 

=	

= 

In our annuity problem, x  = 1/(1  +  p) <  1, so the theorem applies.  Substituting for x, we get an 
annuity value of 

V  =  m  · 
=  m  · 
=  m  · 
· 

=  m

1 
1 − x 
1 
1 − 1/(1  +  p) 
1 +  p
(1  +  p) − 1 
1 +  p
p 

.

Course Notes, Week 8:  Sums, Products & Asymptotics 

5 

Plugging in m  =  $50,  000  and p  = 0.08  gives only $675,  000.  Amazingly, a million dollars today is 
worth much more  than $50,   000  paid every year  forever!  Then again,  if we had a million dollars 
today in the bank earning 8% interest, we could take out and spend $80,   000  a year forever.  So the 
answer makes some sense. 

2.5  Examples 

1 +   1/2 +  1/4 +   1/8 +   · · · = 

We  now  have  closed  form  formulas  for  geometric  sums  and  series.  Some  examples  are  given 
below.  In each case, the solution follows immediately from either Theorem 2.1 (for ﬁnite sums) or 
∞� 
Theorem 2.2 (for inﬁnite series). 
(1/2)i 
∞� 
i=0 
(1/10)i  =  0.9 
0.999999999   .  .   .  =  0.9 
∞� 
i=0 
(−1/2)i 
n−1� 
i=0 
2i 
n−1� 
i=0 
3i 
i=0 

1 
1 − (1/2) 
1 
1 − 1/10 
1 
1 − (−1/2) 
1 − 2n 
1 − 2 
1 − 3n 
1 − 3 

1 +  3 +  9 +  27  +  · · · +  3n−1  = 

1 +   2 +  4 +   8 +   · · · +  2n−1  = 

1 − 1/2 +  1/4 − 1/8 +  · · · = 

3n  − 1 
2 

=  2n  − 1 

=  0.9 

=  1 

(1) 

(2) 

(3) 

(4) 

(5) 

= 

= 

= 

= 

= 

=  2 

10 
9 

=  2/3 

If  the  terms  in a geometric sum or series grow smaller, as  in equation  (1),  then  the sum  is said  to 
be geometrically decreasing.  If the terms in a geometric sum grow progressively larger, as in (4) and 
(5), then the sum is said to be geometrically increasing. 
Here is a good rule of thumb:  a geometric sum or series is approximately equal to the term with greatest 
absolute value.  In equations  (1) and  (3),  the  largest  term  is equal  to 1 and  the sums are 2 and 2/3, 
both  relatively  close  to  1.  In  equation  (4),  the  sum  is  about  twice  the  largest  term.  In  the  ﬁnal 
equation (5), the largest term is 3n−1  and the sum is (3n  − 1)/2, which is only about a factor of 1.5 
greater. 

2.6  Related Sums 

� 
We now know all about geometric sums. But in practice one often encounters sums that cannot be 
i
transformed by simple variable substitutions to the form 
x . 
A non­obvious, but useful way to obtain new summation formulas from old is by differentiating 
� 
or integrating with respect to x. As an example, consider the following sum: 
n
ixi  =  x  +  2x 2  +  3x 3  +
i=1  

+  nx
n

· · ·

This is not a geometric sum, since the ratio between successive terms is not constant. Our formula

for the sum of a geometric sum cannot be directly applied.  But suppose that we differentiate that


6	

formula: 

Course Notes, Week 8:  Sums, Products & Asymptotics 

d  � 
n	
x  = 
i 
n
� 
dx 
i=0 

ixi−1  = 
i=1  

d  1 − x
n+1
1 − x 
dx 
−(n  +  1)xn (1	 − x) − (−1)(1  − xn+1 ) 
(1  − x)2 
−(n  +  1)xn  +  (n  +  1)xn+1  +  1 − x
n+1
(1  − x)2 
1 − (n  +  1)x
n  + 
n+1
nx
(1  − x)2 
� 
�	
Often  differentiating  or  integrating messes  up  the  exponent  of  x  in  every  term.  In  this  case,  we 
ixi−1 ,  but we want  a  formula  for  the  series 
ixi  . 
now  have  a  formula  for  a  sum  of  the  form 
� 
The solution is simple: multiply by x. This gives: 
ix i  =  x  − (n  +  1)xn+1  +  nx
n	
n+2
(1  − x)2 
i=1  

= 

= 

. 

Since we  could  easily  have made  a mistake,  it  is  a  good  idea  to  go  back  and  validate  a  formula 
obtained this way with a proof by induction. 
Notice  that  if  |x|  <  1,  then this series converges to a ﬁnite value even  if there are  inﬁnitely many 
terms. Taking the limit as n  tends inﬁnity gives the following theorem: 
Theorem 2.3.  If |x| <  1, then 
� 
∞
ixi  = 
i=1 

x 
(1  − x)2  . 

As a consequence, suppose there is an annuity that pays im  dollars at the end of each year i  forever. 
For example,  if m  =  $50,  000, then the payouts are $50,   000  and then $100,   000  and then $150,  000 
and so on.  It is hard to believe that the value of this annuity is ﬁnite! But we can use the preceding 
∞� 
theorem to compute the value: 
i=1 

im 
(1  +  p)i 

V  = 

=  m 

=  m

1 
1+p
(1  −  1+p
1  )2
1 +  p
.
2p

The second line follows by an application of Theorem 2.3. The third line is obtained by multiplying 
the numerator and denominator by (1  +  p)2  . 
For  example,  if  m  =  $50,  000,  and  p  = 0.08  as  usual,  then  the  value  of  the  annuity  is  V  = 
$8,  437,  500. Even though payments increase every year, the increase is only additive with time; by 
contrast, dollars paid out  in  the  future decrease  in value exponentially with  time.  The geometric 

Course Notes, Week 8:  Sums, Products & Asymptotics 

7 

decrease swamps out  the additive  increase.  Payments  in  the distant future are almost worthless, 
so the value of the annuity is ﬁnite. 

The important thing to remember is the trick of taking the derivative (or integral) of a summation 
formula.  Of course, this technique requires one to compute nasty derivatives correctly, but this is 
at least theoretically possible! 

3  Book Stacking 

Suppose you have a pile of books and you want  to stack  them on a  table  in some off­center way 
so  the  top  book  sticks  out  past  books  below  it.  How  far  past  the  edge  of  the  table  do  you  think 
you could get the top book to go without having the stack fall over?  Could the top book stick out 
completely beyond the edge of table? 

Most people’s ﬁrst response to this question—sometimes also their second and third responses— 
is  “No,  the  top  book will  never  get  completely  past  the  edge  of  the  table.”  But  in  fact,  you  can 
get the top book to stick out as far as you want:  one booklength, two booklengths, any number of 
booklengths! 

3.1  Formalizing the Problem 

We’ll approach this problem recursively. How far past the end of the table can we get one book to 
stick out?  It won’t  tip  as  long  as  its  center of mass  is over  the  table,  so we  can get  it  to  stick out 
half its length, as shown in Figure 1. 

Figure 1: One book can overhang half a book length. 

Now  suppose  we  have  a  stack  of  books  that  will  stick  out  past  the  table  edge  without  tipping 
over—call that a stable stack.  Let’s deﬁne the overhang of a stable stack to be the largest horizontal 
distance from the center of mass of the stack to the furthest edge of a book.  If we place the center 
of mass of the stable stack at the edge of the table as in Figure 2, that’s how far we can get a book 
in the stack to stick out past the edge. 

So we want a formula for the maximum possible overhang, Bn , achievable with a stack of n  books. 

table12center of massof book8 

Course Notes, Week 8:  Sums, Products & Asymptotics


Figure 2: Overhanging the edge of the table. 

B1  = 

We’ve already observed that the overhang of one book is 1/2 a book length. That is, 
1 
.2
Now  suppose we  have  a  stable  stack  of  n  + 1 books with maximum  overhang.  If  the  overhang 
of  the  n  books  on  top  of  the  bottom  book  was  not  maximum,  we  could  get  a  book  to  stick  out 
further by replacing the top stack with a stack of n  books with larger overhang.  So the maximum 
overhang, Bn+1 , of a stack of n + 1 books is obtained by placing a maximum overhang stable stack 
of n  books on top of the bottom book. And we get the biggest overhang for the stack of n + 1 books 
by placing the center of mass of the n  books right over the edge of the bottom book as in Figure 3. 
So we know where to place the n  + 1st book to get maximum overhang, and all we have to do is 
calculate what it  is.  The simplest way to do that  is to  let the center of mass of the top n  books be 
the  origin.  That way  the  horizontal  coordinate  of  the  center  of mass  of  the whole  stack  of n  + 1 
books will equal the increase in the overhang.  But now the center of mass of the bottom book has 
horizontal coordinate 1/2, so the horizontal coordinate of center of mass of the whole stack of n + 1 
books is 
n  + (1/2) · 1
0 ·
n  + 1 

1
2(n  + 1) . 

=

In other words, 

as shown in Figure 3. 
Expanding equation (6), we have 

Bn+1  =  Bn  + 

1 
2(n  + 1) , 

(6) 

+

1
1 
Bn+1  =  Bn−1  + 
2n  
2(n  + 1) 
1
1 
+ · · ·
=  B1  + 
1 n+1� 1 
2 · 2
2n  
2 
i
i=1 

+

= 

+

1 
2(n  + 1) 

. 

tablecenter of massof the whole stackoverhangCourse Notes, Week 8:  Sums, Products & Asymptotics


9 

Deﬁne 

Figure 3: Additional overhang with n  + 1 books. 
n� 1 
i 
i=1 

Hn  ::= 

. 

Hn  is called the nth Harmonic number, and we have just shown that 

Bn  = 

Hn
2  . 

The  ﬁrst  few Harmonic  numbers  are  easy  to  compute.  For  example,  H1  =  1,  H2  =  1 + 1  =  2 ,
3 
2 
H3  =  1 + 1  + 1  =  6  ,  H4  =  1 + 1  + 1  + 1  =  12  .  The  fact  that H4  is  greater  than  2  has  special 
11  
25
2
3 
2
3
4 
signiﬁcance; it implies that the total extension of a 4­book stack is greater than one full book! This 
is the situation shown in Figure 4. 

Figure 4:  Stack of four books with maximum overhang.


In  the next  section we will prove  that Hn  grows  slowly,  but unboundedly with n.  That means we 
can get books to overhang any distance past the edge of the table by piling them high enough! 

table}2(n+1)1ntopbooks}center of massof topbooksncenter of massof all+1 booksnTable11/21/31/410 

Course Notes, Week 8:  Sums, Products & Asymptotics 

3.2  Evaluating the Sum—The Integral Method 

It would be nice to answer questions like, “How many books are needed to build a stack extending 
100 book lengths beyond the table?”  One approach to this question would be to keep computing 
Harmonic numbers until we found one exceeding 200. However, as we will see, this is not such a 
keen idea. 

Such questions would be settled if we could express Hn  in a closed form. Unfortunately, no closed 
form  is  known,  and  probably  none  exists.  As  a  second  best,  however, we  can  ﬁnd  closed  forms 
for very good approximations to Hn  using the Integral Method.  The  idea of the Integral Method 
is to bound terms of the sum above and below by simple functions as suggested in Figure 5.  The 
integrals of these functions then bound the value of the sum above and below. 

� 
Figure 5:  This ﬁgure  illustrates  the  Integral Method  for bounding a  sum.  The  area under  the “stairstep” 
n
curve over  the  interval  [0,  n] is  equal  to Hn  =
1/i.  The  function 1/x  is  everywhere greater  than or 
i=1 
equal to the stairstep and so the integral of 1/x  over this interval is an upper bound on the sum.  Similarly, 
1/(x  + 1)   is everywhere less than or equal to the stairstep and so the integral of 1/(x  + 1)  is a lower bound 
on the sum. 
� 
The Integral Method gives the following upper and lower bounds on the harmonic number Hn : 
n  1
�  n+1  1 
�  n 
dx  = 1 +  ln  n 
1 + 
1  x 
1 
0  x  +  1 
x 
1 

dx  = ln(n  +  1).

dx  = 

Hn  ≤ 

Hn  ≥ 

(7)

These bounds imply that the harmonic number Hn  is around ln   n. Since ln   n  grows without bound, 
albeit slowly, we can make a stack of books that extends arbitrarily far. 

For example,  to build a  stack extending  three book  lengths beyond  the  table, we need a number 
of books n  so that Hn  ≥ 6. Exponentiating the above inequalities gives 
e Hn−1  ≤ n  ≤ e Hn  − 1.  

This  implies  that we will need somewhere between 149 and 402 books.  Actual calculation of Hn 
shows that 227 books will be the minimum number to overhang three book lengths. 

10123456781 / x1 / (x + 1)Course Notes, Week 8:  Sums, Products & Asymptotics 

11 

3.3  More about Harmonic Numbers 

Hn  = ln n  +  γ  +

In the preceding section, we showed that Hn  is about ln   n. A even better approximation is known: 
1
1 
�(n)
120n
12n2 
2n  
4
Here γ  is a value 0.577215664  .  .  .  called Euler ’s constant, and �(n) is between 0 and 1 for all n. We 
will not prove this formula. 
The shorthand Hn  ∼ ln  n  is used to indicate that the leading term of Hn  is ln  n. More precisely: 
→ 
Deﬁnition 3.1.  For functions f , g  :  R
R, we say f  is asymptotically equal to g , in symbols, 
f (x) ∼ g(x) 

+ 

+ 

iff 

lim  f (x)/g(x) = 1. 
x→∞ 
We  also might write Hn  ∼  ln  n  +  γ  to  indicate  two  leading  terms.  While  this notation  is widely 
used,  it  is not  really  right.  Referring  to  the deﬁnition  of ∼, we  see  that while Hn  ∼  ln  n  +  γ  is  a 
true  statement,  so  is Hn  ∼  ln  n  +  c  where  c  is any constant.  The correct way  to  indicate  that γ  is 
the second­largest term is Hn  − ln  n  ∼ γ . 
The reason that the ∼ notation is useful is that often we do not care about lower order terms.  For 
� � � �

� � � �

example,  if  n  =  100,  then  we  can  compute  H (n)  to  great  precision  using  only  the  two  leading 
terms: 
1
200 
4  Finding Summation Formulas 
�

n
i  =  n(n   +  1)/2  is still a mystery!  Sure, we can prove  this 
The source of  the simple  formula 
i=1 
statement  true  by  induction,  but where  did  the  expression  on  the  right  come  from?  Even more 
� 
inexplicable is the summation formula for consecutive squares: 
n
i2 
i=1  

(2n  +  1)(n  +  1)n 
6 

Hn  − ln  n  − γ | ≤ 
|

1 
120  · 1004 

1 
120000  

1

200


− 

+

= 

<


.


+ 

+ 

n 
6 

n2 
2 

= 
∼ 

n3 
3 
n3 
3  . 
Here is how we might ﬁnd the sum­of­squares formula if we forgot it or had never seen it.  First, 
� 
�

� 
the Integral Method gives a quick estimate of the sum: 
n
i2  ≤ 
x 2  dx  ≤ 
� 
0 
0 
i=1 
n
(n  +  1)3 
i2  ≤

3

i=1 

(x  +  1)2  dx 

1
.− 
3

3 
n
3 

≤ 

n 

n 

12 

� 
Course Notes, Week 8:  Sums, Products & Asymptotics 
i2  ∼ n3/3. To get 
n
These upper and lower bounds obtained by the Integral Method show that 
i=1 
an exact formula, we then guess the general form of the solution. Where we are uncertain, we can 
add parameters a,  b,  c, .  .  .  .  For example, we might make the guess: 
� 
n
i2  = an 3  + bn2  + cn   + d. 
i=1  

If  the  guess  is  correct,  then we  can determine  the parameters  a,  b,  c,  and  d  by plugging  in  a  few 
values for n. Each such value gives a linear equation in a, b, c, and d.  If we plug in enough values, 
we may get a linear system with a unique solution. Applying this method to our example gives: 
→ 
→ 
→ 
→ 

0 =  d
1 =  a  + b  + c  + d
5 =  8a  + 4b  + 2c   + d
14 = 27a  + 9b  + 3c   + d.

n  =  0 
n  =  1 
n  =  2 
n  =  3 

Solving  this  system gives  the  solution  a  =  1/3,  b  =  1/2,  c  =  1/6,  d  =  0.  Therefore,  if  our  initial 
guess at  the  form of  the solution was correct,  then  the summation  is equal  to n3/3 + n2/2 + n/6. 
In fact, our initial guess was correct, this is the right formula for the sum of squares! 

Be careful! After obtaining a formula by this method, always go back and prove it using induction 
or  some other method.  This  is not merely a check  for algebra blunders;  if  the  initial guess at  the 
solution was not of the right form, then the resulting formula will be completely wrong! 

5  Double Sums 

Sometimes we have to evaluate sums of sums, otherwise known as double summations.  Sometimes 
it is easy: we can evaluate the inner sum, replace it with a closed form, and then evaluate the outer 
sum which no longer has a summation inside it. 

But  there’s  a  special  trick  that  is  often  extremely  useful  for  sums,  which  is  exchanging  the  order 
of  summation.  It’s  best  demonstrated  by  example.  Suppose we want  to  compute  the  sum  of  the 
�  � � 
harmonic numbers 
n
n
k
Hk  =
1/j 
k=1  j=1 
k=1 
�  n 
For intuition about this sum, we can try the integral method: 
� 
n
Hk  ≈ 
1
k=1 

ln x  dx  ≈ n  ln n  − n. 

Now let’s look for an exact answer.  If we think about the pairs (k ,  j ) over which we are summing,


Course Notes, Week 8:  Sums, Products & Asymptotics 

13 

they form a triangle: 

j 
1 
1

1 
1 
1 
.  .  . 
1 

k	 1 
2 
3 
4 

n 

2 

3 

4 

5 .  .  . 

n  

1/2

1/2 1/3

1/2 1/3 1/4


1/2 

.  .  . 

1/n  

=

The summation above is summing each row and then adding the row sums.  Instead, we can sum 
the columns and then add the column sums.  Inspecting the table we see that this double sum can 
�  � � 
be written as 
n
n
k
Hk  =
1/j 
� � 
k=1  j=1 
k=1 
n
n
1/j 
�  � 
j=1  k=j 
n
n
1/j 
1 
n� 1 
j=1 
k=j 
(n  − j  + 1)
n� n  − j + 1 
j
j=1 
� n  + 1  � j
j
j=1 
n
n
− 
n� 1  � 
j	
j
j=1 
j=1 
n
− 
1 
j 
j=1 
j=1 
= (n  + 1)Hn  − n.	

= (n  + 1) 

(8) 

=

=

= 

= 

6  Stirling’s Approximation 

The familiar factorial notation, n!, is an abbreviation for the product 
� 
n
i.  
i=1 

This  is  by  far  the most  common  product  in Discrete Mathematics.  In  this  section we  describe  a 
good  closed­form  estimate  of  n! called  Stirling’s  Approximation.  Unfortunately,  all  we  can  do  is 
estimate:  there  is no closed  form  for n! —  though proving so would  take us beyond  the scope of 
6.042. 

14 

Course Notes, Week 8:  Sums, Products & Asymptotics 

6.1  Products to Sums 

A good way  to handle a product  is often  to convert  it  into a sum by  taking  the  logarithm.  In  the 
case of factorial, this gives 

ln(n!)  =  
= 

= 

ln(1 2 3 · · · (n  − 1)  · n)
·
·
� 
ln  1 +  ln  2 +  ln  3 +  · · · +  ln(n   − 1)  +  ln  n 
n
ln  i. 
i=1  

We’ve not seen a summation containing a logarithm before!  Fortunately, one tool that we used in 
evaluating sums is still applicable:  the Integral Method. We can bound the terms of this sum with 
� 
� 
� 
ln  x  and ln(x  +  1)  as shown in Figure 6. This gives bounds on ln(n!)  as follows: 
� 
� 
� 
n
ln(x  +  1)  dx 
=1 
i
0 
1 
n  +  1 
�
� 
ln   i  ≤ (n  +  1) ln

n
+  1 
=1
i
e 
n  +  1 
e 

ln  x  dx  ≤ 
) +   1   ≤ 
n 
�  �
n  ln(
e 
nn 
e 

ln   i  ≤ 

e  ≤ 

≤ 

n! 

n 

n 

n+1 

e. 

The second line follows from the ﬁrst by completing the integrations. The third line is obtained by 
exponentiating. 

Figure 6: This ﬁgure illustrates the Integral Method for bounding the sum


� 
n
i=1 
So  n!  behaves  something  like  the  closed  form  formula  ( e
)n  .  A  more  careful  analysis  yields  an 
n 
unexpected closed form formula that is asymptotically exact: 
� �n 
n  √
e 

Lemma (Stirling’s Formula). 

n!   ∼ 

ln  i. 

2πn, 

Stirling’s Formula describes how n! behaves in the limit, but to use it effectively, we need to know 
how  close  it  is  to  the  limit  for  different  values  of  n.  That  information  is  given  by  the  bounding 
formulas: 

ln(x + 1)ln(x)Course Notes, Week 8:  Sums, Products & Asymptotics 
� �
Fact (Stirling’s Approximation). 
√
n  n 
e

≤ n!  ≤

1/(12n+1) 
e 

2πn 

√

15 

� �
n  n 
e 

2πn 

1/12n
e

.  

The  Approximation  implies  the  asymptotic  Formula,  since  since  e1/(12n+1)  and  e1/12n  both  ap­
proach  1  as  n  grows  large.  These  inequalities  can  be  veriﬁed  by  induction,  but  the  details  are 
nasty. 
The  bounds  in  Stirling’s  formula  are  very  tight.  For  example,  if  n  =  100,  then  Stirling’s  bounds 
�
�100 
are: 
100 
e �
�100 
100 
e 

1/1201 
e

1/1200 
e

200π

√

√

≥ 

≤ 

100! 

100!  

200π

The only difference between the upper bound and the lower bound is in the ﬁnal term.  In partic­
ular e1/1201  ≈  1.00083299  and e1/1200   ≈  1.00083368.  As a result, the upper bound is no more than 
1  +  10−6  times  the  lower  bound.  This  is  amazingly  tight!  Remember  Stirling’s  formula; we will 
use it often. 

6.2  Bounds by Double Summing 

� 
Another way to derive Stirling’s approximation is to remember that ln  n  is roughly the same as Hn . 
This lets us use the result we derived before for  Hk  via double summation. Our approximation 
for Hk  told us  that  ln(k  +  1)  ≤  Hk  ≤  1  +  ln  k .  Rewriting, we ﬁnd  that Hk  − 1  ≤  ln  k  ≤  Hk−1 .  It 
�  � 
follows that (leaving out the i  = 1  term in the sum, which contributes 0), 
n
n
ln  i  ≤  Hi−1

� 
i=2 

i=2  
n−1
= 
Hi

i=1 

=  nHn−1  − (n  − 1) 
≤ n(1  +  ln(n   − 1))  − (n  − 1) 

by (8) 
by (7) 

=  n ln(n  − 1)  +  1,  

roughly  the  same  bound  as we  proved  before  via  the  integral method.  We  can  derive  a  similar 
lower bound. 

7  Asymptotic Notation 

Asymptotic  notation  is  a  shorthand  used  to  give  a  quick  measure  of  the  behavior  of  a  function 
f (n) as n  grows large. 

16 

Course Notes, Week 8:  Sums, Products & Asymptotics 

7.1  Little Oh 
The asymptotic notation ∼ is an equivalence relation indicating that ∼­equivalent functions grow 
at exactly the same rate.  There is a corresponding strict partial order on functions indicating that 
one function grows at a signiﬁcantly slower rate. Namely, 
→ 
R, we say f is asymptotically smaller than g , in symbols, 
Deﬁnition 7.1.  For functions f , g :  R
f (x) =  o(g(x)), 

iff 

lim  f (x)/g(x) = 0. 
x→∞ 

For  example,  1000x1.9  =  o(x2 ),  because  1000x1.9/x2  =  1000/x0.1  and  since  x0.1  goes  to  inﬁnity 
with x and 1000 is constant, we have limx→∞ 1000x1.9/x2  = 0.  This argument generalizes directly 
to yield 
Lemma 7.2.  xa  =  o(xb ) for all nonnegative constants a < b.


Using the familiar fact that log  x < x for all x > 1, we can prove

Lemma 7.3.  log  x =  o(x� ) for all � > 0  and x > 1.


Proof.  Choose � > δ > 0 and let x =  z δ  in the inequality log  x < x. This implies

log  z  < z  /δ =  o(z  � ) 
δ

by Lemma 7.2. 

(9) 

Corollary 7.4.  x =  o(ax ) for any a, b ∈ R with a > 1. 
b

Proof.  From (9), 

for all z  > 1, δ > 0. Hence 

(e b )log  z  < 

z  b  < 

log  z  < z  δ /δ 
� 
elog  a(b/  log  a)  �zδ /δ 
(e b )zδ /δ 
=  a(b/δ  log  a)z δ 
<  a z 

for  all  z  such  that  (b/δ log  a)z δ  <  z .  But  since  z δ  =  o(z ),  this  last  inequality  holds  for  all  large 
enough z . 

Lemma 7.3 and Corollary 7.4 can also be proved easily in several other ways, e.g., using L’Hopital’s 
Rule or the McLaurin Series for log  x and ex . Proofs can be found in most calculus texts. 
Problem 1.  Prove the initial claim that log   x < x for all x > 1 (requires elementary calculus). 
Problem  2.  Prove  that  the  relation,  R,  on  functions  such  that  f Rg iff  f  =  o(g) is  a  strict  partial 
order, namely, R is transitive and asymmetric:  if f Rg then ¬gRf . 
Problem 3.  Prove that f   ∼ g iff f   =  g +  h for some function h =  o(g). 

Course Notes, Week 8:  Sums, Products & Asymptotics 

17 

7.2  Big Oh 

Big Oh is the most frequently used asymptotic notation.  It is used to give an upper bound on the 
growth of a function, such as the running time of an algorithm. 
→ 
R, with g nonnegative, we say that 
Deﬁnition 7.5.  Given functions f , g :  R

iff 

f  =  O(g) 

lim  sup  f (x) /g(x) <  ∞.
|
|
x→∞ 

This deﬁnition2  makes it clear that 
Lemma 7.6.  If f  =  o(g) or f  ∼ g , then f  =  O(g). 
Proof.  lim  f /g  = 0  or lim  f /g  = 1  implies lim  f /g  <  ∞. 

�∼  x 

It  is easy  to  see  that  the  converse of Lemma 7.6  is not  true.  For example,  2x  =  O(x),  but 2x 
and 2x  =�
o(x). 
The usual formulation of Big Oh spells out the deﬁnition of lim  sup  without mentioning it. Namely, 
here is an equivalent deﬁnition: 
Deﬁnition 7.7.  Given functions f , g :  R

R, we say that 

→ 

f  =  O(g) 
iff there exists a constant c  ≥ 0  and an x0  such that for all x  ≥ x0 ,  f (x) ≤ cg(x).
|
| 

This  deﬁnition  is  rather  complicated,  but  the  idea  is  simple:  f (x) =  O(g(x))  means  f (x) is  less 
than  or  equal  to  g(x),  except  that  we’re  willing  to  ignore  a  constant  factor,  viz.,  c,  and  to  allow 
exceptions for small x, viz., x  <   x0 . 
We observe, 

Lemma 7.8.  Assume that g is nonnegative.  If f  =  o(g), then it is not true that g =  O(f ). 

Proof. 

so g =� O(f ). 

2 

g(x)
lim 
x→∞  f (x) 

=

1 
limx→∞ 

f (x)  
g(x)  

1
= = 
0 

∞, 

lim  sup  h(x) ::=  lim  luby≥xh(y). 
x→∞ 
x→∞ 
We  need  the  lim  sup   in  the  deﬁnition  of  O()  because  if  f (x)/g(x)  oscillates  between,  say,  3  and  5  as  x  grows, 
then  f  =  O(g)  because  f  ≤  5g ,  but  limx→∞  f (x)/g(x)  does  not  exist.  However,  in  this  case  we  would  have 
lim  supx→∞  f (x)/g(x)  = 5. 

18 

Proposition 7.9.  100x2  =  O(x2 ). 

Course Notes, Week 8:  Sums, Products & Asymptotics 
� �

� �

100x

2 

≤

Proof.  Choose  c  =  100   and  x0  = 1.  Then  the  proposition  holds,  since  for  all  x  ≥  1, 
100x .

2 

Proposition 7.10.  x2  +  100x  +  10
 =  O(x2 ). 

Proof.  (x2+100x+10)/x2  = 1+100/x+10/x2  and so its limit as x  approaches inﬁnity is 1+0+0  = 1. 
So in fact, x2  + 100x  + 10   ∼ x2 , and therefore x2  + 100x  + 10  =  O(x2 ).  Indeed, it’s conversely true 
that x2  =  O(x2  +  100x  +  10). 

Proposition 7.10 generalizes to an arbitrary polynomial by a similar proof, which we omit. 
· · ·
Proposition 7.11.  For ak  = 0, ak xk  +  ak−1xk−1  +

+  a1x  +  a0  =  O(xk ).

Big Oh notation is especially useful when describing the running time of an algorithm.  For exam­
ple,  the usual algorithm for multiplying n  × n  matrices requires proportional  to n3  operations  in 
the worst case.  This fact can be expressed concisely by saying that the running time  is O(n3 ).  So 
this  asymptotic  notation  allows  the  speed  of  the  algorithm  to  be  discussed without  reference  to 
constant factors or lower­order terms that might be machine speciﬁc.  In this case there is another, 
ingenious matrix multiplication procedure that requires O(n2.55 ) operations.  This procedure will 
therefore be much more efﬁcient on large enough matrices. Unfortunately, the O(n2.55 )­operation 
multiplication procedure is almost never used because it happens to be less efﬁcient than the usual 
O(n3 ) procedure on matrices of practical size.  It is even conceivable that there is an O(n2 ) matrix 
multiplication procedure, but none is known. 

7.3  Theta 

Deﬁnition 7.12. 

f  =  Θ(g) 

iff  f  =  O(g) ∧ g  =  O(f ). 

The statement f  =  Θ(g) can be paraphrased intuitively as “f  and g  are equal to within a constant 
factor.” 
The value of these notations is that they highlight growth rates and allow suppression of distract­
ing factors and low­order terms.  For example, if the running time of an algorithm is 
T (n) = 10n  − 20n  2  +  1, 
3

then 

T (n) =  Θ(n 3 ). 

In this case, we would say that T  is of order n3  or that T (n) grows cubically. 
Another such example is 

π23x−7  +

(2.7x113  +  x9  − 86)4  − 1.083x  =  Θ(3x ).
√
x 

�
Course Notes, Week 8:  Sums, Products & Asymptotics 

19 

Just knowing  that  the  running  time of an algorithm  is Θ(n3 ),  for example,  is useful, because  if n 
doubles we can predict that the running time will by and large3  increase by a factor of at most 8 for 
large n.  In this way, Theta notation preserves information about the scalability of an algorithm or 
system.  Scalability is, of course, a big issue in the design of algorithms and systems. 

7.4  Pitfalls with Big Oh 

There is a long list of ways to make mistakes with Big Oh notation.  This section presents some of 
the ways that Big Oh notation can lead to ruin and despair. 

7.4.1  The Exponential Fiasco 

Sometimes relationships involving Big Oh are not so obvious.  For example, one might guess that 
4x  =  O(2x ) since  4  is  only  a  constant  factor  larger  than  2.  This  reasoning  is  incorrect,  however; 
actually 4x  grows much faster than 2x  . 
Proposition 7.13.  4x  =� O(2x ) 

Proof.  2x/4x  = 2x/(2x2x )  = 1/2x  .  Hence,  limx→∞ 2x/4x  = 0,  so  in  fact  2x  =  o(4x ).  We observed 
earlier that this implies that 4x  =� O(2x ). 

7.4.2  Constant Confusion 

False Theorem 7.14. 

Every  constant  is  O(1).  For  example,  17  =  O(1).  This  is  true  because  if  we  let  f (x)  = 17  and 
g(x)   = 1,  then  there  exists  a  c  >  0  and  an  x0  such  that  |f (x) ≤  cg(x).  In  particular,  we  could 
|
choose  c  = 17 and x0  = 1,  since  17 ≤  17  · 1  for all x  ≥  1.  We  can  construct a  false  theorem  that 
|
|
exploits this fact. 
� 
n
i  =  O(n) 
i=1 

� 
· · ·
n
False proof.  Deﬁne f (n) = 
i  = 1 + 2 + 3 + + n.  Since we have shown that every constant i
is O(1), f (n) =  O(1)  +   O(1)  +   · · · +  O(1)  =  O(n). 
i=1  
� 
i  =  n(n  +  1)/2 =� O(n).
n
Of course in reality 
i=1  
The  error  stems  from  confusion  over what  is meant  in  the  statement  i  =  O(1).  For  any  constant 
i  ∈  N  it  is true  that i  =  O(1).  More precisely,  if f  is any constant function,  then f  =  O(1).  But  in 
this False Theorem, i  is not constant but ranges over a set of values 0,1,. . . ,n  that depends on n. 
And  anyway,  we  should  not  be  adding  O(1)’s  as  though  they  were  numbers.  We  never  even 
deﬁned what O(g) means by itself; it should only be used in the context “f  =  O(g)” to describe a 
relation between functions f   and g . 

3 Since Θ(n   )  only  implies  that  the  running  time,  T (n),  is  between  cn 3  and  dn3  for  constants  0  <  c  <  d,  the  time 
3
T (2n)  could regularly exceed T (n) by a factor as large as 8d/c.  The factor is sure to be close to 8 for all large n  only if 
T (n)  ∼ n 3  . 

20 

Course Notes, Week 8:  Sums, Products & Asymptotics 

7.4.3  Lower Bound Blunder 

Sometimes  people  incorrectly  use  Big  Oh  in  the  context  of  a  lower  bound.  For  example,  they 
might say, “The running time, T (n),  is at  least O(n2 ),” when  they probably mean something  like 
“O(T (n))   =  n2 ,” or more properly, “n2  =  O(T (n)).” 

7.4.4  Equality Blunder 

The notation  f  =  O(g) is  too ﬁrmly  entrenched  to  avoid,  but  the use of  “=”  is  really  regrettable. 
For example,  if f  =  O(g),  it seems quite reasonable to write O(g) =  f .  But doing so might tempt 
us  to  the  following  blunder:  because  2n   =  O(n),  we  can  say  O(n)  = 2n.  But  n  =  O(n),  so we 
conclude that n  =  O(n) = 2n, and therefore n  = 2n.  To avoid such nonsense, we will never write 
“O(f ) =   g .” 

