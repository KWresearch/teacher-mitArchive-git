Massachusetts Institute of Technology 
6.042J/18.062J, Fall ’05: Mathematics for Computer Science 
Prof. Albert R. Meyer and Prof. Ronitt Rubinfeld 

December 7 
revised December 6, 2005, 1252 minutes 

Solutions to In­Class Problems Week 14, Wed. 


Problem  1.  Suppose  you  have  learned  that  the  average  graduating MIT  student’s  total 
number of credits is 200. 

(a)  Knowing  only  this  average,  use  Markov’s  inequality  to  ﬁnd  a  best  possible  upper 
bound for the fraction of MIT students graduating with at least 235 credits.  1 
Solution.  Let X  be a random variable with a distribution equal to that of the graduating 
MIT students’ credit count. We are given that E [X ] =  200. By Markov’s inequality: 

Pr {X  ≥ 235} ≤ 

E [X ]
235 

= 

200 
235 

≈ 0.85 

� 

(b)  Demonstrate that this is a best possible bound by giving a distribution for which this 
bound holds with equality. 
Solution.  The  bound  is  attained  with  equality  at  the  two­point  distribution  which  has 
non­zero values only at 0 and 235, that is, 
Pr {X  =  235}  = 
200 
235 
Pr {X  =  0}  = 
35 
235 
Pr {X  =  x}  =  0  for all other x. 

� 

(c)  Suppose  you  are now  told  that  no  student  can  graduate with  fewer  than  170 units. 
How does  this allow you  to  improve your previous bound?  As before,  show  that  this  is 
the best possible bound. 

Copyright © 2005, Prof. Albert R. Meyer and Prof. Ronitt Rubinfeld. 
1 Ignore the fact that there are practical limits to the amount of time a student can stay at MIT and remain 
sane; That is, assume that there is no bound on the number of credits a student may earn. 

Solutions to In­Class Problems Week 14, Wed. 
2 
Solution.  We can now apply Markov’s  inequality  to  the nonnegative variable Y  =  X  −
170, with expectation E [Y   ] =  E [X  − 170]  =  E [X ] − 170  = 30.  So, 
Pr {X  ≥ 235} =  Pr {X  − 170  ≥ 235  − 170} =  Pr {Y  ≥ 65} 

Therefore: 

Pr {X  ≥ 235}  =  Pr {Y  ≥ 65}
E [Y  ]
≤ 
65 
≤ 
≈ 0.46
30 
65 

As above, we achieve an optimum (equality in the bound) when our distribution consists 
of two spikes:  one at (x  − 170)  =  c  − 170, that is, x  =  235, and one at (x   − 170)   = 0, that is, 
x  =  170. 

Pr {X  =  235}  = 
(200  − 170)/(235  − 170)   = 30/65 
Pr {X  =  170}  =  35/65 
Pr {X  =  x}  = 0  for all other x 

� 

(d)  Now  suppose  you  further  learn  that  the  standard  deviation  of  the  total  credits  per 
graduating  student  is  7.  What  is  the Chebyshev  bound  on  the  fraction  of  students who 
can graduate with at least 235 credits? 
Solution.  The variance of X  is the square of the standard deviation, or 49. The variance of 
Y  is the same as that of X , by the linearity of variance.  That is, Var [Y  ] =  Var [X  − 170]  =  
Var [X ] − Var [170]  = 49  − 0.  (The variance of a constant is 0). 
Pr {X  ≥ 235}	 =  Pr {Y  ≥ 65} 
=  Pr {Y  − E [Y  ]  ≥ 65  − E [Y  ]} 
=	 Pr {Y  − 30  ≥ 35}
Pr {|Y  − 30 ≥ 35}
≤	
|
Var [Y  ]
≤ 
352 
≤ 
49 
1225 

1 
25 

=

This is a much better bound than before!	

� 

Solutions to In­Class Problems Week 14, Wed. 

3 

Problem 2.  (a)  Show that Markov’s Theorem only applies to nonnegative random vari­
ables.  That is, give an example of a random variable to which Markov’s Theorem gives a 
wrong answer. 

Solution.  Here  is  one  possible  answer:  Let  R  be  ­10  with  probability  1/2  and  10  with 
probability 1/2. Then we have: 
·
E [R] =  −10  ·  +  10 
1
1
= 0 
2 
2 
Suppose that we now tried to compute Pr {R  ≥ 5} using Markov’s Theorem: 
E [R]
5

Pr {R  ≥ 5} ≤ 

0 
=  = 0.  
5 

This is the wrong answer! Obviously, R  is at least 5 with probability 1/2. 

� 

(b)  Suppose  R  is  a  random  variable  that  is  always  at  least  −10  and  has  expectation  0. 
Since R  may be negative, Markov’s  theorem does not apply directly.  Still, use Markov’s 
theorem to show that the probability that R  is ≥ 5  is at most 2/3. 
Solution.  Let  T  ::=  R  +  10.  Now  T  is  a  nonnegative  random  variable with  expectation 
E [R  +  10]  =  E [R] + 10  =  10, so Markov’s Theorem applies and tells us that Pr {T  ≥ 15} ≤ 
10/15  = 2/3. But T  ≥ 15   iff R  ≥ 5, so Pr {R  ≥ 5} ≤ 2/3. 
� 

Problem  3.  There  are  n  people  at  a  circular  table  in  a Chinese  restaurant.  On  the  table, 
there are n  different appetizers arranged on a big Lazy Susan. Each person starts munch­
ing on  the appetizer directly  in  front of him or her.  Then someone spins  the Lazy Susan 
so  that  everyone  is  faced  with  a  random  appetizer.  In  class,  we  saw  that  the  expected 
� 
number of people that end up with the appetizer that they had originally is 1. 
Let Xi  be  the  indicator variable  for  the  ith person getting  their own  appetizer back.  Let 
n
Xi .
Sn  be the total number of people who get their own appetizer back, so Sn  = 
i=1 

(a)  What is E [X 2 ]?i 

Solution.  Xi  = 1  with probability 1/n   and 0 otherwise. Thus X 2  = 1  with probability 1/n
i 
� 
and 0 otherwise.  So E [Xi 
2 ] = 1/n. 

(b)  For i  =�

j , what is E [XiXj ]? 

Solutions to In­Class Problems Week 14, Wed. 

4 

Solution.  The probability that Xi  and Xj  are both 1 is 1/n. Thus XiXj  =  1 with probabil­
� 
ity 1/n, and is zero otherwise.  So E [XiXj ] =  1/n. 

(c)  What is E [S 2 ]?n
Solution. 

�  �  � 
E  S 2  = 
E [XiXj ]
n 
i,j 
· 
12 
n 

=  n 
=  n. 
Pr � 
n  =  n 2 � 
Alternatively, we observe directly that 
Pr {Sn  =  n} = 
S 2 
n  =  0 � 
Pr � 
Pr {Sn  =  0} = 
S 2 
E � 
� 
n  − 1 
=  n 2  1 
S 2 
n 
n 
n 

+ 0 · 

and 

so 

(d)  What is Var [Sn ]? 
Solution. 

�  � 
Var [Sn ] =  E  S 2  − E2  [Sn ]
n 
=  n  − 12 
=  n  − 1. 

1 
n 
n  − 1 
n 

=  n. 

, 

� 

� 

(e)  Discuss  the accuracy of  the Chebyshev Bound on  the probability  that Sn  is distance 
x  from its expectation as x  ranges over integers between 1 and n. 
� 
x  ≥ �√
� 
trivial bound of (n  − 1)/  �√
Solution.  The  bound  Var [Sn ] /x2  is  trivial  (>  1)  unless  x2  >  varianceSn ,  that  is,  unless 
n  − 1 + 1   .  In the case that x  equals this minimum value, it still gives yields a near 
n  − 1 + 1  ≈ 1, whereas actually, 
Pr {|Sn  − 1| ≥ x} = 
1
n 

Solutions to In­Class Problems Week 14, Wed. 
for all x  ≤ n  − 1, and 

Pr {|Sn  − 1|
≥ x} = 0 
for x  > n  − 1.  At x  = n  − 1, the Chebyshev Bound is (n   − 1)/(n  − 1)2  = 1/(n  − 1) which 
is still a bit larger than the actual value of 1/n.  Finally, at x  = n, the Chebyshev Bound is 
(n  − 1)/n2  = 1/n  − 1/n2  whereas the actual probability is zero. 
� 

5 

Problem 4.  For any random variable, R, with E [R] =  µ  and Var [R] =  v ,  the Chebyshev 
Bound says that for any real number x  >  0, 
Pr {|R − µ| ≥ x} ≤ 

v 
. 2 
x
Show that for any real number, µ, and real numbers v , x  >  0,  there is an R  for which the 
Chebyshev Bound is tight, that is, 

Pr {|R| ≥ x} = 
v 
. 
2 
x
Hint: Assume µ  = 0 and let R  be three valued with values 0, −x,  and x. 
Solution.  From the hint, we aim to ﬁnd an R  with E [R] = 0 and Var [R] = v  that satisﬁes

equation (1).

Using the further hint that R  takes only values 0, −x,  x, we have

= x} − Pr {R 
= x} − x Pr {R 
= −x} = x (Pr {R 
0 = E [R] = x Pr {R 

= −x})

(1)

so 

Pr {R  = x} = Pr {R  = −x} , 

� 
� 
since x  >  0. Also, 
v  = Var [R] = E  R2  = x 2  Pr {R  = −x} + x 2  Pr {R  = x} = 2x 2  Pr {R  = x} , 

(2) 

so 

Pr {R  = x} = 

v
2x2 

. 

This implies 

−x} − Pr {R 
= x} = 1 − 
Pr {R  = 0} = 1 − Pr {R  =
which completely determines the distribution of R. Moreover, 
Pr {|R| 
≥ x} = Pr {R  = −x} + Pr {R  = x} = 

v
x2  ,

v 
2 
x

which conﬁrms (1). 
Finally, given µ,  x,  and v , if we let R� ::= R + µ, then R�  will be the desired random variable 
� 
for which the Chebyshev Bound is tight. 

Solutions to In­Class Problems Week 14, Wed. 

6 

Problem 5.  The covariance, Cov [X, Y ], of two random variables, X and Y ,  is deﬁned to 
be  E [X Y ] − E [X ] E [Y ].  Note  that  if  two  random  variables  are  independent,  then  their 
covariance is zero. 

(a)  Give an example to show that having Cov [X, Y ] = 0 does not necessarily mean that 
X and Y  are independent. 

Solution.  Let (X, Y ) have joint probability given by the table below: 

X  Y 
−1 
1 
0 
0 
1 
1 

P 
1/3 
1/3 
1/3 

Note that X and Y  are not independent: 
Pr {X  = 1 & Y   = 1} = 1/3 = 2/9 = Pr {X = 1} Pr {Y  = 1} . 

But since X Y   = X and E [X ] = 0, we have 
E [X ] E [Y ] = 0 · E [Y ] = 0 = E [X ] = E [X Y ] . 

Thus Cov [X, Y ] = 0. 

Var [

+ Xn ] = 

Solution. 
X1  + · · ·

� 
� 
(b)  Let X1 , . . . , Xn  be random variables. Prove that 
n
X1  + · · ·
Var [Xi ] + 2 
Cov [Xi , Xj ] .
i=1 
i<j 
� 
� 
X1� 
�(X1  + · · ·
� 
� 
� 
�  � 
+ · · ·
+ Xn )2  − E2  [
+ Xn ] =  E 
2XiXj )  − 
� 
� � 
�
� �
E [Xi ]2  +
2  + (  
=  E 
2 E [Xi ] E [Xj ] 
Xi 
i<j
i 
i<j 
i 
E [Xi ]2  − 
2 E [XiXj ] − 
� 
�  � 
� 
2  +
E  Xi 
2 E [Xi ] E [Xj ] 
i<j 
i 
i 
i<j 
E  X 2  − E [Xi ]2  + 
2(E [XiXj ] − E [Xi ] E [Xj ])
� 
� 
i 
i<j 
i 
Cov [Xi , Xj ] . 
Var [Xi ] + 2 
i<j 
i 

+ Xn ]

Var [

= 

= 

= 

� 

� 

� 

�
