MIT OpenCourseWare 
http://ocw.mit.edu 

18.03 Differentia l Equations, Spring 2006 

Please use the following citation format: 

Arthur Mattuck and Haynes Miller, 18.03 Differentia l Equations, Spring 
2006. (Massachusetts Institute of Technology: MIT OpenCourseWare). 
http://ocw.mit.edu (accessed MM DD, YYYY). License: Creative 
Commons Attribution-Noncommercia l-Share Alike. 

Note: P lease use the actua l date you accessed this materia l in your citation. 

For more information about citing these materia ls or our Terms of Use, visit: 
http://ocw.mit.edu/terms 

MIT OpenCourseWare 
http://ocw.mit.edu 

18.03 Differentia l Equations, Spring 2006 
Transcript – Lecture 29 

We are going to need a few facts about fundamental matrices, and I am worried that 
over the weekend this spring activities weekend you might have forgotten them. So I 
will just spend two or three minutes reviewing the most essential things that we are 
going to need later in the period. What we are talking about is, I w ill try to color code 
things so you will know what they are. 

First of all, the basic problem is to solve a system of equations. And I am going to 
make that a two-by-two system, although practically everything I say today will also 
work for end-by-end systems. Your book tries to do it end-by-end, as usua l, but I 
think it is easier to learn two-by-two first and generalize rather than to wade through 
the complications of end-by-end systems. So the prob lem is to solve it. 

And the method I used last time was to describe something called a fundamenta l 
matrix. 

A fundamental matrix for the system or for A, whichever you want, remember what 
that was. That was a two-by-two matrix of functions of t and whose columns were 
two independent solutions, x1, x2. 

These were two independent solutions. In other words, neither was a constant 
multip le of the other. Now, I spent a fair amount of time showing you the two 
essentia l properties that a fundamental matrix had. We are going to need those 
today, so let me remind you the basic properties of X and the properties by which 
you could recognize one  if you were given one. 

First of all, the easy one, its determinant sha ll not be zero, is not zero for any t, for 
any va lue of the variab le. That simp ly expresses the fact that its two columns are 
independent, linearly independent, not a multip le of each other. The other one was 
more bizarre, so I tried to ca ll a little more attention to it. 

It was that the matrix satisfies a differentia l equation of its own, which  looks a lmost 
the same, except  it's a matrix differential equation. It is not our column vectors 
which are solutions but matrices as a whole which are solutions. In other words, if 
you take that matrix and differentiate every entry, what you get is the same as A 
multip lied by that matrix you started with. 

This, remember, expressed the fact, it was just really formal when you analyzed 
what it was, but it expressed the fact that it says that the columns solved the 
system. The first thing says the columns are independent and the second says each 
column separately is a solution to the system. That is as far, more or less. 

Then I went in another direction and we ta lked about variation of parameters. I am 
not going to come back to variation of parameters today. We are going in a different 
tack. And the tack we are going on is I want to first ta lk a little more about the 
fundamenta l matrix and then, as I said, we will talk about an entirely d ifferent 

method of solving the system, one which makes no mention of eigenva lues or 
eigenvectors, if you can believe that. 

But,  first, the one confusing thing about the fundamenta l matrix is that it is not 
unique. I have carefully tried to avoid ta lking about the fundamenta l matrix because 
there is no "the" fundamental matrix, there is only "a"  fundamenta l matrix. Why is 
that? Well, because these two columns can be any two  independent solutions. And 
there are an  infinity of ways of picking independent solutions. That means there is an 
infinity of possible fundamenta l matrices. 

Well, that is d isgusting, but can we repair it a little b it? I mean maybe they are all 
derivable from each other in some simp le way. And that is, of course, what is true. 
Now, as a prelude to doing that, I would like to show you what I wanted to show you 
on Friday but, aga in, I ran out of time, how to write the genera l solution --

-- to the system. The system I am ta lking about is that p ink system. Well, of course, 
the standard na ïve way of doing it is it's x equals, the general solution is an arb itrary 
constant times that first solution you found, p lus c2, times another arbitrary 
constant, times the second solution you found. Okay. Now, how would you 
abbreviate that using the fundamental matrix? 

Well, I did something very similar to this on Friday, except these were ca lled Vs. It 
was part of the variation parameters method, but I promised not to use those words 
today so I just said nothing. Okay. What is the answer? It is x equals, how do I write 
this using the fundamental matrix, x1, x2? Simp le. It is cap ital X times the column 
vector whose entries are c1 and c2. 

In other words, it is x1, x2 times the column vector c1, c2, isn't it? Yeah. Because if 
you multip ly this think top row, top  row, top  row c1, plus top  row times c2, that 
exactly g ives you the top  row here. And the same way the bottom row here, times 
this vector, gives you the bottom row of that. 

It is just another way of writing that, but it looks very efficient. Sometimes efficiency 
isn't a good thing, you have to watch out for it, but here it is good. So, this is the 
general solution written out using a fundamental matrix. And you cannot use less 
symbols than that. There is just no way. 

But that gives us our answer to, what do a ll fundamenta l matrices look like? 

Well, they are two columns are solutions. The answer is they  look like --

Now, the first column is an arb itrary solution. How do I write an arbitrary solution? 
There  is the general solution. I make it a particular one by g iving a particular value 
to that column vector of arb itrary constants like two, three or minus one, p i or 
something like that. The first guy is a solution, and I have just shown you I can write 
such a solution like X, c1 with a column vector, a particular column vector of 
numbers. 

This is a solution because the green thing says it is. And side by side, we will write 
another one. And now all I have to do is, of course, there is supposed to be a 
dependent. We will worry about that in just a moment. All I have to do  is make this 
look better. Now, I told you last time, by the laws of matrix multiplication, if the first 

column is X c1 and the second column  is X c2, using matrix multiplication that is the 
same as writing it this way. 

This square matrix times the matrix whose entries are the first column vector and 
the second column vector. Now, I am going to call this C. It is a square matrix of 
constants. It is a two-by-two matrix of constants. And so, the fina l way of writing it 
is just what corresponds to  that, X times C. 

And so X is a given fundamenta l matrix, this one, that one, so the most general 
fundamenta l matrix is then the one you started with, and multip ly it by an arbitrary 
square matrix of constants, except you want to be sure that the determinant is not 
zero. 

Well, the determinant of this guy won't be zero, so all you have to do is make sure 
that the determinant of C isn't zero either. In other words, the fundamental matrix is 
not unique, but once you  found one a ll the other ones are found by multip lying it on 
the right by an arbitrary square matrix of constants, which is nonsingular, it has 
determinant nonzero in other words. Well, that was a ll Friday. 

That's Friday leaking over into Monday. And now we begin the true Monday. 

Here is the prob lem. Once aga in we have our two-by-two system, or end-by-end if 
you want to be super genera l. There is a system. What do we have so  far by way of 
solving it? 

Well, if your kid brother or sister when you go home said, a precocious kid, okay, tell 
me how to solve this thing, I think the only thing you will be able to say is well, you 
do this, you take the matrix and then you ca lculate something ca lled eigenva lues and 
eigenvectors. Do you know what those are? I didn't think you d id, blah, blah, blah, 
show how smart I am. 

And you then exp lain what the eigenva lues and eigenvectors are. And then you show 
how out of those make up specia l solutions. And then you take a combination of 
that. In other words, it is algorithm. It is something you do, a process, a method. 
And when it is all done, you have the genera l solution. Now, that is fine for 
calculating particular problems w ith a definite model w ith definite numbers in it 
where you want a definite answer. 

And, of course, a lot of your work in eng ineering and science classes is that kind of 
work. But the further you get on, well, when you start reading books, for example, 
or god forb id start reading papers  in which peop le are telling you, you know, they 
are doing engineering or they are doing science, they don't want a method, what 
they want is a  formula. In other words, the prob lem is to fill  in the blank in the 
follow ing. 

You are writing a paper, and you just set up some elaborate model and A is a matrix 
derived  from that model in some way, represents bacteria doing something or bank 
accounts doing something, I don't know. And you say, as is well-known, the solution 
is, of course, you only have letters here, no numbers. This is a general paper. The 
solution is given by the formula. 

The only trouble is, we don't have a  formula. All we have is a method. Now, people 
don't like that. What I am going to produce for you this period is a formula, and that 

formula does not require the ca lculation of any eigenvalues, eigenvectors, doesn't 
require any of that. It is, therefore, a very popular way to fill in to finish that 
sentence. 

Now the question is where is that formula going  to come from? Well, we are, for the 
moment, clueless. If you are clueless the p lace to look a lways is do I know anything 
about this sort of thing? I mean  is there some special case of this problem I can 
solve or that I have solved in the past? And the answer to that is yes. 

You haven't solved it for a two-by-two matrix but you have solved it for a one-by-
one matrix. A one-by-one matrix a lso goes by the name of a constant. It  is just a 
thing. It's a number. Just putting brackets around it doesn't conceal the fact that it is 
just a number. Let's look at what the solution is for a one-by-one matrix, a one-by-
one case. If we are looking  for a general solution for the end-by-end case, it must 
work for the one-by-one case also. 

That  is a good reason for us starting. That looks like x, doesn't it? A one-by-one 
case. 

Well, in that case, I am trying to solve the system. The system consists of a single 
equation. That is the way the system looks. 

How do you solve that? Well, you were born knowing how to solve that. Anyway, you 
certainly d idn't learn it in this course. You separate variables, blah, blah, blah, and 
the solution is x equa ls, the basic solution is e^(at), and you multip ly that by an 
arbitrary constant. 

Now, that is a formula for the solution. And it uses the parameter in the equation. I 
didn't have to know a specia l number. I didn't have to put a particular number here 
to use that. Well, the answer is that the same idea, whatever the answer I give here 
has got to work in this case, too. But let's take a quick look as to why this works. 

Of course, you separate variables and use ca lculus. I am going to g ive you a slightly 
different argument that has the advantage of generalizing to the end-by-end case. 
And the argument goes as follows for that. It uses the definition of the exponentia l 
function not as the inverse to  the logarithm, which is where the fancy calculus books 
get it from, nor as the na ïve high school method, e^2 means you multiply e by itself 
and e cubed means you do it three times and so on. 

And e to the one-ha lf means you do it a ha lf a time or something. So, the naïve 
definition of the exponential function. Instead, I will use the definition of the 
exponentia l function that comes from an infinite series. Leaving out the arbitrary 
constant that we don't have to bother w ith. e^(at) = 1 + at + a^2 t / 2! + ... 

I w ill put out one more term and let's call  it quits there. If I take this then argument 
goes let's just differentiate it. In other words, what is the d(e^(at))/dt? Well, just 
differentiating term by term it is zero p lus the first term is a, the next term is a^2 t. 

This differentiates to t^2 / 2!. And the answer is that this  is equal to a*(1 + at + 
a^2 t^2 / 2!). 

In other words, it is simp ly e^(at). In other words, by differentiating the series, 
using the series definition of the exponentia l and by differentiating it term by term, I 

can immediately see that is satisfies this d ifferentia l equation. What about the 
arbitrary constant? Well, if you would like, you can include it here, but it is easier to 
observe that by linearity if e to the a t solves the equation so does the constant 
times it because the equation is linear. 

Now, that is the idea  that I am going to use to solve the system in general. What are 
we doing to say? Well, what could we say? The solution to, well, let's get two 
solutions at once by writing a fundamental matrix. 

"A" fundamental matrix, I don't claim it is "the" one, for the system x' = Ax. That is 
what we are trying to solve. And we are going to get two solutions by getting a 
fundamenta l matrix for it. The answer is e^(At). 

Isn't that what it should be? I had a little a. Now we have a matrix. Okay, just put 
the matrix up there. Now, what on earth? The first person who must have thought of 
this, it happened about 100 years ago, what meaning should be given to e to a 
matrix power? Well, clearly the two naïve definitions won't work. 

The only possible meaning you could try for is using  the infinite series, but that does 
work. So this is a definition I am g iving you, the exponential matrix. Now, notice the 
A is a two-by-two matrix multiplying it by t. What I have up here is that it's basically 
a two-by-two matrix. Its entries involve t, but  it's a two-by-two matrix. Okay. We 
are trying to get the ana log of that formula over there. 

Well, leave the first term out just for a moment. The next term is going to surely be 
A times t. This is a  two-by-two matrix, right? What should the next term be? Well, A 
squared times t squared over two factorial. What kind of a guy is that? Well, if A is a 
two-by-two matrix so is A squared. How about this? This is just a scalar which 
multip lies every entry of A squared. 

And, therefore,  this is still a two-by-two matrix. That  is a two-by-two matrix. This is 
a two-by-two matrix. No matter how many times you multip ly A by itself it stays a 
two-by-two matrix. It gets more and more complicated looking but it is a lways a 
two-by-two matrix. And now  I am multiplying every entry of that by the scalar t^3 / 
3!. I am continuing on in that way. What I get, therefore,  is a sum of two-by-two 
matrices. 

Well, you can add two-by-two matrices to each other. We've never made an infinite 
series of them, we haven't done it, but others have. And this is what they wrote. The 
only question is, what should we put in the beginning? Over there I have the number 
one. But I, of course, cannot add the number one to a two-by-two matrices. What 
goes here must be a two-by-two matrix, which is the closest thing to one I can think 
of. What should it be? 

The I. Two-by-two I. Two-by-two identity matrix looks like the natural candidate for 
what to put there. And, in fact, it is the right thing to put there. Okay. Now I have a 
conjecture, you know, purely forma lly, chang ing only w ith a keystroke of the 
computer, all the little a's have been changed to cap ita l A's. And now a ll I have to do 
is wonder if this is going to work. 

Well, what is the basic thing I have to check to see that  it is the fundamenta l matrix? 
The question is, I wrote it down a ll right, but  is this a fundamenta l matrix for the 
system? Well, I have a way of recognizing a fundamenta l matrix when I see one. The 

critica l thing is that it should satisfy this matrix differentia l equation. That is what I 
should verify. 

Does this guy that I have written down satisfy this equation? And the answer is, 
number two is, it satisfies x' = Ax. In other words, p lugg ing in x = e^(At), whose 
definition I just gave you. If I substitute that in, does it satisfy that matrix 
differentia l equation? The answer is yes. 

I am not going to calculate it out because the ca lculation  is actua lly identical to what 
I d id there. The only difference is when I differentiated it term by term, how do you 
differentiate something like this? Well, you differentiate every term in it. But, if you 
work it out, this is a constant matrix, every term of which is multip lied by t^2 / 2!. 
Well, if you d ifferentiate every entry of that constant, of that matrix, what you are 
going to get is A squared times just the derivative of that part, which  is simply t. 

In other words, the formal calculation looks absolutely identica l to that. So the 
answer to this is yes, by the same calculation as before, as for the one-by-one case. 
And now the only other thing to check is that the determinant is not zero. In fact, 
the determinant is not zero at one point. 

That  is all you have to check. What is x(0)? What is the value of the determinant of x 
is e^(At)? What is the va lue of this thing at zero? Here is my function. If I plug  in t 
equa ls zero, what is it equa l to? I. What is the determinant of I? One. It is certainly 
not zero. 

By writing down this infinite series, I have my two solutions. Its columns g ive me 
two solutions to the orig inal system. There were no eigenvalues, no eigenvectors. I 
have a formula  for the answer. What is the formula? It is e^(At). And, of course, 
anybody read ing the paper is supposed  to know what e to the At  is. It means that. 

This is just marvelous. There must be a fly in the ointment somewhere. Only a teeny 
little fly. There is a teeny little fly because it is a lmost impossible to calculate that 
series for a ll reasonable times. However, once in a while  it is. Let me give you an 
example where it is possible to calculate the series and were you get a nice answer. 
Let's work out an examp le. 

By the way, you know, nowadays, we are not back 50 years, the exponential matrix 
has the same status on, say, a Matlab or Maple or Mathematica, as the ordinary 
exponentia l function does. It is just a command you type in. You type in your matrix. 

And you now say EXP of that matrix and out comes the answer to as many decimal 
places as you want. It will be square matrix w ith entries carefully written out. So, in 
that sense, the fact that we cannot calculate it shouldn't bother us. There are 
machines to do the calculations. What we are interested in is it as a  theoretica l tool. 
But,  in order to get any feeling for this at all, we certa inly have to do a few 
calculations. 

Let's do an easy one. Let's consider the system x' = y, y' = x. This is very easily 
done by elimination, but that is forb idden. First of all, we write it as a matrix. It's the 
system A = (0, 1; 1, 0). Here is my A. And so e^(At) is going to be --

A = (0, 1; 1, 0). What we want to ca lculate is we are going to get both solutions at 
once by calculating it one fell swoop e^(At). Okay. E to the At equals. I am going to 

actually write out these guys. Well, obviously the hard part, the part which is 
normally going to prevent us from calculating this series exp licitly, by hand anyway, 
because, as I sa id, the computer can a lways do  it. 

The va lue, how do we ra ise a matrix to a high power? You just keep multiplying and 
multip lying and multiplying. That looks like a  rather forb idding and unpromising 
activity. Well, here it  is easy. Let's see what happens. If that is A, what is A squared? 
I am going to have to calculate that as part of the series. That is going to be (0, 1; 
1, 0)(0, 1; 1, 0) = (1, 0; 0, 1). 

We got saved. It is the identity. Now, from this point on we don't have to do 
anymore calculations, but I w ill do them anyway. What is A^3? Don't start from 
scratch aga in. No, no, no. A^3 = A^2 A. 

And A squared is, in real life, the identity. Of course, you would do a ll this in your 
head, but I am being a good boy and writing it a ll out. This is I, the identity, times A, 
which is A. I will do one more. What is A to the fourth? Now, you would be tempted 
to say A to the fourth is A squared, which is I times I, which is I, but that would be 
wrong. 

A^4 = A^3 A, which is, I have just calculated is A times A, right? And now that is A 
squared, which is the identity. It is clear, by this argument, it is going to continue in 
the same way each time you add an A on the right-hand side, you are going to keep 
alternating between the identity, A, the next one will be identity, the next w ill be A. 

The end result is that the first term of the series is simply the identity; the next term 
of the series is A, but it is multiplied by t. I will keep the t on the outside. 
Remember, when you multip ly a matrix by a sca lar, that means multip ly every entry 
by that scalar. This is the matrix (0,  t; t, 0). 

I w ill do a coup le more terms. The next term would be, well, A squared we just 
calculated as the  identity. That looks like this. Except now I multiply every term by 
t^2 / 2!. All right. I'll go for broke. The next one will be this times t^3 / 3!. And, 
fortunately, I have run out of room. 

Okay, let's calculate then. 

What is the fina l answer for e^(At)? I have an infinite series of two-by-two matrices. 
Let's look at the term in the upper left-hand corner. It is 1 + 0t + 1t^2 / 2! + 0t. It 
is going to be, in other words, 1 + t^2 / 2! p lus the next term, which  is not on the 
board but I think you can see, is this. And it continues on in the same way. 

How about the lower left term? Well, that is 0 + t + 0 + t^3 / 3! and so on. It  is t + 
t^3 / 3! + t^5 / 5!. And the other terms in the other two corners are just the same 
as these. This one, for example, is 0 + t + 0 + t^3 / 3!. And the lower one is 1 + 0 
+ t^2 and so on. 

This is the same as t^2 / 2! and so on, and up here we have t + t^3 / 3! and so on. 
Well, that matrix doesn't look very square, but it is. It is infinitely long physically, but 
it has one term here, one term here, one term here and one term there. 

Now, a ll we have to do is make those terms look a little better. For here I have to 
rely on the culture, which you may or may not posses. You would know what these 

series were if only they a lternated their signs. If this were a negative, negative, 
negative then the top would be cos(t) and this would be sin(t), but they don't. 

So they are the next best thing. They are what? Hyperbolic. The topic is not cosine t, 
but cosh(t). The bottle is sinh(t). And how do we know this? Because you remember. 
And what if I don't remember? Well, you know now. That is why you come to class. 

Well, for those of you who don't, remember, this is e^t + e^-t. It should be over 
two, but I don't have room to put in the two. This doesn't mean I will omit it. It just 
means I w ill put it in at the end by multip lying every entry of this matrix by one-half. 

If you have forgotten what cosh(t) = (e^t + e^(-t)) / 2. And the similar thing for 
sinh t. There is your first exp licit exponential matrix calculated according to the 
definition. And what we have found is the solution to the system x' = y, y' = x. 

A fundamental matrix. In other words, cosh t and sinh t satisfy both solutions to that 
system. Now, there is one thing people love the exponentia l matrix in particular for, 
and that is the ease with which it solves the initial value problem. It is exactly what 
happens when studying the sing le system, the single equation x' = Ax, but let's do it 
in genera l. 

Let's do it in general. What is the initial va lue problem? Well, the initia l va lue 
problem is we start with our old system, but now I want to plug in initial cond itions. I 
want the particular solution which satisfies the initial condition. Let's make it zero to 
avoid complications, to avoid a lot of notation. This  is to be some starting va lue. This 
is a certa in constant vector. 

It is to be the value of the solution at zero. And the prob lem is find what x(t) is. 
Well, if you are using the exponentia l matrix it is a joke. It is a joke. Shall I derive it 
or just do  it? All right. The genera l solution, let's derive it, and then I w ill put up the 
fina l formula in a box so that you w ill know it is important. 

What is the genera l solution? Well, I did that for you at the beginning of the period. 
Once you have a fundamental matrix, you get the general solution by multiplying it 
on the right by an arb itrary constant vector. The genera l solution is going to be x = 
e^(At). That is my super fundamental matrix, found w ithout eigenva lues and 
eigenvectors. 

And this should be multip lied by some unknown constant vector c. The only question 
is, what should the constant vector c be? To find c, I w ill p lug in zero. When t is zero, 
here I get x of zero, here I get e^(A0) c. Now what is this? This is the vector of 
initia l conditions? What  is e to the A times zero? Plug in t equals zero. What do you 
get? I. 

Therefore, c  is what? c = (x)0. It is a tota l joke. And the solution is, the initial value 
problem is x = e^(At) (x)0. 

It is just what it would have been at one variable. The only d ifference  is that here we 
are a llowed to put the c out front. In other words, if I asked you to put in the initial 
cond ition, you would probably write x = x0 e^(At). And you would be tempted to do 
the same thing here, vector x equa ls vector x zero times e to the At. Now, you 
cannot do that. 

And, if you try to Matlab will hiccup and say illegal operation. What is the illegal 
operation? Well, x is a column vector. From  the system it is a column vector. That 
means the initia l conditions are a lso a column vector. You cannot multiply a column 
vector out front and a square matrix afterwards. You cannot. If you want to multip ly 
a matrix by a column vector, it has to come afterwards so you can do z ing, zing. 

There  is no zing, you see. You cannot put it in front. It doesn't work. So it must go 
behind. That is the only p lace you might get tripped up. And, as I say, if you try to 
type that in using Matlab, you will immed iately get error messages that it is illegal, 
you cannot do that. Anyway, we have our solution. There is our system. 

Our initial va lue problem anyway is in pink, and  its solution using the exponential 
matrix is in green. Now, the only prob lem is we still have to ta lk a little b it more 
about ca lculating this. Now, the princip le warning with an exponential matrix is that 
once you have gotten by the simp lest things involving the fact that it solves systems, 
it g ives you the fundamenta l matrix for a system, then you start flexing your 
muscles and say, oh, well,  let's see what else we can do w ith this. 

For example, the reason exponentials came  into being in the first p lace was because 
of the exponential law, right? I will kill anybody who sends me emails saying, what is 
the exponentia l law? The exponential law would say that e^(A + B) = e^A * e^B. 
The law of exponents, in other words. 

It is the thing that makes the exponentia l function d ifferent from all other functions 
that it satisfies something like that. Now, first of all, does this make sense? That is 
are the symbols compatible? Let's see. This is a  two-by-two matrix, this is a two-by-
two matrix, so it does make sense to multiply them, and the answer will be a two-
by-two matrix. How about here? This is a two-by-two matrix, add this to  it. It is still 
a two-by-two matrix. e to a two-by-two matrix still comes out to be a two-by-two 
matrix. 

Both sides are legitimate two-by-two matrices. The only question is, are they equa l? 
And the answer is not in a pig's eye. How could this be? Well, I didn't make up these 
laws. I just obey them. I w ish I had time to do a little calculation to show that it is 
not true. 

It is true in certain special cases. It  is true in the specia l case, and this is pretty 
much if and only if, the only case  in which  it is true is if A and B are not arb itrary 
square matrices but commute w ith each other. You see, if you start writing out the 
series to try to check whether that  law is true, you w ill get a bunch of terms here, a 
bunch of terms here. 

And you w ill find that those terms are pair-wise equa l only if you are allowed to let 
the matrices commute with each other. In other words, if you can turn AB + BA = 
2AB then everything will work fine. But if you cannot do that it w ill not. Now, when 
do two square matrices commute with each other? The answer is almost never. 

It is just a lucky accident if they do, but there are three cases of the lucky accident 
which you should know. The three cases, I feel justified ca lling it "the" three cases. 
Oh, well, maybe I shouldn't do that. The three most significant examples are, 
example number one, when A is a constant times the identity matrix. 

In other words, when A is a matrix that looks like this. That matrix commutes with 
every other square matrix. If that is A, then this law is always true and you are 
allowed to use this. Okay, so that is one case. Another case, when A  is more general, 
is when B = -A. 

I think you can see that that is going to work because A*-A = -A*A. Yeah, they are 
both equal to A^2, except w ith a negative sign in front. And the third case is when B 
is equal to the inverse of A because A A^(-1) = A^(-1) A. They are both the identity. 
Of course, A must have an inverse. 

Okay, let's suppose it does. Now, of them this is, I think, the most important one 
because it leads to this law. That is forbidden, but there is one case of it which is not 
forbidden and that is here. What will it say? Well, that will say that e^(A - A) = 
e^(A) e^(-A). 

This is true, even though the genera l law is fa lse. That is because A and negative A 
commute w ith each other. But now what does this say? What is e to  the zero matrix? 
In other words, suppose I take the matrix that is zero and plug it into the formula for 
e? What do you get? e to the zero times t is I. It has to be a two-by-two matrix if it 
is going to be anything. 

It is the matrix I. This side is I. This side is the exponentia l matrix. And what does 
that show? It shows that the inverse matrix, the e to the A, is e to the negative A. 
That  is a very useful fact. This is the main survivor of the exponentia l law. 

In genera l it is fa lse, but this standard corollary to the exponential law is true, is 
equa l to e^(-A), just what you would dream and hope would be true. Okay. I have 
exactly two and a half minutes left in which to do the impossib le. All right. The 
question is, how do you calculate e^(At)? 

You could use series, but it rarely works. It is too hard. There are a  few examples, 
and you will have some more for homework, but in general it is too hard because it 
is too hard to ca lculate the powers of a general matrix A. There is another method, 
which is useful only for matrices which are symmetric, but like that --

Well, it is more than symmetric. These two have to be the same. But you can handle 
those, as you will see from the homework prob lems, by breaking it up this way and 
using the exponential law. This would be (0, b; b, 0). See, these two matrices 
commute w ith each other and, therefore, I could use the exponential law. This leaves 
all other cases. And here is the way to handle all other cases. All other cases. 

In other words, if you cannot calculate the series, this trick doesn't work, I have 
done as follows. You start w ith an arbitrary fundamenta l matrix, not the exponential 
matrix. You multiply it by its value at zero, that  is a constant matrix, and you take 
the inverse of that constant matrix. It w ill have one because, remember,  the 
fundamenta l matrix never has the determinant zero. 

So you can a lways take its inverse-ready va lue of t. Now, what property does this 
have? It is a fundamental matrix. How do I know that? Well, because I found a ll 
fundamenta l matrices for you. Take any one, multiply it by a square matrix on the 
right-hand side, and you get still a fundamental matrix. And what is its va lue at 
zero? 

Well, it is x of zero times x of zero  inverse. Its value at zero is the identity. Now, 
e^(At) has these same two properties. 

Namely, it is a fundamental matrix and its va lue at zero  is the identity. 

Conclusion, this is e^(At). And that is the garden variety method of calculating the 
exponentia l matrix, if you want to g ive it explicitly. Start with any fundamental 
matrix ca lculated, you should forgive the expression using eigenvalues and 
eigenvectors and putting the solutions into the columns. Eva luate it at zero, take its 
inverse and multip ly the two. 

And what you end up with has to be the same as the thing ca lculated w ith that 
infinite series. Okay. You will get lots of practice for homework and tomorrow. 

