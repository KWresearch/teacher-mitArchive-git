18.03 Class 33, Apr 28, 2010
 
Eigenvalues and eigenvectors  
 
1. Linear algebra 
2. Ray solutions 
3. Eigenvalues 
4. Eigenvectors 
5. Initial values 
 
 
[1]  Prologue on Linear Algebra. 
 
Recall  [a b ; c d] [x ; y] = x[a ; c] + y[b ; d] : 
 
A matrix times a column vector is the linear combination of  
the columns of the matrix weighted by the entries in the column vector. 
 
When is this product zero?  
 
One way is for  x = 0 = y.  If  [a ; c]  and  [b ; d]  point in  
different directions, this is the ONLY way. But if they lie along a  
single line, we can find  x  and  y  so that the sum cancels.  
 
Write  A = [a b ; c d]  and  u = [x ; y] , so we have been thinking 
about  A u = 0  as an equation for  u . It always has the "trivial" 
solution  u = 0 = [0 ; 0] :  0  is a linear combination of the two  
columns in a "trivial" way, with  0  coefficients, and we are asking 
when it is a linear combination of them in a different, "nontrivial" way. 
  
We get a nonzero solution  [x ; y]  exactly when the slopes of the vectors 
[a ; c]  and  [b ; d]  coincide:  c/a = d/b ,  or  ad - bc = 0.  This  
combination of the entries in  A  is so important that it's called the 
"determinant" of the matrix: 
 
      det(A) = ad - bc 
 
We have found:  
 
Theorem:  Au = 0  has a nontrivial solution exactly when  det A = 0 . 
 
If A is a larger *square* matrix the same theorem still holds, with 
the appropriate definition of the number  det A . 
 
 
[2]  Solve  u' = Au : for example with  A = [-2 1 ; -4 3] . 
 
The Mathlet "Linear Phase Portraits: Matrix entry" shows that some  
trajectories seem to be along rays from the origin. (We saw these also 
in the rabbits example on Monday. They were not present in the Romeo and 
Juliet example, though!) That is to say, we are going to look for a  
solution of the form 
 
       u(t)  =  r(t) v ,  v  not  0 
 
One thing for sure:   u'(t)  also points in the same or reverse direction: 
 

      u'(t)  =  r'(t) v 
   
Use the equation to express  u'(t)  in terms of  A  and  v : 
 
      u'(t)  =  A u(t)  =  A r v  =  r A v 
 
       r' v  =  r A v 
 
So  Av  points in the same or the reverse direction as  v :  
 
       A v  =  lambda v 
 
for some number  lambda.  This letter is always used in this context. 
An *eigenvalue* for  A  is a number  lambda such that  A v = lambda v 
for some nonzero  v . A vector  v  such that  A v = lambda v  is an 
eigenvector for value  lambda.  [Thus the zero vector is an eigenvector 
for every  lambda.  A number is an eigenvalue for  A  exactly when it 
possesses a nonzero eigenvector.] 
 
I showed how this looks on the Mathlet  Matrix/Vector. 
 
 
[3]  This is a pure linear algebra problem:  A is a square matrix, and we  
are looking for nonzero vectors  v  such that  A v = lambda v  for some  
number lambda.  Let's try to find  v .  In order to get all the  v's 
together, right the right hand side as  
 
      lambda v  =  (lambda I) v 
 
where  I  is the identity matrix  [1 0 ; 0 1] , and  lambda I  is the 
matrix with  lambda  down the diagonal.  Then we can put this on the left: 
 
      0  =  A v - (lambda I) v  =  (A - lambda I) v 
 
Don't forget, we are looking for a nonzero  v . We have just found an 
exact condition for such a solution:  
 
      det(A - lambda I)  =  0 
 
This is an equation in  lambda ;  we will find  lambda  first, and then  
set about solving for  v  (knowing in advance only that there IS a nonzero 
solution). 
 
In our example, then, we subtract  lambda  from both diagonal entries 
and then take the determinent: 
 
            A - lambda I   =  [ -2 - lambda , 1 ; -4 , 3 - lambda ] 
 
      det ( A - lambda I ) =  (-2-lambda)(3-lambda) + 4  
 
                           =  1 - lambda + lambda^2 - 2 
 
                           =  lambda^2 - lambda - 2 
 
This is the "characteristic polynomial"   
 
      p_A(lambda)  =  det( A - lambda I )  

 
of  A ,  and its roots are the "characteristic values" or "eigenvalues"  
of  A . 
 
In our case,  p_A(lambda)  =  (lambda + 1)(lambda - 2) 
 
and there are two roots,  lambda_1 = -1  and  lambda_ 2 = 2 .  
 
(The order is irrelevant.) 
 
 
[4]   Now we can find those special directions. There is one line for  
lambda_1  and another for  lambda_2 .  We have to find nonzero solution   
v  to 
 
      (A - lambda I) v  =  0 
 
eg  with  lambda = lambda_1 = -1 ,  A - lambda = [ -1  1 ; -4 4 ] 
 
There is a nontrivial linear relation between the columns:   
 
      A [ 1 ; 1 ]  =  0 
 
All we are claiming is that  
 
      A [ 1 ; 1 ]  =  - [ 1 ; 1 ] 
 
and you can check this directly.  Any such  v  (even zero)  is called 
an "eigenvector" of  A. 
 
It means that there is a ray solution of the form  r(t) v   
where   v = [1;1] .  We have 
 
      r' v  =  r A v  =  r lambda v 
 
so (since  v  is nonzero)  
 
      r'  =  lambda r 
 
and solving this goes straight back to Day One:  
 
       r  =  c e^{lambda t}  
 
so for us   r = c e^{-t}  and we have found our first straight line  
solution: 
 
       u  =  e^{-t} [1;1]  
 
In fact we've found all solutions which occur along that line: 
 
       u  =  c e^{-t} [1;1] 
 
Any one of these solutions is called a "normal mode." 
 
General fact: the eigenvalue turns out to play a much more important role 
than it looked like it would: the ray solutions are *exponential* 
solutions,  e^{lambda t} v ,  where  lambda  is an eigenvalue for 

the matrix and  v  is a nonzero eigenvector for this eigenvalue. 
 
The second eigenvalue,  lambda_2 = 2 , leads to 
 
      A - lambda I  =  [ -4 1 ; -4 1 ] 
 
and   [ -4 1 ; -4 1 ] v  =  0   has nonzero solution  v = [1;4] 
 
so  [1;4]  is a nonzero eigenvector for the eigenvalue  lambda = 2 , 
and there is another ray solution 
 
      e^{2t} [1;4]  
 
 
[5]  The general solution to  u' = Au  will be a linear combination of 
the two eigensolutions (as long as there are two distinct eigenvalues). 
 
In our example, the general solution a linear combination of the normal 
modes: 
 
u = c1 e^{-t} [1;1] + c2 e^{2t} [1;4] 
 
We can solve for  c1  and  c2  using an initial condition:  say for example 
  
u(0) = [1 ; 0].  Well, 
 
u(0) = c1 [1 ; 1] + c2 [1 ; 4] = [ c1 + c2 ; c1 + 4 c2 ]   
 
and for this to be  [1 ; 0]  we must have  3 c2 = -1 : c2 = -1/3 ; 
so  c1 = - 4 c2 = 4/3 : 
 
u(t) = (4/3) e^{-t} [1 ; 1] - (1/3) e^{2t} [1 ; 4] . 
 
When  t  is very negative, -10, say, the first term is very big and the 
second tiny: the solution is very near the line through [1 ; -1].  
As  t  gets near zero, the two terms become comparable and the solution 
curves around. As  t  gets large, 10, say, the second term is very big 
and the first is tiny: the solution becomes asymptotic to the line through 
[1 ; 1].  
 
 
 
    
  
 
 
 
 
 
 
 

MIT OpenCourseWare 
http://ocw.mit.edu 

18.03 Differential Equations(cid:13)(cid:10)�� 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

