18.03 Class 34, April 30, 2010
 
Complex or repeated eigenvalues 
 
1. Eigenvalues and coefficients 
2. Complex eigenvalues 
3. Repeated eigenvalues 
4. Defective and complete 
 
 
[1]  We were solving  u' = Au , with  A = [ a b ; c d ] :   
A = [ -2 1 ; -4 3 ] for example. Or  u  could be a 3D vector,  
and  A  a  3x3  matrix.  
 
(1) Find the eigenvalues = roots  lambda_1, lambda_2   
of the characteristic polynomial 
 
       p_A(lambda) = det( A - lambda I ) 
 
(2) For each eigenvalue, find a nonzero eigenvector: 
 
   ( A - lambda_1 I ) v_1 = 0      ( A - lambda_2 I ) v_2 = 0    
 
Normal modes:  u_1 = e^{\lambda_1 t} v_1 ,  u_2 = e^{\lambda_2 t} v_2 . 
 
(3) General solution is linear combination of these:  u = c_1 u_1 + c_2 u_2 . 
                   
 
There are a few problems with this, but before pointing them out let me make 
three comments: 
 
(a)  Any multiple of an eigenvector is another eigenvector for the same 
eigenvalue; they form a line, an "eigenline."    
 
(b) The zero vector is an eigenvector for every value lambda, whether   
lambda  is an eigenvalue or not.  Most of the time  0  is the ONLY eigenvector 
for value  lambda ;  lambda  is an eigenvalue exactly when there is a  
*nonzero* eigenvector for that value. 
 
(c)    p_A(lambda) = det( A - lambda I ) 
 
                   = lambda^2 - (a+d) lambda - (ad-bc) 
 
The sum of the diagonal terms of a square matrix is the "trace" of  A ,  
tr A ,  so  
 
      p_A(lambda)  =  lambda^2 - (tr A) lambda + (det A) 
 
In our example,  tr A = 1  and  det A = -2 , and  
 
      p_A(lambda)  =  lambda^2 - lambda - 2  .  
 
 
[2]  There may be no ray solutions. Romeo and Juliet provided one example. 
Or, what about 
 
      A  =  [ 1 2 ; -2 1 ] .  

 
Let's apply the method and see what happens.  tr(A) = 2 , det(A) = 5,  so 
 
      p_A(lambda) = lambda^2 - 2 lambda + 5 = (lambda - 1)^2 + 4  
 
which has roots  lambda_1 = 1 + 2i,  lambda_2 = 1 - 2i. 
 
We could abandon the effort at this point, but we had so much fun and  
success with complex numbers earlier that it seems we should carry on. 
 
Find an eigenvector for  lambda_1 = 1 + 2i : 
 
      A - (1+2i)I : [ - 2i 2 ; -2 -2i ][ ? ; ? ]  =  [ 0 ; 0 ] 
 
Standard method: use the entries in the top row in reverse order with 
one sign changed:  [ 2 ; 2i ]  or, easier, in this case,   
 
      v_1   =  [ 1 ; i ]. 
 
This is set up so the top entry in the product is  0 . We have a chance 
to check our work (mainly the calculation of the eigenvalues) by 
seeing that the bottom entry in the product is  0  too: 
 
      -2 . 1 - 2i . i  =  0 
 
[ 1 ; i ]  is a vector with complex entries. OK, so be it. It's hard 
to visualize, perhaps, and doesn't represent a point on the plane, but 
we can still compute with it just fine. 
 
Since  lambda_2 = conjugate of  lambda_1,  
an eigenvector for  lambda_2  is given by the conjugate of  v_1: 
 
      v_2  =  [ 1 ; -i ]   
 
So the normal modes 
                                        _ 
      v(t) = e^{(1+2i)t} [ 1 ; i ]  ,   v(t) = e^{(1-2i)t} [ 1 ; -i ] .  
 
As in the case of second order equations, the real and imaginary parts 
of solutions are again solutions,  
                _                            _ 
      u1 = (v + v)/2 = Re(v)  ,    u2 = (v - v)/(2i) 
 
and we really only need to write down one of the normal modes. 
 
So these are real solutions: 
 
       u  =  e^{(1+2i)t} [ 1 ; i ]  
 
          =  e^t ( cos(2t) + i sin(2t) ) ( [1;0] + i[0;1] )    so 
 
      u1  =  Re(u)  =  e^t ( cos(2t) [1;0] - sin(2t) [0;1] )  
 
          =  e^t [cos(2t) ; - sin(2t)]                     and 
 
      u2  =  Im(u)  =  e^t ( cos(2t) [0;1] + sin(2t) [1;0] )  
 

          =  e^t [sin(2t) ; cos(2t)] 
 
These are two independent real solutions. Both spiral around the origin, 
clockwise, while fleeing away from it exponentially. They satisfy 
 
      u1(0)  =  [1;0]   ,   u2(0)  =  [0;1] . 
 
I showed their trajectories on the Mathlet Linear Phase Portraits: Matrix  
Entry.  
 
The general real solution is  
 
      a u1 + b u2  ,  a,  b real . 
 
It is very hard for me to visualize the fact that all 
those spirals are linear combinations of any two of them. 
 
 
Summary: Nonreal eigenvalues lead to spiral solutions. 
Positive real parts lead to solutions going to infinity with t ("unstable") 
Negative real parts lead to solutions going to zero with  t  ("stable") 
Zero real parts lead to solutions parametrizing ellipses. 
 
So we discover that the possibility of complex eigenvalues really isn't a 
failure of the method at all. There are in fact ray solutions, but they are 
complex and don't show up on our real phase plane. 
 
 
[3]  Second problem with our method: Illustrated by 
 
      A  =  [ -2 1 ; -1 0 ]  
 
      p_A(lambda)  =  lambda^2 + 2 lambda + 1 = (lambda + 1)^2 
 
which has only one root, "repeated":  lambda_1 = lambda_2 = -1. 
 
Still, find an eigenvector:   
 
  A - (-1)I  =  [ -1 1 ; -1 1 ][ ? ; ? ]  =  [ 0 ; 0 ] :   v  =  [ 1 ; 1 ] 
  
or any nonzero multiple. ALL eigenvectors for  A  lie 
on the line containing  0  and [ 1 ; 1 ].  I showed a picture of the  
phase portrait, which shows only one pair of opposed ray trajectories. 
 
So there is (up to multiples) only one normal mode: 
 
       u_1  =  e^{-t} [ 1 ; 1 ]  
 
But we need another solution. Here is how to find one; I won't go into 
details, just give you the method. 
 
Write down the same matrix  A - lambda_1 I  but now find a vector  w  such 
that 
 
      (A - lambda1 I) w  =  v . 
 
Then  

 
      u_2  =  e^{lambda_1 t} (t v + w)  
 
is a second solution.   
 
In our case: 
 
      [ -1 1 ; -1 1 ] [ ? ; ? ]  =  [ 1 ; 1 ] 
 
has solution  [0;1] ,  so 
 
      u_2  =  e^{-t} ( t [1;1] + [0;1] )  
 
           =  e^{-t} [ t ; t+1 ] 
   
[0;1]  isn't the only vector that works here;  [0;1] + c v  does too 
for any constant  c. It doesn't matter which one you pick. 
 
With this choice,  u_1(0) = [1;1]  ,   u_2(0) = [0;1] . 
 
The general solution is 
 
      u  =  a u_1 + b u_2 .  
 
To learn more about all this you should take 18.06. 
 
 
Didn't get to talk about this on Friday: 
 
[4]  A matrix with a repeated eigenvalue but only one lineful of  
eigenvectors is called "defective."  A matrix can have a repeated  
eigenvalue and not be defective:  
 
      A  =  [ 2 0 ; 0 2 ]   
 
for example has characteristic polynomial  
 
      p(lambda)  =  lambda^2 - 4 lambda + 4  =  (lambda - 2)^2 
 
so  lambda_1 = lambda_2 = 2.  To find an eigenvector consider 
 
      A - lambda_1 I :  [ 0 0 ; 0 0 ] [ ? ; ? ]  =  [ 0 ; 0 ] 
 
Now ANY vector is an eigenvector! Instead of only one line you get the 
entire plane. For any vector  v ,   
 
      e^{2t} v 
 
is a solution, and every solution is a normal mode. This is called the 
"complete" case.   
 
In the 2x2 case, if the eigenvalue is repeated you are in the defective 
case unless the matrix is precisely   [ \lambda_1 , 0 ; 0 , \lambda_1 ] 
 
 
 
 

MIT OpenCourseWare 
http://ocw.mit.edu 

18.03 Differential Equations(cid:13)(cid:10)�� 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

