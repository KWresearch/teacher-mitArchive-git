18.03 Class 36, May 5, 2010
 
The matrix exponential: initial value problems. 
 
1. Definition of e^{At} 
2. Computation of e^{At} 
3. Uncoupled example 
4. Defective example 
5. Exponential law 
 
 
[1]  Recall from day one: 
 
(a)  x' = rx  with initial condition  x(0) = 1  has solution  x  =  e^{rt} .  
 
x' = rx  with any initial condition has solution  x = e^{rt} x(0) . 
 
Later, we decided to *define*  e^{it}  as the solution to 
 
(b)  x' = ix  with initial condition   x(0) = 1 .  
 
Following Euler, a solution is given by   cos t + i sin t ,  so we found that 
 
             e^{it} = cos(t) + i sin(t) 
 
(c)  Now we are studying  u' = A u  .  Let's try to *define*  
 
   The solution to  u'  =  Au  with initial condition   u(0)   
   is   u  =  e^{At}u(0).  (**) 
 
Note that the initial value  u(0)  is a vector, and  u(t)  is a vector  
valued function.  So the expression  e^{At}  must denote a matrix, or  
rather a matrix valued function. 
 
What could  e^{At}  be?  For a start, what is its first column?   
Recall that the first column of any matrix  B  is  the product  B[1;0] , 
and  [ b_1   b_2 ] [1;0] = b_1 , so combining this with (**) we see: 
 
The first column of  e^{At}  is the solution to  u' = Au  with  
u(0) = [1;0]. 
 
Similarly, 
 
The second column of  e^{At}  is the solution to  u' = Au  with   
u(0)= [0;1]. 
 
This is the DEFINITION of  e^{At} . It makes  (**)  true for all  u(0) , 
because  e^{At} u(0)  is a solution (being a linear combination of the 
columns of  e^{At} ,  which are solutions), and when  t = 0  we get 
 
         e^{A0} u(0) = I u(0) = u(0) . 
 
 
[2]  Computation of  e^{At}  
 
We need a method for computing it, though. To explore this we'll use the 
 

Example:  A = [ 1 1 ; 0 2 ] . 
 
This is upper triangular, so its eigenvalues are given by the diagonal  
entries:  lambda = 1,  lambda = 2.  The  (tr,det)  pair lies in the 
upper right quadrant, below the critical parabola; the phase portrait 
is an unstable node. 
 
Find eigenvectors: 
 
  lambda_1  =  1 :   A - I : [ 0 1 ; 0 1 ] [?;?]  =  [0;0] :  v_1  =  [1;0] 
 
  lambda_2  =  2 :  A - 2I : [-1 1 ; 0 0 ] [?;?]  =  [0;0] :  v_2  =  [1;1] 
 
Two independent solutions are given by   
 
      u_1  =  [ e^t ; 0 ] ,  u_2  =  [ e^{2t} ; e^{2t} ] 
 
and the general solution is  
 
      u  =  c_1 [ e^t ; 0 ] + c_2 [ e^{2t} ; e^{2t} ] 
 
We could go ahead and solve for  c_1  and  c_2  to get solutions with 
the desired initial conditions. What follows is a clever way to do that. 
 
There is a compact way to write this linear combination: it is 
 
      u  =  [ e^t , e^{2t} ; 0 , e^{2t} ] [ c_1 ; c_2 ].    (***) 
 
This matrix is a "fundamental matrix" for the system: its columns are 
independent solutions. Such a matrix will be denoted by  Phi(t); so here 
 
      Phi(t) = [ e^t , e^{2t} ; 0 , e^{2t} ] 
 
Phi(t)  behaves very much like we want  e^{At}  to behave; its columns 
are solutions, even independent ones, and the general solution is given by 
 
      Phi(t) [ c_1 ; c_2 ] 
 
The matrix exponential  e^{At}  is a fundamental matrix: it is the fundamental 
matrix  Phi(t)  such that  Phi(0) = I .  
 
Our Phi(t)  does not evaluate this way.  To fix this, I claim we should form 
 
      Phi(t) Phi(t)^{-1}  
 
Explanation:  If  B  is a square matrix, you can ask whether it has an  
*inverse* matrix, a matrix  B^{-1}  such that 
 
       B B{-1} = I   and   B^{-1} B = I  
 
(either implies both). The answer, as for numbers, is not always. 
It turns out that there is an inverse exactly when  det(B)  is not zero. 
 
In the  2x2  case,  B = [ a b ; c d ]  and  
 
      [ a b ; c d ]^{-1}  =  (1/det(A)) [ d -b ; -c a ]. 
 

We can check this: 
 
      [ a b ; c d ] [ d -b ; -c a ] = [ ad-bc 0 ; 0 ad-bc ] = (det B) I 
 
Now let's see: each column  Phi(t) Phi(0)^{-1}  is a linear combination 
of the columns of  Phi(t) ,  so it's a solution. What remains is to check 
the normalization; but  Phi(0) Phi(0)^{-1} = I . 
 
Conclusion:  
 
      e^{At}  =  Phi(t) Phi(0)^{-1} 
 
where  Phi(t)  is ANY fundamental matrix for  A.2A 
 
 
In our example,  Phi(0) = [ 1 1 ; 0 1 ] ,  Phi(0)^{-1} = [ 1 -1 ; 0 1 ] , 
and so    
 
      e^{At} = [ e^t e^{2t} ; 0 e^{2t} ] [ 1 -1 ; 0 1 ]  
 
              =  [ e^t , e^{2t} - e^t ; 0 , e^{2t} ]. 
 
 
[3]  Uncoupled example:  Suppose  A = [ a 0 ; 0 d ] . 
The eigenvalues are  lambda_1 = a  and  lambda_2 = d 
 
I can see the eigenvectors:  v_1 = [ 1 ; 0 ] ,  v_2 = [ 0 ; 1 ] . 
 
Basic solutions are  e^{lambda_1 t} [ 1 ; 0 ] 
                     e^{lambda_2 t} [ 0 ; 1 ]  
 
so  Phi(t) = [ e^{lambda_1 t} 0 ; 0 e^{lambda_2 t} ]  
 
and this is already normalized: so it is the matrix exponential. 
 
 
[4]  Defective example. 
 
Sometimes the matrix exponential can be a bit unexpected. For example: 
 
     A = [ 0 1 ; 0 0 ]  
 
Then  tr A = 0  and  det A = 0 ,  so the only eigenvalue is  0 ,  with 
multiplicity  2 .  This is not a diagonal matrix, so it is defective, 
and we could find solutions by the standard method. However, it is also 
a companion matrix, for the second order equation    x" = 0 . 
Solutions of this are easy! x_1 = 1 , x_2 = t .  So basic solutions to  
 
     u' = A u  
 
are  u_1 = [ x_1 ; x_1' ] = [ 1 ; 0 ] 
     u_2 = [ x_2 ; x_2' ] = [ t ; 1 ] 
 
     Phi(t) = [ 1 t ; 0 1 ]  
 
This satisfies  Phi(0) = I , so   
 

     e^{At} = [ 1 t ; 0 1 ]  
 
  
[5]  We can take  t  to be a specific value, of course: eg  t = 1 : 
 
     e^[ 0 1 ; 0 0 ] = [ 1 1 ; 0 1 ]  
 
and this lets us define  e^A  for any square matrix  A . 
 
Then  e^0 = I , as you might expect, but watch out:  
 
      e^A e^B = e^{A+B}   *provided that*  AB = BA  
 
So for example  (e^A)^n = e^{nA} 
 

MIT OpenCourseWare 
http://ocw.mit.edu 

18.03 Differential Equations(cid:13)(cid:10)�� 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

