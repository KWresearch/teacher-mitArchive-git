18.03 Class 9, Feb 22, 2010 
 
Linear vs Nonlinear: a debate 
 
[1] Linearization near equilibrium 
[2] Exponential Response Formula (first order) 
[3] Potential blow-up of solutions to a nonlinear equation 
 
 
Hour Exam I Wednesday. Locations for both lectures: 
 
     10-250 A -- G 
     Walker H -- Z 
 
Office hours as usual on Wednesday 
 
 
Review: Nonlinear vs Linear 
 
 y'(t) = F(t,y(t))      vs     r(t) x'(t) + p(t) x(t) = q(t)    
   
This is in the form of a debate, between Linn E. R. (on the right) 
and Chao S. (on the left). I'll take a vote at the end. 
 
Linn: I'd like to begin by making the point that there is a solution  
procedure for linear equations, which reduces solution of any linear  
equation to integration: Multiply the equation through by a factor so that 
the two terms become the two terms in  (d/dx)(ux) . Then integrate. 
 
Sometimes you can just *see* this:  t^2 x' + 2t x = (d/dt)(t^2 x)  for example. 
 
If we are in *reduced* standard form, so  r = 1 , then this can be done 
systematically: 
 
We seek  u(t)   such that  u (x' + px) =  (d/dt) (ux)  
i.e.  pu = u' : separable, with solution  u = e^{\int p(t) dt}   
(any constant of integration will do here). Then integrate both sides of  
 
     (d/dt) (ux) = uq  :    x = u^{-t} int uq dt 
 
The constant of integration is in this integral, so the general solution  
has the form  
 
     x = x_p + c u^{-1} .    
 
Another lovely feature of linear equations is that the constant of integration 
in the solution of a linear equation always appears right there.  
 
The "associated homogeneous equation" is  x' + px = 0 : separable, 
with solution  x_h = e^{-\int p(t) dt} . Look! this is the reciprocal of  
the integrating factor! Wonderful! 
 
In most applications,  u{-1}  falls off to zero 
as  t  gets large; the term  c u^{-1}  is a "transient." 
 
 
Chao: That's a lot of integration ....  I'm more interested in the general 

behavior of solutions, rather than an incomprehensible expression of them  
as integrals or a boring expression of them in terms of sin, cos, exponential. 
 
I prefer arguments like this: take an equation like  y' = y^2 - x . 
This doesn't have a single solution which you, Linn, in your linear cave, 
have anything to say about. But I can look at the direction field, 
recognize that there is a funnel along  y = -\sqrt(x) , so all solutions 
near there are trapped and are asymptotic to  - sqrt(x) .... and even argue 
that they are all ultimately a bit larger than  - sqrt(x) . No integration 
involved, but very good information.  
 
Or take an equation like the logistic equation 
 
       y' = k_0 ( 1 - y/p) y 
 
This is an auotomous equation, and remains so even if I allow a harvest 
rate, even one depending up on  y : 
 
       y' = k_0 ( 1 - y/p) y - a(y)  
 
This equation gives genuine insight into real population dynamics. 
By looking at the phase line it is easy to analyze the behavior of  
solutions, in a way useful for policy makers. 
 
 
Linn: Well, now, most of the time a system is near equilibrium. 
Engineers get very anxious when their systems get too far from  
equilibrium. Let's look at your nice nonlinear logistic equation. 
As you said, there's a critical point at  y = p , and so an equilibrium  
solution. Just how does the system relax to this equilibrium? 
 
Let's write  y = p + u  and change variables:   
 
     1 - y/p = - u/p  
 
     u' = - k_0 u/p (p-u) = - k_0 u + k_0 u^2/p 
 
For small  u  the second term is *very* small, and can be ignored. 
This is called LINEARIZING the equation near equilibrium. Near equilbrium 
solutions to the nonlinear equation behave a lot like  p + u  where 
u  is a solution to the *linear* equation  
 
     u' = - k_0 u 
 
so we can say that the population relaxes to equilibrium exponentially, 
as  e^{-k_0 t}  approaches 0.  
 
 
Chao: There you go again with your fancy exponentials. You think you 
know all about them, but your computer has to compute their values, after 
all, and the methods it uses are no different than the methods used to  
compute the values of linear equations: Euler's method, or its fancier 
variants.  
 
 
Linn: Speaking of fancy exponentials, I'd like to point out that  
smart people almost never use integrating factors to integrate linear  

equations with constant coefficients: 
 
     x' + k x = q(t) 
 
Yeah, the integrating factor  is  e^{kt} , but even I don't like the 
integrals that come out. But there are these great tricks, Chao! Suppose 
q(t) = B e^{rt} .  Then, be optimistic! maybe there's an exponential  
solution!   
           x_p = A e^{rt}  
 
          x_p' = A r e^{rt} 
          ----------------- 
  
      B e^{rt} = A (k+r) e^{rt} 
 
so   A = B / (k+r) :     
 
ERF      x_p = B e^{rt} / (k+r)   is a solution to  x' + kx = B e^{rt} 
 
This is the "Exponential Response Formula." It works as long as  k+r  is not 0. 
 
 
Chao: Bravo. 
 
 
Linn:  And what's better, did I ever tell you about the complex exponential? 
r can be a complex number! Euler told us that  
 
           e^{i theta} = cos(theta) + i sin(theta) 
 
-- it's a point on the unit circle in the complex plane --- so trig functions 
are incorporated into the complex exponential!! 
 
So to solve        x' - x = 3 cos(t)  
 
I replace it by a different equation  
 
                   z' - z = 3 e^{it} 
 
of which it is the real part. Then use optimism or the ERF  
(with  k = -1 ,  r = i , B = 3 ): 
 
                   z_p = 3 e^{it} / (-1+i)  
 
and just find the real part  x_p = Re(z_p)  
 
        z_p  = 3 (-1-i)/2 (cos(t) + i sin(t))  
 
so      x_p = -(3/2) cos(t) + (3/2) sin(t)  
  
For the general solution we just add in the general sol to the homogeneous 
equation, which is  c e^t . It's a little funny to call this a "transient," 
and I don't, but it does give you the general solution. 
 
 
Chao: You know, this method results in these sums of sines and cosines, 
which is very nice but I want to know what they look like. The "nonlinear" 

view of sines and cosines writes 
 
         a cos(omega t) + b sin(omega t) = A cos(omega t - phi) 
 
where  A, phi  are the polar coordinates of the point  (a,b) . 
 
In your example,  A = 3 sqrt(2)/2  and  phi = 3 pi/4 : so the solution is 
easy to draw and compare with the input signal. 
 
 
Linn, I want to point out another charming feature of solutions to many 
nonlinear equations. ... Take a simple one like   y' = y^2  
for example. This is separable:  y^{-2} dy = dx ,  - y^{-1} = x + c , 
 
     y = 1 / (c - x)  
 
 
So the IVP  with  y(0) = 1  has c = 1  ,  y = 1/(1-x) . Its graph is  
asymptotic to the vertical line at  x = 1 : it goes off to infinity in  
finite time. It ENDS. The equation  y = 1/(1-x)  actually represents TWO  
solutions: one for  x < 1 , and another for x > 1 . If we are to say that 
a solution of a differential equation is determined by an initial value, 
we have to require that the graph be connected.  
 
 
Linn: You call that a feature? That never happens to solutions to my  
equations. If they are going to go south on me, I know it from the 
coefficients or the input signal. As long as  p(t)  and  q(t)  are 
nice and finite (and r(t) is nonzero) so are all solutions.  
They live as long as I do! 
 
 
Chao:  Well, the world really is nonlinear. Newton's law of gravitation is  
highly nonlinear. This kind of explosion actually happens in the case of  
Newton's laws: Jeff Xia showed that in a certain 5-planet system two of the  
planets behaves more or less like    (1/t) sin(1/t) ,  oscillating with  
increasing amplitude and increasing frequency as  t ---> 0  (from  t < 0 ) . 
 
Solutions to linear equations are not nearly as diverse and exciting! 
 
 
Who wins? Linn E. R. or Chao S.? 
 
In both classes, Linear won but Nonlinear was more enthusiastic. 
 
 
 
 
 
 
 

MIT OpenCourseWare 
http://ocw.mit.edu 

18.03 Differential Equations���� 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

