EXAM  1 

Instructions:  You  will  have  approximately  50  minutes  for  this  exam.  The  test  is  closed  book, 
closed notes  and  calculators  are not  allowed.  The point value of  each problem  is written next  to  the 
problem – use your time wisely.  Partial credit will be given.  You may use either pencil or ink.  If you 
need  extra  paper,  raise  your  hand  (I  also  have  a  stapler  and  paper  clips  for  attaching  extra  sheets). 
If  you  have  any  questions,  raise  your  hand.  Please  show  all  work  unless  instructed  otherwise. 

Problem  1(15  points)  In  each  of  the  following  cases,  you  are  given  a  ﬁeld  F,  an  F-vector  space 
V ,  and  a  subset W  ⊂ V .  State whether  or  not W  is  an  F-vectors  subspace  of  V .  If  it  is  a  subspace, 
compute  its  dimension.  You  need  not  give  rigorous  justiﬁcation,  but  do  show  all  work. 
� 
� 
�� 
|z1  + z 2  = 0 
. Here  z  is  the  complex  conjugate,  i.e. 

(a)(5 points) F = C, V  = C2 , W  = 
a + ib = a − ib. 

z1 
z2 

Consider  the  vector 

It  is  NOT  a  subspace.  To  prove  this,  we  will  show  that W  is  not  stable  for  scalar multiplication, 
i.e.  we  will  ﬁnd  a  vector  v  in W  and  a  scalar  λ ∈ C  such  that  λv  is  not  in W . 
�  � 
i
(1) 
v = 
.
i 
It  is  easy  to  check  that  v  is  in  W  (this  is  just  the  identity  i + i  =  i + (−i)  =  0).  Let  λ  =  −i  and 
consider  the  scalar multiple 
�  �  � 
� 
−i ∗ i  = 
1
−i ∗ i 
. 
1 
It  is  easy  to  see  that  λv  is  not  in W  because  1 + 1 = 1 + 1 = 2 = 0.  Thus 
scalar  multiples. 
Subspace?:  NO. 

W  is  not  stable  for  taking 

λv = −iv = 

(2) 

Dimension?:  N/A. 

� 
� 
�� 
|z1  + z 2  = 0 
z1 
(b)(5  points)  F  =  R,  V  =  C2 ,  W  = 
.  Here  scalar  multiplication  of  a 
z2 
vector  is  deﬁned  by 
� 
�  � 
� 
= 
r 
. 
In  this  case  W  is  a  vector  subspace.  First  of  all  observe  that  the  zero  vector  is  in  W  because  of 
the  identity  0 + 0 = 0 + 0 = 0. 

ra + irb 
rc + ird 

a + ib 
c + id 

(3) 

Date :  October  2001. 

1 

�
2 

(4) 

EXAM  1 
� 
� 
Next  suppose  that  we  have  two  vectors  in W ,  say 
z1 
, w = 
z2 
� 
We  will  prove  that  v + w  is  also  in W .  Observe  that 
z1  + ζ1 
z2  + ζ2 

v + w = 

� 

v = 

(5) 

� 
ζ1 
. 
ζ2 
� 
. 

� 
� 
To  see  that  v + w  is  in W ,  observe  that 
(6) 
(z1  + ζ1 ) + (z2  + ζ2 ) = (z1  + ζ1 ) +  z1  + ζ2  . 
This  follows  since  complex  conjugation  commutes  with  addition.  Because  addition  is  associative, 
� 
� 
we  can  rewrite  the  right-hand  side  of  this  equation  as 
(7) 
(z1  + z2 ) +  ζ1  + ζ2  . 
By the assumption that v and w are in W , each of the terms in parentheses vanishes.  So we conclude 
v + w  is  also  in W . 

Finally,  suppose  that  we  have  a  vector  v  in  W  (with  the  same  notation  as  above),  and  suppose 
that  r ∈ R  is  some  real  number.  We  will  show  that  rv  is  also  in W .  Indeed,  we  have 
� 
� 
rz1 
rv = 
(8) 
. 
rz2 

And  we  check  that 
rz1  + rz2  = rz1  + rz2  = r (z1  + z2 ) . 
(9) 
The  ﬁrst  step  above  follows  since  scaling  by  a  REAL  number  commutes  with  complex  conjugation 
(which  is  clear  from  the  deﬁnition  of  complex  conjugation).  The  second  step  follows  since  multi­
plication  distributes  with  respect  to  addition.  By  the  assumption  that  v  is  in  W ,  we  have  that 
z1  + z2  =  0.  So  we  conclude  that  r(z1  + z2 )  =  r0  =  0,  i.e.  rv  is  in  W .  Thus  we  have  checked  the 
three  axioms  necessary  to  prove  that W  is  a  vector  subspace  of  V . 

Subspace?:  YES. 

v = 

(10) 

� 
�  � 
� 
In  order  to  compute  the  dimension,  we  write  a  general  vector  v  in  V  in  the  form 
a1  + ib1 
z1  = 
. 
a2  + ib2 
z2 
Then  the  condition  for  v  to  be  contained  in W  is  exactly 
0 + i0 = (a1  + ib1 ) + (a2  + ib2 ) = (a1  + ib1 ) + (a2  − ib2 ) = (a1  + a2 ) + i (b1  − b2 ) . 
(11) 
The  ﬁrst  step  above  is  by  deﬁnition  of  complex  conjugation,  and  the  second  is  by  associativity  of 
addition  and  distributivity  of  multiplication  with  addition.  This  condition  is  equivalent  to  the  pair 
� 
of  real  linear  conditions 
a1  + a2  = 0 
b1  − b2  = 0 
�  �  � 
� 
Solving  this  system  of  linear  equations,  we  conclude  that  a  basis  for W  consists  of  the  vectors 
1 −1 
i 
, 
. 
i 

(13) 

(12) 

Therefore W  is  2-dimensional. 

EXAM  1 

3 

(15) 

f1  = 

Dimension?:  2. 
(c)(5  points)  F = R,  V  is  the  vector  space  consisting  of  all  2 × 2 matrices  with  real  entries 
� 
� 
a b 
(14) 
A = 
� 
� 
c d 
with  addition  and  scalar  multiplication  deﬁned  in  the  usual  way,  and  W  =  A ∈ V  : A + A
†  = 0  . 
†  denotes  the  transpose  matrix  of  A. 
Here  A
This  is  a  vector  subspace  of  V .  The  easiest  way  to  see  this  is  to  observe  that  we  may  identify  V 
with  the  vector  space  R4 ,  where  the  basis  vectors 
� 
� 
� 
� 
� 
� 
� 
� 
0 0
0 0 
0 1
1 0 
, f4  = 
, f2  = 
, f3  = 
,
0 1 
1 0 
0 0 
0 0 
are  identiﬁed  with  the  standard  basis  vectors  of  R4 ,  e1 , . . . , e4  respectively.  In  other  words,  we 
identify  a  2 × 2 matrix  with  a  4-vector  by  the  rule

⎛

⎞

�

�

a
⎜⎜⎝
 ⎟⎟⎠

b 
c

d

†  = 0  is  precisely  that 
�  � 
�  � 
� 
Now  the  condition  that  A + A
0 0 
a c 
a b 
=
+ 
0 0 
c d 
b d 
⎧ ⎪⎪⎨ 
Gathering  terms,  this  is  the  same  as  the  four  linear  conditions 
a + a = 0 
b + c = 0
⎪⎪⎩

c + b = 0

d + d = 0

Identifying matrices  with  4-vectors  as  above,  we  see  that  these  equations  exactly  give  the  nullspace 
⎛ 
⎞
of  the matrix 
2 0 0 0

0 1 1 0
 ⎠
 .

⎝

0 0 0 2 
Since the nullspace of a matrix  is always a vector subspace, we conclude that W  is a vector subspace 
of  V . 

� 
. 

b 
a 
c d 

↔ 

. 

(19)


(16) 

(17) 

(18)


Subspace?:  Yes. 

We  can  easily  put  the  matrix  above  in  reduced  row  echelon  form  by  simply  scaling  the  ﬁrst  and 
third  rows  by  1 :2 
⎛ 
⎞
1 0 0 0

0 1 1 0
 ⎠
 . 
⎝

0 0 0 1 
So  the  rank  of  this matrix  is  3.  By  the  rank-nullity  theorem  (which  simply  says  that  the  number  of 
leading variables plus the number of free variables equals the number of total variables), we conclude 

(20)


EXAM  1 
4 
that the dimension of the nullspace is 4 − 3 = 1.  In fact,  it is easy to see that a basis for the nullspace 
is  given  by  the  vector/matrix 
⎞

⎛

� 
� 
⎟⎟⎠

0
⎜⎜⎝ 
↔ −1 0 
0 1
1
−1

.
0


(21) 

Dimension?:  1. 

.


(22) 

Problem  2(20  points)  In  this  problem,  F  =  R.  Consider  the  real  vector  space  V  =  R5 .  Deﬁne 
the  vector  subspaces W1  ⊂ V  and W2  ⊂ V  as  follows.  We  deﬁne W1  to  be  the  span  of  the  vectors 
⎞⎛⎞⎛⎞⎛ 
⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

,
 ⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

,
 ⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

1
0
0 
0

0

0

0

1

0

0

0

0

0
0
1 
We  deﬁne W2  to  be  the  span  of  the  vectors 
⎞⎛⎞⎛⎞
⎛ 
⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

,
 ⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

,
 ⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

1
0 
1

0

1

1

1

0

0
1 
Using  any method  you  like  (as  long  as  you  show  your  work),  compute  a  spanning  set  for  the  vector 
subspace W1  ∩ W2  ⊂ V . 

1
−1

1
−1

1


(23) 

. 

(24)


Of  course we  could  use  our  algorithm  from  the  homework  to  ﬁnd  the  intersection.  But  it  is much 
simpler  to  observe  that  the  vectors  in W1  are  simply  those  of  the  form 
⎛

⎞

⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

X1 
0 
X2 
0 
X3 
On  the  other  hand,  every  vector  in W2  is  a  linear  combination 
⎛⎞⎛
⎞⎛
⎛ 
⎞
⎞
Y1 ⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

+ Y2 ⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

+ Y3 ⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

=
 ⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

0 
1
0 
1

1 
1

1 
0

0
1

The  condition  that  this  vector  be  contained  in W1  is  exactly  the  vanishing  of  the  two  coordinates: 
−Y1  + Y2  = 0, −Y1  + Y3  = 0. 

Y1  + Y2 
−Y1  + Y2 
Y1  + Y2  + Y3 
−Y1  + Y3 
Y1  + Y3 

1
−1

1
−1

1


(26) 

(25) 

. 

. 

EXAM  1 

5 

(27) 

(28) 

(29) 

(30) 

. 

. 

(a)(10  points) 

We may  rewrite  these  two  conditions  as  Y2  = Y1  and  Y3  = Y1 .  Substituting  Y1  in  for  Y2  and  Y3 ,  we 
conclude  that  the  vectors  in W1  ∩ W2  are  precisely  those  of  the  form 
⎛ 
⎞
⎜⎜⎜⎜⎝ 
⎟⎟⎟⎟⎠

2Y1 
0 
3Y1 
0 
2Y1 
So  we  conclude  that  a  basis  for W1  ∩ W2  consists  of  the  single  vector 
⎞⎛ 
⎜⎜⎜⎜⎝ 
⎟⎟⎟⎟⎠

2 
0 
3 
0

2

Problem  3(20  points)  In  each  case  below,  ﬁnd  all  solutions  to  the  system  of  equations AX  = C . 
Please  show  all  work,  but  you  don’t  need  to  give  a  rigorous  proof  of  your  answer. 
⎛

⎞

�
 �

�
� 
⎝

⎠ =
2  2  1 
9
1  1  2 
9 
� � � �  �

� 
9

9

Now  we  perform  Gauss-Jordan  elimination.  We  begin  by  setting  B1  =  A
ﬁrst  and  second  row  to  get  a  leading  1  in  the  ﬁrst  column: 
� � � �
 �

�

1 1 2

9 
B2  = 
.
9 
2 2 1

Next  we  subtract  2  times  the  ﬁrst  row  from  the  second  row  to  get

� � � �

�

�

9 
1 1 
2

0 0  −3

−9 
B3  = 
. 
−1  to  get  the  row  echelon matrix 
� � � �
 �

�

3 
1 1 2

9 
B4  = 
0 0 1

3 
Finally  we  subtract  2  times  the  second  row  from  the  ﬁrst  row  to  get  a  reduced  row  echelon matrix

� � � �
 �

�

1 1 0

3 
R =

.
0 0 1

3 
Written  using  variables  instead  of matrix  notation,  the  reduced  row  echelon  system  is 

�

1x1  + 1x2  + 0x3  = 3 
0x1  + 0x2  + 1x3  = 3 

� .  First  we  transpose  the 

We  form  the  augmented matrix  A

Next  we  scale  the  second  row  by 

�  = 

A

2 2 1

1 1 2


(34) 

(35) 

.

.

(31) 

(32) 

(33) 

X1 
X2 
X3 

.

� : 

R  :

6 

EXAM  1 

(36)	

(37)	

(38) 

(b)(10  points) 

Here  the  leading  variables  are  x1  and  x3  and  the  free  variable  is  x2 .  We  bring  the  terms  involving 
� 
free  variables  to  the  right-hand  side  of  the  equation  to  get 
x1  = 3 − x2 
x3  = 3 
⎞ 
⎞ ⎛ 
⎛
Therefore  the  general  solution  of  the  system  is 
3 − x2 
x2  ⎠ . 
X  = ⎝  X2  ⎠ = ⎝ 
X1 
3 
X3 
⎛ ⎞ 
⎛
⎞ 
� 
�
2 1 
2
⎝  2 1	 ⎠  X1  = ⎝  4  ⎠ . 
X2 
1 2 
5 
We  could  perform  Gauss-Jordan  elimination  to  this  matrix  to  solve  the  problem,  but  actually 
there  is  no  need.  The  ﬁrst  row  of  the matrix  gives  the  equation 
(39)	
2X1  + X2  = 2. 
The  second  row  of  the matrix  gives  the  equation 
(40)	
2X1  + X2  = 4. 
But  the  left-hand  side  of  each  equation  is  the  same.  So,  if  there  were  any  solutions  we  would  have, 
by  transitivity  of  equality,  that  2  =  4,  which  is  absurd.  Therefore  we  conclude  that  there  are  no 
solutions  to  this  system  of  equations. 
Problem  4(15  points)  Over  the  ﬁeld  F2  =  Z/2Z  =  {0, 1},  the  two-element  ﬁeld,  consider  the 
2 .  Deﬁne  the  two  vectors  v1 , v2  ∈ V  by 
vector  space  V  = F3 
⎛ ⎞ 
⎛ ⎞ 
1
0 
v1  = ⎝  0  ⎠ , v2  = ⎝  1  ⎠ . 
1
0 
⎛ ⎞ 
v = ⎝  b  ⎠ , 
a 
c 
such  that  v1 , v2 , v  forms  a  basis  for  V .  Please  show  all work  and  explain  your  answer,  but  you  need 
not  give  a  100%  rigorous  proof. 
(Hint:  There  are  exactly  four  such  vectors.) 

(41)	
Write  down  all  vectors  v ∈ V , 

(42)	

It  is  actually  simpler  (conceptually  anyway)  to  try  to  ﬁnd  the  vectors  w  such  that  v1 , v2 , w 
DOESN’T  form  a  basis.  The  complement  of  this  set  in  V  will  be  the  set  of  vectors  we  are  looking 
for. 

Since  the  dimension  of  V  is  3,  a  theorem  from  the  book  tells  us  that  any  set  of  3  linearly 
independent  vectors  is  a  basis  for  V .  So  v1 , v2 , w  fails  to  be  a  basis  precisely  when  v1 , v2 , w  are 
linearly  dependent.  On  the  other  hand,  clearly  v1 , v2  are  linearly  independent  (this  is  “visible” 
by  observing  that  each  has  a  zero  coordinate  where  the  other  has  a  coordinate  equal  to  one).  By 
another  theorem  from  the  book,  adding  a  new  vector  to  a  linearly  independent  set  of  vectors  gives 

EXAM  1 

7 

a  linearly dependent  set  if and only  if  the new vector  is  in  the  span of  the other vectors.  So  v1 , v2 , w 
is  linearly  dependent  iﬀ  w  is  in  the  span  of  v1 , v2 ,  i.e.  ⎛
⎞ 
w = c1v1  + c2v2  = ⎝  c2  ⎠ , 
c1 
(43)	
c1 
for  some  pair  of  scalars  c1 , c2  ∈  F2 .  Another  way  to  characterize  this  set  is  as  the  collection  of 
vectors  whose  ﬁrst  coordinate  is  equal  to  the  third  coordinate.  So  the  complement  of  this  set  is 
the  collection  of  vectors  whose  ﬁrst  coordinate  is  diﬀerent  from  the  third  coordinate.  So  the  set  of 
⎛ ⎞ 
⎛ ⎞ 
⎛ ⎞ 
⎛ ⎞ 
vectors  v  for  which  v1 , v2 , v  is  a  basis  for  V  is  the  following  set: 
1
1
0
0 
v1  = ⎝  0  ⎠ , v2  = ⎝  1  ⎠ , v3  = ⎝  0  ⎠ , v4  = ⎝  1  ⎠ . 
0
0
1
0 
Problem  5(30 points) (a)(10 points) Given an m × n matrix A over a ﬁeld F, deﬁne the nul lspace 
of  A. 
The  nullspace  of  A  is  deﬁned  to  be  the  set  of  vectors  X  ∈ Fn  such  that  AX  = 0. 

(44)	

The  nullspace  has  basis 

In  each  of  the  following  parts,  state  whether  the  sentence  is  true  or  false.  If  the  sentence  is  false, 
provide  a  counterexample  (and  say  a  word  or  two  about  why  the  counterexample  works).  If  the 
sentence is true, explain why it is true.  Your arguments should be convincing, but need not be 100% 
rigorous. 
(b)(10  points)  If  U  is  an  invertible  n × n matrix,  then  the  nullspace  of  A  equals  the  nullspace  of 
AU . 
This  is  false.  Consider  the  2 × 2 matrix 
A = 
(45)	

� 
� 
1 0
. 
0 0 
�  � 
0
X  = 1 
. 
� 
� 
On  the  other  hand,  consider  the  invertible matrix  (which  is  even  an  elementary matrix): 
0 1
. 
1 0	
� 
� 
0 1
. 
0 0	
�  � 
1
Y  = 0 
. 
Since  Y  is  not  contained  in  the  span  of  X ,  we  conclude  that  the  nullspace  of  A  does  not  equal  the 
nullspace  of  AU . 
(c)(10  points)  If U  is  an  invertible m × m matrix,  then  the  nullspace  of A  equals  the  nullspace  of 
U A. 

The  product  of  A  and  U  is  the matrix 

The  nullspace  of  AU  has  basis 

(46)	

(47) 

(48) 

(49)	

U  = 

AU  = 

8 

EXAM  1 

This  is  true.  For  one  thing,  this  is  a  theorem  in  the  text  (which  you  are  allowed  to  quote without 
citing  the  speciﬁc  number).  If  we  wanted,  this  is  also  quite  simple  to  prove:  we  will  prove  that  the 
nullspace  of  U A  contains  the  nullspace  of  A  and  vice  versa. 

To  see  that  the nullspace  of U A  contains  the nullspace  of A,  observe  that  for  any  vector X  in  the 
nullspace  of  A,  we  have 
(50) 
(U A)X  = U (AX ) = U 0 = 0. 
The  ﬁrst  step  follows  from  associativity  of  matrix  multiplication,  and  the  second  step  follows  from 
the  assumption  that  X  is  in  the  nullspace  of  A.  We  conclude  that  X  is  in  the  nullspace  of  U A. 
Therfore  the  nullspace  of  U A  contains  the  nullspace  of  A.  The  same  argument  works  in  reverse  by 
−1 (U A),  so  A  is  the  product  of  U A  with  an  invertible matrix  on  the  left.  We 
observing  that  A = U 
conclude  that  the  nullspace  of  A  equals  the  nullspace  of  U A. 

Extra  Credit(5  points)  Only  attempt  this  if  you  have  solved  all  of  the  other  problems  and 
checked  your  answers.  Prove  that  for  an  m × n  matrix  A  and  an  n × p  matrix  B ,  the  rank  of  AB 
is  at  most  the  minimum  of  the  rank  of  A  and  the  rank  of  B .  Please  justify  your  answer,  but  your 
argument  need  not  be  100%  rigorous. 

A theorem proved in lecture is that, with the notation from above, the rowspace of AB  is contained 
in  the  rowspace  of  B .  Therefore  the  dimension  of  the  rowspace  of  AB  is  at  most  the  dimension  of 
the  rowspace  of  B ,  i.e.  rank(AB ) ≤  rank(B ).  It  is  also  a  theorem  from  the  book  that  the  rank  of 
a  matrix  is  the  same  as  the  rank  of  its  transpose,  in  particular  rank(AB )  =  rank(AB )† .  Another 
theorem  from  the  book  (just  a  simple  calculation  really),  shows  that  the  transpose  of  a  product  of 
matrices  equals  the  product  of  the  transposes  in  the  opposite  order,  in  particular  (AB )†  =  B
†
† . 
A
† ) ≤ rank(A
†
† ).  And 
Now again using the ﬁrst theorem mentioned above, we conclude that rank(B
A
† ) =  rank(A).  Putting 
once  again  using  the  second  theorem mentioned  above, we  know  that  rank(A
the  pieces  together,  we  have 
† ) ≤ rank(A
†
† ) = rank(A). 
rank(AB ) = rank(AB )†  = rank(B
(51) 
A
So  we  have  both  inequalities  rank(AB )  ≤  rank(B )  and  also  rank(AB )  ≤  rank(A).  So  the  rank  of 
AB  is  at most  the minimum  of  the  rank  of  A  and  the  rank  of  B . 

Have  a  great  weekend,  see  you  next  week! 

