18.700 LECTURE  NOTES,  11/12/04 

Contents 

1.  Resultants 
2.  Discriminants 
3.  A  fact  about  polynomials 
4.  The  fundamental  theorem   of  algebra  
5.  The  regular  matrices  are   dense 
6.  The  Cayley­Hamilton   Theorem 

1 
3 
5 
6 
7 
8 

1.  Resultants 
Let  F  be  a  ﬁeld,  let  d, e   ≥  0  be  integers,  and  let  f (x), g(x)  ∈  F[x] be   polynomials 
of  degrees  ≤  d  and   ≤  e  respectively  (possibly  the  zero  polynomial).  How  can   one 
tell  if   f  and  g  have  a  common  factor  of  positive  degree? 

Proposition  1.1.  Assume  either  f  is  nonzero  of  degree  d or  g is  nonzero  of  degree 
e.   There  is  a  common  factor  of  f  and  g of  positive  degree  iﬀ  there  exist  polynomials 
f1  and  g1  of  degrees  < e and  < d respectively,  not  both  zero,  such  that  f · g1  + g · f1  = 
0. 
Proof.  (⇒) Suppose  there  exists  a   common  factor  of  positive  degree,  say  h.  Denote 
c  =  deg(h)  >  0.  Let  f1  = (f /h)  and  g1  =  −(g/h).  Then  f1  and  g1  have   degrees 
< d and   < e  respectively,   and, 
f · g1  +  g · f1  =  −x c−1 (f g/h) +  x c−1 (f g/h) = 0. 
Since  at  least  one  of  f  and   g  is  nonzero,  at  least  one  of  f1  and  g1  is  nonzero. 
(⇐)  Let  f1  and   g1  be  polynomials  of  degrees  <  d  and  <  e  respectively  such  that 
f · g1  +  g · f1  =  0,  i.e.,  f · g1  =  −g · f1 ,  and  which  are  not  both  zero.  At   least  one 
of  f  and   g  is  nonzero  of  degree   d,  resp.  e;  without  loss  of  generality,  assume  f  is 
nonzero  of  degree  d.  If  g1  is  nonzero,  then  f  · g1  is  nonzero,  which  implies  f1  is 
nonzero. Since  at  least  one  of  f1 , g1  is  nonzero,  it  follows  that  f1  is  nonzero. 
Consider  the  irreducible  decomposition   of  f ,  say  f  =  p d1  · · · pdr  where  p1 , . . . , pr
are  non­proportional   irreducible   polynomials  of  positive   degree  and  d1  · deg(p1 ) + 
r 
1 
+ dr  · deg(pr ) =  d.   For  every  i = 1, . . . , r ,  let  ci  ≥ 0  be   the  greatest  integer  such 
· · ·
that  p ci  divides  f1 . Then  c1  · deg(p1 ) + 
+  cr  · deg(pr ) ≤  deg(f1 ) <  d.  So  there 
· · ·
exists  1  ≤  i  ≤  r  such   that  ci  <  di .  Then  pi  divides  (f /pi  ) · g1  =  −g · (f1 /pi  ), 
i 
ci 
ci
but  pi  does  not  divide  (f1 /pi  ).  Therefore   pi  divides  g ,  i.e.,  pi  is  a  polynomial  of 
ci
� 
positive  degree  that  factors   both  f  and  g . 

This  suggests  a  linear  algebra  solution  to  this  polynomial  problem. 
1 

Deﬁnition  1.2.  Let  f , g  ∈  F[x] be  polynomials  of  degrees  ≤  d  and  ≤  e.  Let 
P d−1  ⊕ P e−1  be  the  F­vector  space   of  pairs  (f1 , g1 )  of  polynomials  of  degrees  < d 
and   < e  respectively,   where  (f1 , g1 ) +  (f2 , g2 ) = (f1  + f2 , g1  + g2 ) and   a · (f1 , g1 ) = 
(a · f1 , a · g1 ).  The  associated  linear  transformation   is  T(f ,d),(g ,e)  :  P d−1  ⊕ P e−1  → 
P d+e−1  ,  T(f ,d),(g ,e) (f1 , g1 ) =  f · g1  +  g · f1 ,  where   P d+e−1  is  the  F­vector  space   of 
polynomials  of  degree  d +  e − 1. 
Corollary  1.3.  Assume  either  f  is  nonzero  of  degree  d  or  g  is  nonzero  of  degree 
e.   The  polynomials  f  and  g  have  a  common factor  of  positive  degree  iﬀ   T(f ,d),(g ,e) 
has  nul lity   ≥ 1. 
Proof.  The  kernel  of  T(f ,d),(g ,e)  is  the  set  of  pairs  (f1 , g1 ) such  that  f · g1  + g · f1  = 0. 
By  Proposition   1.1,   there   exists  such  a   pair  iﬀ  f  and  g  have   a  common  factor  of 
� 
positive  degree.  
Let  B  denote  the  ordered   basis  of  P d−1  ⊕ P e−1  , 
B = ((1, 0), (x, 0), . . . , (x d−1  , 0), (0, 1), (0, x), . . . , (0, x e−1 )). 
Let  C  denote  the  ordered   basis  of  P d+e−1  ,  C  =  (1, x, . . . , xd+e−1 ).  Let  f (x) = 
· · ·
· · ·
⎧⎨ 
adxd  +
+  b1x +  b0 .  Denote  by  A(f ,d),(g ,e)
+  a1x +  a0  and  let  g(x) =  bexe  +
the  (d +  e) × (d +  e) matrix, 
1  ≤ i ≤ d,  and  i ≤ j ≤ i +  e, 
A(f ,d),(g ,e) (i, j ) =  ⎩ 
bj−i , 
aj+d−i , d +  1  ≤ i ≤ d +  e,  and  i − d ≤ j ≤ i, 
otherwise  
0, 
� 
� 
In   other  words,  A(f ,d),(g ,e)   is  the   partitioned   matrix, 
Bd,(g ,e) 
,
Be,(f ,d) 

A(f , d), (g , e) = 
⎛ 
where  Bd,(g ,e)  is  the  d × (d +  e)  matrix, 
⎜⎜⎜⎝ 
bd−1 
bd 
. . . 
b1 
b0 
. . . 
0 
bd−2 
bd−1 
b0 
. . . 
. . . 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0 
. . . 
b0 
b1 
. . . 
⎛ 
⎞ 
e,(f ,d)  is  the  e × (d +  e)  matrix, 
⎟⎟⎟⎠ 
⎜⎜⎜⎝ 
0 
0 
0 
ad 
. . . 
a0  a1 
. . . 
. . . 
0 
0 
0 
ad−1  ad 
. . . 
. . . 
. . . 
a0 
.

.
.
.
.
.
.
.
.
.
. 
.
.
.
.
. 
.
.
.
.
.
.
. 
.
.
. 
.
.
0 
0
0 
0
a0 
. . . 
. . . 
ad 
. . . 
Lemma  1.4.  The  matrix   representative  [T(f ,d),(g ,e) ]C ,B  is  A(f ,d),(g ,e) . 
Proof.  For  every  i = 1, . . . , d,  T(f ,d),(g ,e) (xi−1  , 0)  =  xi−1 g .  The  coordinate   vector  of 
xi−1 g with   respect  to  C  is  the  vector  of  coeﬃcients.  Thus [xi−1 g ]j  is  the  coeﬃcient 
of  xj−1  in   xi−1 g ,  i.e.,  the  coeﬃcient  of  xj−i  in  g .  This  is  0  unless  0   ≤  j − i ≤  e, 
and   then  it  equals  bj−i . 
Similarly,   for  i =  d + 1, . . . , d + e,  T(f ,d),(g ,e) (0, xi−d−1 ) =  xi−d−1 f .  The   coordinate 
vector  of   xi−d−1 f  with   resepct   to   C  is  the   vector  of  coeﬃcients.  Thus [xi−d−1 f ]j 
2 

0 
be 
.
. 
. 
be+2−d 

be 
be−1 
. 
. 
. 
be+1−d 

and   where  B

⎞ 
⎟⎟⎟⎠ 
,

0 
0 
.

.

.

be 

. . . 
. . . 
.
.
.
. . . 

Bd,(g ,e)  = 

B

e,(f ,d) 

= 

is  the  coeﬃcient  of  xj−1  in   xi−d−1 f ,  i.e.,  the   coeﬃcient  of  xj+d−i  in  f .  This  is  0 
unless  0  ≤ j +  d − i  ≤ d,   and  then  it   equals  aj+d−i . 
� 
Deﬁnition  1.5.  The  resultant  of  (f , d) and  (g , e),  Res((f , d), (g , e))  is  the   determi­
nant  of  the  (d +  e) × (d +  e) matrix  A(f ,d),(g ,e) . 
Theorem  1.6.  Assume  either  f  is  nonzero  of  degree  d  or  g  is  nonzero  of  degree  e. 
The  polynomials  f  and  g  have  a  common  factor  iﬀ  Res((f , d), (g , e))  = 0. 
Proof.  By  Corollary  1.3,   f  and  g  have   a  common  factor  iﬀ  null(T(f ,d),(g ,e) ) >  0.  Of 
course, null(T(f ,d),(g ,e) )  =  null(A(f ,d),(g ,e) ).  Because  A(f ,d),(g ,e)  is  a  square   matrix, 
by  Theorem  2.1.12  it  has   a  nonzero  nullvector  iﬀ  the  determinant  is  0,  i.e.,  iﬀ 
� 
Res((f , d), (g , e))  = 0. 
⎞
⎛ 
Example  1.7.  Let  d  =  e  =  2.  The   resultant  of  f (x) =  a2x2  +  a1x  +  a0  and 
g(x) =  b2x2  +  b1x  +  b0  is,  
Res((f , 2), (g , 2))  =  det  ⎜⎜⎝ 
⎟⎟⎠ , 
i.e.,  
1  − 2b0 b2 ) +  a1 b0 b2  − a1a2 b0 b1  +  a2 b2 
2  − a0a1 b1 b2  +  a0a2 (b2
Res((f , 2), (g , 2))  =  a0 b2
2 
2
2
0 . 
Just  as  a  further  example,  if  f  = (x−1)(x+1)  =  x2−1 and g  = (x−1)2  =  x2−2x+1, 
then   Res((f , 2), (g , 2))  equals,  
12  12  − 1 0 · (−2)  1 +  1 · (−1)((−2)2  − 2 1 · 1)  +  02  1 1 − 0 · (−1)  1 · (−2)  +  (−1)2  12 
·
·
· 
·
·
·
· 
· 
= 1 +  0 − 2 +  0 +  0 +  1  = 0. 
Also,   if   f  = (x  − 1)(x  +  1)  =   x2  − 1  and  g  =  x  = 0x2  +  1x  +  0,  considered   as  a 
polynomial   of  degree  ≤ 2,  then  Res((f , 2), (g , 2))  equals, 
12  .  .  .  0 − 1 0 1 0 +  1 · (−1)(11  − 2 0 · 0)  +  02  0 0 − 0 · (−1)  0 · 1 +  (−1)1  01 
·
·
· 
·
·
·
· 
· 
= 0 − 0 +  (−1)  +  0 − 0 +  0  =  −1. 
This  is  nonzero,  as  dictated  by  Theorem  1.6. 

0 
b2 
b1 
b0 
0 
b0 
b2 
b1 
0 
a0  a1  a2 
0 
a0  a1  a2 

2.  Discriminants 
A  particularly  interesting   case   is  when   g  =  f � ,  the  formal  derivative   of  f  with 
respect  to  x. 
Deﬁnition  2.1.  The  formal   derivative  with  respect  to  x  is  the  unique  F­linear 
transformation   d/dx  :  F[x]  →  F[x]  such  that   d/dx(xn ) =  nxn−1  for  every  n  ≥  1, 
and   d/dx(1)  = 0. 
Remark  2.2.  If  F =  R  or  C,  the  formal  derivative   can   be  interpreted   as  the  usual 
derivative  of  the  associated  function,  or  the  holomorphic   derivative  of  the   associ­
ated   holomorphic  function.   But   for  F  a  general  ﬁeld,  e.g.,  Z/pZ,  there  is  no  such 
interpretation.  In   this   case  the   formal  derivative  is  simply  the  linear  transformation 
deﬁned  above. 
Lemma  2.3.  The  formal  derivative  with  respect  to  x  is  the  unique  F­linear  trans­
formation  such  that  both,  
(i)  d/dx(x) = 1,  and, 

3 

(ii)  for  every  f (x),  g(x) ∈ F[x],  d/dx(f  · g) =  f  · d/dx(g) +  g  · d/dx(f ). 
Proof.  First  of  all,   d/dx(x)  = 1x1−1  = 1x0  =  1.  By  linearity,  to  prove   that  
d/dx(f  · g) =  f  · d/dx(g) +  g  · d/dx(f ),  it  suﬃce  to   consider  the   case   that  f  =  xm 
and  g  =  x .  Up   to  relabeling,  assume  that  m  ≤  n. 
If  m  =  0,  then   f  = 1  
n
and   d/dx(f  · g) =  d/dx(g)  = 1   · d/dx(g).  Because  d/dx(f )  =  0,  this  equation  is 
d/dx(f  · g) =  f  · d/dx(g) +  g  · d/dx(f ). 
Therefore  assume  m  ≥  1.  Then  d/dx(xm  xn ) =  d/dx(xm+n ),  which  by  deﬁnition 
·
m+n−1
.  Also  d/dx(f ) =  d/dx(xm ) =  mxm−1  and  d/dx(g) = 
equals  (m  +  n)x
d/dx(xn ) =  nxn−1 .  Therefore, 
d/dx(f ·g) = (m+n)x m+n−1  =  x 
·
·
f ·
d/dx(g)+g ·d/dx(f ).
(mx m−1 ) = 
(nx n−1 )+x 
n
m
→
Next  suppose  that  T  :  F[x] 
F[x]  is  a  linear  transformation  such  that  T (x)  = 1
and   T (f  · g) =  f T (g) +  g T (f )  for  every  f ,  g   ∈ F[x].  First  of  all, 
·
·
T (1)  =  T (1  · 1)  = 1 T (1)  +  1 · T (1)  =  T (1)  +  T (1).
·
By  cancellation,  T (1)  =  0.  The  claim   is  that  for  every  integer  n  ≥  0,  T (xn ) = 
nxn−1 .  This   will  be  proved by induction   on  n.  For  n  = 1,  this  is  the  ﬁrst  hypothesis. 
Therefore  assume  n  >  1  and  the   result  is  true  for  n  − 1.  Then,  by  the   second 
hypothesis, 
·
·
·
T (x n ) =  T (x x  n−1 ) =  x T (x n−1 ) +  x n−1  T (x).
By  the  induction   hypothesis,  T (xn−1 )  = (n  −  1)xn−2  .  By  the   ﬁrst  hypothesis, 
T (x) =  1.  Thus, 

T (x n ) =  x  · (n  − 1)x n−2  +  x n−1  1 =  n x n−1  .
·
·
Therefore  the  claim  is   proved by induction   on   n.  Since  1,  x,  .  .  .  is  a  basis  for  F[x] and 
T (xn ) =  d/dx(xn )  for  every  n  ≥ 0,  the   linear  transformation   T  equals  d/dx. 
� 
Proposition  2.4.  If  f (x),  a  polynomial  of  positive  degree,  factors  as   a  product  of 
linear  factors,   then  f (x)  has  a  repeated  linear  factor  iﬀ  f (x)  and  d/dx(f (x))  have 
a  common  factor  of  degree  d  ≥ 1. 
Proof.  Let  f (x)  = (x   −  λ1 )e1  · · · (x  −  λs )es  ,  where   λ1 , .  .  .  ,  λs  are  distinct  and  
� 
� 
e1 , .  .  .   ,  es  are  positive  integers.  Then, 
s
· 
ei  · (x  − λ1 )ei−1 
f � (x) = 
1≤j≤s,j=i 
i=1 
If  ei  >  1,  then  each  term  in  the  sum   is  divisible  by  (x  − λi )ei−1 .  So   (x  − λi )ei−1  is 
a  common  factor  of  f (x) and  f � (x) of  positive  degree. 
� 
� 
On   the  other  hand,   suppose   each  ei  = 1.  Then  for  every  i  = 1, .  .  .  ,  s,  x − λi  divides, 
(x  − λk ). 
gi  =
�� 
1≤j≤s,j=i  1≤k≤s,k=j 
If   x  − λi  divides  f � (x),  then   x  − λi  divides  the  diﬀerence, 
(x   − λj ), 
f � (x) − gi (x) = 
1≤j≤s,j=i 
which   is  ridiculous.   Therefore   no  x  − λi  divides  f � (x).  Because  the  irreducible 
factors  of   f (x) are  x  − λ1 , .  .  .  ,  x  − λs ,  there  is  no  common  factor  of  f (x) and   f � (x) 
� 
of  positive  degree. 

(x  − λj )ej .

4 

�
�
�
Deﬁnition  2.5.  Let  f (x) be  a   nonzero  polynomial  of  degree  e  ≥ 1.  The  discrimi­
nant of  f  is  Disc(f ) =  Res((f ,  e),  (df /dx,  e  − 1)). 
Theorem  2.6.  Suppose  that  f (x) factors  as  a  product  of  linear  factors.  Then   f (x) 
has  a  repeated  linear  factor   iﬀ  Disc(f ) = 0. 

Proof.  By  Proposition   2.4,   f (x) has  a  repeated  linear  factor  iﬀ  f  and  df /dx  have   a 
common   factor   of  positive  degree.  The   derivative  df /dx  is  a  polynomial  of  degree 
≤ e  − 1.  By  Theorem  1.6,  f  and  df /dx  have   a  common   factor  of  positive   degree  iﬀ 
the  resultant  Res((f ,  e),  (df /dx,  e  − 1))  is  zero. 
� 
Example  2.7.  Let  f (x) =  ax2  +  bx  +  c  where  a  =�
0.  Then  f � (x) = 2ax  +  b  and 
⎛ 
⎞
the  discriminant  of  f  is  Res((f ,  2),  (f � ,  1)), 
0 ⎠ , 
det ⎝ b 
b 
a 
c
2a  
2a  
0 
b 

i.e.,  

Disc(f ) =  c(4a  2 ) − b(2ba   − ba) =  a(4ac  − b2 ). 
Because  a  is  nonzero,  it  is  customary  to   factor  it  from   the  expression,  giving   (−1/a) · 
Disc(f ) =  b2  − 4ac,  familiar  from  the   quadratic   formula. 
Example  2.8.  Let  f (x)  be  a  cubic  polynomial.  If  2  and  3  are   diﬀerent  from  0  in  
F,   there  is  a  straightforward  linear  change  of  coordinates  xnew   =  µxold  − λ  so  that 
f (x)  has  the  form,  
f (x) =  x 3  +  ax   +  b. 
⎛ 
⎞
The  derivative  of   f (x) is  f � (x) = 3x2+a.  So   the  discriminant  of  f  is  Res((f ,  3),  (f � ,  2)), 
⎜⎜⎜⎜⎝ 
⎟⎟⎟⎟⎠ 
0 
1 
0 
b  a 
1 
b  a  0 
0 
0 
0 
3 
0 
a 
, 
0  a 
0 
3 
0 
3 
0  a  0 
0 
i.e.,   expanding  down  the  ﬁrst  column, 
Disc(f ) = 27b2  +  4a . 
3
In  particular,  if  f (x) = (x  − 1)2 (x  +  2)  =  x3  − 3x   +  2,  the  discriminant  is  27(2)2  + 
4(−3)3  =  108  − 108  =  0.  And   for  f (x) = (x  − 1)(x   + 1)x  =  x3  − x,  the   discriminant 
is  27(0)2  +  4(−1)3  =  −4.  Observe  that  −4  equals  0   in  F  iﬀ  2  =  0,  i.e.,  −1  = 1,  in  
which   case  f (x) = (x  − 1)2x  does  have   a   repeated   linear  factor. 

det 

3.  A  fact   about  polynomials 
Let  x1 , .  .  .  ,  xn  be  variables,  and  let  f (x1 , .  .  .  ,  xn )  ∈  F[x1 , .  .  .  ,  xn ] be   a  polynomial 
→
F  by  substituting   in  for 
in   n  variables.  There  is  an  associated   function  f  :  Fn 
the  variables.  
Proposition  3.1.  If  F  has  inﬁnitely  many  elements,  e.g.,  F  =  R  or  F  =  C,  then 
for  every  integer  n  ≥  0,  the   only  polynomial   in  n  variables  that  gives  rise  to  the 
zero  function  is  the  zero  polynomial. 

5 

Proof.  For  n  =  0,  f  =  a  for  some   a  ∈  F.  The  associated  function  sends  the  
singleton set  F0  =  {∗}  to  a  ∈  F. Thus  f  is  the   zero   function  iﬀ  a  = 0  iﬀ  f  is  the  
zero  polynomial. 
Next  assume  n  =  1.  Clearly  the   zero  polynomial  gives  the   zero  function.  Assume 
· · · 
+  a1x +  a0  where  d ≥  0  and 
f (x)  is  a  nonzero  polynomial,  i.e.,  f (x) =  adxd  +
ad  =�
0.  Since  deg(f ) =  d,   f (x)  has  at  most  d distinct  linear  factors,  so   at  most  d 
distinct  roots.  But  F  has  inﬁnitely  many  elements.  Therefore  there  exists  λ  ∈  F 
such that  f (λ) =�
0. 
By  way  of  induction,  assume  n  >  1  and   assume  the  result  is  known   for  n −  1. 
Clearly  the  zero  polynomial  gives  rise   to   the   zero   function. Thus  assume   f  is  not 
� 
the  zero  polynomial.  The  polynomial  f  can  be  expanded   as, 
d
fi (x1 , . . . , xn−1 )xn , 
f (x1 , . . . , xn ) = 
i
i=0 
where  d  ≥  0  and   fd (x1 , . . . , xn−1 )  is  a  nonzero   polynomial.  By  the   induction 
hypothesis,  there  exist  λ1 , . . . , λn−1  ∈  F  such  that  fd (λ1 , . . . , λn−1 ) =  0.  Consider 
� 
the  polynomial, 
d
i 
aix . 
i=0 
By  construction,   ad  =�
0.  So  by  the  same   argument  as  in   the  last   paragraph,  there 
exists  λ  ∈  F  such   that  g(λ)  =  0.  Therefore  f (λ1 , . . . , λn−1 , λ)  =  0,  which  proves 
� 
the  proposition   by  induction   on  n. 
Corollary  3.2.  Let  f (x) ∈ Z[x1 , . . . , xn ]  be  a  polynomial   with  integer  coeﬃcients. 
→
C  is  the  zero 
Then  f (x)  is  the  zero  polynomial  iﬀ  the  associated  function  f  :  Cn 
function. 

g(x) =  f (λ1 , . . . , λn−1 , x) = 

Proof.  Thinking  of   integers  as  just  special  complex  numbers,  the  polynomial  f 
is  also  a  polynomial   in  C[x1 , . . . , xn ].  For  every  n­tuple  of  nonnegative  integers, 
e0 , . . . , en  ≥ 0,  the  coeﬃcient  of  x e0 
· · · 
xen  in  f  is  0  considered  as  an  integer  iﬀ   it  is 
n
0 
0  considered  as  a  complex  number. Therefore  f  is  0  as  an  element  in   Z[x1 , . . . , xn ] 
iﬀ   f  is  0  as  an  element  in  C[x1 , . . . , xn ].  By  Proposition  3.1,  f  is  0  as  an  element 
� 
in   C[x1 , . . . , xn ]  iﬀ   f  is  0  as  a  function. 

4.  The  fundamental  theorem  of  algebra 
Deﬁnition  4.1.  A  ﬁeld  F  is  algebraical ly   closed  if  every  nonconstant   polynomial 
in   F[x] factors  as  a  product  of  linear  polynomials. 
Theorem  4.2  (The  fundamental  theorem  of  algebra).  The  ﬁeld  of  complex  numbers 
is  algebraical ly  closed. 

This  is  an   important  theorem  about  the  complex  numbers.  Every  proof  has  some  
element  of  analysis.  A  simple  proof,  given  in  most  complex  analysis  courses,  is 
the  following:  Let  f (z ) be  a  polynomial  of  degree  d  ≥  1  with  no  zero.  Then 
1/f (z )  is  a  holomorphic   function   on   C.  For  every  �  >  0,  there   exists  R  > 
max(1, a0 /2d, . . . , ad−1 /2d, (2/� ad )1/d ) such that  if  z ≥ R,  then  f (z ) =
|
|
|
|
|
|
| | 
|
|
|ad ||z | |1+ 
d
| ≥ |
≥ 1/�.  Therefore  the   restriction   of  1/(f (z
|
+  . . .
Rd/2
))  to  the   circle  
ad−1 /z
ad
=  R}  has  maximum  modulus  ≤ �.  By  the  Maximum  Modulus  Theorem, 
{z  ∈ C||z | 
6 

�
�
�
1/ f (z ) ≤ � on  the  interior.   Since   this  holds  for  every  � > 0,  it  follows  that  1/ f (z )
|
|
| 
|
is  identically  zero  on   all  of  C,  which  is  absurd.  This  contradiction  proves  that  f (z ) 
has  a  zero.  Factoring  the  corresponding  linear  polynomial  from  f (z ) and  repeating, 
it  follows  that  f (z ) is  a  product  of  linear  polynomials. 

5.  The  regular  matrices  are   dense 
Deﬁnition  5.1.  Let  n ≥ 1 be  an  integer.  An  n×n matrix  A ∈ Matn×n (F) is  regular 
if  cA (X ) factors  as  (X − λ1 )
n )  for  distinct  elements  λ1 , . . . , λn  ∈ F.
· · · (X − λ

Let  (ai,j   1  ≤  i, j ≤ n)  be  n2  variables.  Let  f  ∈ F[ai,j  1  ≤  i, j ≤ n] be   a  polynomial 
|
|
in  these  variables.  This  deﬁnes  a   function  f  :  Matn×n (F) → F. 
Proposition  5.2.  If  F  is  inﬁnite  and  algebraical ly  closed,  and  if  f  is  zero  on  al l 
regular  matrices,  then  f  is  the  zero  polynomial. 
Proof.  Let  A be  any  n×n matrix.  Because   F is  inﬁnite,  there  exist  distinct  elements 
� 
λ1 , . . . , λn  ∈ F.  Let  B  be  the  matrix, 

B (i, j ) = 

λi ,
0,

i =  j 
i =�
j

, 

i.e.,   B  is  the  diagonal  matrix  with  entries  λ1 , . . . , λn .  Let  t  be   a  variable  and  
consider  the  matrix  At  with   entries  in  F[t]  given  by, 
At  =  tB  +  (1  − t)A. 
Denote  by  cAt  (X )  the  polynomial  det(X In  − At ),  which  is  a  polynomial  in  F[t, x]. 
This  polynomial   is, 
cAt  (X ) =  X n  − trace(At )X n−1  +  · · · +  (−1)det  (At ). 
In  particular,  for  every  value  of  t,  this  is  a  nonzero  polynomial  of  degree   n.  By 
Theorem  2.6,  cAt  (X ) has  a  repeated  linear  factor  iﬀ  Disc(cAt  (X ))  is  zero.  Now  the 
entries  of  cAt  (X )  are  linear  polynomials  in   t,  thus  Disc(cAt  (X ))  is  a  polynomial  of 
degree  at  most  n in   t.  Moreover,  for  t = 1,  cAt  (X ) =  cB (X ) = (X − λ1 )
· · · (X − λ
n )
has  distinct  linear  factors.  So,  by  Theorem  2.6,  Disc(cAt  (X ))  is  nonzero  when   t = 1. 
Hence  Disc(cAt  (X ))  is  not  the   zero  polynomial  in  t.  Therefore   the  polynomial 
Disc(cAt  (X ))  has  at  most  n  distinct  zeros.  So  there   are   at  most  n  distinct  values 
of  t  for  which  At  is  not  regular,  i.e.,  At  is  regular  for  inﬁnitely  many  values  of  t. 

Consider  the  polynomial  in  t,  f (At ).  Unless  f (At )  is  the  zero   polynomial  in  t, 
there  are  only  ﬁnitely  many  values  of  t  where  f (At )  =  0.  By  the   last  paragraph, 
there  are  inﬁnitely  many  values  of  t where  At  is  regular.  By  hypothesis,  f (At ) = 0 
whenever  At  is  regular.  So  f (At )  =  0  for  inﬁnitely  many  t,  proving  f (At )  is  the  
zero  polynomial   in  t. 

In  particular,   plugging  in   t = 0,  A0  =  A  and  f (A)  is  the  value  of   f (At )  at  t  =  0. 
Since  f (At )  is   the  zero  polynomial,  f (A)  =  0.  Since   this  holds  for  every  n × n 
� 
matrix  A,  by  Proposition   3.1,  f  is  the   zero   polynomial. 

7 

6.  The  Cayley­Hamilton  Theorem 

The  Cayley­Hamilton   theorem  is  the   key  to  proving   Jordan   canonical  form. For  any 
particular  matrix,  the   Cayley­Hamilton  theorem  can  be   veriﬁed   directly.  However, 
the  abstract   proof  that  works  for  all  matrices  at  once  is  rather  involved,  using  all 
the  ideas   we  have  developed  to   this  point.  The  idea  is  to  interpret  the   Cayley­
Hamilton   theorem  as  a  sequence   of  identities  of   polynomials  in  the  variables  (ai,j ) 
with   integer  coeﬃcients.   By  Corollary  3.2,  it   then  suﬃces  to   prove  these  identities 
are  true  as  identities   of  functions  on  Matn×n (C).  By  the  fundamental  theorem   of 
algebra  and  Proposition  17,   it  then  suﬃces  to   prove  these  identities  of  functions  on 
the  set  of  regular   matrices.   This  follows  immediately  from  what  we  know  about  
diagonal  matrices.  

→
V   be  a  linear  operator  on  a  ﬁnite­dimensional  F­vector 
Lemma  6.1.  Let  T  :  V
space.  If  T  is  diagonalizable,   then  cT  (T )  is  the  zero  operator. 
Proof.  Let  B  be  an  ordered   basis  for  T  with  respect  to  which   the   matrix  represen­
� 
tation   is  diagonal,  say 

i =  j, 
λi ,
A = [T ]B,B (i, j ) = 
i =  j 
0,
Then   cA (X )  = (X − λ1 )
· · · (X − λ
n ).  Because  A  is  a  diagonal  matrix,  it  follows 
easily  by  induction   on  deg(f )  that  for  any  polynomial  f (x)  ∈  F[x],  f (A) is  the 
� 
diagonal  matrix, 
i =  j, 
f (λi ),
f (A)(i, j ) = 
i =  j 
0,
For  every  i = 1, . . . , n,  cA (λi ) =  0,  because  X − λi  is  a   factor  of  cA (X ).  Therefore  
� 
cA (A) = 0.  Of  course [cT  (T )]B,B  =  cA (A),  thus  cT  (T ) is  the   zero  operator. 
→
Corollary  6.2.  Let  T 
V  be  a  linear  operator   that  is  regular,  i.e.,  every 
:  V
matrix   representation  is  regular.  Then  cT  (T )  is  the  zero  operator. 
� 
Proof.  As  proved  in  lecture,   every  regular  linear  operator  is  diagonalizable. 
Deﬁnition  6.3.  Let  n ≥ 1  be  an  integer  and  let  (ai,j  1  ≤ i, j ≤ n) be  n2  variables. 
|
Let  A  be  the  n × n  matrix  with  entries  in  Z[ai,j  1  ≤
|
i, j ≤  n],  A(i, j ) =  ai,j .  The
generic  characteristic   polynomial  is  the  polynomial  cA (X ) := det(X In−A),  which is 
a  polynomial  with  integer   coeﬃcients  in   the  variables  (ai,j  1  ≤ i, j ≤ n) and X .  The 
|
generic  Cayley­Hamilton  matrix  is  the  matrix  with  entries  in   Z[ai,j  1  ≤  i, j ≤  n],
|
cA (A). 
Theorem  6.4   (The  Cayley­Hamilton   theorem).  Every  entry  of  the  generic  Cayley­
Hamilton  matrix   is  0. 
Proof.  Every  entry  is  a  polynomial  in  the   variables  (ai,j  1  ≤  i, j ≤  n).  By  Corol­
|
lary  3.2,  it  suﬃces  to  prove   every  entry  gives  the  zero  function  on  Matn×n (C).  By 
Theorem  4.2,   the   fundamental  theorem  of   algebra,  C  satisﬁes  the  hypotheses  of 
Proposition   5.2.  So   by  Proposition  5.2,  it  suﬃces  to  prove  that  for  every   regular  
matrix  B  ∈  Matn×n (C),  the  value  of  the  entry  on   B  is  0.  Substituting  in  the 
entries  of  B  for  the  ai,j   s,  the  value  of  cA (A)  is  cB (B ).  By  Corollary  6.2,  cB (B ) 
is  the  zero  matrix,   i.e.,   every  entry  is  zero.  Therefore   every  entry  of   the   generic  
� 
Cayley­Hamilton  matrix  is  0.  

8 

�
�
Corollary  6.5.  For  every  ﬁeld  F,  not  necessarily  algebraical ly  closed   or  inﬁnite, 
for  every  integer  n  ≥ 1,  for  every  matrix  B  ∈ Matn×n (F),  cB (B ) is  the  zero  matrix. 
→ 
V  on  a  ﬁnite­dimensional  F­vector 
Therefore,  for  every   linear  operator  T  :  V
space,  cT  (T ) = 0. 
Proof.  The  operation  of  multiplications,  additions  and  subtractions  computing  cB (B ) 
is  exactly  the  same  as  the   operation  of  multiplications,  additions  and  subtractions 
computing  the  generic  Cayley­Hamilton  matrix,  but  with   the  entries  of  B  substi­
tuted   for  the  variables  (ai,j   ).  Therefore  cB (B )  is  obtained  from   cA (A)  by  substi­
tuting  in  the  entries   of  B  for  the  variables  (ai,j  ).  By  Theorem   6.4,  every  entry   of 
cA (A) is   the  zero  polynomial.   Therefore  the  value   on   B  is  0,  i.e.,  cB (B ) is  the  zero  
� 
matris.  

9 

