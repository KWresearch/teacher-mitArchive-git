18.700 LECTURE  NOTES,  11/10/04 

Contents 

1.  Direct  sum  decompositions 
2.  Generalized  eigenspaces 
3.  The  Chinese  remainder  theorem  
4.  Linear  independence  of  generalized  eigenspaces 

1 
3 
5 
8 

1.  Direct   sum  decompositions 

if  it  is  linearly 

Deﬁnition   1.1.  Let  V  be  an  F­vector  space.  Let  W  = (W1 , . . . , Ws ) be  an   ordered 
s­tuple  of  vector  subspaces  of  V . 
(i)  An  ordered  s­tuple  of  vectors  in   W ,  (w1 , . . . , ws ),  is  an  ordered  s­tuple  of 
vectors  in  V  such   that  for  every  i = 1, . . . , s,  wi  ∈ Wi . 
(ii) The  ordered   s­tuple  W  is  linearly  independent  if  the  only  ordered  s­tuple 
of  vectors   in  W ,  (w1 , . . . , ws ),  satisfying  w1  +
· · ·
+  ws  =  0  is  (0, . . . , 0).
(iii)  The  ordered   s­tuple  W  spans  V ,  or  is  spanning,  if  for  every  vector  v  ∈ V , 
there  exists   an  ordered   s­tuple  of   vectors  in   W ,  (w1 , . . . , ws ),  satisfying 
· · ·
w1  +
+  ws  =  v.
(iv)  The  ordered  s­tuple  W  is  a   direct  sum  decomposition  of  V 
independent  and   spans  V . 
Remark   1.2.  Please  do  not  confuse  the  notion   of  linearly  independent  (resp.  span­
ning)  collection  of  subspaces   of  V  with  the   notion   of  linearly  independent  (resp. 
spanning)  collection   of  vectors  in   V .  These  are  closely  related,  but  diﬀerent. 
Proposition  1.3.  An  ordered  s­tuple  of  subspaces   of  V ,  W  = (W1 , . . . , Ws ),  is 
linearly  independent  iﬀ   for  every   ordered  s­tuple  (B1 , . . . , Bs )  of  linearly  indepen­
dent  ordered  subsets  Bi  ⊂  Wi ,  the  concatenation  B  =  B1  ∪  · · · ∪ Bs  is  a  linearly 
independent  ordered  subset  of  V . 
Proof.  (⇒)  Assume  W  is  linearly  independent.  For  every  i = 1, . . . , s,  deﬁne   ei  = 
#Bi ,  which  is  0  if  Bi  is  empty.  Deﬁne  e  =  e1  +
· · · 
+  es .  For  every  i  = 1, . . . , s 
such   that  ei  > 0,  denote  Bi  = (w(i,1) , . . . , w(i,ei ) ).  If  e = 0,  then  B =  ∅.  Otherwise, 
by  deﬁnition,  B  is  the  unique  ordered  set  of  vectors  B = (v1 , . . . , ve )  such  that  for 
every  i = 1, . . . , s  with   ei  > 0  and   every  1  ≤ j ≤ ei ,  ve1+···+ei−ei+j  =  w(i,j ) . 
If   B =  ∅,  it  is  vacuously  linearly  independent.  Thus  assume   e > 0.  Let  (c1 , . . . , ce ) 
be  a  linear  relation  among  B .  For  every  i  = 1, . . . , s  with  ei  >  0  and  every  1  ≤
j ≤  ei ,  deﬁne  c(i,j )  =  c
+ei−ei+j .  If  ei  =  0,  deﬁne  wi  =  0.  If  ei  >  0,  deﬁne  
e1+···
� 
�  � 
� 
· · ·
+  c(i,ei )w(i,ei ) .  Then, 
wi  =  c(i,1)w(i,1)   +
ei
e
s
(  
ckwk  =
ce1+···+ei−ei+j we1+···+ei−ei+j ) = 
wi . 
1≤i≤s,ei>0  j=1 
i=1 
k=1 

0  = 

1 

wi . 

0  = 

is  a  vector  subspace,  wi  ∈  Wi ,  i.e., 
For  every  i  = 1, . . . , s,   because  Wi  ⊂  V 
(w1 , . . . , ws )  is  an  ordered  s­tuple  of  vectors  in  W .  Because  W  is  linearly  inde­
· · · 
· · · 
=  ws  =  0.  So   for  every 
pendent  and   because  w1  +
+  ws  =  0,  w1  =
i  = 1, . . . , s  with   ei  >  0,  (c(i,1) , . . . , c(i,ei ) )  is  a  linear  relation  among   Bi .  By  hy­
pothesis,  Bi  is  linearly  independent.  Therefore   c(i,1)  =
· · · 
=  c(i,ei )  =  0.  So  for 
every  i  = 1, . . . , s  such that  ei  >  0,  and  for  every  1  ≤  j ≤  ei ,  c(i,j )  =  0,  i.e.,  the  
linear  relation   (c1 , . . . , ce )  is  (0, . . . , 0).  Since  the  only  linear  relation   among  B  is 
the  trivial  linear  relation,   it  is  linearly  independent. 
(⇐)  Assume   that  for   every  ordered  s­tuple  (B1 , . . . , Bs )  of  linearly  independent 
ordered subset  Bi  ⊂ Wi ,  the  concatenation   B =  B1 ∪· · ·∪Bs  is  a  linearly  independent 
ordered subset  of  V .  Let  (w1 , . . . , ws ) be  an   ordered  s­tuple  of  elements  in  W .  For 
every  i = 1, . . . , s,  if   wi  =  0,  deﬁne   Bi  =  ∅,  otherwise   deﬁne  Bi  = (wi ).  Each  Bi  ⊂
|
Wi  is  a  linearly  independent  subset.  By  construction,  B = (wi 1  ≤  i ≤ s, wi  =  0). 
By  hypothesis,  this  is   linearly  independent.  If  B  is  nonempty,  then, 
�  � 
s
wi  = 
wi∈B 
i=1 
Therefore  (1, . . . , 1)  is  a  nontrivial  linear  relation  among   B .  This  contradiction  
proves   that  B  is  empty,  i.e.,   w1  =
=  ws  =  0.  Therefore  W  is  linearly  indepen­
· · · 
� 
dent. 
Proposition  1.4.  An  ordered  s­tuple  of  subspaces   of  V ,  W  = (W1 , . . . , Ws ),  spans 
V  iﬀ  for  every  ordered  s­tuple  (B1 , . . . , Bs )  of  spanning  subsets  Bi  ⊂ Wi ,  the  union 
B =  B1  ∪ · · · ∪ B
s  is  a  spanning  set  for  V .
Proof.  (⇒) Assume  W  spans  V .  Let  v  ∈ V  be   any vector.  If  v  =  0,  by  convention 
it  is  in   span(B) (even   if  B =  ∅).  Assume  v  =  0.  Because  W  is  spanning,  there   exists 
an   ordered   s­tuple  of  vectors  in  W ,  (w1 , . . . , ws ),  such  that   v  =  w1  + + ws .  For 
· · ·
every  i  = 1, . . . , s  with   wi  =  0,  because   Bi  is  a  spanning  set  for  Wi ,  there   exists 
ei  >  0,  vectors  w(i,1) , . . . , w(i,ei )  ∈  Bi  and  scalars  c(i,1) , . . . , c(i,ei )  ∈  F  such  that 
· · ·
+  c(i,ei )w(i,ei ) .  Then  the  following  collection  of  vectors  is  a 
wi  =  c(i,1)w(i,1)   +
collection  in   B , 
(w(i,j ) 1  ≤ i ≤ s, wi  =  0, 1  ≤ j ≤ ei ). 
|
For  the  choice  of  coeﬃcients,  
(c(i,j ) 1  ≤ i ≤ s, wi  =  0, 1  ≤ j ≤ ei ), 
|
� 
the  linear  combination   of  these  vectors  in  B  is, 
1  ≤ i ≤ s, wi  =  0wi  =  v. 

So  v  ∈ span(B). 
(⇐)  Assume  that  for   every  ordered  s­tuple  (B1 , . . . , Bs )  of  spanning  subsets  Bi  ⊂
Wi ,  the  union   B  is  a  spanning  set  for  V .  For  i = 1, . . . , s,  deﬁne  Bi  =  Wi .  This  is 
certainly a  spanning subset  of  Wi .  By  hypothesis,  B = 
W1  ∪ · · · ∪ W
s  is  a   spanning 
set  for  V . 
Let  v  ∈ V  be  a  vector.  If  v  =  0,  then  (0, . . . , 0)  is  an  ordered  s­tuple  of  vectors  in  
· · ·
+  0.  Therefore  assume  v  =  0.  Because   B  is  a  spanning 
W  such   that  v  =  0  +
2 

�
�
�
�
�
�
�
set  for  V ,  exists  an   integer   e  >  0,  nonzero  vectors  v1 , . . . , ve  ∈  B ,  and   scalars 
c1 , . . . , ce  ∈ F  such that, 
· · ·
+  ceve .
v  =  c1v1  +
For  every  j = 1, . . . , e,  let  1  ≤ i(j ) ≤ s  be   an   integer  such   that  vj  ∈ Wi(j ) ;  because 
B =  W1  ∪ · · · ∪ W
s ,  there  is  at  least  one  such   i(j ).  For  every  i,  if  i(j ) =�
j =  1, . . . , e,   deﬁne  wi  =  0.  Otherwise  deﬁne, � 
i for  every 
wi  = 
cj vj . 
1≤j≤e,i(j )=i 
For  every  i  =  1, . . . , s,  because   Wi  ⊂  V 
is  a  vector  subspace,  wi  ∈  Wi . 
s� 
s�  � 
e� 
(w1 , . . . , ws )  is  an  ordered  s­tuple  of  vectors  in  W .  And, 
cj vj  = 
( 
wi . 
i=1 
i=1  j,i(j )=i 
j=1 

cj vj ) = 

Thus 

v  = 

Therefore  W  spans  V . 
� 
Proposition  1.5.  An  ordered  s­tuple  of  subspaces  of  V ,  W  = (W1 , . . . , Ws ),  is  a 
direct  sum  decomposition  of   V  iﬀ  for  every   ordered  s­tuple  (B1 , . . . , Bs )  of  bases  Bi 
for  Wi ,  the  union  B =  B1  ∪ · · · ∪ Bs  is  a  basis   for  V . 
Proof.  This  follows  from  Proposition  1.3  and  Proposition  1.4. 

� 

2.  Generalized  eigenspaces 
Deﬁnition   2.1.  Let  T  :  V  → V  be   a   linear  operator. For  every  integer  n ≥ 0,  the 
→
nth  iterate  of   T  is  the  linear  operator  T n  :  V
V  recursively  deﬁned   by  T 0  = IdV  ,
◦
· · ·
+  a1x +  a0  ∈  F[x],
and  T n+1  =  T T n  .  For  every  polynomial  f (x) =  anxn  +
V ,  is  deﬁned   to  be   f (T ) =  anT n  + +· · ·
→ 
the  associated  linear  operator,  f (T ) :  V
a1T +  a0 IdV  . 
(i)  For  every  pair  of  polynomials  f (x), g(x) ∈ F[x],  f (T ) + 
Proposition  2.2. 
g(T ) = (f +  g)(T ). 
(ii)  For  every  polynomial  f (x)  ∈  F[x]  and  every   scalar  a  ∈  F,  a ·  (f (T ))  = 
(a · f )(T ). 
(iii)  For  every  pair  of   polynomials  f (x), g(x) ∈ F[x],  f (T ) ◦ g(T ) = (f · g)(T ) = 
g(T ) ◦ f (T ). 
n  + · · ·
n  + · · ·
Proof.  (i) Let  f (x) =  anx
+ a1x + a0  and  let   g(x) =  bnx
+ b1x + b0 .  By 
deﬁnition  of  polynomial  addition,  f + g = (an  + bn )xn  + · · · + (a1  + b1 )x + (a0  + b0 ). 
By  deﬁnition   of  the  associated  linear  operator, 
(f +  g)(T ) = (an  +  bn )T n  +  · · · +  (a1  +  b1 )T +  (a0  +  b0 )IdV  . 
By  associativity  and  commutativity   of  addition  of  operators,  and  by  distributivity  
of  scalar  multiplication  of  operators  and   addition  of  operators,  this  equals, 
· · ·
· · ·
(anT n  +
+  a1T +  a0 IdV  ) +  (bnT n  +
+  b1T +  b0 IdV  ).
By  deﬁnition  of  the   associated   linear  operator,  this  is  f (T ) + g(T ),  i.e.,  (f + g)(T ) = 
f (T ) +  g(T ). 
n  + · · ·
(ii)  Let  f (x) =  anx
+ a1x+ a0 .  By  deﬁnition   of  scalar  product   of  polynomials, 
a · f (x)  = (aan )xn  +  · · · +  (aa1 )x +  (aa0 ).  By  deﬁnition  of  the   associated   linear 
operator, 
(a · f )(T ) = (aan )T n  +  · · · +  (aa1 )T +  (aa0 )IdV  . 
3 

By  distributivity  of  multiplication   of  scalars  and   scalar  multiplication  of   operators, 
this  is, 
a · (anT n ) +  . . . a · (a1T ) +  a · (a0 IdV  ). 
By  distributivity  of  scalar  multiplication  of  operators  and  operator  addition,  this 
is,  
a · (anT n  +
· · ·
+  a1T +  a0 IdV  ).
By  deﬁnition  of  the  associated  linear  operator,  this  is  a · (f (T )),  i.e.,  (a · f )(T ) = 
a · (f (T )). 
(iii)  The  claim  is  that  for  every  pair  of  integers  m, n ≥ 0,  T m  T n  =  T m+n .  This 
◦
◦
T n  = IdV  ◦ T n  = 
is  proved   by  induction   on  m.  For  m = 0,  T 0  = IdV  so  that  T 0
.  Thus  assume  m  >  0  and   assume  the  result  is  true   for  smaller  values 
T n  = 
T 0+n 
of  m.  By  the  recursive  deﬁnition,  T m  T n  = (T ◦ T m−1 ) ◦ T n .  By  associativity  of 
◦
composition  of  linear  transformations,  (T ◦ T m−1 ) ◦ T n  =  T ◦ (T m−1  T n ).  By  the  
◦
induction   hypothesis,   T m−1  ◦ T n  =  T m−1+n  .  Therefore  T m  T n  = 
◦ 
◦
T T m−1+n  .
By  the  recursive  deﬁnition,  T m+n  =  T  ◦  T m−1+n ,  i.e.,  T m
◦ 
T n  = 
. So 
T m+n
m  +  n  =  n  +  m, 
the  claim  is  proved   by  induction   on   m. 
In  particular,  since  
T m  T n  =  T m+n  =  T n+m  =  T n  ◦ T m  .
◦
· · · 
· · · 
Let  f (x) =  amxm  +
+  b1x +  b0 .  Let 
+  a1x +  a0  and  let  g(x) =  bnxn  +
N  =  max(m, n).  For  i > m,  deﬁne  ai  =  0.  For  j > n,  deﬁne   bj  =  0.  By  deﬁnition  
� � 
of  polynomial  multiplication,  
N
k
(f · g)(x) =
ai bk−i )x . 
( 
k
k=0  i=0 
� � 
By  deﬁnition  of  the   associated  linear  operator, 
n
k
(f · g)(T ) =
ai bk−i )T k  . 
( 
k=0  i=0 
◦
By  the  claim  above,   T k  =  T i  T k−i  .  Together  with  commuativity,  associativity 
� � 
� � 
and  distributivity,  the   formula  above   is, 
n
k
n
k
◦
(aiT i ) ◦ (bk−iT k−i )).
ai bk−iT i  T k−i ) = 
( 
(
k=0  i=0 
k=0  i=0 
� 
� 
By  distributivity  of  addition  and   composition  of  linear  transformations,  this  is, 
m
n
aiT i ) ◦ ( 
( 
bj T j ). 
j=0 
i=0 
By  the  deﬁnition  of  the  associated  linear  operator,  this  is  f (T )◦g(T ),  i.e.,  (f ·g)(T ) = 
f (T ) ◦ g(T ). 
� 
Deﬁnition   2.3.  For  every  linear  operator  T  :  V  → V  and  every  polynomial  f (x) ∈
F[x],  the  associated  generalized  eigenspace  is  ET ,f  = ker(f (T )).  For  every  λ  ∈  F 
and   every  integer  n ≥ 0,  the  nth  generalized  λ­eigenspace  is  E (n)  =  ET ,(x−λ)n  .
T ,λ  
(i)  For  every  pair  of  polynomials   f (x), g(x) ∈ F[x],  ET ,f  ∩
Proposition  2.4. 
ET ,g  ⊂ ET ,f +g . 
(ii)  For  every  polynomial  f (x)  ∈  F[x]  and  every  scalar  a  ∈  F,  ET ,f  ⊂  ET ,a·f . 
If  a =�
0,  then  ET ,f  =  ET ,a·f . 

4 

(iii)  For  every  pair  of   polynomials   f (x), g(x) ∈ F[x],  ET ,f  +  ET ,g  ⊂ ET ,f ·g . 
Proof.  (i) For  every  v  ∈ ET ,f ∩ET ,g ,  f (T )(v) =  g(T )(v) =  0.  By  Proposition  2.2(i), 
(f  +  g)(T )(v) =  f (T )(v) +  g(T )(v) =  0  +  0  =  0.  Therefore  v  ∈  ET ,f +g ,  i.e., 
ET ,f  ∩ ET ,g  ⊂ ET ,f +g . 
(ii)  For  every  v  ∈ ET ,f ,  f (T )(v) =  0.  By  Proposition  2.2(ii), 
(a · f )(T )(v) =  a · (f (T )(v))  =  a · 0  =  0. 
Therefore  v  ∈  ET ,a·f ,  i.e.,   ET ,f  ⊂  ET ,a·f .  If  a  =  0,  then  f  =  a−1  · (a · f )  so  that 
also  ET ,f  ⊂ ET ,a·f ,  i.e.,  ET ,f  =  ET ,a·f . 
(iii)  For  every  v  ∈ ET ,f ,  f (T )(v) =  0.  By  Proposition  2.2(iii), 
(f · g)(T )(v) = (g · f )(T )(v) = (g(T ) ◦ f (T ))(v) =  g(T )(f (T )(v))  =  g(T )(0) =  0. 
So  v  ∈ ET ,f ·g ,  i.e.,  ET ,f  ⊂ ET ,f ·g .  By  the  same  argument,  ET ,g  ⊂ ET ,f ·g .  Because 
ET ,f ·g  is  a  vector  subspace,   ET ,f  +  ET ,g  ⊂ ET ,f ·g . 
� 
T ,λ   ⊂  · · · ⊂ V . 
T ,λ   ⊂ E (2) 
T ,λ   ⊂ E (1) 
Corollary  2.5.  For  every  λ ∈ F,  {0} = 
E (0) 
Proof.  For  every  integer   n,  by Proposition  2.4(iii),  E (n)  =  ET ,(x−λ)n  ⊂ ET ,(x−λ)n+1  = 
T ,λ  
� 
E (n+1)  
. 
T ,λ 
Deﬁnition  2.6.  For  every  λ ∈ F,  the  generalized  λ­eigenspace  is  E (∞)  = 
∪n≥0E (n) 
T ,λ .
T ,λ  
Proposition  2.7.  For  every  λ ∈ F,  the  generalized  λ­eigenspace  E (∞)  is  a  vector 
T ,λ  
subspace   of  V . 
T ,λ   ,  the  subset   E (∞)  is  nonempty.  For  every 
T ,λ  ⊂  E (∞)
Proof.  Because  {0}  = 
E (0) 
T ,λ  
v, w  ∈  E (∞) 
T ,λ  ,  there  exist  integers  m, n  ≥  0  such  that  v  ∈  E (m)  and  w  ∈  E (n) 
T ,λ . 
T ,λ  
T ,λ   ⊂ E (N ) .  Since  E (N )  is  a  vector 
T ,λ   , E (n) 
Let  N  =  max(m, n).  By  Corollary  2.5,  E (m) 
T ,λ  
T ,λ  
T ,λ   ,  i.e.,  E (∞)  is  stable  under  addition  of  elements. 
T ,λ  ⊂  E (∞) 
subspace,  v  +  w  ∈  E (N ) 
T ,λ  
By  a  similar  argument,  E (∞) 
is  stable  under  scalar  multiplication   of   elements.  So 
T ,λ 
E (∞) 
� 
is  a  vector  subspace  of  V . 
T ,λ 

3.  The  Chinese  remainder  theorem 

Ancient  Chinese  generals   discovered  a  beautiful  and   eﬃcient  method   for  counting 
large  numbers   of  soldiers   very  quickly  –  a   task  of  great  importance  in   determining 
the  number  of   losses  after  a  battle.  Let  n1 , . . . , ns  be   integers  such   that  for  every 
1  ≤  i <  j  ≤  s,  the  pair   of  integers  (ni , nj )  have  no  common  factor.  For  every 
i = 1, . . . , s have  the  N  soldiers  line  up  in  rows  of  size   ni .  Do  not  count  the  number 
of  rows!  Instead,   count  the  remainder  ri ,  i.e.,  the  numer  of  soldiers  who  cannot  line 
up   in  a  row  of  size  ni .  There  exist  integers  a1 , . . . , as  depending  only  on  n1 , . . . , ns 
such  that  for  every  integer  N ,  N  is  congruent  to   a1 r1  + · · · + as rs ,  modulo  n1 
· · ·
ns .
If   n1  =  10, n2  =  11, n3  =  13,  the   numbers  are  a1  =  −429, a2  =  650, a3  =  −220,  so  
that  N  =  −429r1 +650r2 −200r3 ,  modulo  1430.  These  may seem  like   large   numbers, 
but  it  is   much  faster  for  a  mathematician   to  compute  the  product  on  an  abacus 
than  to  count  the  soldiers   by  brute  force.  Since   the   general  presumably  knows 
the  number  of  soldiers  to  within   a  range  of  ±715,  this  method  allows  the   general 
to  compute  precisely  the  number  of  soldiers  very  quickly.  We  will  use  the   same  
5 

�
method   to  prove  that  for  distinct  λ1 , . . . , λs  ∈ F,  the  ordered   s­tuple  of  generalized 
eigenspaces,  W  = (E (∞)  , . . . , E (∞)  ),  is  linearly  independent. 
T ,λs 
T ,λ1
Deﬁnition   3.1.  For  an  s­tuple  of  polynomials  in  F[x],  (f1 , . . . , fs ),  not  all  zero, 
a  greatest  common  factor  is  a  polynomial  f (x)  of   maximal  degree   such  that  f (x) 
· · · 
divides  fi (x) for  every  i  = 1, . . . , s.  If  f1  =
=  fs  =  0,  the   greatest  common 
factor  is  deﬁned  to  be   0.  An   s­tuple  of  polynomials  (f1 , . . . , fs )  is  coprime  if  there  
g1  · f1  + 
s  ·
· · ·
exist  polynomials  (g1 , . . . , gs ) such  that  1  = 
+  g
fs .
Theorem  3.2  (The  Chinese  remainder  theorem).  An  s­tuple  of  polynomials   (f1 , . . . , fs ) 
is  coprime   iﬀ  1  is  a  greatest  common  factor. 
Proof.  (⇒) Assume  (f1 , . . . , fs ) is  coprime,  i.e.,  1  =  g1  f1  +
· · ·
s  ·
·
+  g
fs .  Let  f  be  
any  common  factor   of  (f1 , . . . , fs ).  For  every  i,  because   fi  is  divisible  by  f ,  also 
gi · fi  is  divisible  by  f .  Because   every  gi · fi  is  divisible  by  f ,  the   sum   g1 · f1 + . . . g fs
s  ·
is  divisible  by  f ,  i.e.,  1  is  divisible   by  f .  But  the   only  polynomials  dividing  1  are  
nonzero  constants.  Thus  1  is  a  greatest  common   factor  of  (f1 , . . . , fs ). 
(⇐) Assume  1 is  a  greatest  common   factor  of   f1 , . . . , fs .  The  claim  is  that  (f1 , . . . , fs ) 
is  coprime.  This   will  be   proved  by  induction  on  s.  If  s  =  1,  then   f1  is  a   greatest 
common   factor  of  (f1 ).  Since  also  1  is  a  greatest  common  factor,  deg(f1 ) =  0,  i.e., 
f1  is  a  scalar.  By  deﬁnition,   the  greatest   common   factor  of  (0)   is  0,  so   f1  is  nonzero. 
·
Since  F  is  a  ﬁeld,   there  exists  a   scalar  g1  such  that  1  =  g1  f1 ,  i.e.,  (f1 ) is  coprime. 
Next  assume  s =  2.  If  necessary,  permute   f1  and  f2  so  that  deg(f1 ) ≥ deg(f2 ).  By 
hypothesis,  1  is  a  greatest common  factor  of  (f1 , f2 ).  If  f2  = 0,  then  1  is  a   greatest 
common   factor  of   (f1 )  and   by  the  last  case  there  exists  g1  such  that  1  =  g1  · f1 . 
Putting  g2  = 0,  1  =  g1  · f1  +  g2  · f2 ,  i.e.,  (f1 , f2 )  is  coprime.  Therefore  assume 
f2  =�
0,  which   implies   f1  =  0.  The  claim  in  this  case  will  be  proved  by  induction   on 
deg(f2 ).  If  deg(f2 ) =  0,   i.e.,   if  f2  is  a  nonzero  scalar,  there   exists  a  nonzero   scalar 
Setting  g1  = 0,  1   =  g1  · f1  +  g2  · f2 .  Thus,  by  way  of 
g2  · f2 . 
g2  such   that  1  = 
induction,  assume  deg(f2 ) > 0  and  assume  the   result  is  known   for  smaller  values  of 
deg(f2 ). 
By  the  division  algorithm,  there   exist  polynomials  q(x)  and  r(x)  with  deg(r)  < 
deg(f2 )  such  that  f1 (x) =  q(x)f2 (x) +  r(x). 
If  h  is  a   common  factor  of  (f2 , r), 
then   h is  also  a  factor  of  qf2  and so   of  the   sum   qf2  +  r ,  i.e.,  h is  a   common  factor 
of  (f1 , f2 ).  Conversely,   if   h  is  a  common  factor  of  (f1 , f2 ),  then  h  is  a  common 
factor  of  −qf2  and  so  of  the  sum  f1  +  (−qf2 ),  i.e.,  h is  a  common  factor  of  (f2 , r). 
So  the  common   factors   of  (f1 , f2 )  are  precisely  the   common  factors  of   (f2 , r).  By 
hypothesis,  1  is  a  greatest  common  factor  of  (f2 , r).  Since  deg(r)  <  0,  by  the 
induction   hypothesis  there  exist  polynomials  g1 , g � such  that  1  =  g1 f2  + g1 r .  Deﬁne 
�
�
�
g1  =  g � and  g2  =  g1  − qg2 .  Then, 
2 
�
�
2 
g1  f1  +  g2  f2  =  g2 (qf2  +  r) +  (g1  − qg2 )f2  =  g1 f2  +  g �
·
·
�
�
�
�
2 r  = 1.
So  (f1 , f2 )  is  coprime.   So  the  claim  is  proved  by  induction   on  deg(f2 )  if  s = 2. 
By  way  of  induction,  assume  s > 2  and   the  result  is  known  for  smaller  s.  Let  h be  
a  greatest common  factor  of  (f1 , . . . , fs−1 ).  Then  1  is  a  greatest  common  factor  of 
(f1 /h, dots, fs−1 /h).  By  the  induction  hypothesis,  there  exist  elements  g1 , . . . , g �
�
s−1 
such   that, 
· · ·
1 =  g � · (f1 /h) + 
s−1  · (fs−1 /h),
+  g �
1 
6 

�
i.e.,  

fj . 

·
· · ·
h =  g � f1  +
+  g �
s−1 fs−1 .
1 
The  greatest  common  factor  of  (f1 , . . . , fs−1 , fs )  is  the  greatest  common  factor 
of  (h, fs ).  By  hypothesis,   1  is  a   greatest  common  factor  of  (h, fs ).  By  the   case 
1  h +  g ��

1  , g �� such  that   1  =   g ��
s  =  2  above,  there  exist  elements  g ��
2  fs .  Deﬁning  
2
s−1  and  deﬁning  gs  =  g ��
1  g �
2 , . . . , gs−1  =  g ��
1  g �
1 , g2  =  g ��
1  g �
g1  =  g ��
2  ,

· · ·
· · ·
1  h + g ��
s−1 fs−1 ) + g ��
+ gs−1 fs−1  + gs fs  =  g ��
2  fs  =  g ��
�
+ g �
2  fs  = 1.
g1 f1  +
1  (g1 f1  +
� 
So  (f1 , . . . , fs )  is  coprime.   The   theorem  is  proved  by  induction   on   s. 
Corollary  3.3.  Let  s  ≥  2  and  let  (f1 , . . . , fs )  be  nonzero  polynomials.  For   every 
� 
i = 1, . . . , s,  deﬁne,  
ri (x) = 
fj (x). 
1≤j≤s,j=i 
If  for  every  1  ≤ i < j ≤ s,  1  is  a  greatest  common  factor  of  (fi , fj ),  then  (r1 , . . . , rs ) 
is  coprime. 
Proof.  By  Theorem  3.2,  it  suﬃces  to  prove  that  1   is  a  greatest   common  factor  of 
(r1 , . . . , rs ).  This  will  be  proved  by  induction  on  s.  For  s  = 2,  (r1 , r2 )  = (f2 , f1 ). 
By  hypothesis,  1  is  a  greatest  common   factor. Thus,  by way  of   induction,  assume  
s  >  2  and  the  result  is   known   for  smaller  values  of  s.  Consider  the   sequence 
� 
(f1 , . . . , fs−1 ).  The  hypothesis  holds  for  this  collection.  For  every  i = 1, . . . , s − 1, 
denote 
� = 
ri
1≤j≤s−1,j=i 
�
�
By  the  induction   hypothesis,   1  is  a  common factor  of  (r1 , . . . , rs−1 ).  Therefore  fs 
is  a  greatest  common   factor  of  (fs r1 , . . . , fs r �
�
s−1 ) = (r1 , . . . , rs−1 ).  So  the   common  
· · ·
factors  of   (r1 , . . . , rs−1 , rs )  are  the   common  factors  of  (fs , rs )  = (fs , f1 
fs−1 ).
· · ·
Let  h be  an  irreducible  common  factor  of  (fs , f1 
fs−1 ).  Because  h is  irreducible 
· · ·
fs−1 ,  there  exists  1   ≤ i ≤ s − 1  such  that   h divides  fi ,  i.e.,  h is  a 
and  divides   f1 
common  factor   of  (fi , fs ).  By  hypothesis,  1  is  a   greatest  common  factor  of  (fi , fs ). 
· · ·
Therefore  h is  a  nonzero  scalar. So  for  every   common factor  h  of  (fs , f1 
fs−1 ),
every  irreducible  factor   of  h is  a  nonzero  scalars,  i.e.,  h is  a  nonzero  scalar.  Therefore  
1  is  a  greatest common  factor  of  (r1 , . . . , rs ).  The   corollary  is  proved  by  induction 
� 
on  s. 
Corollary   3.4.  For  every  s­tuple  of  distinct  scalars  λ1 , . . . , λs  ∈  F  and  every 
� 
integer  n > 0,  for  every   i = 1, . . . , s  deﬁne, 
ri (x) = 
1≤j≤s,j=i 

(x − λj )n  . 

Then  (r1 , . . . , rs )  is  coprime.  
Proof.  For  every  1  ≤  i  <  j  ≤  n,  1   is  a  greatest   common   factor  of  (x − λi )n  and 
(x − λj )n .  To  see  this,  ﬁrst  observe, 
1  = (x − λi )/(λj  − λi ) +  (x − λj )/(λi  − λj ). 
�� �
� 
� 
Raising  both   sides  to  the  power  2n − 1  and  using   the  binomial  theorem  gives, 
��2n−1 2n−1  (−1)i (x − λi )2n−1−i (x − λj )i−n /(λj  − λi )2n−1 
� 
�
� 
n−1 2n−1  (−1)i (x − λi )n−1−i (x − λj )i /(λj  − λi )2n−1 
(x − λi )n 
1  = 1n  = 
i=1 
i 
(x − λj )n  .
+ 
i=n
i 
7 

�
�
�
In  other  words,   ((x − λi )n  , (x − λj )n ) is  coprime.  By  the  easy  part  of  Theorem  3.2, 
� 
1  is  a  greatest  common  factor. 

4.  Linear  independence   of  generalized  eigenspaces 
Let  s ≥ 2  be  an  integer.   Let  λ1 , . . . , λs  ∈ F be   distinct,  and   let  n > 0  be  an   integer. 
Deﬁne  r1 , . . . , rs  as  in   Corollary  3.4.  By  Corollary   3.4,  there   exist  polynomials 
g1 , . . . , gs  such   that, 

· · ·
1 =  g1 (x)r1 (x) + 
+  gs (x)rs (x).
→
Let  T 
V   be  a  linear   operator.  The   identity  above  gives  an  identity  of 
:  V
associated  linear  operators, 
IdV  = 1(T ) =   g1 (T ) ◦ r1 (T ) + 
+  gs (T ) ◦ rs (T ).
· · ·
Lemma  4.1.  For  every  1  ≤  i ≤  s  and  every  wi  ∈ E (n)  ,  for  every  1  ≤  j ≤  s  with
T ,λi
j =�
i,  rj (T )(wi ) =  0. 
Proof.  Because  j =  i,  by  construction  rj (x) =  q(x)(x − λi )n  for  some  q(x) ∈ F[x], 
� 
speciﬁcally, 
1≤k≤s,k=i,j 
By  Proposition  2.2(iii),   rj (T ) =  q(T ) ◦ (T − λi IdV  )n .  By  hypothesis,  wi  ∈ E (n) 
:=
T ,λi 
ker((T − λi IdV  )n ).  Thus  rj (T )(wi ) =  q(T )((T − λi IdV  )n (wi ))  =  q(T )(0) =  0.  � 
�
Lemma  4.2.  For  every  i = 1, . . . , s  and  every  wi  ∈ E (n) 
,  gi (T ) ◦ ri (T )(vi ) =  vi . 
T ,λi
j=1  gi (T ) ◦ ri (T )(wi ).  By  Lemma   4.1, 
s
Proof.  By  the  identity,   wi  =  IdV  (wi ) = 
for  every  j =  i,  gi (T ) ◦ ri (T )(wi ) =  gi (T )(ri (T )(wi ))  =  gi (T )(0) =  0. 
� 

(x − λk )n  . 

q(x) = 

Denote  by  W  the  ordered   s­tuple   of   vector  subspaces  of  V ,  W  = (E (n)  , . . . , E (n)  )
T ,λs 
T ,λ1
Proposition  4.3.  For  every  ordered  s­tuple  of  vectors  in  W ,  (w1 , . . . , ws ),  denot­
+  ws ,  for  every  i = 1, . . . , s,  wi  =  gi (T ) ◦ ri (T )(v).
· · ·
ing  v  =  w1  +
Proof.  Because  gi (T ) ◦ ri (T )  is  a   linear  operator, 
� 
� 
s
s
gi (T ) ◦ ri (T )(v) =  gi (T ) ◦ ri (T )(  wj ) = 
gi (T ) ◦ ri (T )(wj ). 
j=1 
j=1 
By  Lemma  4.1,   if  j =  i then   gi (T ) ◦ ri (T )(wj ) =  gi (T )(ri (T )(wj ) =  gi (T )(0) =  0. 
Therefore, 
gi (T ) ◦ ri (T )(v) =  gi (T ) ◦ ri (T )(wi ). 
By  Lemma  4.2,  gi (T ) ◦ ri (T )(wi ) =  wi .  Therefore   gi (T ) ◦ ri (T )(v) =  wi . 
� 
Theorem  4.4.  Let  T  :  V  →  V  be  a  linear  operator. For  every  integer  s  ≥  1  and 
every  ordered  s­tuple  of  distinct  scalars   (λ1 , . . . , λs ),  the  ordered  s­tuple  of  vector 
subspaces  W  = (E (∞)  , . . . , E (∞)  )  is  linearly  independent. 
T ,λs 
T ,λ1
8 

�
�
�
�
Proof.  If  s =  1,  this  is  trivial:  for  any subspace  W  of  V ,  (W ) is  linearly  independent. 
Thus  assume  s  ≥  2.  Let  (w1 , . . . , ws )  be  an  ordered  s­tuple  of  vectors  in  W  such 
that  0  =  w1  + · · · + ws .  By  deﬁnition,  for  every  i = 1, . . . , s,  E (∞)  =  ∪n≥0E (n)  .  So 
T ,λi 
T ,λi 
for  every  i = 1, . . . , s,  there  exists  an  integer  ni  > 0 such   that  wi  ∈ E (ni )  . Deﬁne  
T ,λi 
n  =  max(n1 , . . . , ns ).  By  Corollary  2.5,  for  every  i  = 1, . . . , s,  wi  ∈  E (n) 
.  By 
T ,λi 
Proposition  4.3,   for  every  i = 1, . . . , s, 
wi  =  gi (T ) ◦ ri (T )(0) =  0. 
Since  (w1 , . . . , ws ) = (0, . . . , 0),  W  is  linearly  independent. 

� 

9 

