18.700 LECTURE  NOTES,  11/15/04 

Contents 

1.  The  generalized  eigenspace  decomposition 
2.  Operators  commuting with  T 
3.  The  semisimple­nilpotent  decomposition 
4.  Jordan  normal  form 

1 
2 
5 
7 

1.  The  generalized   eigenspace  decomposition 

→
V   be  a  linear  operator  on  a   ﬁnite  dimensional  vector  space.  Denote  the  
Let  T  :  V
· · ·
factorization   of   cT  (X ) by  f1 (X )e1 
fs (X )es  ,  where  every  fi (X ) is  an  irreducible, 
monic  polynomial  of  degree  di  > 0, 
· · ·
fi (X ) =  X di  +  ai,di−1X di−1  +
+  ai,1X +  ai,0 ,
where  f1 , . . . , fs  are  distinct  and  where   e1 , . . . , es  are  positive   integers. 
Remark  1.1.  The  case  we  are  most  interested  in   is  where  each   fi (X ) =  X − λi ,  for 
distinct  elements  λ1 , . . . , λs  ∈ F.  This  is  always  the   case  if the   ﬁeld  F is  algebraically 
closed,  e.g.,  if  F =  C. 
Lemma  1.2.  If  s  ≥  2,  then  for  every  1  ≤  i < j ≤  2,  the  polynomials   f ei  and  f
ej
i
j 
are   coprime.  
Proof.  Because  fi  and fj  are  irreducible,  the  only factors  of  each are  scalar  multiples 
of  1  and   fi ,  resp.  1  and  fj .  Since  fi  =�
fj  and  each  are  monic,  they  are  not  propor­
tional  (i.e.,  they  are  linearly  independent).  Therefore   1   is  a  greatest  common  factor 
of  fi  and  fj .  By  the  Chinese  Remainder  Theorem,  Theorem  3.2  from  11/10/04, 
there  exist  polynomials   gi , gj  such  that  1  =  gi fi  +  gj fj .  Let  e  =  maxei ,ej .  Then 
12e−1  = (gi fi  +  gj fj )2e−1  .  Expanding  the  right­hand­side  using  the  multinomial 
formula,  each term  is   divisible   by f k f 2e−1−k  for  some   0  ≤ k ≤ 2e−1.  At  least  one  of 
i
j 
k , 2e − 1 − k is  at  least  e.   Gathering  terms,  the  equation  is  1  = 12e−1  =  hi f e  + hj f e 
i
j 
,  where   h� =  hi f e−ei  ,
for  some  polynomials   hi , hj .  Therefore  1  =  h�
i f ei  +  h� fj 
ej
j
i
i
i 
h� =  hj f e−ej .  By  the  Chinese   Remainder  Theorem  again,  f ei  and  fj 
ej
are  co­
j
j 
i 
� 
prime.  
Notation   1.3.  For  every  i = 1, . . . , s,  denote   Ei  =  ET ,fi (X )ei  = ker(fi
ei (T )),  as  in 
Deﬁnition   2.3  from  11/10/04.  Denote  W  = (E1 , . . . , Es )  as  in   Deﬁnition   1.1  from 
� 
11/10/04.  If  s ≥ 2,  for  every  i = 1, . . . , s,  denote, 
ri (X ) = 
fj (X )ej , 
1≤j≤s,j=i 

as  in  Corollary  3.3  from  11/10/04. 

1 

�
Proposition  1.4.  If  s  ≥  2,  there  exist  polynomials   g1 (X ), . . . , gs (X )  such  that 
· · ·
1 =  g1 (X )r1 (X ) + 
+  gs (X )rs (X ).
Proof.  This  follows  from   Corollary  3.3  from  11/10/04  and   Lemma   1.2. 

� 

Theorem  1.5.  The  s­tuple  of  subspace  (E1 , . . . , Es )  is  a  direct  sum  decomposition 
of  V .  Moreover,  if  s ≥ 2  then  gi (T ) ◦ ri (T ) :  V
→
V  has  image  Ei  and  is  the  unique 
linear  operator  whose  restriction   to  Ei  is  the  identity  and  whose  restriction  to  Ej 
is  0  for  1  ≤ j ≤ s, j =  i. 
Proof.  First consider  the  case   that  s =  1.  Then  cT  (X ) =  f1 (X )e1 .  By  the   Cayley­
Hamilton   Theorem,   Corollary  6.5  from  11/12/04,  cT  (T ) is  the  zero  operator.  There­
e1 (T ))  is  all   of  V .  So  W  = (V ) is  clearly  a  direct  sum  decomposition  
fore  E1  = ker(f1 
of  V . 
Next  assume  s  ≥  2.  By  the  same  arguments  from  Section  4  from  11/10/04,  the 
restriction  of  gi (T )◦ri (T ) to  Ej  is  the   identity if  j =  i and is  the  zero  transformation 
if  j =  i.   By  the  same  argument  as  in   Theorem  4.4  from  11/10/04,  W  is  linearly 
independent.   The  claim  is  that  Image(gi (T ) ◦ ri (T ))  is  contained  in  Ei .  This  is  the  
ei (T ) ◦ gi (T ) ◦ ri (T )  is  the   zero  linear  operator.  Of  course 
same  as  saying  that  fi
ei  · gi  ri )(T ) = (gi  f ei  ri )(T ) =  gi (T ) ◦ cT  (T ).  By  the  Cayley­Hamilton 
·
·
·
this  is  (fi
theorem,   cT  (T )  is   the  zero  operator,  thus  gi (T ) ◦ cT  (T )  is  the  zero  operator,  so 
i
Image(gi (T ) ◦ ri (T ))  ⊂ Ei . 
Because  1  =  g1  · r1  +
· · · 
s  ·
+  g
rs ,  there  is  an   equation  of  linear  operators  IdV  = 
+  gs (T ) ◦ rs (T ).  For  every  v  ∈ V ,  this  gives, 
g1 (T ) ◦ r1 (T ) + 
· · ·
v  =  g1 (T ) ◦ r1 (T )(v) + 
+  gs (T ) ◦ rs (T )(v).
· · ·
By  the  last  paragraph,  each vi  :=  gi (T )◦ri (T )(v) is  in  Ei .  Therefore   W  is  spanning. 
Since  it  is  both linearly  independent  and spanning,  W  is  a   direct  sum   decomposition. 
To  see  that  gi (T ) ◦ ri (T )  is  the   unique   linear  operator  whose  restriction  to  every 
Ej  is  either  IdEi  if  j =  i  or  else  the  zero  operator  if  j =�
is→
i,  assume  πi  :  V
V 
also  such  an  operator.  Then  πi  − gi (T ) ◦ ri (T ) is  a  linear  operator  whose  restriction  
to  every  Ej  is  the  zero  operator,  i.e.,  ker(πi  − gi (T ) ◦ ri (T ))  contains  Ej  for  every 
j  = 1, . . . , s.   Since  these  subspaces  span   V ,  the  kernel  contains  all  of  V ,  i.e., 
πi  − gi (T ) ◦ ri (T )  is  the  zero  operator.  Therefore  πi  =  gi (T ) ◦ ri (T ). 
� 

2.  Operators  commuting  with  T 
→ 
◦
◦
V  commute  if  T T �  =  T �  T ,  and 
Deﬁnition   2.1.  Linear  operators  T , T �  :  V
the  pair  (T , T � ) is  a  commuting  pair. 
(i)  For  every  commuting  pair  (T1 , T � ) and  every  commuting  pair 
Lemma  2.2. 
(T2 , T � ),  (T1  +  T2 , T � )  is  a  commuting  pair. 
(ii)  For  every  commuting   pair  (T , T � ),  for  every  a ∈ F,  (a ·T , T � ) is  a  commuting 
pair.  
(iii)  For  every commuting  pair  (T1 , T � ) and  every   commuting  pair  (T2 , T � ),  (T2  ◦
T1 , T � )  is  a  commuting  pair. 
(iv)  For  every  commuting  pair  (T , T � )  of  linear  operators   on  V ,  and  for  every 
f (X ) ∈ F[X ],  (f (T ), T � )  is  a  commuting  pair  of  linear  operators   on   V . 
2 

�
�
Proof.  (i)  By  distributivity  of  composition   and  addition,  (T1  +  T2 ) ◦ T �  =  T1  ◦
T �  +  T2  ◦ T � .  By  hypothesis,  T1  ◦ T �  =  T �  ◦ T1  and  T2  ◦ T �  =  T �  T2  so  that 
◦
T1  ◦ T � + T2  ◦ T �  =  T �  T1  + T �  T2 .  By  distributivity  of  composition   and   addition, 
◦
◦
T �  T1  +  T �  ◦ T2  =  T �  ◦ (T1  +  T2 ).  Therefore  (T1  +  T2 ) ◦ T �  =  T �  ◦ (T1  +  T2 ),  i.e., 
◦
+  T2 , T � )  is  a  commuting  pair. 
(T1 
(ii)  By  distributivity  of  scalar  multiplication   and  composition,  (a · T ) ◦ T �  =  a · 
◦ 
T �  =  T �  T ,  thus  a ·  (T
◦
◦
T � ) =  a ·  (T �  T ).  By 
◦
◦
T � ).  By  hypothesis,  T
(T
· (T �  T ) =  T �
◦ (a · T ).
◦
distributivity  of  scalar  multiplication   and  composition,  a
Therefore  (a · T ) ◦ T �  =  T �  ◦ (a · T ),  i.e.,  (a · T , T � ) is  a  commuting  pair. 
(iii) By  associativity  of  composition,  (T1  ◦ T2 ) ◦ T �  =  T1  ◦ (T2  ◦ T � ).  By  hypothesis, 
T2  ◦  T �  =  T �  ◦  T2 ,  so  that  T1  ◦  (T2  ◦  T � ) =  T1  ◦  (T �  T2 ).  By  associativity  of 
◦
composition,  this  is  (T1  ◦  T � ) ◦  T2 .  By  hypothesis,  T1  ◦  T �  =  T �  T1 ,  so   that 
◦
(T1  ◦ T � ) ◦ T2  = (T � ◦ T1 ) ◦ T2 .  By  associativity  of  composition,  this  is 
T � ◦ (T1  ◦ T2 ). 
Therefore  (T1  ◦ T2 ) ◦ T �  =  T �  ◦ (T1  ◦ T2 ),  i.e.,  (T1  ◦ T2 , T � )  is  a  commuting  pair. 
(iv)  The  ﬁrst  claim  is   that  for  every  integer  n  ≥  0,  (T n , T � )  is  a  commuting  pair. 
This  is  proved  by  induction  on  n. For  n  = 0 this  is  trivial  because   T 0  =  IdV  and 
clearly  IdV  ◦ T �  =  T �  =  T �  IdV  .  By  way  of  induction,  assume  n > 0  and   the   result 
◦
is  known  for  n − 1,  i.e.,   (T
n−1 , T � )  is  a  commuting  pair.  By  hypothesis,  (T , T � )  is 
◦
a  commuting  pair.  By  (iii),   (T T n−1 , T � )  is  a   commuting  pair,  i.e.,  (T n , T � )  is  a  
commuting  pair,   proving  the   claim  by  induction  on  n. 
· · ·
· · ·
+  a1T +  a0 IdV  .
Now  let  f (X ) =  anX n  +
+  a1X +  a0 .  Then  f (T ) =  anT n  +
By  the  last  paragraph,  each  (T k , T � ) is  a  commuting   pair.  Repeatedly  applying   (i) 
� 
and   (ii),  (f (T ), T � )  is  a  commuting  pair. 
Proposition  2.3.  For  every  commuting  pair  (T , T � )  of  linear  operators  on  V ,  for 
every  f (X )  ∈  F[X ],  T � (ET ,f (X ) )  ⊂  ET ,f (X ) .  In  particular,  T (ET ,f (X ) )  ⊂  ET ,f (X ) 
because   (T , T )  is  a  commuting  pair. 
Proof.  By  Lemma  2.2(iv),   (f (T ), T � )  is  a  commuting  pair.  Therefore,  for  every 
v  ∈  ET ,f (X ) ,  f (T )(T � (v)  = (f (T ) ◦ T � )(v)  equals  (T �  f (T ))(v) =  T � (f (T )(v)).
◦
Because  v  ∈  ET ,f (X ) ,  f (T )(v) =  0.  Thus  f (T )(T � (v))  =  T � (0) =  0.  Therefore 
T � (v) ∈ ET ,f (X ) ,  i.e.,  T � (ET ,f (X ) ) ⊂ ET ,f (X ) . 
� 
Notation   2.4.  For  every  commuting  pair  (T , T � )  and   every  f (X ) ∈  F[X ],  denote 
(X )  :  ET ,f (X )  →  ET ,f (X )  the  unique  linear  operator  that  agrees  with  the 
�
by  Tf
restriction   of  T �  to  ET ,f (X ) . 

· · ·
The  relevance  is   the   following.  Let  cT  (X ) =  f1 (X )e1 
fs (X )es  .  For  every  i  = 
1, . . . , s,  let  Bi  be  an   ordered  basis  for  Ei  =  ET ,f i
ei  of  size   ni  =  dim(Ei ).  Let 
s .  By  Proposition   1.5  from  11/10/04  and   by  Theorem  1.5,  B  is  an 
B =  B1  ∪ · · · ∪ B
n1  + · · ·
+ ns .  Let  n = (0, n1 , n1  + n2 , . . . , n1  +
ordered   basis  for  V ,  in  particular  n =
· · ·
· · ·
· · ·
+  ni , . . . , n1  +
+  ns )  be  the   partition  of  n  associated   to  n  =  n1  +
+  ns .
Denote  by  A  the  n × n­matrix, [T ]B,B .  For  every  i  = 1, . . . , s,  denote  Ti  =  Tf  ei  , 
and  denote  by  Ai  the  ni  × ni ­matrix, [Ti ]Bi ,Bi . 
i 
Deﬁnition  2.5.  Let  j be  a  partition   of  n.  A  (j , j )­partitioned  n × n­matrix,  B ,  is 
a  diagonal  block  matrix   if  Bi,j  is  the   zero  matrix  unless  i =  j . 
3 

� 
Proposition  2.6.  The  (n, n)­partitioned  matrix  A has  blocks, 
i =  j, 
Ai ,
Ai,j  = 0ni ,nj ,
i =�
j 
In  other  words,   A is  a  diagonal  block  matrix  and  the  diagonal  block  Ai,i  equals  Ai . 
Proof.  For  every  i  = 1, . . . , s  and  every  1  ≤  l  ≤  ni ,  the   (n1  +
· · · 
+  ns ) − ns  +  l
column  of  A is  the  B­coordinate   vector  of  T (vi,l  ),  where   vi,l  is  the   lth  vector  in  Bi . 
By  Proposition   2.3,   T (vi,l )  ∈  E
,  thus  it  is  a  linear  combination  of  vectors  in 
ei 
T ,f i
Bi .  Therefore  the  only  nonzero  entries  of   the  (n1  + + ns ) − ns  + l column  occur 
· · ·
in  the  (i, i)­block.   This  proves  that  Ai,j  = 0ni ,nj  if  i =�
j .  Moreover,  since   Ti  agrees 
ei  ,  the  B­coordinate  vector  of  T (vi,l )  equals  the 
with   the  restriction   of  T  to  ET ,fi
B­coordinate  vector  of  Ti (vi,l ).  This  proves  Ai,i  =  Ai . 
� 
· · ·
Corollary   2.7.  The  characteristic  polynomial   of  T  is  cT  (X ) =  cT1 (X )
cTs  (X ).
· · ·
cAs  (X ),  by 
Proof.  For  a  block  matrix  A  as  above,  clearly  cA (X ) =  cA1 (X )
� 
cofactor  expansion.  
Corollary  2.8.  Assume  cT  (X ) = (X − λ1 )e1 
· · · (X − λ
s )es  .  For   every  i = 1, . . . , s,
cTi (X )  = (X − λi )ei .  In  particular,  dim(E (ei )  ) =  ei ,  the  algebraic  multiplicity  of 
T ,λi
λi . 
Proof.  Let  X − µ be  a  linear  factor  of  cTi  (X ),  i.e.,  µ is  an  eigenvalue  for  Ti .  There 
exists  a  nonzero  µ­eigenvector  v  for  Ti .  The  claim  is  that  for  every  integer  e  ≥  0, 
(Ti  − λi Id)e (v)  = (µ − λi )e  v.  For  e  =  0  this  is  obvious.  By   way  of  induction, 
· 
assume  e  > 0  and  the  result  is  true   for  e − 1.  Then,  by  deﬁnition  of  (Ti  − λi Id)e  , 
(Ti  − λi Id)e (v) = (Ti  − λi Id)e−1 (Ti (v) − λi  v).  By  hypothesis,  Ti (v) =  µ · v.  So  this 
·
is  (Ti  − λi Id)e−1 ((µ − λ) v),  which  by  linearity equals  (µ − λi ) · (Ti  − λi Id)e−1 (v).
·
By  the  induction   hypothesis,   this  is  (µ − λi ) · ((µ − λi )e−1  v).  By  distributivity  of 
·
multiplication   of   scalars  and  scalar  multiplication,  this  is  ((µ − λi )(µ − λi )e−1 ) · v  = 
(µ − λi )e  v.  So  the  claim  is  proved  by  induction   on  e.
·
By  deﬁnition,  (Ti  −  λi Id)ei 
is  the  zero  operator.  Thus,  (µ −  λi )ei  v  = (Ti  −
· 
λi Id)ei (v) =  0.  Since  v  =  0,  (µ − λi )ei  =  0.  Therefore  µ =  λi ,  i.e.,  the   only  linear 
factor  of  cTi  (X )  is  X − λi . 
By  Corollary  2.7,   cTi (X )  factors  cT  (X ),  in  particular  it  is  a  product   of  linear 
factors.  Since  the   only  linear  factor  of  cTi (X ) is  X − λi ,  cTi (X ) = (X − λi )ni ,  where  
cTs  (X ) = (X −λ1 )n1 
· · ·
· · · (X −λ
ni  =  dim(E (ei )  ).  Therefore  cT  (X ) =  cT1 (X )
s )ns  .
T ,λi
Since  also  cT  (X )  = (X − λ1 )e1 
· · · (X − λ
s )es  ,  for  every  i = 1, . . . , s,  ni  =  ei ,  i.e., 
�
dim(E ei 
) equals   the  algebraic  multiplicity  ei . 
T ,λi 
· · ·
Remark  2.9.  It  is  true,  more  generally,  that  if  cT  (X ) =  f1 (X )e1 
fs (X )es  is  the 
the  irreducible  decomposition,  then  for  every  i = 1, . . . , s,  cTi (X ) =  fi (X )ei  .  The  
simplest  proof  uses   “base­change  to   the  algebraic   closure”   and  Corollary  2.8. 
· · ·
f es  .  Let  W  = (E1 , . . . , Es ),
Let  (T , T � )  be  a  commuting  pair.  Let  cT  (X ) =  f e1 
ei  .  Let  f (X ) ∈ F[X ]  and  for  every  i = 1, . . . , s,  deﬁne  WT � ,f (X )  = 
s
1 
where  Ei  =  ET ,f i
(ET � ,f (X ),1 , . . . , ET � ,f (X ),s ) by  ET � ,f (X ),i  =  ET � ,f (X )  ∩ Ei . 
Proposition   2.10.  The  sequence  WT � ,f (X )  is  a  direct  sum  decomposition  of  ET � ,f (X ) . 
4 

�
Proof.  Because  W  is  linearly  independent,  and   because   each   ET � ,f (X ),i  is  contained 
in   Ei ,  also  WT � ,f (X )  is  linearly  independent.  Let  v  ∈  ET � ,f (X ) .  By  Theorem  1.5, 
there  exists  an  ordered  s­tuple  of  vectors  in   W ,  (v1 , . . . , vs )  such  that  v  =  v1  + 
· · ·
+  vs .
The  claim  is  that  for  each  i = 1, . . . , s,  vi  ∈ ET � ,f (X ) .  By  Lemma  2.2(iv),  (T , f (T � )) 
is  a  commuting  pair.  For  every  i = 1, . . . , s,  denote  v� =  f (T � )(vi ).  By  Proposi­
� ∈ Ei .  And, 
i 
tion   2.3,   vi
· · ·
· · ·
· · ·
v� + + v� =  f (T � )(v1 ) + + f (T � )(vs ) =  f (T � )(v1  + + vs ) =  f (T � )(v) =  0,
1 
s
where  the  ﬁrst  inequality  is  the  deﬁnition,  the  second  is  by  linearity,  the   third 
is  by  deﬁnition,  and  the   last  is  the   hypothesis  that  v  ∈  ET � ,f (X ) .  Because   W
=  v� =  0.  Therefore  each  vi  ∈  ET � ,f (X ) ,  i.e., 
· · · 
is  linearly  independent,  v� =
vi  ∈  ET � ,f (X ),i .  So  WT � ,f (X )  spans  ET � ,f (X ) .  Thus  WT � ,f (X )  is  a  direct  sum 
s
1 
� 
decomposition   of  ET � ,f (X ) . 

3.  The  semisimple­nilpotent  decomposition 
→ 
is  nilpotent  of  index  e  if  N e  is 
Deﬁnition   3.1.  A  linear  operator  N 
:  V
V 
→
is  nilpotent  if  there   exists  an  
the  zero  operator.  A   linear  operator  N  :  V
V 
→
integer  e  >  0  such  that  N  is  nilpotent  of  index  e.  A  linear  operator  S  :  V
V 
is  semisimple  if  there  exists   a  ﬁnite  ordered  basis  B  for  V  such  that [S ]
is  a  
B,B
diagonal  matrix.  
→
For  a  linear   operator  T  :  V
V ,  a  semisimple­nilpotent  decomposition  is  a   pair 
(S, N )  of  a  semisimple  and  nilpotent  matrix  such  that  
(i)  T  =  S +  N , 
(ii)  (T , S )  is  a  commuting  pair,  and  
(iii)  (T , N )  is  a  commuting  pair. 
Lemma  3.2.  Let  (S, T )  be  a  commuting  pair  where  S  is  diagonalizable.  Then  for 
every  f (X ) ∈ F[X ],  ET ,f (X )  has   a  basis   of  S ­eigenvectors. 
Proof.  Let  cS (X )  = (X − λ1 )e1 
· · · (X − λ
s )es  .  Choosing   a   basis  with   respect  to 
which   S  is  diagonal,  it  is  clear  that  for  every  i  = 1, . . . , s,  E (ei )  =  ES,λi ,  the  
S,λi 
λi ­eigenspace  of  S .  So  the  sequence   W  = (E (e1 ) , . . . , E (es ) )  is  the  sequence  of
S,λs 
S,λ1
∩
∩ ET ,f (X ) , . . . , E (es ) 
λi ­eigenspaces  for  S .  By  Proposition  2.10,  WT ,f (X )  = (E (e1 ) 
S,λi 
S,λi 
ET ,f (X ) ) is  a  direct  sum  decomposition  of  ET ,f (X ) .  For  every  i = 1, . . . , s,  let  Bi  be  
∩ ET ,f (X ) .  Because  these  vectors  are  contained  in  E (ei )  ,
an  ordered   basis   for  E (ei ) 
S,λi 
S,λi
they  are  λi ­eigenvectors   of   S .  By  Proposition  1.5  from  11/10/04,  the   concatenation  
B =  B1  ∪ · · · ∪ B
s  is  an  ordered   basis  for  ET ,f (X )  consisting  of   eigenvectors  for  S .  �
Theorem  3.3.  Assume  cT  (X ) = (X − λ1 )e1 
· · · (X − λ
s )es  .  There  exists  a  unique 
semisimple­nilpotent  decomposition  (S, N )  for  T . 
Proof.  Existence:  By  Theorem   1.5,  W  = (E (e1 )  , . . . , E (es )  )  is  a   direct  sum   de­
→
T ,λs 
T ,λ1
composition  of  V .  Deﬁne  S  :  V
V  to  be   the   unique   linear  operator  such   that 
is  λi Id.  For  the  basis  B  of
for  every  i  = 1, . . . , s,   the  restriction  of  S  to  E (ei ) 
T ,λi 
Proposition  2.6,   S  is  diagonal. 
For  every  i = 1, . . . , s,   the  restriction  of  T S to  E (ei )  is  Ti  ◦ (λi Id)  =  λ · (Ti  ◦ Id)  = 
◦
T ,λi
λi ·Ti  = (λi Id)◦Ti ,  which is  the  restriction  of  S T  to  E (ei )  .  So  for  every  i = 1, . . . , s,
◦
T ,λi
5 

the  restriction   of  T  ◦ S − S ◦ T  to  E (ei ) 
is  the   zero   operator,  i.e.,  E (ei ) 
is  in  the  
T .  Since  W  spans  V ,  T
◦ S − S ◦
◦ S − S ◦
T ,λi 
T ,λi 
T  is  the   zero   operator,  i.e., 
kernel  of  T
◦
◦ 
T S =  S T .
Deﬁne  N  =  T  −  S .  Because  (S, T )  is  a  commuting   pair,  and  because  (T , T ) 
is  a  commuting  pair,   by  Lemma   2.2,  (N , S )  is  a  commuting  pair.  Therefore  
N (E (ei )  )  ⊂  E (ei )  by  Proposition  2.3.  Moreover,  denoting  by  Ni  the  restriction 
T ,λi
T ,λi 
of  N  to  E (ei )  ,  by  deﬁnition   N ei  = (Ti  − λi Id)ei  is  the  zero  linear  operator.  Denote  
i 
T ,λi
e  =  max(e1 , . . . , es ).  Then  for  every  i  = 1, . . . , s,  N e  is  the  zero  linear  operator, 
i 
i.e.,  E (ei )  is  in  the  kernel  of  N e .  Since  W  spans  V ,  N e  is  the   zero  operator,  i.e.,  N
T ,λi 
is  nilpotent  of   index  e. 
Uniqueness:  Let  (S � , N � )  be  a   semisimple­nilpotent  decomposition  of   T .  By 
)  ⊂  E (ei )  and  N � (E (ei )  )  ⊂  E (ei )  .
Proposition  2.3,   for  every  i = 1, . . . , s,  S �
(E (ei )
T ,λi 
T ,λi
T ,λi 
T ,λi
� ◦ (λi Id)  = 
� the  restrictions  of  S �  and  N �  to  E (ei )  .  Then  Si
Denote  by  S � and   Ni
i 
T ,λi 
(λi Id)  ◦ Si
� ◦ Si  =  Si  ◦ Si
� ,  i.e.,  Si
� for  every  i = 1, . . . , s.  Thus  (S � , S ) is  a   commuting 
pair.  
Since  (S � , S )  is  a  commuting  pair  and  (T , S )  is  a  commuting  pair,  by  Lemma   2.2, 
(N � , S ) is  a  commuting  pair.   Since   also  (N � , T ) is  a  commuting   pair,  by  Lemma  2.2  
(N � , N ) is  a  commuting  pair.   Let  N  be   nilpotent   of  index  e and  let  N �  be   nilpotent  
of  index  e� .  Because  N  and  N �  commute,  the  binomial  theorem  applies  and  (N − 
N � )e+e� −1  =  B N e  + C ◦ (N � )e�  for  some  linear  operators  B and  C .  Because  N  is
◦
nilpotent  of  index  e  and   N �  is  nilpotent  of  index  e� ,  B N e  +  C ◦ (N � )e�  = 0 +  0, 
◦ 
i.e.,   (N − N � )e+e� −1  is  the  zero   operator.  By  hypothesis, 
S + N  =  T  =  S �  + N � ,  so 
S � − S =  N − N � .  Thus  (S � − S )e+e� −1  is  the   zero   operator.  In  particular,  for  every 
� − λi Id)e+e� −1  is  the   zero  operator. 
i = 1, . . . , s,  (Si
� ­eigenvectors.  If  v  is  a   µ­eigenvector 
By  Lemma  3.2,   there  is  a  basis  of  E (ei )  of  Si
T ,λi 
� ,  by  the  same  argument  as  in  the   proof   of  Corollary  2.8,  µ =  λi .  Thus  E (ei ) 
for  Si
T ,λi 
� ,  i.e.,  S � =  λi Id   for  every  i = 1, . . . , s.  Therefore 
has  a  basis   of  λi ­eigenvectors   for  Si
S �  =  S ,  and  so  also  N �  =  T  − S �  =  T  − S  equals  N .  So  (S, N )  is  the   unique 
i 
� 
semisimple­nilpotent  decomposition   of  T . 

Corollary  3.4.  Let  (S, N )  be  the  semisimple­nilpotent  decomposition   of  T .  For 
every  linear  operator   T � ,  (T , T � )  is  a  commuting  pair  iﬀ 
) ⊂ E (ei )  for  every  i = 1, . . . , s,  and 
(i)  T �
(E (ei )
T ,λi 
T ,λi
(ii)  (N , T � )  is  a  commuting  pair. 
) ⊂ E (ei )  .
Proof.  If  (T , T � )  is  a  commuting  pair,  then  by  Proposition   2.3,  T �
(E (ei )
T ,λi
T ,λi 
Denote  by T � the  restriction   of  T  to  E (ei )  .  Then  for  every  i = 1, . . . , s,  T � commutes 
i 
i 
T ,λi 
� commutes  with   Si .  Therefore  T �  commutes  with  S ,  i.e.,  (S, T � ) is 
with   λi Id,   i.e.,   Ti
a  commuting  pair.  By  Lemma  2.2,  since   (T , T � )  and  (S, T � )  are  commuting   pairs, 
also  (N , T � )  is  a  commuting  pair. 
Conversely,   suppose  that  T �  satisﬁes  (i)  and   (ii).  By  the  argument   above,  Ti
� com­
mutes  with  Si  for  every  i = 1, . . . , s,  i.e.,  T �  commutes  with   S .  Since   (S, T � )  and  
(N , T � ) are  commuting  pairs,  by  Lemma  2.2,  also  (T , T � )  is  a  commuting  pair.  � 
6 

4.  Jordan  normal  form 
Let  N  :  V  →  V  be  a  nilpotent  operator.  For  every  integer  e  ≥  0,  deﬁne   E (e)  = 
E (e)   = ker(N e ).
N ,0 
Lemma  4.1.  For  every  integer  e ≥ 0,  N (E (e+1) ) ⊂ E (e) .  For  every  integer   e ≥ 1, 
E (e−1)  ⊂ E (e) . 
Proof.  If  v  ∈  E (e+1) ,  then  N e (N (v))  =  N e+1 (v) =  0,  therefore  N (v)  ∈  E (e) . 
Therefore  N (E (e+1) ) ⊂ E (e) .  Clearly  E (e−1)  ⊂ E (e) . 
� 
Notation  4.2.  Denote  F (0)  =  N (E (1) )  ⊂  E (0) .  For  every  integer  e  ≥  1,  denote  
F (e)   N (E (e+1) ) +  E (e−1)  ⊂ E (e) .
= 
Lemma  4.3.  For  every  integer  e  ≥ 0,  there  exists  a  vector  subspace  G(e)  ⊂ E (e) 
so  that  (F (e) , G(e) )  is  a  direct  sum  decomposition  of  E (e) . 
Proof.  For  every  integer  e,   let  B be   a  basis  for  F (e) .  This  is  a  linearly  independent  
set  of  vectors  in   E (e) .  By  the  basis  extension  theorem,  there  exists  a  collection 
of  vectors  B �  in   E (e)   so  that  B ∪ B �  is  a  basis  for  E (e) .  Deﬁne  G(e)  =  span(B � ). 
By  Proposition  1.5  from  11/10/04,  (F (e) , G(e) )  is  a   direct  sum  decomposition  of 
� 
E (e) . 
Notation   4.4.  For  every  integer  e  ≥  0,  denote  by  Ae  = (ve,1 , . . . , ve,re  )  an  
ordered   basis  for   G(e) ,  possibly  empty  (in  which   case   deﬁne  re  =  0).  For  ev­
ery  vector  ve,j  ,  for  every  i  = 1, . . . , e,  deﬁne   ve,j,i  =  N e−i (ve,j  ),  and   deﬁne 
Be,j   = (ve,j,1 , . . . , ve,j,e ).  Deﬁne   Ee,j  =  span(Be,j  ).  Deﬁne  W  to  be   the  sequence 
of  all  nonzero  subspaces   Ee,j  . 
Theorem  4.5.  For  every  e  ≥ 0  and  every   1  ≤ j ≤ re ,  N (Ee,j  ) ⊂ Ee,j  .  Moreover 
W  is  a  direct  sum  decomposition  of  V . 
Proof.  By  construction  N  maps  each  element  of  Be,j  either  to   0  or  to  another 
element  of  Be,j   .  Therefore  N  maps  Ee,j  into  itself. 
� 
For  every  e  ≥  0  and   every  1  ≤  j  ≤  re ,  let  v(e,j )  ∈  Ee,j  be   a   vector  such  that  
(e,j )  v(e,j )  =  0.  Let  l  ≥  0  be   the   largest  integer  such  that  at  least  one   element 
� 
�
N l (v(e,j ) ) =  0,  if  such   an  integer  exists.  For  every  (e, f ),  deﬁne   v(
e,j )  =  N l (v(e,j ) . 
e,j )  ∈ Ee,j  ,  and  of  course  
�
�
By  the  ﬁrst  paragraph,  each   v(
e,j )  =  N l (0) =  0. 
(e,j )  v(
e,j )  ∈ E (1)  for  every  (e, j ).  Therefore,  for  every  (e, j ),  v(
e,j )  =  a(i,j )  · 
�
�
Moreover,  v(
v(e,j,1)   for  some  a(e,j )  ∈ F. 
� 
Let  e ≥ 0  be  the  least  integer  such  that  for  some  j ,  a(e,j )  =  0.  Denote, 
we  = 
a(e,j )v(e,j ) , 
� � 
1≤j≤re 
N e� −e (ae� ,j  · ve� ,j ).
w� = 
� 
e 
e� >e   1≤j≤re� 
Then   we  ∈ N (E (e+1) ) ⊂ F (e) ,  and  N e−1 (we  +  we ) equals 
�
�
(e,j )  a(e,j )v(e,j,1)  =  0. 
e ) − we  is  in   F (e) .  Of   course   also 
e  ∈ E (e−1)  ⊂ F (e)  So   we  = (we  +  w�
Thus  we  + w�
�
we  ∈ G(e) .  Since  (F (e) , G(e) ) is  linearly  independent  by construction,  we  =  0.  But 
7 

and  denote, 

�
�
by  construction,   (v(e,1) , . . . , v(e,re ) )  is  linearly  independent.  Therefore   a(e,j )  = 0 
for  every  1  ≤ j ≤ re .  This  is  a  contradiction,  proving   the  integer  l does  not  exist. 
Since  there  is  no  integer  l  ≥  0  such  that  for  some (e, j ),  N l (v(e,j )  is  nonzero,  in 
particular  for   l  =  0,  for   every  (e, j ),  v(e,j )  =  N 0 (v(e,j ) )  is  the  zero   vector.  This 
proves  W  is  linearly  independent.  By  construction  W  is  clearly  spanning,  thus  W
� 
is  a  direct  sum  decomposition  of  V . 
Notation  4.6.  For  every  integer  e  ≥  0  and  every  1   ≤  j  ≤  re ,  denote  by  Ne,j  : 
Ee,j  → Ee,j   the  restriction   of  N  to  Ee,j  . 
Deﬁnition  4.7.  For  every  integer  n ≥ 1,  the  nilpotent  Jordan   block  of  size  n is  the 
� 
n × n  matrix  J0,n  with, 
1,  1  ≤ i ≤ n − 1, j =  i +  1,
J0,n (i, j ) = 
0, 
otherwise 
For  every  integer  n  ≥  1  and   every  element  λ ∈  F,  the   Jordan  block  of  size  n  and 
eigenvalue  λ is  the  n × n  matrix  Jλ,n  =  λIn  +  J0,n . 
Proposition   4.8.  For  every  e ≥ 0  and  every  1  ≤ j ≤ re ,  the  matrix  representative 
[Ne,j  ]Be,j  ,Be,j  is  the  nilpotent   Jordan  block  of  length  e,  J0,e . 
Proof.  Denote  J  = [Ne,j   ]Be,j  ,Be,j  .  By  construction,  Ne,j  (ve,j,1 ) =  0,  therefore  the 
ﬁrst column  of  J  is  the  zero  vector.  And  for  every  i = 1, . . . , e − 1,  Ne,j  (ve,j,i+1 ) = 
ve,j,i .  Therefore  the  (i +  1)st  column  of  J  is  the  ith  standard   basis  vector.  This  is 
� 
precisely the  deﬁnition  of  J0,e . 
Corollary  4.9.  There  exists  a  basis   B  for  V  so  that  the  matrix  representative 
[N ]B,B  is  a  diagonal  block   matrix  whose  diagonal   blocks   are  al l  nilpotent  Jordan 
blocks. 
Proof.  By  Theorem  4.5,   and  by  Proposition   1.5  from  11/10/04,  the  concatenation 
B  of  all  the  sets  Be,j   is  a  basis  for  V . By  Theorem   4.5,  the   matrix  representative 
[N ]B,B  breaks  up   into  diagonal  blocks, [Te,j  ]Be,j  ,Be,j  .  By  Proposition  4.8,  each  of 
� 
these  blocks  is  a  nilpotent  Jordan   block. 
Corollary   4.10.  Let  T  :  V  → V  be  a  linear  operator  such  that  cT  (X ) = (X − λ)n  . 
There  exists  an  ordered  basis  B  for  V  such  that  [T ]B,B  is  a  diagonal  block  matrix 
whose  diagonal  blocks  are  al l  Jordan  blocks  of  eigenvalue  λ. 
Proof.  Deﬁne  N  =  T  − λIdV  .  By  the  Cayley­Hamilton  theorem,  N n  is  the  zero 
operator,  i.e.,  N is  nilpotent.  By  Corollary  4.9,  there   exists  a  basis  B for  V  such that 
[N ]B,B  is  a  diagonal  block  matrix  whose  diagonal  blocks  are  all  nilpotent  Jordan 
blocks  J0,e .  Of  course [λIdV  ]B,B  =  λIn .  Therefore [T ]B,B  =  λIn  + [N ]B,B  is  a 
diagonal  block  matrix  whose  diagonal  blocks  are   all  λIe  + J0,e ,  i.e.,  a  Jordan   block 
� 
with   eigenvalue  λ.  
Theorem  4.11.  Let  T  :  V  → V  be  a  linear  operator  with  cT  (X ) = (X −λ1 )e1  · · · (X −
λs )es  .  There  exists  an  ordered  basis  B  for  V  such  that  [T ]B,B  is  a  diagonal  block 
matrix,  whose  diagonal  blocks  are 
) 
(Jλ1 ,e1,1 , . . . , Jλ1 ,e1,m1  , Jλ2 ,e2,1 , . . . , Jλ2 ,e2,m2  , . . . , Jλs ,es,1 , . . . , Jλs ,es,ms 
where  every  for  every  i = 1, . . . , s,  ei,1  ≥  · · · ≥ ei,mi  ≥ 1  and  ei,1  + · · · + ei,mi  =  ei . 
The  matrix  [T ]B,B  is  unique  up  to  reordering  of  (λ1 , . . . , λs ),  but  the  ordered  basis  
B  is  typical ly  not  unique. 

8 

Proof.  Existence  of   an  ordered   basis  B  such  that [T ]B,B  is  a  diagonal  matrix  of 
Jordan  blocks  follows  from  Proposition  2.6  and  Corollary  4.10.  To  compute   the  
number  of  Jordan  blocks  Jλi ,e ,  and  form  Ni  :  E (ei )  ,  deﬁne  E (e) , F (e)  ⊂  E (ei )  as
T ,λi
T ,λi 
above.  Then  the  number  of  Jλi ,e ­blocks  is  dim(E (e) ) − dim(F (e) ).  This  depends 
only  on   T ,  not  on   a  choice  of  basis.  Therefore  the   sequence  of   lengths  of  Jλi ,e ­blocks 
is  canonically  determined  by  T .  Reorder  the  subbases  putting  T  into  diagonal  block 
form  so  that  ei,1  ≥  · · · ≥ ei,mi  ≥ 1  for  every  i = 1, . . . , s. 
� 
⎛ 
→⎞ 
Example  4.12.  Let  λ ∈ F  and  let  T  =  TA  :  F3 
F3  where,
A =  ⎝  0  λ 
0  ⎠ . 
0 
1 
λ 
0 
1  λ 
⎛
⎞ 
One  basis  putting  A into  Jordan  canonical  form  is  the  set  of   columns  of  the  matrix, 
1  ⎠ . 
P1  =  ⎝  0 
1 
0 
0 
0 
0 
1 
0 
⎞ 
⎛
Another  basis   putting  A  into  Jordan  canonical  form   is  the  set  of  columns  of  the 
matrix, 
1  ⎠ . 
P2  =  ⎝  0 
1 
0 
1 
0 
1 
0 
1 
⎛
⎞ 
−1 ,  where  J  is  the   Jordan  normal  form, 
In  both   cases,  APi  =  PiJ ,  i.e.,  A =  PiJ Pi
J  =  ⎝  0  λ 
1  ⎠ . 
1 
0 
λ 
0 
0  λ 
The  Jordan  normal   form  J is  uniquely  determined,  but  the   change­of­basis  matrices 
P1  and  P2  are  not  uniquely  determined. 

9 

