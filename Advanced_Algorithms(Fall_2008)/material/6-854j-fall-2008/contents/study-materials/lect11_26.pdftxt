MIT OpenCourseWare
http://ocw.mit.edu 

6.854J / 18.415J Advanced Algorithms 
Fall 2008
��

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.415/6.854 Advanced Algorithms 

November  26, 2001 

Lecturer:  Michel  X .   Goernans 

Lecture   19  
Scribe:  Shalini Agarwal,  Shane  Swenson 

Previously,  we  discussed solving a  system  of  equations  of  t h e  form: 

(mod 2)  : Xi,Xj, xk E  {O,1} 

The   goal  is  t o   satisfy  as many  equations  as  possible.  I t   is  easy  t o   satisfy  a t   least  half  of  them  by 
assigning all xi  = 0 or  all xi  = 1. In   this  lecture, we  consider t h e  related  problem  of  satisfying  a  set 
of  linear equations 
(mod 2 ) ,  each  having  exactly  2 variables.  We  call this  problem  Lin-2-Mod-2. 

Given rn equations  on  n  variables  of  t h e  form: 

(mod 2) 

Assign  a  value from  { O , l }   t o  each x i   so t h a t   t h e  maximum  number  of  equations  a re  satisfied, 

1.1  Simple 0.5-approximation algorithm 

This 0.5-approximation  algorithm  finds a solution t h a t   satisfies a  number  of  equations greater than  
or  equal t o  half  of  t h e  number  satisfied  by  t h e  optimal solution. 

1.1.1  Randomized Algorithm 

Select  a n  assignment  uniformly  a t   random. 

There a re  2,  possible  assignments so 

1 
P T ( x ~= a i  Vi) = - V(a1, a2, ...,a,).
2" 

In  order  t o  bound  t h e  expected  number  of  equations  satisfied  by  this  assignment  let 

Ii = 

1  if  equation  i  is  satisfied 
0  otherwise 

Claim  1  E I I i] = 

Proof  of  claim: 

EIIi]= P r [equa t ion  i is  satisfied] 
Equation  i has  th e   form  x j  + xk 
(mod  2 )   where  ci  E  (0, I) .   Since xj is  independent  of  xk 
and  P r [ x j  = 11= i ,  
P r [equa t ion  i is  satisfied]  =  P r [equa t ion  i  is  satisfiedlxk] 

C i  

Now  consider  t h e  expected  number  of  equations  satisfied. 
m 
E[# equations  satisfied]  =  E[CI,] 

where OPT  is  the   number  of  equations  satisfied  by  the  optimum  assignment. 

1 .1 .2   Derandomization 

We  now  present  two ways  of  derandomizing the  above randomized algorithm.  We  can either use  the  
method  of  conditional  expectations  t o   assign  values  t o   variables  one-by-one  or  we  can  reduce  the  
sample space t o  one of  polynomial  size and  perform  a n  exhaustive  search. 

Method  of conditional expectations 
Let  W  = # equa t ions   s a t i s f i e d .   Following t h e  formula  for  conditional expectations: 

1
since P r [ x l  = 01  = P r [ x l  = 11= 2 .  
Suppose  EIW1xl  = 0]  is  th e   maximum  in  t h e   above  expression.  Then  we  set  X I  = 0  and  
examine  the   next  variable, 22.  Consider  the   equation: 

By  choosing th e   variable  assignment  t h a t   yields th e  maximum  conditional E [W ]  in  each  s tep  
we  have: 

-OPT  5 E [W ]  < E[W121] 5 E [W l x l , $21  < ... < E [W l x l , ~ 2
1
2 
so our  assignment  is  a  0.5-approximation. 

,-.-,x,]

Now  we  show how  t o  compute E [W lx l ,  x2, ...,x k ] .  From  above, 

and  if  equation  i  has  th e  form  x j  + xk 

ci 

(mod 2)  then 

i f x j > t t x 2 1 c > t  
E [ I i l x l , x 2 ,  ...,x t ]  =  1  if  i , j  5 t a n d  xi + x j  
0  otherwise 
So t o  compute E[Wlxl, x2, ...,st]we  simply add   1for  all equations  already  satisfied  and   $  for 
all equations  not  yet  determined. 

(mod  2) 

ci 

Reducing the  sample space 
In th e  analysis of  our randomized  algorithm, we  used two properties of  our random assignment. 

- Pair-wise  independence:  P r [ ( x i  = a i )  A  (x   = a j ) ]  = P r [ x i  = ail  . P r [ x   = a j] Qi,j ,  a i ,  a j  

We  will  now  construct  a  polynomial-size  sample  space  t h a t   preserves  these  two  properties. 
If  we  choose  assignments  uniformly  from  this  sample  space,  our  previous  analysis  of  E [W ]  
remains valid  and  we  have 

1 

Thus if  we  exhaustively search t h e  sample space we  a re  guaranteed  t o  find an  assignment  such 
t h a t  

1 

Assume n  has the  form  (power of  2) - 1. If  no t ,  round  i t   up  t o  th e  next  such number.  We  can 
construct  a  set  of  "Hadamard"  matrices  recursively as follows: 

where  J is  th e   appropriately  dimensioned matrix  with  every  entry  equal t o  1. 
For  example, here  is H 2 :  

Since n  is  (power of  2)  - 1, let  n  + 1 = q  = 2'.  We  define  our  distribution  of  assignments  as 
follows.  Let  a l ,  a2, ...,a,  be  th e   elements  of  any  column  of  HI, excluding  t h e   top   element  of 

t h a t   column.  T h a t   is, if  we  number  the   rows of  Hk beginning  with  0,  a1  is  always taken  from 
row  1 ,  a2  is  always taken  from  row  2 ,  etc.  and  all  a i   a re  taken  from  t h e  same  column of  Hk. 

Then  let 

1
P r [ x i  = a i  Vi] = -. 
4 

Claim  2  Using  this distribution, P r [ x i  = I] = f  Vi. 

Proof of  claim:  We  use  induction  t o  show t h a t   each  row, other  th an   the   top   row,  of  H n  has  a n  
equal  number  of  1s and   0s  for  all  n.  Since we  choose columns  of  HI, uniformly  a t   random,  th is   is 
sufficient  t o  prove th e   claim. 

Base  case: 

L 
J 
The   second row  of  H I   has  an  equal number  of  1s and  0s. 

Inductive  step:  Assume  each  row,  other  th an   th e   top   row,  of  Hi  has  a n   equal  number  of  I s  
and  0s  and  consider  the   rows  of H i+ l .  

The re  a re  two  cases t o  consider. 

- If  t h e  row  is  in  t h e  top  half  of  H i+ l ,   bu t   isn't th e   top  row,  i t   has  identical  left  and  right 
halves,  each  of  which  a re   comprised  of  a n   equal  number  of  I s  and   0s  by  th e   inductive 
hypothesis. 
- If  th e   row  is  in  th e   bottom  half  of  H i+ l ,   each  1 in  th e   left  half  corresponds  with  a  0  in 
t h e  same position  in t h e  right  half  and  vice  versa. 

Thus  every row, except  for  th e  top   row, of  Hi+1  has  a n  equal number  of  1s and  0s. 

Claim 3  Using  this distribution, xi  and  x j   are  pairwise  independent for  all  i , j .  

Proof of claim:  We  use induction on  n  t o  show t h a t   if  we  vertically match  any two distinct  rows, 
excluding  th e  top  row, of  H n  we  have  a n  equal number  of  00, 01,  10, and   11 pairs. 

Base  case: 

Inductive  step:  Assume each  pair  of  rows,  excluding  th e   top   row, of  Hi  has  an  equal number 
of  00, 01,  10, and   11 pairs  and   consider  the   pairs  of  rows of  Hi+l. 

The re  a re  five  cases t o  consider. 

- If  bo th   rows  a re  in  the   top   half  of  Hi+1  t h e  claim follows  from  th e   inductive hypothesis. 
- If  bo th   rows  a re   in  th e   bottom  half  of  Hi+1  and  neither  row  is  the   top   row  of  t h a t   half, 
th e  claim holds for t h e  left  halves of  the  pair  by  th e  inductive hypothesis.  Since th e  right 
halves  a re  merely  inversions of  t h e   left  halves,  t h e   claim  also  holds  for  t h e   right  halves 
and  therefore  applies  t o  th e  pair  of  rows  along their  entire  length. 
- If  one row  is taken  from the   top  half  of  Hi+1 and  the  other  is taken  from t h e  bottom  half, 
they  correspond  t o  different  rows  in  H i ,  and   neither  row  corresponds t o  th e   top   row  of 
H i ,  th e  claim  holds  for  th e   left  halves  by  th e   inductive  hypothesis.  For  t h e  right  halves, 
apply  th e   inductive  hypothesis  t o  th e   original  rows  and   replace  00  with  01,  01 with  00, 
10 with  11, and   11with  10.  This proves th e   claim for  the   pair  of  rows  along  their  entire 
length. 
- If  one row is taken from  the   top  half  of  Hi+1 and  th e  other row  is taken from  the  bo t tom  
half  and  they  correspond t o  th e  same row  in H i ,  from  th e  proof  of  t h e  previous  claim we 
find exactly 2i-1  00 and   2i-1  11 pairs  in the   left  halves of  the  rows  and  2'-l  01  and  2i-1 
10 pairs  in  t h e  right  halves  of  th e  rows. 
- If  one of  th e  rows is in  the  bottom  half  of  Hi+1and  corresponds  t o  th e  top   row of  H i ,  we 
again use  t h e  previous  claim  t o  find  1matched  with  a n  equal number  of  1s and  0s  in  the  
left  halves  and   0 matched  with  a n  equal  number  of  1s and   0s  in  th e   right  halves  of  the  
rows. 

We  have  now  proved  th e   necessary  properties  of  this  distribution  t o  apply  our  previous  reasoning 
and  deduce  t h a t   if  we  sample uniformly  from  th is  new  sample space 

and  therefore the re  exists a n  assignment  in this  sample space such  t h a t  

Since t h e  sample space contains less  t h a n  2n  possibilities,  we  can exhaustively  search  i t   for  such a n  
assignment. 

1.2  0.878-approximation algorithm based on "semidefinite" programming 

In this algorithm, we  convert the  original Lin-2-Mod-2 problem into a slightly different, more familiar 
graph  problem.  We  define  the   graph G  t o   contain  a  vertex  for  each  variable,  a  solid edge  for  each 
equation  valued  a t   1,and   a  dashed  edge for  each equation  valued  a t  0. 

To  solve this  problem,  we  find  a  partition  of  G ,  such  t h a t   t h e  maximum  number  of  solid edges are 
cut  and  th e  minimum  number  of  dashed  edges a re  cut.  We  call  this  partition  MAXCUT. 

1.2.1  MAXCUT 

Given  a  graph  G  = (V,E ) ,  find  S C  V  : max d ( S ) ,  where  d ( S )  = IS (S )I  =# of  cut  edges.  More 
formally, we  can  rewrite MAXCUT  as: 

such t h a t  

1.2.2  Relaxation 

max  C -( i , j ) ~ E  
The  constraint  can be  rewritten  as Xi  E R  :  = 1 .  
Since th e  MAXCUT problem  as s ta ted  above is ha rd  t o  solve, we  find a relaxation t h a t  can be solved 
in  almost  polynomial  time.  This  relaxation  is  defined as: 
1 - uiuj 
~  =l 
~
l
vi  E  IWP a n d   l
l  1 ,  Vi. 
This  relaxation  gives  a n   upper  bound  on  th e   MAXCUT  a s   i t   is  maximizing  t h e   same  objective 
function  subject  t o  weaker constraints. 

such t h a t  

1 .2 .3   Example 

For  example  consider C 5 ,a  graph  comprised of  a  5-cycle.  The  rnax  cut  for  this graph is 4.  Figure  1 
shows C s  and  the   optimal solution  t o  i ts  relaxation  for p = 2.  In  this  case,  Lvi  = $(i  - 1 ) .  Thus 

and  

1- v i v j   - 5 (1  - cos F)
= 4.52
-
2 
( i , j ) € E  

Figure  1:  Cs and  t h e  relaxation  for p = 2 

1.2.4  Randomized Algorithm 

In  order  t o  convert  our  relaxed  solution  t o  a  cut  we  choose a hyperplane  in  Rp  and   assign  1 t o  all 
vectors  on  one side of  it  and  -1 t o  all vectors on  th e   other. 
= {x E Rp  : 1  I Z   1 1   = 1). Then  let 

Select vector r  uniformly  a t  random  from 

2 i  = 

1  i f r . u c > O  
-1  otherwise 

and  

Theorem 4  If  we choose  S  i n  this  w a y ,  
E [ d ( S ) ]> 0 .878UB  > 0 . 8 7 8 0 P T .  

