MIT OpenCourseWare
http://ocw.mit.edu 

6.854J / 18.415J Advanced Algorithms 
Fall 2008
��

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.415/6.854  Advanced Algorithms 

28 November 2001 

Lecturer:  Michel  X .   Goernans 

Lecture   20 
Scribe:  David  Liben-Nowell  and   Bill  Th ies  

Review 

In  th e  previous  lecture, we  were  considering t h e  LIN-2-MOD-2 problem, which  is  as follows. We  are 
given  a  set  of  equations,  and  our  goal is t o  satisfy  as many  of  them   as possible: 
xi + X j  
T h a t  is, each equation  s ta tes  t h a t  t h e  sum of  exactly two variables  is congruent  (modulo 2 )  t o  either 
0  or  1 .   Restricting  our  a t ten t ion   t o   the   case  when  each  sum  is  congruent  t o   1, this  problem  is 
equivalent  t o  th e   following instance  of  MAX-CUT. Given  a  graph  G  = (V, E) with  a  vertex  u  for 
each  variable  x  and   a n  edge  (vi, ~ j )for  each  equation  xi + x j   1 (mod 2),  find  th e   set  of  vertices 
S C V  t h a t  maximizes  the   number  of  edges crossing t h e  boundary  of  t h e  set: 

{0 ,1 )   (mod  2) 

max  d ( S )
s c v  
where d ( S )  = IS(S)I  is  the   number  of  edges crossing -&.om S t o  V  \ S 

We  then   established t h e  following upper  bound  (UB) for  MAX-CUT: 

We  will  refer  t o  th is  maximization problem  as SDP ,  which  s tands  for  "semi-definite programming" 
as we  will  see  later.  In   th e   case when p  = 1 ,  UB  is  clearly  equal t o  t h e  value of  t h e  maximum  cut; 
we  showed  t h a t   for  larger p ,   th e   expression  above  still  remains  a n  upper  bound.  Finally,  we  used 
this  expression  t o   sketch  out  a  0.878-approximation  algorithm  for  MAX -CUT .  In  th is   lecture,  we 
formulate  the   details of  this  algorithm  and  prove its  correctness. 

2 

A  0.878-Approximation  Algorithm  for  Max-Cut 

The   0.878-approximation  algorithm  t h a t   we  consider  is  due  t o   Goemans  and  Williamson  [2].  An 
outline  of  the   algorithms  is  as follows: 

Algorithm  for  Max-Cut : 

1. Solve th e  SDP  problem  (Equation 3)  t o  obtain  vectors  vi. 

2.  We'd  like  t o  separate the   vectors  into  two groups t h a t   are  "maximally  separated"  in order  t o  
determine  t h e  cut  S c V.  As  discussed  in  th e   last  lecture,  we  can  do this  with  a  hyperplane 
in RP,  and  split  th e  vectors based  on which  side of  the  hyperplane they  fall.  However, since we 
can  ro ta te   all of  t h e  vectors  vi  without  changing th e  goodness of  t h e  solution, it  is not  a good 
idea t o  fix  a  single hyperplane  t o  do this  separation since some cases will  elicit  a  poor  split. 
Instead, we  select a  unit  vector  r  uniformly  a t  random  from  th e  p-dimensional  sphere Sp-l = 
{x E  RP  : llxll  = 1).  (We denote  this  sphere by  Sp-l since Sl  is  a  circle in  two  dimensions.) 

Figure  I :  One can select a point  uniformly  a t  random  from  th e  surface of  S2,th e  three-dimensional 
unit  sphere, by  choosing uniformly  a t   random  8  from  [0,2;lr]and  h  from  [-I, 11 and  then  projecting 
th e  point  horizontally  onto t h e  surface  of  t h e  sphere. 

3 .   Select t h e  cut  S as those  vectors  which  fall on  one  side of  r:  S = {i E V  : r  vi 4 0). 
There are th ree   issues in  t h e  above algorithm  t h a t  we  need  t o  address.  In  Section  5, we  present 
a method  for solving th e  SDP  problem  as required in  step 1 .  In  Section 3 we  show how t o  randomly 
select  the   unit  vector  r  as required  in  s tep   2,  and   in  Section  4  we  analyze  selection  of  th e   cut  t o  
show t h a t   i t   is, in  fac t ,  a  0.878-approximation  algorithm. 

3  Choosing  a Random  Vector 

I t   is  not  entirely  trivial  t o   choose  a  vector  t h a t   is  uniformly  distributed  across  th e   points  of  a 
multi-dimensional  sphere.  For  instance,  in  th ree   dimensions,  choosing bo th   la t i tude   and   longitude 
coordinates  uniformly  a t   random  will  not  yield  points  t h a t   are  uniformly  distributed  across  the  
surface  of  the   ea r th .   This  is  because  a  given  range  of  la t i tude  covers  a  smaller and   smaller  portion 
of  the   earth's  surface  as  one  nears  th e   poles-thus,  if  la t i tude  were  chosen  uniformly,  more  points 
would  end  up  near  the   poles  th an   near  th e  equator. 

3.1  3-D  Solution 
For  the   case  of  th ree   dimensions,  a  uniform  spherical  distribution  can  be  obtained  by  choosing 
different  coordinates  uniformly  a t   random.  These  coordinates  are  1) an   angle  8 E  [O,27r] and   2)  a 
height  h  E  [-I, 11.  The  random  vector  can then  be obtained  by projecting  horizontally  on to  a  sphere 
from  th e   corresponding  position  on  a n  enclosing cylinder  (see Figure  1 ) .   In  other  words,  one  can 
take  8  as t h e  longitude  and   a s i n ( h )  as th e  latitude. 

3.2  General  Solution 
In the  general  case, one can generate r from Sp-l uniformly  a t  random by  choosing each coordinate 
according  t o  N ( 0 ,I ) , t h e   normal  distribution  with  mean  0  and   s tanda rd   deviation  1 .   Using  oc  t o  
denote  proportionality, we  have  by  t h e  definition  of  a  normal  distribution t h a t :  

We  can  see t h a t   this  gives t h e  desired  result  if  we  consider  the  normal  distribution in p  dimensions: 
f (XI, x 2 , . . .xp )  

ce-+(x:+":+-.-+x;) 

(5) 

Figure  2:  Two vectors  ui  and   u j   with  r i n ,  t h e  component  of  the   random  vector  r  which  lies  in  the  
plane  of  ui  and  uj. 

along any  given axis: 

where  c  denotes  some  constant.  Noting  t h a t   the   distance  of  any  point  from  t h e   origin  is  d  = 
we  can  see  t h a t   the   distribution  depends  only  on  d  instead  of  th e   position 
- 4d2
f (dl 
(6) 
ce 
Thus, selecting each coordinate from this distribution will give vectors t h a t  a re  uniformly distributed 
within  t h e  unit  sphere Sp - l .  

4  Analyzing  the   Algorithm 
We  now  tu rn   our  attention  t o   proving  t h a t   our  choice  of  S = {i  E  V  :  r  . ui  > 0)  does  yield  a 
0.878-approximation  algorithm for MAX-CUT,given t h a t  vi  a re  a  solution  t o  SD P   (Equation 3) and  
r  is  chosen  randomly  a s   described  above.  As  a  first  s tep ,   we  will  need  a  closed  form  expression 
for  t h e  expected  value  of  t h e  cu t ,  given our  random  choice of  r .   For  th is ,  we  t u r n   t o  t h e  following 
theorem: 
x{i,j) 
Theorem 1 F o r  any   s e t   of  u n i t  v e c t o r s  Vi,  E [ d ( S ) ]  = 
t E
Proof:  Let  us  introduce  a n   indicator  variable  Iij whose  value  is  1 if  ( i ,j )   is  in  the   cut  and   0 
otherwise.  Then we  have  t h a t ,  by  th e  definition  of  d and  of  expectation: 

Now  let  us  consider  this  last probability-that  exactly one of  a pair  of  vertices  i and  j  a re  contained 
in S. This will be the  case if  the  unit  vectors  ui  and  u j  were separated by  t h e  hyperplane orthogonal 
t o  our random  vector  r .  Let  us  split r  up  into two components:  1) rin, th e  component  t h a t   is in  the  
plane of  vi  and  v j ,  and  2) rou t ,t h e  component  t h a t  is orthogonal to  ui  and  u j .   We  a re  not  concerned 
with  rou t  because  i ts  dot product with ui  and  u j  is zero; therefore, our  decision as t o  whether or not 
t o  include  i  and  j  in  S depends only  on  ri,. 
Thus ,   we  restrict  our  a t ten t ion   t o   ri,  (see  Figure  2).  Because  r  was  chosen  by  a  spherically 
symmetric distribution, we  know t h a t  ri,  is uniformly distributed within  a circle in th e  space spanned 

Figure  3:  The   segment  S T  is  orthogonal  t o  Tin, and  th e  angle  9  is measured  between  ui  and  uj. 

by  ui  and  u j .   ( I t  might  not  be  S1, th e   unit  circle,  since r in   could  be  shorter  th an   1.  However, this 
does not  affect  th e  proof  since we  a re  only  interested  in  th e  direction of  Tin.) 
Now, our cut will  separate i and  j  if  and  only if  ui  and  u j  fall on opposite sides of  th e  segment S T  
which  is  orthogonal  t o  T in   (see Figure  3 ) .   This is  a  probability  t h a t   is  straightforward  t o  compute. 
Let  8  denote  th e   angle  between  ui  and   v j .   Since bo th   ui  and   v j   are  distributed  uniformly  within 
t h e  circle, 8  is  distributed  uniformly within  [O,27~].And  since T i n ,   and   therefore th e   endpoint  T ,  is 
oriented  a t   a n  angle with  a  uniform  distribution between  [0,2;rr],we  can  see  t h a t   T  will  fall within 
9 with  a probability  of  9 / 2 ~ .Bu t   since S T  will  also separate vi  and  uj  if  S falls within  t h e  angle  9 ,  
t h e  probability  t h a t   vi  and  u j   a re  separated is  2 0 1 2 ~= 8 / ~ .Noting  t h a t   vi  .v j  = cos(0) because  vi 
and  u j   a re  unit  vectors, we  have t h a t  8 = U C O S ( U ~ . v j )   and  thus  P r  [(i,j )  E 6 (S ) ]  = U C O S ( U ~  . v j ) /T   as 
desired. 
This yields th e  following corollary, which  seems quite surprising: 

Corollary  2  F ix ing  p  and   allowing  t h e  ui's  t o  v a r y ,   de f ine  
acos(vi - v j )
Z = max  C 
7i-
( i , j ) €E  

T h e n  Z  i s   equal  t o  O P T ,   t h e   op t ima l   value   of  MAX -CUT .  

Proof:  W e s h ow t h a t   Z S O P T  and   Z 2 O P T :  
Z 5 O P T  because,  by Theorem 1,C ( i , jt E   a
a c o s ( v i  . v j )
is th e  expected value of  a cut  for a given 
d 
set  of  vi's.  Thus ,  for  t h e  set  of  vi's yiel  ing th e  maximum,  Z ,  there must  be  some cut with  a 
value a t  least  as great  as t h e  expected  (average) value,  and  this  value  can  be  a t  most  O P T .  
To  show Z  2 O P T ,  consider th e   elements  S of  t h e  optimal cu t .   Select a  unit  vector  w  in Rp, 
and   set  ui  = w  if  i  E  S and  ui  = -w otherwise.  Then  C( i , j)EE  
a c o s ( v i - w j )
= O P T  since ~i  and  
v j   a r e  opposite  (with UCO S (U ~. v j )   = T ) for  edges  ( i , j )  across  the   cu t ,  bu t   they  a re   identical 
(with ~ C O S ( U ~. v j )  = 0)  for  other  edges. 

I7 
Thus ,  we  have  seen  t h a t   we  could  solve th e  MAX -CUTproblem  by  solving Equation  7,  bu t   (as 
fa r   as  we  know)  th is   can't be   done  in  polynomial  time.  Instead,  we  would  like  t o   solve  t h e  easier 
SDP   problem  in  Equation  3,  which  can  be  done  in  (almost)  polynomial  time,  and   then   use  th e  
randomized  cut  t o  obtain  an   approximation  of  th e   optimum.  We  formalize  th e   correctness of  th is  
approach with  th e  following theorem: 

Figure  4:  The  linearized  approximation  compared  t o  the   exact  objective  h c t i o l z .   The  original 
objective  function  is always a t  least  0.878  as great  as th e  linearized version, 

Theorem 3  For  any  set  of vectors  v i :  

where a = r n i n x E [ - l f l l ~
rn 0.87856 

Proof:  The &st  equality  is  from  Theorem  1 and  included only  for  clarity.  The  inequality  holds 
term-wise on  each component of  th e  sum, as we  can show with  simple algebraic manipulation: 

aces (s)/7r 
( 1  - 4 1 2  

( 1 - vi  u j ) / 2

This  inequality  is  shown graphically in Figure 4. 
Now,  if  we  could  solve th e  SDP problem, we  would  obtain  U B  = max C ( i , j lEE   I. Instead 
1 -v i   .v -
consider t h a t  we  solve SDP within  some margin  of  error e ,  since finding the  o p tm a l  solution might 
require  more  th an   polynomial  time,  but  we  can  still  find  a  nearly-optimal  solution.  Then,  by 
Theorem 3, we  have t h a t  our randomized algorithm produces E [d(S)] 2 a (1- 6)U B  2 0.878 * OPT 
for  sufficiently small c.  Thus, our  algorithm  is  a  0.878-approximation algorithm  for MAX-CUT. 
As an  extension, we could generalize th e  problem t o  consider any instance of  L I N - ~ - M o D - ~(above 
we  consider only equations congruent t o  1,whereas L IN -Z -MOD -~also allows equations congruent t o  
l + v i . v j   te rm
0).  This simply involves modifying the  objective function in th e  SDP  problem t o  add  a 
for every equation x i  +xj  0  mod  2.  These terms will have an  expected value of  1- a c o s ( v i . u j ) / r  
instead  of  acos(vi .v j ) / r ,  but  th e  approximation  guarantee will  be  preserved since 1- a c o s ( z ) / r  is 
a t  least  as great  as a ( l  + x)/ 2   in  [- 1,1] (see Figure 5 ) .  
Remarks. We  conclude th e  analysis with  a  few  remarks  about work  related  t o  th e  algorithm: 

1. Feige  and   Schechtman  [I] identified  graphs  G,  = (V, E) such  t h a t   inf,  OP T IUB   = a  B 
0.87856.  This shows th a t  our analysis of  th e  algorithm is tight.  Th a t  is, given our estimation of 
th e  upper bound  U B , we  cannot do be t te r  than  an  a-approximation algorithm on every inpu t ,  

Figure  5:  The  linearized  approximation  compared  t o   the  exact  objective  function  for  equations 
congruent to  0 in t h e  original L I N - ~ - M o D - ~problem.  The or ig ina l objective fund ion  is still at least 
0.878 as great as the linearized version. 

since  otherwise  there  would  be  an instance  of  G, for  which  we  would  exceed  th e   optimum. 
However,  this  does  not  preclude  there  being  tighter  estimations  of  U B  th a t   would  allow  a 
better  approximation  algorithm. 
2 .   	 H h t a d   [3] established t h a t  there is no  (16/17+  E)-approximation algorithm  for MAX-CUTfor 
all E > 0 unless  P = NP .   Given t h a t   16/17 w  0.942,  this leaves a  gap down  t o  0.878 between 
which  it  is not  known where the   limit  of  approximability lies.  However, th e  gap is  smaller for 
L I N - ~ - M o D - ~ ,for which  Hgstad  found  t h a t   there  is  no  (11/12 + E)-approximation algorithm 
(11/12 R 0.916). 

3.  	 The algorithm given above is a randomized one; i t  is messy t o  obtain a deterministic algorithm 
instead.  As in th e  last lecture, th e  method of  conditional expectations can be applied.  However, 
it  is  more  complicated  because  th e   vi7s do  not  have  discrete  coordinates;  rather,  they  can 
assume continuous values  instead.  A  technique  for  derandolnizing th e   algorithm is  described 
in  [4]. 

5 

How  do  we  solve  the  SDP? 

The last  missing piece  in the   algorithm  is  a mechanism for  solving th e  SDP: 

We  will  sketch a  solution t o  this  problem based  upon  an  extension of  a  polynomial-time algorithm 
for linear  programming - either ellipsoid or  interior  point  suffices.  (For concreteness, we  will  focus 
on this particular  program  rather th an  th e  more general class of  semidefinite programs  (SDPs) th a t  
we  can solve in this  way.) 
For  a  matrix M ,  let  M 
0  denote  t h a t   M  is  positive  semidefinite.  Recall  from  linear  algebra 
V a  aTM U   2 0. 
t h a t ,  by  definition, we  have M 
0 
Consider a  matrix Y  E  RnXn so t h a t  Yij  = vi  . v j   for  1 5 i, j  5 n.  Thinking  about  this matrix, 
we  can reformulate  th e  program  (10) as follows: 

1- Yij
max  -
2 
( G j )EE 

s e t .   Yii  = 1 
y. . = y . .  
23 
3 2  
Y  k 0. 

(Generally, a  semidefinite program  has  a  linear objective function,  linear constraints on  Y ,  and  the  
restriction  t h a t  Y 
0 be  symmetric.) 

Claim 4  Programs  (10)  and  ( 1 1 )   are  equivalent. 
Proof:  Given  a  solution  vl ,.. . ,v, 
t o   ( l o ) ,  we  produce  a  solution  t o   (1I )  by  setting 
All  of  t h e  constraints of  (11) are satisfied: 
Y,i  = vi  .vi  = llvill  = 1 by  t h e  constraint of  (10). 

j  = vi  . v j . 

Yij  = vi  v j  = V j   vi  = Yji  by  t h e  commutativity  of  inner  product. 

For  any vector  a ,  

since xiaivi  is  a  scalar.  So Y 
Given a  solution Y  t o   ( l l ) ,  we  produce th e  vectors  vi  as follows.  Recall from  linear algebra t h a t  
0  iff  there  is  a  Cholesky factorization  Y  = VVT ,  where  V  E  RnXP,where p  = rank(Y)  5 n .  
Y 
Take  as  vector  vT  th e   i t h  row  of  V.  Then  vi  - v j   = ( V v T ) i j  = Y,j.  By  t h e   constraint  of  (11) t h a t  
Yii  = I ,  we  have  th e   desired  condition t h a t   llvi 1 1   = 1 .  

0. 

So  any  feasible  solution  t o   one  program  can  be  converted  t o   a  feasible  solution  t o   t h e   other 
program,  where X j  = vi  . v j , and  thus   the   values of  th e  objective functions a re  also identical.  Thus 
t h e  programs  a re  equivalent. 
Cholesky factorization  can  be  accomplished  in  O (n3 )  time  by  Gaussian  elimination.  So we  now 
need  only  solve  version  (11) of  th e   program.  Observe  t h a t   th e   semidefinite  program  is  actually 
almost  a  linear  program:  the   only  constraint  t h a t   doesn't  fit  into  a  linear  program  is  Y  k 0,  i.e., 
aTY a  = xix ja i a j X j   2  0  for  every  a .   Bu t   this  is  a  linear  constraint  for  any  particular  a ;  there 
are just  infinitely many  such  a 's ,  and  therefore infinitely many  linear  constraints. 
Observe  t h a t   t h e   set  of  feasible  solutions  t o   t h e   SDP   is  convex:  if  A  and   B  a re  feasible,  then 
AA  + (1- A )  B is  trivially  symmetric  and  has ones on  its  diagonal, and ,  for  any  a ,  

so  long  as  A  E  [0, 11,  since  A, B 
0.  (This  is  a  special  case  of  convex  programming  - a  linear 
objective function over  a  convex set .) 
We  can  solve the   SDP  by  adapting  interior  point  or  ellipsoid: 

[interior point.]  In  LP, we'd added  a  penalty  function  p C jlog (x j )   t o  our  objective  function; 
for  SDP,  we  add   t h e   penalty  p log (d e t (Y ) )   = p C j  log(Xj).  (Y   is  symmetric  and   positive 
semidefinite  iff  all  of  Y's  eigenvalues  a r e  positive;  de t (Y )   2  0  bounds  Y's  eigenvalues  away 
from  0.) 
Th e  man t ra ,   which  is  only  partially  false:  "SDP  is  L P  over t h e  eigenvalues." 

a  [ellipsoid.] We  need  t o  be  able t o  answer  t h e  question  "is  Y  feasible?",  and ,  if  no t ,  produce  a 
violated  constraint.  We  can  do  th is   by  computing t h e   eigenvalues of  Y;  if  X j   < 0,  then   take  
a  = v j   t h e  corresponding  eigenvector.  Then   a T y a  < 0, violating Y t0. 
Finding t h e  initial and  final  ellipsoids  is  not  so  simple,  bu t   it  can be  done. 

We  may  lose  t h e  factor of  E  in  solving  t h e   SDP  because  t h e  optimal  solution  may  be  i r ra t iona l  
(unlike in  LP, where  the re  was nice form of  t h e  solution in te rms  of  ratios of  small integers); we  have 
t o  give up  before we  actually  reach optimality. 

References 

[I] U.  Feige  and  G.  Schechtman.  On  t h e  optimality of  t h e   random  hyperplane rounding  technique 
for MAX  CUT .   Technical  repo r t ,  Weizmann  Ins t i tu te ,   Rehovot,  Israel,  2000. 

[2]  M.  X.  Goemans  and   D.  P.  Williamson.  Improved  Approximation  Algorithms  for  Maximum 
Cu t   and   Satisfiability  Problems  Using  Semidefinite  Programming.  J. Assoc.  Comput.  Mach., 
42:1115-1145,1995. 

[3]  J.  HBstad.  Some  optimal  inapproximability  results.  In  P roc .   29th  ACM  Symp.  on   Theory  of 
Computing,  1997. 

[4]  S.  Maha jan   and   H.  Ramesh.  Derandomizing  semidefinite  programming  based  approximation 
algorithms.  In   IEEE Symposium  on  Foundations  of  Computer  Science, pages  162-169,  1995. 

