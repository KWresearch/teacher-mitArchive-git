MIT OpenCourseWare
http://ocw.mit.edu 

6.854J / 18.415J Advanced Algorithms 
Fall 2008
�� 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.415/6.854  A d v a n c e d  A l g o r i t h m s  

October  3 ,  2001 

Lec tu re r :   M iche l   X .   G o em a n s  

Lecture  7 
Scribe:   N i c k   H a n s s e n s   a n d   N i c k   M a t s a k i s  

1  Properties  of  Barrier  Problem 

Last  lecture,  we  used  the   strict  convexity  of  t h e   logarithmic  barrier  function  t o   show  t h a t   B P ( p )  
has  a t   most  one  solution.  Now  we  will  prove  t h a t   a  solution  exists.  We  assume  throughout  and 
without  loss of  generality  t h a t   the   rank  of  A = m  where  A  is rn x  n .  
T h e o r e m  1  A s s u m e   t h a t  3 2  > 0  : Ax = b  (2.e.  B P ( p )  i s  feasible) ,   and   3  S  > 0,  3 y  : ATjj + S = C .  
T h e n  B P ( p )  i s  f in i te   and   h a s   a  u n i q u e   s o l u t i o n .  

P r o o f  o f   T h e o r e m   1: 
Take any  x  > 0 such  t h a t   Ax  = b .   We  have: 

=  ( s ^ T + j j T ~ ) ~ - p C l n ~ i  
j 
=  S T x + e T ~ x- p C l n x j  
j 
=  jjT b + C ( i j   x j  - p ln x j ) ,  
j 

( th is  sum  cannot  be  arbitrarily negative) 

implying  t h a t   th e  objective function is  lower bounded  by  a  constant  (this follows from t h e  fact  t h a t  
for p  > 0, S j   x + p l n x  tends t o  +oo  as x  goes t o  0 or t o  + m ) .   Therefore  th e  infimum of  B P ( p )  is 
finite for every p  > 0.  To show t h a t  th e  infimum is a t ta ined   ( t h a t  the re  exists an  optimum  solution), 
i t   is sufficient t o  notice  t h a t  th e  argument  above also leads t o  upper  and  lower bounds on x j  in order 
t o  have  a  value  below  th e   one  for  2 ,  which  means  t h a t   we  can  restrict  our  a t ten t ion   t o  a  compact 
set; this  implies t h a t   t h e  infimum  is  a t ta ined .   Finally,  we  have  shown  last  t ime  t h a t   if  a n  optimum 
solution  exists then   i t   is  unique. 
For  any  p  > 0,  the   unique  solution  t o  B P ( p )  is  called th e  p-center. 

2  Karush, Kuhn,  and  Tucker  (KKT )   Conditions 

Remember  the  optimality  conditions  from  last  lecture.  The  solution x  is optimum  for B P ( p )  if  3 y 
such t h a t  

where 

By  setting  s  t o  be  pXV1e ,  these  conditions  can be  re-written  as 3 y, s  such t h a t  

2.1  Definition  of  Algorithm 
To  find  the   p-center,  we  need  t o  solve  (1)-(3).  However,  the   constraints  (3)  are  quadratic, making 
this  ha rd   t o   solve1.  Instead,  in  order  t o   find  (or  approximate)  t h e   p-center,  we  use  a n   iterative 
method  based  on  Netwon's method.  We  assume  we  have  a  solution  t h a t   satisfies  (1)  and   ( 2 ) ,  but 
not  necessarily  (3). We  will  then  linearize equations  (3) around our  values of  x  and   s ,  and  solve the  
corresponding  linear  system.  This  gives  us  new  values  for  x  and   s  and   we  proceed.  We  will  show 
t h a t   if  we  s t a r t   "close enough"  from  the  p-center  then   after  this  upda te   step we  will  be even closer, 
and  this  iterative process will  converge t o  the   p-center  of  B P ( p ) .  

2.2  Upd a t e  Derivation 
Replacing  x ,  y  and  s with 

and  ignoring  Ax   . A s  in  (3 ) ,  we  arrive a t  

AAx  =  0 ,  
A ~ A Y + A S=  0, 
x j  . s j  + A x j  - s j  + x j  . A s j   =  /L. 
We  claim  this  system has th e  unique  solution, 

where 

l i f   such  a sys tem  was  easy t o  solve then   th i s  would  give  a simple algorithm  for linear  programming  by  se t t ing  p  t o  
0  and   replacing  t h e  s tr ic t   inequalities  by  inequalities. 

Indeed,  (6) implies t h a t   A x j  + x j s j l A s j   = p s j '   - x j , or  in  vector  notation, 

Premultiplying  by  A, using  (4) and  th e  fact  t h a t  Arr: = b,  we  get 

Observe  t h a t   this  is  not  a  square system  of  equations  (bu t  we  have  m  equations  in  n  unknowns). 
Substituting  A s  by  -ATAy  (because of  (5 ) ) ,  we  get 

is  a n  m  x  m  matrix  of  rank  m  since  A  has  rank  m  and   X  and   S-'  are  diagonal 
But  AXS-'AT 
is  invertible and  we  derive  (7).  (5) then 
matrices with  positive  diagonal elements.  Thus AXS-'AT 
immediately implies  (8),  and   (10) implies  (9). 
At  each  s tep ,  then ,  replace  x  and  s with  th e  values x + Ax   and   s + A s   (y can always be  derived 
from  x  and   s ) .  We  will  show t h a t  this  iteration will  converge t o  t h e  p-center  of  B P ( p ) .  

3  Definitions and  Properties 

3 .1   Prox im ity  Measure 
Let  o ( x ,  s ,p )  = 1 lv 1 1   be  th e  proximity  measure where  vj = 
- 1 .  Note  t h a t   this  will  be  zero  a t  
t h e  p-center.  We  will  show t h a t   llvll decreases with  each  iteration. 

3.2  d s   and  d x  
As  ( x , S ,p )  + (x  + A x ,  s + A s ,  p ) ,  our  proximity vector  v becomes w  where: 

-

p + A x j  - A s j  
- 1 
lu 
-- A x j  - A s j   which  we  are hoping will  be  small. 
P 
For  t h e  analysis, i t  will  be  useful  t o  rescale  th e   x-space  and   th e   s-space so t h a t   the   th e   current 
iterates  x  and   s  a re   equal,  bu t   in  a  way  t h a t   x j s j   remains  constant.  For  this,  we  will  rescale 
by  f i .Rescaling  Ax   and   A s ,   we  express  w j   a s  w j   = d x j   . d s j   where  d x j   = (3. c)and  
component  j  of  any  vector  in  the   x-space  by 
and   component  j  of  any  vector  in  the   s  space 

3 .3   Propert ies  
Property  1  A x l A s .  

Proof  of  Property  1:  This  is  t ru e   because  Ax   is  in  th e   nullspace  of  A  while  A s   is  in  the  
columnspace of  AT.  Indeed, premultiplying  (5) by  AxT  and  using  (4),  we  get  t h a t  A xTA s  = 0. 
Observe  t h a t   although  x + Ax   and   s + A s   do  not  necessarily  satisfy  (and  will  no t )   t h a t   ( x j  + 
A x j ) ( s j  + A s j )   = p ,   on  average  they  do  since  t h e   duality  gap  (x  + AX)^ ( s  + A s )   = C j( x js j  + 
A x j s j  + x j A s j  + A x j A s j )  = n p  by  th e   above property  and   (6). 

Property  2  d x l d s .   

Proof of  Property  2:  dxTds  = C jd x j  - ds  .  - 'C jA x j  . A s j  = 0, using  t h e  definition  of  d x j , 

3 - P  
d s j   and  P rope r ty   1. 

Property  3 

Proof of Property  3: 

4  Theorems  2  and  3 
Theorem 2  If a ( x ,  s , p )  < 1, then x + Ax  > 0  and   s + A s  > 0. 
Theorem 3  If a ( x , s ,p )  < a < 1, then o ( x  + AX ,s + A s ,  p )  < -. 
These  two  theorems  guarantee  t h a t ,   provided  a ( x , s, p )   is  sufficiently small,  repeated  updates 
x + A x ,  s + A s   given  in  th e   algorithm  will  converge t o  th e   p-center.  Theorem  2 was  not  proved  in 
this  lecture.  The  proof  for  theorem  3  is  provided  below.  Theorem  3  shows  t h a t   th e   convergence is 
quadratic  (provided we  a re  close enough). 
Proof  of  Theorem  3:  We  have  t h a t   o 2(x  + A x ,  s  + A s ,  p )   = 1  lw1I2  = C jW:  = C jdx:  . ds:. 
Using the   fact  t h a t   4  a  . b  < ( a  + b ) 2   and  P rope r ty  2, 

Using property 3 ,  

Taking  th e   square roo t ,  we  get 

Now,  consider  these  two  terms.  The   first,  x.v z ,   is  equal  t o   a2  by  definition.  The   second, 
?  3
m ax j  p /  ( x j  . s j ) ,  is  a t  most  1/( 1 - a )  by  the   following argument: 

since  1- a  is  strictly  positive  under  th e   conditions  of  the   theorem.  Using  these  upper  bounds  in 
( l l ) ,  we  can  conclude t h e  proof  by  s ta t ing ,  

a ( x  + Ax ,  s + A s ,  p )   5 

a2  
2 - ( 1 - 0 ) '  

Corollary 4  I f a  < $,  then -< a < $. 

This corollary  gives us  a  necessary  initial  bound  on  a t o  guarantee convergence. 

5 

Theorem  5 

Theorems 2 and  3 show t h a t  the  updates Ax ,  A s  given in th e  algorithm will converge t o  t h e  p-center. 
However,  instead  of  making  t h e  Newton  updates  t o  converge t o  t h e  p-center  for  a  fixed  value of  p ,  
we  take  one  s tep  t o  get  closer  t o  t h e  p-center  ( a  becomes now  a 2 / ( 2 ( 1  - a ) ) )  and   then  decrease  p 
(since our  goal  is  let  p.  tend  t o  0)  in  such  a  way  t h a t  our  new  i te ra te  is within  th e  original  value of 
a bu t   with  respect  t o  th e  updated  p .   Theorem  5 shows  how  th e   proximity  measure  changes  as we 
change p .  
Theorem 5  Suppose  xT s  = n p  and  a = a ( x , s ,p ) ,   then o ( x ,  s ,  p ( 1 -   8 ) )  = &do2  + O2   .n .  

Observe t h a t  our new i te ra te  $ +A x   and  s +  A s  satisfy th e  condition t h a t   ( x + A x ) ~  ( s f   A s )  = n p ,  
and  hence  Theorem  5  can be  applied  t o  see how  th e   proximity  measure  changes when  we  modify p 
Proof of  Theorem 5:  Let  v'  be  t h e  vector  v after  having  changed  p ,  i.e.  v(i = a- 1 .  We 
after a  Newton  iterate. 
have  t h a t  

Thus u'  = &v  + m e .  
6
Our assumption t h a t  z T s  = n p  can be translated into u T e  = 0 since u T e  = C . u . = C j(y 1)
- = 
3  3 
- -  
z T s   n .  Therefore, we  get  that 
P 

Taking  square roots, we  get  th e  desired  result. 

