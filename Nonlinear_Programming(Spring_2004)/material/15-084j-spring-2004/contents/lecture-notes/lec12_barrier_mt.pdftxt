Conditional Gradient Method,  plus  Subgradient 
Optimization 

Robert M.  Freund 

March,  2004 

2004 Massachusetts Institute of Technology.

1 

1	 The Conditional-Gradient Method for Constrained 
Optimization  (Frank-Wolfe  Method) 

We  now  consider  the  following  optimization  problem: 

P  :	 minimizex  f (x) 
x ∈ C . 

s.t. 

We  assume  that  f (x)  is  a  convex  function,  and  that  C  is a convex set. 
Herein we describe the conditional-gradient method for solving P , also called 
the  Frank-Wolfe  method.  This  method  is  one  of  the  cornerstones  of  opti-
mization,  and  was  one  of  the  ﬁrst  successful  algorithms  used  to  solve  non-
linear  optimization  problems.  It  is  based  on  the  premise  that  the  set  C 
is  well-suited  for  linear  optimization.  That  means  that  either  C  is  itself  a 
system  of  linear  inequalities  C  =  {x  |  Ax ≤  b},  or  more  generally  that  the 
problem: 

T
LOc  :  minimizex  c x 
x ∈ C 

s.t. 

is  easy  to  solve  for  any  given  ob jective  function  vector  c. 
This  being  the  case,  suppose  that  we  have  a  given  iterate  value  ¯x  ∈  C . 
Let  us  linearize  the  function  f (x) at  x = ¯x.  This  linearization  is: 
x)T (x − ¯
x) + ∇f ( ¯
z1 (x) := f ( ¯
x)  , 
which  is  the  ﬁrst-order  Taylor  expansion  of  f (·) at  x.  Since  we  can  easily 
¯
do  linear  optimization  on  C ,  let  us  solve: 

x)T (x − ¯
x) + ∇f ( ¯
LP  :  minimizex  z1 (x) = f ( ¯
x) 
x ∈ C , 

s.t. 

which  simpliﬁes  to: 

2 

LP  :  minimizex  ∇f ( ¯x)T x 
x ∈ C . 

s.t. 

∗
Let  x  denote  the  optimal  solution  to  this  problem.  Then  since  C  is 
∗
is  also  in  C , and  we  can 
a  convex  set,  the  line  segment  joining  x  and  x 
¯
perform  a  line-search  of  f (x)  over  this  segment.  That  is,  we  solve: 
∗  − ¯
x + α(x 
LS  :  minimizeα  f ( ¯
x)) 
0 ≤ α ≤ 1  . 

s.t. 

α  denote  the  solution  to  this  line-search  problem.  We  re-set  ¯
Let  ¯
x:
∗  − ¯
x ←  ¯ α(x 
x)
x + ¯
¯

and  repeat  this  process. 
The  formal  description  of  this  method,  called  the  conditional  gradient 
method  (or  the  Frank-Wolfe) method,  is: 

3 

Step  0:  Initialization.  Start  with  a  feasible  solution  x0  ∈  C . Set 
k = 0.  Set  LB ← −∞. 
Step  1:  Update  upper  bound.  Set  U B ← f (xk ).  Set  ¯x ← x . 
k
Step  2:  Compute  next  iterate. 

–  Solve  the  problem 

x) + ∇f ( ¯
x)T (x − ¯
z¯ = minx  f ( ¯
x)
x ∈ C , 

s.t. 

∗  denote  the  solution. 
and  let  x 
–  Solve  the  line-search  problem: 

∗  − ¯
minimizeα  f ( ¯x + α(x
x)) 
0 ≤ α ≤ 1  , 

s.t. 

and  let  ¯α  denote  the  solution. 
k+1  ← ¯
∗  − ¯
x)
x + ¯
α(x 
–  Set  x
Step  3:  Update  Lower  Bound.  Set  LB ← max{LB , z¯}. 
Step  4:  Check  Stopping  Criteria.  If  |U B − LB | ≤ ,  stop.  Oth-
erwise,  set  k ← k + 1  and  go  to  Step  1. 

The  upper  bound  values U B  are  simply  the  ob jective  function  values  of 
the iterates f (xk ) for k = 0, . . ..  This is a monotonically decreasing sequence 
because  the  line-search  guarantees  that  each  iterate  is  an  improvement  over 
the  previous  iterate. 
The  lower  bound  values  LB  result  from  the  convexity  of  f (x)  and  the 
gradient  inequality  for  convex  functions: 
x)T (x − ¯
x)  for  any  x ∈ C . 
f (x) ≥ f ( ¯
x) + ∇f ( ¯

4 

Therefore 

min f (x) ≥ min f ( ¯
x) + ∇f ( ¯
x)T (x − ¯
x) =  ¯
z , 
x∈C
x∈C 
and  so  the  optimal  ob jective  function  value  of  P  is  bounded  below  by  ¯z . 
The  following  theorem  concerns  convergence  of  the  conditional  gradient 
method: 

Theorem  1.1  Conditional  Gradient  Convergence  Theorem  Suppose 
that  C  is  a  bounded  set,  and  that  there  exists  a  constant  L  for  which 
(cid:8)∇f (x) − ∇f (y)(cid:8) ≤ L(cid:8)x − y(cid:8) 
for  al l  x, y ∈ C .  Then  there  exists  a  constant  Ω > 0  for  which  the  fol lowing 
is  true: 
f (x k ) − min f (x) ≤ 
Ω 
x∈C 
k 

.q.e.d. 

1.1  Proof  of  Theorem  1.1 

1.2 

Illustration  of  the  Conditional  Gradient  Method 

Consider  the  following  instance  of  P : 

P  :  minimize  f (x) 
x ∈ C , 

s.t. 

where 

and 

f (x) = f (x1 , x2 ) = −32x1  + x1  − 8x2  + x2 
2
4

C  = {(x1 , x2 )  |  x1  − x2  ≤ 1,  2.2x1  + x2  ≤ 7, x1  ≥ 0, x2  ≥ 0}  . 
� 
� 
Notice  that  the  gradient  of  f (x1 , x2 )  is  given  by  the  formula: 
− 32 
3 
4x1 
2x2  − 8 

∇f (x1 , x2 ) = 

.

5 

∇f (0.5, 3.0) = 

Suppose  that  xk 
= ¯
x  = (0.5, 3.0)  is  the  current  iterate  of  the  Frank-
Wolfe  method,  and  the  current  lower  bound  is  LB  = −100.0.  We  compute 
x) = f (0.5, 3.0) = −30.9375  and  we  compute  the  gradient  of  f (x) at  ¯
�  � 
� 
� 
f ( ¯
x:
−31.5 
− 32  =  −2.0 
3 
4x1 
2x2  − 8 
We  then  create  and  solve  the  following  linear  optimization  problem: 
LP  : ¯z = minx1 ,x2  −30.9375 − 31.5(x1  − 0.5) − 2.0(x2  − 3.0) 
x1  − x2  ≤ 1

2.2x1  + x2  ≤ 7

x1  ≥ 0

x2  ≥ 0  .


.

s.t.	

The  optimal  solution  of  this  problem  is: 
∗ 
∗	
∗ 
x  = (x1 , x2 ) = (2.5, 1.5)  , 
and  the  optimal  ob jective  function  value  is: 
z¯ = −50.6875  . 
Now  we  perform  a  line-search  of  the  1-dimensional  function 
− ¯
∗  − ¯
− ¯
x))  =  −32( ¯
∗
∗	
x1 ))4
x1 )) + ( ¯
x1  + α(x1 
x1  + α(x1 
x + α(x 
f ( ¯
− ¯
− ¯
−8( ¯
∗	
∗
x2 ))2
x2  + α(x2 
x2  + α(x2 
x2 )) + ( ¯
over  α  ∈  [0, 1].  This  function  attains  its  minimum  at  α  = 0.7165  and  we 
¯
therefore  update  as  follows: 

←  ¯ α(x 
∗ − ¯
x) = (0.5, 3.0)+0.7165((2.5, 1.5)−(0.5, 3.0)) = (1.9329, 1.9253)
x+ ¯	

k+1 
x 
and 

LB ← max{LB , z¯} = max{−100, −50.6875} = −50.6875  . 
The  new  upper  bound  is 
U B = f (x k+1 ) = f (1.9329, 1.9253) = −59.5901  . 
This  is  illustrated  in  Figure  1. 

6 

8 

7 

6 

5 

4 

3 

2 

1 

0 

−1 
−1

−0.5 

0 

0.5 

1 

1.5 

2 

2.5 

3 

