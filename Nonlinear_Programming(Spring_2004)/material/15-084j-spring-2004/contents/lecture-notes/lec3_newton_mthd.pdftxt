Newton’s  Method  for  Unconstrained  Optimization 

Robert   M.   Freund  

February,  2004 

2004 Massachusetts Institute of Technology.

1 

1  Newton’s  Method 

Suppose  we  want  to  solve:


(P:) 

min f (x)

x ∈ (cid:3) . 
n

At  x = ¯x,  f (x)  can  be  approximated  by:

x) + ∇f ( ¯
f (x) ≈ h(x) := f ( ¯
(x − ¯
x)T (x − ¯
x)(x − ¯
1

x)tH ( ¯
x), 
x) + 
2
which  is  the  quadratic  Taylor  expansion  of  f (x) at  x  = ¯x.  Here  ∇f (x) is 
the  gradient  of  f (x) and H (x)  is  the  Hessian  of  f (x). 
Notice  that  h(x)  is  a  quadratic  function,  which  is  minimized  by  solving 
∇h(x) = 0.  Since  the  gradient  of  h(x) is: 
x)(x − ¯
∇h(x) = ∇f ( ¯
x) + H ( ¯
x)  , 

we  therefore  are  motivated  to  solve: 
x)(x − ¯
∇f ( ¯
x) + H ( ¯
x) = 0  , 

which  yields 

−1∇f ( ¯
x − ¯
x = −H ( ¯
x). 
x)
The direction −H ( ¯
x)−1∇f ( ¯
x)  is  called  the Newton  direction,  or  the Newton
step  at  x = ¯x.

This  leads  to  the  following  algorithm  for  solving  (P):


Newton’s  Method: 
Step  0  Given  x0 , set  k ← 0 
Step  1  dk  = −H (xk )−1∇f (xk ).  If  dk  = 0,  then  stop. 
Step  2  Choose  step-size  αk  = 1. 

2


Step  3  Set  xk+1  ← xk  + αk dk , k ← k + 1.  Go  to  Step  1. 

Note  the  following: 
•	 The  method  assumes  H (xk )  is  nonsingular  at  each  iteration.

•	 There  is  no  guarantee  that  f (x
k+1 ) ≤ f (x

k ). 
•	 Step  2  could  be  augmented  by  a  line-search  of  f (xk  + αdk ) to ﬁnd  an 
optimal  value  of  the  step-size  parameter  α. 

Recall  that  we  call  a matrix  SPD  if  it  is  symmetric  and  positive  deﬁnite. 
Proposition  1.1  If  H (x)  is  SPD  and  d := −H (x)−1∇f (x)  (cid:8)= 0,  then  d is 
a  descent  direction,  i.e.,  f (x + αd)  < f (x)  for  al l  suﬃciently  smal l  values 
of  α. 
Proof:   It  is  suﬃcient  to  show  that  ∇f (x)td = −∇f (x)tH (x)−1∇f (x) < 0. 
This will  clearly  be  the  case  if H (x)−1  is  SPD.  Since H (x)  is  SPD,  if  v (cid:8)= 0, 
−1
−1 
−1 
v) = v tH (x
v)tH (x)(H (x
)
)
)
0 < (H (x
thereby  showing  that H (x)−1  is  SPD. 

v , 

1.1  Rates  of  convergence 

A  sequence  of  numbers  {si}  exhibits  linear  convergence  if  limi→∞ si  = ¯s 
and 
|si+1  − s¯|
|si  − s¯|
lim 
i→∞ 
If  δ =  0  in  the  above  expression,  the  sequence  exhibits  superlinear  conver-
gence. 
A sequence of numbers {si} exhibits quadratic convergence if limi→∞ si  = 
s¯ and 
|si+1  − s¯|
= δ < ∞. 
i→∞  |si  − s¯|2 
lim 

= δ < 1. 

3 

Linear   convergence:  

(cid:1) (cid:2)i 
1.1.1  Examples  of  Rates  of  Convergence  
1 
: 0.1,  0.01,  0.001,  etc.  s¯ = 0. 
10 
|si+1  − s¯| 
|si  − s¯| 

= 0.1. 

si  = 

=

Superlinear   convergence:  
|si+1  − s¯| 
|si  − s¯| 

i!
= 
(i + 1)! 
(cid:1) (cid:2)(2i )
1 
Quadratic  convergence:   si  = 
10 
s¯ = 0. 

|si+1  − s¯| 
|si  − s¯|2 

= 

(102i )2

102i+1

= 1.

si  =  i! : 1,  1 
1 
1
1
1
2 ,  6 ,  24  ,  125  , etc.  s¯ = 0. 
→ 0 as  i → ∞. 

1 
i + 1 

: 0.1, 0.01, 0.0001, 0.00000001, etc. 

1.2  Quadratic  Convergence  of  Newton’s  Method 

We  have  the  following  quadratic  convergence  theorem.  In  the  theorem,  we 
use  the  operator  norm  of  a matrix M : 
(cid:11)M (cid:11) := max {(cid:11)M x(cid:11) | (cid:11)x(cid:11) = 1} . 
x 

Theorem  1.1  (Quadratic  Convergence  Theorem)  Suppose f (x) is twice 
continuously  diﬀerentiable  and  x  is  a  point  for  which ∇f (x  ) = 0.  Suppose 
∗
∗
that  H (x)  satisﬁes  the  fol lowing  conditions: 
•  there  exists  a  scalar  h >0  for  which  (cid:11)[H (x 
∗ )]−1(cid:11) ≤  1 
h 
•  there  exists  scalars  β >  0  and  L >  0  for  which  (cid:11)H (x) − H (x  )(cid:11) ≤ 
∗
L(cid:11)x − x  (cid:11) for  al l  x satisfying  (cid:11)x − x  (cid:11) ≤ β 
∗
∗
(cid:4) 
(cid:3) 
Let  x satisfy  (cid:11)x − x 
∗(cid:11) < γ := min  β , 2h  , and  let  xN  := x − H (x)−1∇f (x).
3L 
(cid:1) 
(cid:2) 
Then: 
∗(cid:11) ≤ (cid:11)x − x 
(i)  (cid:11)xN  − x 
∗(cid:11)2 
L
2(h−L(cid:4)x−x ∗ (cid:4)) 
(ii)  (cid:11)xN  − x  (cid:11) < (cid:11)x − x  (cid:11) < γ 
∗
∗

4 

(iii)  (cid:11)xN  − x  (cid:11) ≤ (cid:11)x − x 
∗(cid:11)2
∗

(cid:1)  (cid:2) 
3L
2h 

Example  1:   Let  f (x) = 7x −  ln(x).  Then  ∇f (x) =  f 
(cid:5) (x) = 7 −  1  and 
(cid:5)(cid:5) (x) =  1 
∗ 
x 
1
H (x) =  f 
2  .  It  is  not  hard  to  check  that  x  =  7  = 0.142857143  is 
x
the  unique  global  minimum.  The  Newton  direction  at  x  is 
(cid:5) 
(cid:6) 
(cid:5) (
−1∇f (x) = − 
= x − 7x . 
= −x 
7 − 
d = −H (x)
1 
)
f 
x
2
2
(cid:5)(cid:5)
(
) 
x 
x
f
Newton’s  method  will  generate  the  sequence  of  iterates  {x }  satisfying: 
k
x  = x k  + (x k  − 7(x k )2 ) = 2x k  − 7(x
k+1 
k )2 

. 

Below  are  some  examples  of  the  sequences  generated  by  this  method  for 
diﬀerent  starting  points. 

xk 
xk  xk 
k  xk 
0.01 
0.1 
0 
0 
1.0 
1  −5.0 
0.0193 
0.13 
0 
2  −185.0 
0.03599257 
0 
0.1417 
3  −239, 945.0 
0.14284777 
0.062916884 
0 
4  −4.0302 × 1011  0 
0.142857142  0.098124028 
5  −1.1370 × 1024  0 
0.142857143  0.128849782 
6  −9.0486 × 1048  0 
0.142857143  0.1414837 
7  −5.7314 × 1098  0 
0.142857143  0.142843938 
8  −∞ 
0.142857143  0.142857142 
0 
9  −∞ 
0 
0.142857143  0.142857143 
10  −∞ 
0.142857143  0.142857143 
0 

By  the  way,  the  “range  of  quadratic  convergence”  for  Newton’s  method 
for  this  function  happens  to  be 
x ∈ (0.0  ,  0.2857143)  . 

5 

Example  2:  f (x) = − ln(1 − x1  − x2 ) − ln x1  − ln x2 . 
⎡ 
⎤ 
−  1 
⎢ ⎣ 
⎥ ⎦ , 
1 
1−x1−x2 
x1 
−  1 
1 
1−x1−x2 
(cid:1) 
(cid:2)2 
(cid:1) 
x2 
1 
1 
+ 
1−x1−x2 
x1 
(cid:2)2 
(cid:2)2 

∇f (x) = 
(cid:2)2 

⎡ ⎢ 
⎢ ⎢ 
⎣ 

(cid:1) 
1 
1−x1−x2 
(cid:1) 
H (x) = 
1 
(cid:2) 
1−x1−x2 
∗ ) = 3.295836866. 
,  f (x 

(cid:1) 
∗  =  1 
3 , 1 
x 
3 

⎤ ⎥ 
⎥ ⎥ . 
(cid:2)2  ⎦ 

(cid:2)2 
(cid:1) 
1 
+ 
x2 

(cid:1) 
1 
1−x1−x2 

(cid:11)xk  − x 
∗(cid:11) 
xk 
k  xk 
2 
1 
0.58925565098879 
0.05 
0  0.85 
1  0.717006802721088  0.0965986394557823  0.450831061926011 
0.238483249157462 
2  0.512975199133209  0.176479706723556 
0.0630610294297446 
3  0.352478577567272  0.273248784105084 
4  0.338449016006352  0.32623807005996 
0.00874716926379655 
−5 
7.41328482837195e
5  0.333337722134802  0.333259330511655 
−8 
1.19532211855443e
6  0.333333343617612  0.33333332724128 
−16 
7  0.333333333333333  0.333333333333333 
1.57009245868378e

Comments: 
•	 The  convergence  rate  is  quadratic: 
(cid:11)xN  − x  (cid:11) 
∗
≤∗(cid:11)2
3L 
(cid:11)x − x 
2h 
•  We  typically  never  know  β , h, or L.  However,  there  are  some  amazing 
(cid:13) 
exceptions,  for  example  f (x) = − 
n 
ln(xj ),  as  we  will  soon  see. 
j=1 
•	 The  constants β , h, and L depend  on  the  choice  of norm used,  but  the 
method  does  not.  This  is  a  drawback  in  the  concept.  But  we  do  not 
know  β , h, or  L  anyway. 
(cid:4)xN −
∗ (cid:4) 
x (cid:4)2  ≤
•	 In  the  limit  we  obtain 
(cid:4)x−x 
∗

L
2h 

6 

•	 We  did  not  assume  convexity,  only  that H (x  )  is  nonsingular  and  not 
∗
∗
badly  behaved  near  x  . 
•	 One  can  view  Newton’s  method  as  trying  successively  to  solve 
∇f (x) = 0 

by  successive  linear  approximations. 
•	 Note  from  the  statement  of  the  convergence  theorem  that  the  iterates 
of  Newton’s  method  are  equally  attracted  to  local  minima  and  local 
maxima.  Indeed,  the  method  is  just  trying  to  solve ∇f (x) = 0. 
•	 What if H (xk ) becomes increasingly singular (or not positive deﬁnite)? 
In  this  case,  one  way  to  “ﬁx”  this  is  to  use 

H (x k ) + I  .	
(1) 
•	 Comparison  with  steepest-descent.  One  can  think  of  steepest-descent 
as   → +∞ in  (1)  above. 
•	 The  work  per  iteration  of  Newton’s  method  is  O(n3 ) 
•	 So-called  “quasi-Newton  methods”  use  approximations  of  H (xk ) at 
each  iteration  in  an  attempt  to  do  less  work  per  iteration. 

2  Proof  of  Theorem  1.1 

√ 
The  proof  relies  on  the  following  two  “elementary”  facts.  For  the  ﬁrst  fact, 
let  (cid:11)v(cid:11) denote  the  usual  Euclidian  norm  of  a  vector,  namely  (cid:11)v(cid:11) :=  vT v . 
The  operator  norm  of  a matrix M  is  deﬁned  as  follows: 
(cid:11)M (cid:11) := max {(cid:11)M x(cid:11) | (cid:11)x(cid:11) = 1} . 
x 

Proposition  2.1  Suppose  that M  is  a  symmetric  matrix.  Then  the  fol low-
ing  are  equivalent: 
−1(cid:11) ≤  1 
1.  h > 0  satisﬁes  (cid:11)M
h 
2.  h > 0  satisﬁes  (cid:11)M v(cid:11) ≥ h · (cid:11)v(cid:11) for  any  vector  v 

7 

You  are  asked  to  prove  this  as  part  of  your  homework  for  the  class. 

Proposition  2.2  Suppose  that  f (x)  is  twice  diﬀerentiable.  Then 
(cid:14)  1 
[H (x + t(z − x))] (z − x)dt  . 
∇f (z ) − ∇f (x) = 
0 
Proof:   Let φ(t) := ∇f (x+ t(z −x)).  Then φ(0) = ∇f (x) and φ(1) = ∇f (z ), 
(cid:2) (t) = [H (x + t(z − x))] (z  − x).  From  the  fundamental  theorem  of 
and  φ 
calculus,  we  have: 
∇f (z ) − ∇f (x) =  φ(1) − φ(0) 
(cid:14)  1 
0 (cid:14)  1 
0 

[H (x + t(z − x))] (z − x)dt  . 

(cid:2) 
φ  (t)dt 

= 

=

Proof of Theorem 1.1:  We  have: 
xN  − x  =  x − H (x)−1∇f (x) − x 
∗ 
∗
=  x − x  + H (x)−1  (∇f (x  ) − ∇f (x)) 
∗
∗
(cid:14)  1 
0 

=  x − x  + H (x)−1 
∗
(cid:14)  1 
0 

=  H (x)−1 

∗  − x))] (x  − x)dt  (from  Proposition  2.2) 
∗ 
[H (x + t(x 

∗  − x)) − H (x)] (x  − x)dt 
∗ 
[H (x + t(x 

8 

Therefore 
(cid:11)xN  − x  (cid:11) ≤ (cid:11)H (x)−1(cid:11)
∗

(cid:14)  1 
(cid:11) [H (x + t(x 
∗  − x)) − H (x)] (cid:11)(cid:11)(x  − x)(cid:11)dt 
∗ 
(cid:14)  1 
0 
≤ (cid:11)x  − x(cid:11)(cid:11)H (x)−1(cid:11) 
L · t · (cid:11)(x 
∗  − x)(cid:11)dt 
∗
0  (cid:14)  1 
=  (cid:11)x  − x(cid:11)2(cid:11)H (x)−1(cid:11)L 
∗ 
0 
∗(cid:11)x  − x(cid:11)2(cid:11)H (x)−1(cid:11)L 
2 

tdt 

= 

(from  Proposition  2.1) 

We  now  bound  (cid:11)H (x)−1(cid:11). Let  v  be  any  vector.  Then 
(cid:11)H (x)v(cid:11)  =  (cid:11)H (x  )v + (H (x) − H (x  ))v(cid:11) 
∗
∗
≥ (cid:11)H (x  )v(cid:11) − (cid:11)(H (x) − H (x  ))v(cid:11) 
∗
∗
≥  h · (cid:11)v(cid:11) − (cid:11)H (x) − H (x  )(cid:11)(cid:11)v(cid:11) 
∗
≥  h · (cid:11)v(cid:11) −L(cid:11)x  − x(cid:11) · (cid:11)v(cid:11) 
∗
= (h − L(cid:11)x  − x(cid:11)) · (cid:11)v(cid:11)  . 
∗
Invoking  Proposition  2.1  again,  we  see  that  this  implies  that 
−1(cid:11) ≤ 
(cid:11)H (x)
1 
h − L(cid:11)x ∗  − x(cid:11) 
Combining  this  with  the  above  yields 
(cid:11)xN  − x  (cid:11) ≤ (cid:11)x  − x(cid:11)2 
∗
∗ 
L 
2 (h − L(cid:11)x ∗  − x(cid:11)) 
, 
which  is  (i)  of  the  theorem.  Because  L(cid:11)x  − x(cid:11) <  3  we  have: 
∗ 
2h
L(cid:11)x  − x(cid:11) 
∗ 
(cid:2) (cid:11)x  − x(cid:11) = (cid:11)x  − x(cid:11)  ,
<  (cid:1) 
2
h 
(cid:11)xN  − x 
∗ − x(cid:11) 
∗ (cid:11) ≤ (cid:11)x 
∗ 
∗
3 
2 (h − L(cid:11)x ∗  − x(cid:11)) 
2  h −  2h 
3 

. 

9 

which  establishes  (ii)  of  the  theorem.  Finally,  we  have 
(cid:1) 
(cid:2)  = (cid:11)x  −x(cid:11)2 
≤ (cid:11)x  −x(cid:11)2 
(cid:11)xN −x  (cid:11) ≤ (cid:11)x  −x(cid:11)2 
∗ 
∗
∗ 
∗ 
L 
h − 2h 
2 
3 

L 
2 (h − L(cid:11)x ∗  − x(cid:11)) 

3L
2h

, 

which  establishes  (iii)  of  the  theorem. 

3  Newton’s  Method  Exercises 

1.  (Newton’s Method)  Suppose  we  want  to  minimize  the  following  func-
tion:

f (x) = 9x − 4 ln(x − 7)

over  the  domain X  = {x | x > 7} using  Newton’s  method. 
(a)  Give  an  exact  formula  for  the Newton  iterate  for  a  given  value  of 
x. 
(b)  Using  a  calculator  (or  a  computer,  if  you  wish),  compute  ﬁve 
iterations  of  Newton’s  method  starting  at  each  of  the  following 
points,  and  record  your  answers: 
•  x = 7.40 
•  x = 7.20 
•  x = 7.01 
•  x = 7.80 
•  x = 7.88 
(c)  Verify empirically that Newton’s method will converge  to  the op-
timal  solution  for  all  starting  values  of x in  the  range  (7, 7.8888). 
What  behavior  does  Newton’s  method  exhibit  outside  of  this 
range? 

2.  (Newton’s Method)  Suppose  we  want  to  minimize  the  following  func-
tion:

f (x) = 6x − 4 ln(x − 2) − 3 ln(25 − x)

over  the  domain X  = {x | 2 < x < 25} using  Newton’s  method. 
(a)  Using  a  calculator  (or  a  computer,  if  you  wish),  compute  ﬁve 
iterations  of  Newton’s  method  starting  at  each  of  the  following 
points,  and  record  your  answers: 

10 

•  x = 2.60 
•  x = 2.70 
•  x = 2.40 
•  x = 2.80 
•  x = 3.00 
(b)  Verify empirically that Newton’s method will converge  to  the op
-
timal  solution  for  all  starting  values  of  x  in  the  range  (2, 3.05).

What  behavior  does  Newton’s  method  exhibit  outside  of  this

range?


3.  (Newton’s  Method)  Suppose  that  we  seek  to  minimize  the  following 
function: 
f (x1 , x2 ) = −9x1−10x2+θ(− ln(100−x1−x2 )−ln(x1 )−ln(x2 )−ln(50−x1+x2 )), 
where  θ  is  a  given  parameter,  on  the  domain  X  =  {(x1 , x2 )  |  x1  >

0, x2  >  0, x1  + x2  <  100, x1  − x2  <  50}.  This  exercise  asks  you  to 

implement  Newton’s  method  on  this  problem,  ﬁrst  without  a  line-

search,  and  then  with  a  line-search.  Run  your  algorithm  for  θ  = 10

and  for  θ = 100,  using  the  following  starting  points.

•  x0  = (8, 90)T . 
•  x0  = (1, 40)T . 
•  x0  = (15, 68.69)T . 
•  x0  = (10, 20)T . 
(a)  When  you  run  Newton’s  method  without  a  line-search  for  this

problem and with these starting points, what behavior do you observe?

(b)  When  you  run  Newton’s  method  with  a  line-search  for  this  prob
-
lem,  what  behavior  do  you  observe?


4.  (Pro jected  Newton’s  Method)  Prove  Proposition  6.1  of  the  notes  on 
Pro jected  Steepest  Descent. 

5.  (Newton’s Method) In class we described Newton’s method as a method 
for  ﬁnding  a  point  x  for  which  ∇f (x  )  =  0.  Now  consider  the  fol
-
∗
∗
lowing  setting,  where  we  have  n  nonlinear  equations  in  n  unknowns

x = (x1 , . . . , xn ):


11 

g1 (x) = 0 
.
.
. 
.
.
. 
.
.
. 
gn (x) = 0  , 
which  we  conveniently  write  as 

g(x) = 0  . 

Let  J (x)  denote  the  Jacobian  matrix  (of  partial  derivatives)  of  g(x). 
Then  at  x = ¯x  we  have 

x + d) ≈ g( ¯
x) + J ( ¯
g( ¯
x)d , 

this  being  the  linear  approximation  of  g(x) at  x  = ¯x.  Construct  a 
version  of Newton’s method  for  solving  the  equation  system  g(x) = 0. 

6.  (Newton’s Method) Suppose that f (x) is a strictly convex twice-continuously 
diﬀerentiable  function,	 and  consider  Newton’s  method  with  a  line-
x)]−1∇f ( ¯
x, we compute the Newton direction d = −[H ( ¯
x)
search.  Given ¯
and  the  next  iterate  ˜x  is  chosen  to  satisfy: 

x := arg min f ( ¯
x + αd)  . 
˜
α 

Prove  that  the  iterates  of  this  method  converge  to  the  unique  global 
minimum  of  f (x). 

7.  Prove  Proposition  2.1  of  the  notes  on  Newton’s  method. 

8.  Bertsekas,  Exercise  1.4.1,  page  99. 

12 

