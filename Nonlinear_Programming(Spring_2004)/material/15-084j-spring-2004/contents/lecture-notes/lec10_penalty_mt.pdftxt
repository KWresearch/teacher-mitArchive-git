Penalty  and  Barrier  Methods 
for  Constrained  Optimization 

Robert   M.   Freund  

February,  2004 

2004 Massachusetts Institute of Technology.

1 

1 

Introduction 

Consider  the  constrained  optimization  problem  P : 

P  :  minimize  f (x) 
x 
gi (x) ≤ 0,
s.t. 

i = 1, . . . , m 

i = 1, . . . , k 

hi (x) = 0,
x ∈ (cid:4) , 
n
whose  feasible  region  we  denote  by 
F  := {x ∈ (cid:4) | gi (x) ≤ 0, i = 1, . . . , m, hi (x) = 0, i = 1, . . . , k}. 
n
Barrier  and  penalty  methods  are  designed  to  solve  P  by  instead  solving  a 
sequence  of  specially  constructed  unconstrained  optimization  problems. 
In  a penalty method,  the  feasible  region  of P  is  expanded  from F  to 
all  of  (cid:4)n ,  but  a  large  cost  or  “penalty”  is  added  to  the  ob jective  function 
for  points  that  lie  outside  of  the  original  feasible  region  F . 
In  a  barrier  method,  we  presume  that  we  are  given  a  point  xo  that 
lies  in  the  interior  of  the  feasible  region  F ,  and  we  impose  a  very  large  cost 
on  feasible points  that  lie ever closer  to  the boundary of F ,  thereby creating 
a  “barrier”  to  exiting  the  feasible  region. 

2  Penalty  Methods 

Consider  the  constrained  optimization  problem  P : 

P  :  minimize  f (x)

x

gi (x) ≤ 0,
s.t. 

hi (x) = 0,
x ∈ (cid:4) . 
n

2 

i = 1, . . . , m 

i = 1, . . . , k 

By  converting  the  constraints  “hi (x) = 0”  to  “hi (x)  ≤  0, −hi (x)  ≤  0”,  we 
can  assume  that  P  is  of  the  form 

P  :  minimize  f (x) 
x 
g(x) ≤ 0, 
s.t. 
x ∈ (cid:4) , 
n
where  we  write  g(x) := (g1 (x), . . . , gm (x))T  for  convenience. 
Deﬁnition:  A  function  p(x) : (cid:4)n  → (cid:4) is  called  a  penalty  function  for  P  if 
p(x)  satisﬁes: 
•  p(x) = 0  if  g(x) ≤ 0 and 
•  p(x) > 0 if  g(x)  (cid:6)≤ 0. 

Example:  

(cid:1) 
m 
(max{0, gi (x)})2  . 
i=1 

p(x) = 

We  then  consider  solving  the  following  penalty  program: 

P (c)  :  minimize  f (x) + cp(x) 
x 
x ∈ (cid:4)
s.t. 
n
for  an  increasing  sequence  of  constants  c  as  c  →  +∞.  Note  that  in  the 
program  P (c)  we  are  assigning  a  penalty  to  the  violated  constraints.  The 
scalar  quantity  c  is  called  the  penalty  parameter. 
Let  ck  ≥ 0, k  = 1, . . . , ∞,  be  a  sequence  of  penalty  parameters  that 
satisﬁes ck+1  > ck  for all k and limk→∞ ck  = +∞. Let q(c, x) = f (x)+cp(x), 
∗  denote 
and  let  xk  be  the  exact  solution  to  the  program  P (ck ),  and  let  x 
any  optimal  solution  of  P . 

The following Lemma presents some basic properties of penalty meth-

ods: 

3 

Lemma   2.1   (Penalty   Lemma).  

1.	 q(ck , xk ) ≤ q(ck+1 , x

k+1 )
2.	 p(xk ) ≥ p(x

k+1 )

3.  f (xk ) ≤ f (x
k+1 )

4.  f (x  ) ≥ q(ck , xk ) ≥ f (xk ) 
∗

Proof:  

1.  We  have 

k+1 ) = f (x k+1 ) + ck+1p(x k+1 ) ≥ f (x k+1 ) + ck p(x k+1 )
q(ck+1 , x

≥ f (x k ) + ck p(x k ) = q(ck , x k )


2. 

and


f (x k ) + ck p(x k ) ≤ f (x k+1 ) + ck p(x

k+1 )
k+1 ) ≤ f (x
f (x k+1 ) + ck+1p(x 
k ) + ck+1p(x k ) 
k )p(xk+1 ), whereby p(xk ) ≥ p(xk+1 ). 
k )p(xk ) ≥ (c
k+1  − c
k+1  − c
Thus (c	
3.  From  the  proof  of  (1.),


k+1 ) ≥ f (x

f (x k+1 ) + ck p(x 
k ) + ck p(x k ). 
k+1 ) ≥ f (x
But  p(xk ) ≥ p(xk+1 ),  which  implies  that  f (x
k ). 
4.  f (xk ) ≤ f (xk ) + ck p(xk ) ≤ f (x  ) + ck p(x  ) = f (x  ). 
∗
∗
∗

q.e.d. 

The  next  result  concerns  convergence  of  the  penalty  method. 

4 

Theorem  2.1  (Penalty  Convergence  Theorem).  Suppose  that  f (x), 
g(x), and  p(x),  are  continuous  functions.  Let  {x }, k  = 1, . . . , ∞, be  a 
k
sequence  of  solutions  to  P (ck ).  Then  any  limit  point  ¯x  of  {x }  solves  P  . 
k

Proof:   Let  ¯x  be  a  limit  point  of  {x }.  From  the  continuity  of  the  functions 
k
involved,  limk→∞ f (xk ) = f ( ¯x).  Also,  from  the  Penalty  Lemma, 

Thus 

q  :=  lim  q(ck , x k ) ≤ f (x  ). 
∗	
∗
k→∞ 
(cid:2)	
(cid:3) 
lim  ck p(x k ) =  lim  q(ck , x k ) − f (x k ) = q  − f ( ¯x). 
∗
k→∞ 
k→∞ 
But  ck  → ∞,  which  implies  from  the  above  that 
lim  p(x k ) = 0. 
k→∞ 

x) ≤ 0,
Therefore,  from  the  continuity  of  p(x) and  g(x),  p( ¯	
x) = 0,  and  so  g( ¯
that  is,  x  is  a  feasible  solution  of  P .  Finally,  from  the  Penalty  Lemma, 
¯
f (xk )  ≤  f (x  )  for  all  k , and  so  f ( ¯	
x)  ≤  f (x  ),  which  implies  that  x  is  an 
∗
∗	
¯
optimal  solution  of  P . 
q.e.d. 

An  often-used  class  of  penalty  functions  is: 
(cid:1) 
m 
[max{0, gi (x)}]
,  where  q ≥ 1. 
q
i=1 

p(x) = 

(1) 

We  note  the  following: 
•	 If  q  = 1,  p(x)  in  (1)  is  called  the  “linear  penalty  function”.  This 
function  may  not  be  diﬀerentiable  at  points  where  gi (x) = 0  for  some 
i. 
•	 Setting  q = 2  is  the most  common  form  of  (1)  that  is used  in  practice, 
and  is  called  the  “quadratic  penalty  function”  (for  obvious  reasons). 

5 

•  If  we  denote 
g + (x) = (max{0, gi (x)}, . . . , max{0, gm (x)})T , 
then  the  quadratic  penalty  function  can  be  written  as 

p(x) = (g + (x))T (g + (x)). 

2.1  Karush-Kuhn-Tucker  Multipliers   in  Penalty   Methods 

The  penalty  function  p(x)  is  in  actuality  a  function  only  of  g+ (x),  where 
g+ (x) = max{0, gi (x)}  (the  nonnegative  part  of  gi (x)),  i = 1, . . . , m.  Then 
we  can  write  p(x) = γ (g+ (x)),  where  γ (y)  is  a  function  of  y ∈ ((cid:4)m )+  . 
Two  examples  of  this  type  of  penalty  function  are 
(cid:1) 
m 
yi , 
i=1 
which  corresponds  to  the  linear  penalty  function,  and 
(cid:1) 
m 
2
yi , 
i=1 

γ (y) = 

γ (y) = 

which  corresponds  to  the  quadratic  penalty  function. 

Note  that  even  if  γ (y)  is  continuously  diﬀerentiable,  p(x) might  not 
be  continuously  diﬀerentiable,  since  g+ (x)  is  not  diﬀerentiable  at  points  x 
where  g + (x) = 0  for  some  i.  However,  if  we  assume  the  following: 
i 

∂ γ (y) 
∂ yi 

= 0  at  yi  = 0,

i = 1, . . . , m, 

(2)

then  p(x)  is  diﬀerentiable  whenever  the  functions  gi (x)  are  diﬀerentiable, 
i = 1, . . . , m,  and  we  can  write 
m (cid:1) ∂ γ (g+ (x))
∇p(x) = 
∂ yi
i=1 

∇gi (x). 

(3)

6 

Now  let  xk  solve  P (ck ).  Then  xk  will  satisfy 
∇f (x k ) + ck ∇p(x k ) = 0, 
m (cid:1) 
i=1 

∇f (x k ) + ck 

that  is, 

∂ γ (g+ (xk )) 
∇gi (x k ) = 0. 
∂ yi 

Let  us  deﬁne 

Then 

∂ γ (g+ (xk )) 
u k 
i  = ck 
. 
∂ yi 
m (cid:1) 
i ∇gi (x k ) = 0, 
∇f (x k ) + 
u k 
i=1 
and  so  we  can  interpret  the  uk  as  a  sort  of  vector  of  Karush-Kuhn-Tucker 
multipliers.  In  fact,  we  have: 

(4) 

Lemma   2.2   Suppose  γ (y)  is  continuously  diﬀerentiable  and  satisﬁes  (2), 
and  that  f (x)  and  g(x)  are  diﬀerentiable.  Let  uk  be  deﬁnied  by  (4).  Then 
k  → ¯
x, and  x  satisﬁes  the  linear  independence  condition  for  gradient 
if  x
¯
k  → ¯
vectors  of  active  constraints,  then  u
u,  where  u  is  a  vector  of  Karush-
¯
Kuhn-Tucker  multipliers  for  the  optimal  solution  ¯x  of  P . 

Proof:   From  the Penalty Convergence Theorem,  ¯x  is an optimal  solution of 
x) <  0}. For  i ∈ N ,  gi (xk ) <  0 
x) = 0} and  N  =  {i  | gi ( ¯
P . Let  I  =  {i  | gi ( ¯
for  all  k  suﬃciently  large,  so  uk  = 0  for  all  k  suﬃciently  large,  whereby 
¯ui  = 0  for  i ∈ N . 
i 
From  (4)  and  the  deﬁnition  of  a  penalty  function,  it  follows  that 
k  ≥ 0 for  i ∈ I ,  for  all  k  suﬃciently  large,  . 
ui 
k  → ¯
u  as  k  → ∞.  Then  ¯ui  = 0  for  i  ∈ N .  From  the 
Suppose  u
continuity  of  all  functions  involved, 
(cid:1) 
m 
ui ∇g(x k ) = 0 
∇f (x k ) + 
k
i=1 

7 

vi ∇gi (x k ) = 
k

vi ∇gi (x k ) = 
k 

implies 

(cid:1) 
m 
∇f ( ¯
ui∇g( ¯
x) + 
¯
x) = 0. 
i=1 
ui  = 0  for  all  i ∈ N . Thus 
u ≥ 0 and  ¯
From  the  above  remarks, we  also  have  ¯
¯u  is  a  vector  of  Karush-Kuhn-Tucker  multipliers.  It  therefore  remains  to 
k  → ¯
u  for  some  unique  ¯u. 
show  that  u
Suppose  {uk }
k=1  has  no  accumulation  point.  Then  (cid:10)uk (cid:10) → ∞. But 
∞
then  deﬁne 
k 
u
k  = 
(cid:10)uk (cid:10) , 
v 
and  then  (cid:10)vk (cid:10) = 1  for  all  k ,  and  so  the  sequence  {vk }∞  has  some  accumu-
v .  For  all  i ∈ N ,  vi  = 0  for  k  large,  whereby  ¯
vi  = 0  for  i ∈ N ,
k=1 
k 
lation  point  ¯
(cid:4) 
(cid:5) 
(cid:1)
(cid:1) 
(cid:1)
and 
m 
m
k  ∇gi (x k ) = 
ui (cid:10)uk (cid:10)
i∈I
i=1 
i=1 
for  k  large.  As  k  → ∞, we  have  xk  →  ¯x,  vk  → v¯, and  (cid:10)uk (cid:10) → ∞, and  so 
the  above  becomes 
(cid:1) 
v¯i∇gi ( ¯x) = 0, 
i∈I 
and  (cid:10)v¯(cid:10)  =  1,  which  violates  the  linear  independence  condition.  Therefore 
{uk } is  a  bounded  sequence,  and  so  has  at  least  one  accumulation  point. 
Now  suppose  that  {u } has  two  accumulation points,  ˜
k 
u.  Note 
u  and  ¯
ui  = 0  for  i ∈ N , and  so 
¯
ui  = 0  and  ˜
(cid:1) 
(cid:1) 
ui∇gi ( ¯
ui∇gi ( ¯
x) = −∇f ( ¯
x) = 
¯
˜
x), 
i∈I 
i∈I
(cid:1) 
ui )∇gi ( ¯
ui  − ˜
x) = 0. 
( ¯
i∈I 
ui  − ˜
ui  = 0  for  all  i  ∈ I , and  so 
But  by  the  linear  independence  condition,  ¯
¯
u = ˜
ui  = ˜
ui .  This  then  implies  that  ¯
u. 

−∇f (xk )
(cid:10)uk (cid:10) 

so  that 

8 

q.e.d. 

Remark.   The  quadratic  penalty  function  satisﬁes  the  condition  (2),  but 
that  the  linear  penalty  function  does  not  satisfy  (2). 

2.2  Exact  Penalty  Methods 

The  idea  in  an  exact  penalty  method  is  to  choose  a  penalty  function  p(x) 
and  a  constant  c  so  that  the  optimal  solution  ˜x  of  P (c) is also an optimal 
solution  of  the  original  problem  P . 

p(x) := 

Theorem  2.2  Suppose  P  is  a  convex  program  for  which  the  Karush-Kuhn-
Tucker  conditions  are  necessary.  Suppose  that 
(cid:1) 
m 
(gi (x))+  . 
i=1 
Then  as  long  as  c  is  chosen  suﬃciently  large,  the  sets  of  optimal  solutions 
of  P (c)  and  P  coincide.  In  fact,  it  suﬃcies  to  choose  c > maxi{ui 
},  where 
∗
∗ u  is  a  vector  of  Karush-Kuhn-Tucker  multipliers. 
Proof:   Suppose  ˆx  solves  P .  For  any  x ∈ Rn  we  have: 
(cid:6)
(cid:6) 
(gi (x))+  ≥  f (x) + 
∗ 
m 
m
m (cid:6)  ∗
(ui gi (x))+

q(c, x) = f (x) + c 
i=1 
i=1

≥  f (x) + 
m (cid:6)  ∗
ui gi (x) 
i=1 
≥  f (x) + 
x)T (x − ˆ
x) + ∇gi ( ˆ
m (cid:6)  ∗ 
ui (gi ( ˆ
x)) 
i=1 
x)T (x − ˆ
∇gi ( ˆ
=  f (x) + 
x) 
ui 
(cid:6) 
=  f (x) − ∇f ( ˆ
x)T (x − ˆ
i=1 
x) 
≥  f ( ˆ
m 
x))+  = q(c, ˆ
x) + c 
x) = f ( ˆ
(gi ( ˆ
x). 
i=1 

x) ≤ q(c, x)  for  all  x,  and  therefore  ˆ

Thus  q(c, ˆ
x  solves  P (c).

9


and  so 

(5) 

x))+  = f ( ˆ
(gi ( ˆ
x), 

x solves  P (c).  Then  if  ˆ
Next  suppose  that  ¯
x solves  P , we have: 
(cid:1) 
(cid:1) 
m 
m
x))+  ≤ f ( ˆ
x) + c 
(gi ( ¯
x) + c 
f ( ¯
i=1 
i=1 
(cid:1) 
m 
x) − c 
x) ≤ f ( ˆ
x))+  . 
f ( ¯
(gi ( ¯
i=1 
However,  if  ¯x is  not  feasible  for  P ,  then 
m (cid:6)  ∗ 
x − ˆ
x)  ≥  f ( ˆ
x) + ∇f ( ˆ
x)T ( ¯
x)
f ( ¯
x) − 
x − ˆ
u  ∇gi ( ˆ
m (cid:6)  ∗
x)T ( ¯
=  f ( ˆ
x)
i 
i=1 
≥  f ( ˆ
x) − gi ( ¯
(cid:6)  ∗ 
(cid:6) 
x) + 
ui (gi ( ˆ
x)) 
i=1 
x) − 
x) − c 
m 
m
=  f ( ˆ
ui gi ( ¯
x) > f ( ˆ
i=1 
i=1 
(cid:6) 
which  contradicts  (5).  Thus  x  is  feasible  for  P .  That  being  the  case, 
¯
x) ≤ f ( ˆ
x) − c 
m 
x))+  = f ( ˆ
(gi ( ¯
x)  from  (5),  and  so  ¯
x solves  P .
f ( ¯
i=1 
q.e.d. 

x))+  , 
(gi ( ¯

2.3  Penalty  Methods  for   Inequality and  Equality  Constraints  

The presentation of penalty methods has assumed either that the problem P 
has  no  equality  constraints,  or  that  the  equality  constraints  have  been  con-
verted  to  inequality  constraints.  For  the  latter,  the  conversion  is  easy  to do, 
but  the  conversion  usually  violates  good  judgement  in  that  it  unnecessarily 
complicates the problem.  Furthermore,  it can cause the linear independence 
condition to be automatically violated for every feasible solution.  Therefore, 
instead  let  us  consider  the  constrained  optimization  problem  P  with  both 
inquality  and  equality  constraints: 

10 

P  :  minimize  f (x) 
x 
g(x) ≤ 0, 
s.t. 

h(x) = 0, 
x ∈ (cid:4) , 
n
where g(x) and h(x) are vector-valued functions, that is, g(x) := (g1 (x), . . . , gm (x))T 
and  h(x) := (h1 (x), . . . , hk (x))T  for  notational  convenience. 
Deﬁnition:  A  function  p(x) : (cid:4)n  → (cid:4) is  called  a  penalty  function  for  P  if 
p(x)  satisﬁes: 
•  p(x) = 0  if  g(x) ≤ 0 and  h(x) = 0 
•  p(x) > 0 if  g(x)  (cid:6)
≤ 0 or  h(x)  (cid:6)
= 0. 

The  main  class  of  penalty  functions  for  this  general  problem  are  of 
the  form: 
(cid:1) 
(cid:1) 
k 
m
[max{0, gi (x)}]q  + 
|hi (x)| ,  where  q ≥ 1. 
q
i=1 
i=1 

p(x) = 

All  of  the  results  of  this  section  extend  naturally  to  problems  with 
equality  constraints  and  for  penalty  functions  of  the  above  form.  For  ex-
ample,  in  the  analogous  result  of  Theorem  2.2,  it  suﬃces  to  choose  c > 
max{u1 , . . . , um , |v  |, . . . , |vk 
|}.
∗ 
∗
∗
∗ 
1

3  Barrier  Methods 

Deﬁnition.  A  barrier  function  for  P  is  any  function  b(x) :  (cid:4)n  → (cid:4)  that 
satisﬁes 
•  b(x) ≥ 0  for  all  x  that  satisfy  g(x) < 0,  and 

11 

•  b(x) → ∞ as  limx maxi{gi (x)} →0. 

The  idea  in  a  barrier method  is  to  dissuade  points  x  from  ever  approaching 
the  boundary  of  the  feasible  region.  We  consider  solving: 

B (c)  :  minimize  f (x) +  1 b(x)
c 

s.t. 

Example: 

g(x) < 0, 
x ∈ (cid:4) . 
n
for  a  sequence  of  ck  →  +∞.  Note  that  the  constraints  “g(x)  <  0”  are 
eﬀectively  unimportant  in  B (c),  as  they  are  never  binding  in  B (c). 
m (cid:1) 
1 
−gi (x)
i=1 
Suppose  g(x) = (x − 4, 1 − x)T , x ∈ (cid:4) .  Then 
1
1 
1
x − 1 
4 − x 
. 
Let  r(c, x) = f (x) +  1 b(x)  .  Let  the  sequence  {ck } satisfy  ck+1  > ck
and  ck  → ∞ as  k → ∞. Let  xk  denote  the  exact  solution  to  B (ck ) . 
c 
The following Lemma presents some basic properties of barrier meth-

b(x) = 

b(x) = 

+ 

ods. 

Lemma   3.1   (Barrier   Lemma).  
1.  r(ck , xk ) ≥ r(ck+1 , x
k+1 )

2.  b(xk ) ≤ b(x
k+1 )

3.  f (xk ) ≥ f (x
k+1 )

12 

4.  f (x  ) ≤ f (xk ) ≤ r(ck , xk ). 
∗

Proof:  

1. 

2. 

and 

r(ck , x k ) = f (x k ) + 
≥ f (x k+1 ) + 

1 
ck+1 

b(x k ) 

b(x k ) ≥ f (x k ) + 
1
ck 
1 
ck+1 

b(x k+1 ) = r(ck+1 , x k+1 ) 

f (x k ) + 

b(x k ) ≤ f (x k+1 ) + 
1 
ck 

1 
b(x k+1 ) 
ck 

f (x k+1 ) + 

1 
ck+1 

b(x k+1 ) ≤ f (x k ) + 

1 
ck+1 

b(x k ). 

(cid:8) 
(cid:7) 
(cid:7) 
Summing  and  rearranging,  we  have 
b(x k ) ≤
− 
1
1
ck 
ck 

1 
ck+1 

− 

1 
ck+1 

(cid:8) 
k+1 ).
b(x 

k+1 ) ≥ b(x
k ). 
Since  ck  < ck+1 ,  it  follows  that  b(x

3.  From  the  proof  of  (1.), 

b(x k ) ≥ f (x k+1 ) + 
1 
1 
b(x k+1 ). 
f (x k ) + 
ck+1 
ck+1 
But  from  (2.),  b(xk ) ≤ b(xk+1 ).  Thus  f (xk ) ≥ f (xk+1 ). 

4.  f (x  ) ≤ f (xk ) ≤ f (xk ) +  1  b(xk ) = r(ck , xk ). 
∗
ck 

13 

q.e.d. 

Let  N (, x)  denote  the  ball  of  radius    centered  at  the  point  x.  The 
next  result  concerns  convergence  of  the  barrier method. 

Theorem  3.1  (Barrier  Convergence   Theorem).   Suppose  f (x),  g(x), 
and  b(x)  are  continuous  functions.  Let  {xk }, k  = 1, . . . , ∞,  be  a  sequence 
∗  of  P  for 
of  solutions  of  B (ck ).  Suppose  there  exists  an  optimal  solution  x 
which  N (, x  ) ∩ {x  |  g(x) < 0} (cid:6)= ∅  for  every   > 0  .  Then  any  limit  point 
∗
¯x  of  {x }  solves  P . 
k

Proof:   Let  ¯x  be  any  limit  point  of  the  sequence  {x }.  From  the  continuity 
k
x) ≤ 0.  Thus 
of f (x) and g(x),  limk→∞ f (xk ) = f ( ¯
x) and limk→∞ g(xk ) = g( ¯
¯x  is  feasible  for  P . 

x) ≤ f (x  )+. 
∗
x) < 0 and f ( ˜
For any  > 0 , there exists ˜
x such that g( ˜
For  each  k , 

x) ≥ r(ck , x k ). 
x) ≥ f ( ˜
1 
1
∗
b( ˜
b( ˜
x) + 
) +  + 
f (x 
ck 
ck 
Therefore for k suﬃciently large, f (x  )+ 2 ≥ r(ck , xk ), and since r(ck , xk ) ≥ 
∗
∗
f (x  )  from  (4.)  of  the  Barrier  Lemma,  then 
) + 2 ≥  lim  r(ck , x k ) ≥ f (x  ). 
∗
∗
f (x 
k→∞ 

This  implies  that 

We  also  have 

∗
lim  r(ck , x k ) = f (x  ). 
k→∞ 

∗
f (x 

) ≤ f (x k ) ≤ f (x k ) + 

1 
b(x k ) = r(ck , x k ). 
ck 

Taking  limits  we  obtain 

∗
f (x 

) ≤ f ( ¯x) ≤ f (x 
∗

), 

14 

whereby  ¯x  is  an  optimal  solution  of  P . 
q.e.d. 

A  typical  class  of  barrier  functions  are: 
(cid:1) 
m 
(−gi (x))
−q ,  where  q > 0. 
i=1 

b(x) = 

3.1  Karush-Kuhn-Tucker  Multipliers   in  Barrier   Methods 

Let 

b(x) = γ (g(x)), 

where  γ (y) :  (cid:4)m  → (cid:4),  and  assume  that  γ (y)  is  continuously  diﬀerentiable 
for  all  y < 0.  Then 
m (cid:1) ∂ γ (g(x))
∇gi (x),
∇b(x) = 
∂ yi
i=1 
and  if  xk  solves  B (ck ),  then ∇f (xk ) +  1  ∇b(xk ) = 0,  that  is, 
ck 
1  (cid:1) ∂ γ (g(xk )) ∇gi (x k ) = 0. 
m 
ck 
∂ yi
i=1 

∇f (x k ) + 

(6) 

Let  us  deﬁne 

Then  (6)  becomes: 

1  ∂ γ (g(xk )) 
k ui  = 
. 
ck 
∂ yi 
m (cid:1) 
∇f (x k ) + 
i ∇gi (x k ) = 0. 
u k 
i=1 

(7) 

(8) 

Therefore  we  can  interpret  the  uk  as  a  sort  of  vector  of  Karush-
Kuhn-Tucker  multipliers.  In  fact,  we  have: 

15 

Lemma   3.2   Let  P  satisfy  the  conditions  of  the  Barrier  Convergence  The-
orem.  Suppose  γ (y)  is  continuously  diﬀerentiable  and  let  uk  be  deﬁned  by 
(7).  Then  if  xk  →  ¯
x  , and  ¯
x  satisﬁes  the  linear  independence  condition  for 
gradient  vectors  of  active  constraints,  then  uk  →  ¯
u  is  a  vector  of 
u  ,  where  ¯
Karush-Kuhn-Tucker  multipliers  for  the  optimal  solution  ¯x  of  P . 

Proof:  

x) = 0} and N  = {i  | gi ( ¯
k  → ¯
x) < 0}. For 
x  and  let  I  = {i  | gi ( ¯
Let  x
all  i ∈ N , 
→ 0 as  k → ∞, 
1  ∂ γ (g(xk ))
k ui  = 
∂ yi 
ck 
since  ck  → ∞ and  gi (xk ) → gi ( ¯
x))  
x) < 0,  and  ∂ γ(g ( ¯
∂ yi 
all  i, and  k  suﬃciently  large. 

is  ﬁnite.  Also  ui 
k

≥ 0 for 

u ≥ 0,  and  ¯
u  as  k  → ∞.  Then  ¯
Suppose  uk  →  ¯
ui  = 0  for  all  i  ∈ N . 
From  the  continuity  of  all  functions  involved,  (8)  implies  that 
(cid:1) 
m 
x) = 0, u ≥ 0,  u g( ¯
ui∇gi ( ¯
∇f ( ¯
T 
¯
x) = 0.
¯
¯
x) + 
i=1 
Thus  u  is  a  vector  of  Kuhn-Tucker  multipliers.  It  remains  to  show  that 
¯
uk  →  ¯
u  for  some  unique  u.  The  proof  that  uk  →  ¯
u  for  some  unique  u  is
¯
¯
exactly  as  in  Lemma  2.2. 
q.e.d. 

4  Exercises 

1.  Show  that  the  penalty  method  problem  can  be  formulated  as  follows: 
ﬁnd  sup  inf {f (x) + cp(x)} . 
c≥0  x∈X
Next  show  that  this  problem  is  equivalent  to: 
inf  sup{f (x) + cp(x)} . 
x∈X c≥0

ﬁnd 

16 

2.  To  use  a  barrier  function  method,  we  must  ﬁnd  a  point  x  ∈ X  that 
satisﬁes  gi (x) <  0, i = 1, . . . , m.  Consider  the  following  procedure  for 
computing  such  a  point: 
Initialization:  Select  x1  ∈ X , set  k ← 1

Main  Iteration:

Step  1.	 Let  I  :=  {i  |  gi (xk )  <  0}.
If  I  =  {1, . . . , m}, stop  with  xk 
satisfying  gi (x) < 0, i = 1, . . . , m.  Otherwise,  select  j /∈ I  and  go 
to  Step  2. 
Step  2.  Use  a  barrier  function  method  to  solve  the  following  problem 
starting  from  xk : 

s.t.	

minimizex  gj (x) 
gi (x) ≤ 0 for  i ∈ I 
x ∈ X . 
Let xk+1  be an optimal  solution of  this problem.  If gj (xk+1 ) ≥ 0, 
stop;  there  is  no  solution  x  satisfying  x  ∈ X  and  gi (x)  <  0, i  = 
1, . . . , m.  Otherwise,  set  k ← k + 1  and  repeat  Step  1. 
Show  that  the  above  procedure  stops  in  at  most  m  main  iterations 
with  a  point  x  ∈ X  that  satisﬁes  gi (x) <  0, i = 1, . . . , m,  or  with  the 
valid  conclusion  that  no  such  point  exists. 
3.  Consider  the  problem  to  minimize  c x  sub ject  to  Ax  =  b  and  x  ≥ 0, 
t
where  A  ∈ (cid:4)m×n  and  rank(A) = m.  Suppose  we  are  given  a  feasible 
solution  ¯x > 0  and  consider  the  following  logarithmic  barrier  problem 
parameterized  by  the  barrier  parameter  : 
(cid:6) 
B (θ) :  minimizex  c x − θ 
n 
T
j=1 

ln(xj ) 

s.t.	

Ax = b 
x > 0. 

Let  us  denote  the  ob jective  function  of  B (θ) by  fθ (x). 

17 

(a)  Consider  the  second-order  Taylor  series  approximation  of  fθ (x) 
at  x = ¯x: 

x) + ∇fθ ( ¯
x)T d +  1 dT ∇2fθ ( ¯
P T  :  minimized  fθ ( ¯
x)d
2

Ad = 0  . 
s.t. 
Let  X  denote  the  n × n  diagonal  matrix  whose  diagonal  entries 
¯
are  precisely  the  components  of  ¯x: 
⎞ 
⎛ ¯x1 
⎟ ⎟ ⎟ ⎠ 
⎜ ⎜ ⎜ ⎝ 
0 
. . . 
. 
0 

0 
. . . 
0 
. . . 
. . . 
. . . 
. . .  ¯xn 

¯ X  := 

0 
¯x2 
. . . 
0 

x)?  for ∇2fθ ( ¯
Using this notation, what is the formula for ∇fθ ( ¯
x)?
(b)  Derive  a  closed  form  solution  for  the  solution  dˆ of  P T. 
(c)  The  standard  linear  optimization  dual  problem  is  given  by: 

D  :  maximizeπ ,s  bT π 

s.t.	

AT π + s = c 
s ≥ 0. 
Now  consider  the  following  problem:

P R  :  minimizeπ ,s  (cid:10) 1 X s − e(cid:10)
¯

θ 

s.t. 

AT π + s = c . 

Construct a  closed  form  solution  ( ˆ s) to P R,  and  show  that  the 
π , ˆ

direction  of  part  (3b)  satisﬁes:

x − 
dˆ = ¯

1 
¯ 
X 2 ˆ
s . 
θ 

18 

