Quadratic  Functions,  Optimization,  and  Quadratic 
Forms 

Robert   M.   Freund  

February,  2004 

2004 Massachusetts Institute of Technology.

1 

1  Quadratic  Optimization 

2 

A quadratic optimization problem is an optimization problem of the form: 

(QP) :  minimize  f (x) :=  1  xT Qx + c x
T
2

s.t. 

x ∈ (cid:3) . 
n

Problems  of  the  form  QP  are  natural  models  that  arise  in  a  variety 
of  settings.  For  example,  consider  the  problem  of  approximately  solving 
an  over-determined  linear  system  Ax  =  b,  where  A  has  more  rows  than 
columns.  We might  want  to  solve: 

(P1 ) :  minimize  (cid:4)Ax − b(cid:4) 

s.t. 

x ∈ (cid:3) . 
n

Now notice that (cid:4)Ax−b(cid:4)2  = xT AT Ax−2bT Ax+bT b, and so this problem 
is  equivalent  to: 

(P1 ) :  minimize  xT AT Ax − 2bT Ax + bT b 

s.t. 

x ∈ (cid:3) , 
n

which  is  in  the  format  of  QP. 
A symmetric matrix is a square matrix Q ∈ (cid:3)n×n  with the property that 
Qij  = Qj i  for  all  i, j  = 1, . . . , n  . 

3 

We  can  alternatively  deﬁne  a matrix  Q  to  be  symmetric  if 

QT  = Q . 

We denote  the  identity matrix  (i.e., a matrix with all 1’s on  the diagonal 
⎛ 1 0 
⎞ 
and  0’s  everywhere  else)  by  I ,  that  is, 
. . .  0  ⎟ 
⎜ 0 1 
⎜ ⎜ ⎝ 
⎟ ⎟ ⎠ 
. . .  0 
. . . 
. . . 
. . . 
. . . 
, 
0 0 
. . .  1 

I  = 

and  note  that  I  is  a  symmetric matrix. 
The  gradient  vector  of  a  smooth  function  f (x) : (cid:3)n  → (cid:3) is  the  vector  of 
⎞
⎛
ﬁrst  partial  derivatives  of  f (x): 
⎜
⎟ 
.  ⎟
⎜ 
∂ f (x) 
⎝
⎠ 
∂x1 
.. 
∂ f (x) 
∂xn 

∇f (x) := 

. 

The  Hessian  matrix  of  a  smooth  function  f (x) :  (cid:3)n  → (cid:3) is  the  ma-
trix  of  second  partial  derivatives.  Suppose  that  f (x) :  (cid:3)n  → (cid:3) is  twice 
diﬀerentiable,  and  let 

∂ 2f (x)
∂xi  ∂xj 
Then  the  matrix H (x)  is  a  symmetric  matrix,  reﬂecting  the  fact  that 

[H (x)]ij  := 

. 

∂ 2f (x) 
∂xi  ∂xj 

= 

∂ 2f (x) 
∂xj  ∂xi 

. 

A  very  general  optimization  problem  is: 

4 

(GP) :  minimize  f (x) 

s.t. 

x ∈ (cid:3) , 
n

where  f (x) :  (cid:3)n  → (cid:3)  is  a  function.  We  often  design  algorithms  for  GP 
by  building  a  local  quadratic  model  of  f (·) at a given point  x  = ¯x. We 
form  the  gradient ∇f ( ¯x)  (the  vector  of  partial  derivatives)  and  the  Hessian 
H ( ¯x) (the matrix of second partial derivatives), and approximate GP by the 
following  problem  which  uses  the  Taylor  expansion  of  f (x) at  x  = ¯x  up  to 
the  quadratic  term. 

x)(x − ¯
2 (x − ¯
x)T (x − ¯
x) + ∇f ( ¯
(P2 ) :  minimize  f˜(x) := f ( ¯
x) +  1 
x)T H ( ¯
x) 

s.t. 

x ∈ (cid:3) . 
n

This  problem  is  also  in  the  format  of  QP. 

Notice  in  the  general model QP  that we  can  always  presume  that Q  is  a 
symmetric matrix,  because: 

1
T x  Qx =  x T (Q + QT )x
2 
¯
and  so  we  could  replace  Q  by  the  symmetric matrix  Q :=  1 
2 (Q + QT ). 

Now  suppose  that


1 
x T Qx + c T x

2 
where  Q  is  symmetric.  Then  it  is  easy  to  see  that: 
∇f (x) = Qx + c 

f (x) := 

and 

H (x) = Q . 

5 

Before  we  try  to  solve  QP,  we  ﬁrst  review  some  very  basic  properties  of 
symmetric matrices. 

2	 Convexity,  Deﬁniteness  of  a  Symmetric  Matrix, 
and  Optimality  Conditions 
•	 A  function  f (x) : (cid:3)n  → (cid:3) is  a  convex  function  if 
f (λx+(1−λ)y) ≤ λf (x)+(1−λ)f (y) for  all x, y ∈ (cid:3) ,  for  all λ ∈ [0, 1]. 
n
•	 A  function  f (x)  as  above  is  called  a  strictly  convex  function  if  the 
inequality  above  is  strict  for  all  x  (cid:8)= y  and  λ ∈ (0, 1). 
•	 A  function  f (x) : (cid:3)n  → (cid:3) is  a  concave  function  if 
f (λx+(1−λ)y) ≥ λf (x)+(1−λ)f (y) for  all x, y ∈ (cid:3) ,  for  all λ ∈ [0, 1]. 
n
•	 A  function  f (x)  as  above  is  called  a  strictly  concave  function  if  the 
inequality  above  is  strict  for  all  x  (cid:8)= y  and  λ ∈ (0, 1). 

Here  are  some  more  deﬁnitions: 

•	 Q  is  symmetric  and  positive  semideﬁnite  (abbreviated  SPSD  and  de-
noted  by  Q (cid:10) 0)  if 

T x	 Qx ≥ 0  for  all  x ∈ (cid:3)n  . 
•	 Q  is  symmetric  and  positive  deﬁnite  (abbreviated  SPD  and  denoted 
by  Q (cid:11) 0)  if 
T x	 Qx > 0  for  all  x ∈ (cid:3)n , x  (cid:8)= 0  . 

1 
Theorem  1   The  function  f (x) :=  xT Qx + cT x  is  a  convex  function  if  and 
2
only  if  Q  is  SPSD. 

6 

Proof:   First,  suppose  that  Q  is  not  SPSD.  Then  there  exists  r  such  that 
r
T Qr  <  0.  Let  x  =  θr .  Then  f (x) =  f (θr) = 1 
2 θ2 rT Qr + θcT r  is  strictly 
concave  on  the  subset  {x  |  x  =  θr}, since  rT Qr  <  0.  Thus  f (·)  is  not  a 
convex  function. 
Next,  suppose  that  Q  is  SPSD.  For  all  λ ∈ [0, 1],  and  for  all  x, y , 
f (λx + (1 − λ)y) =  f (y + λ(x − y)) 
(y + λ(x − y))T Q(y + λ(x − y)) + cT (y + λ(x − y))
λ2 (x − y)T Q(x − y) + λcT x + (1 − λ)c
T Qy + λ(x − y)T Qy + 
T Qy + λ(x − y)T Qy + 
λ(x − y)T Q(x − y) + λcT x + (1 − λ)c

=

≤ 

=


1 
2

y

1 
2

1 
2

1 
2

T

y

y

T

1
2

y

(1 − λ)yT Qy + λcT x + (1 − λ)c
1
1 
λxT Qx + 
= 
2
2
=  λf (x) + (1 − λ)f (y)  , 

T

y

thus  showing  that  f (x)  is  a  convex  function. 
Corollary  2   f (x)  is  strictly  convex  if  and  only  if  Q (cid:11) 0. 

f (x)  is  concave  if  and  only  if  Q (cid:12) 0. 

f (x)  is  strictly  concave  if  and  only  if  Q ≺ 0. 

f (x)  is  neither  convex  nor  concave  if  and  only  if  Q  is  indeﬁnite. 

Theorem  3   Suppose  that  Q  is  SPSD.  The  function  f (x) :=  1 
T
T Qx + c x 
2 x

∗	
∗
attains  its  minimum  at  x	
if  and only if  x  solves  the  equation  system: 
∇f (x) = Qx + c = 0  . 

7

∗ satisﬁes Qx
∗ + c = 0. Then for any x, we have:
Proof: Suppose that x
∗ + (x − x
∗ ))
f (x) = f (x
∗ + (x − x
∗ + (x − x
∗ + (x − x
∗ )) + cT (x
∗ ))T Q(x
∗ ))
= 1
2 (x
∗ + cT (x − x
∗ + (x − x
2 (x − x
∗ )T Q(x − x
∗ ) + cT x
∗ )T Qx
∗ )T Qx
∗ + 1
∗ )
= 1
2 (x
2 (x − x
∗ )T Q(x − x
∗ + (x − x
∗ ) + cT x
∗ + c) + 1
∗ )T (Qx
∗ )T Qx
∗
= 1
2 (x
2 (x − x
∗ )T Q(x − x
∗ )T Qx
∗ + cT x
∗ + 1
∗ )
= 1
2 (x
∗ )T Q(x − x
2 (x − x
∗ ) + 1
∗ )
= f (x
≥ f (x
∗ ) ,

∗ is a minimizer of f (x).
thus showing that x
∗ + c (cid:8)= 0.
∗ is a minimizer of f (x), but that d := Qx
Next, suppose that x
Then:
∗ + αd) + cT (x
∗ + αd)T Q(x
∗ + αd) = 1
∗ + αd)
f (x
2 (x
∗ + 1
∗ + αdT Qx
∗ )T Qx
∗ + αcT d
= 1
2 α2dT Qd + cT x
2 (x
∗ + c) + 1
∗ ) + αdT (Qx
2 α2dT Qd
= f (x
∗ ) + αdT d + 1
2 α2dT Qd .
= f (x

But notice that for α < 0 and suﬃciently small, that the last expression
∗ ), and so f (x
∗ + αd) < f (x
∗ ). This contradicts
will be strictly less than f (x
∗ is a minimizer of f (x), and so it must be true that
the supposition that x
∗ + c = 0.
d = Qx
Here are some examples of convex quadratic forms:

• f (x) = xT x

8

• f (x) = (x − a)T (x − a)
• f (x) = (x − a)T D(x − a), where
⎛
⎜⎝ d1
0

D =

⎞
⎟⎠

0

dn

. . .

is a diagonal matrix with dj > 0, j = 1, . . . , n.
• f (x) = (x − a)T M T DM (x − a), where M is a non-singular matrix and
D is as above.

3 Characteristics of Symmetric Matrices

−1 . Note that if M is

−1M x = xT x = (cid:4)x(cid:4)2 ,

A matrix M is an orthonormal matrix if M T = M
orthonormal and y = M x, then
(cid:4)y(cid:4)2 = yT y = xT M T M x = xT M
and so (cid:4)y(cid:4) = (cid:4)x(cid:4).
A number γ ∈ (cid:3) is an eigenvalue of M if there exists a vector ¯x (cid:8)= 0
such that M ¯x = γ ¯x. ¯x is called an eigenvector of M (and is called an
eigenvector corresponding to γ ). Note that γ is an eigenvalue of M if and
only if (M − γ I ) ¯x = 0, ¯x (cid:8)= 0 or, equivalently, if and only if det(M − γ I ) = 0.
Let g(γ ) = det(M − γ I ). Then g(γ ) is a polynomial of degree n, and so
will have n roots that will solve the equation
g(γ ) = det(M − γ I ) = 0 ,

including multiplicities. These roots are the eigenvalues of M .

Proposition 4 If Q is a real symmetric matrix, al l of its eigenvalues are
real numbers.

Proof: If s = a + bi is a complex number, let ¯s = a − bi. Then s · t = ¯s · ¯t,
s is real if and only if s = ¯s, and s · ¯s = a2 + b2 . If γ is an eigenvalue of Q,
for some x (cid:8)= 0, we have the following chains of equations:

9

Qx = γx
Qx = γx
¯Q · ¯x = ¯γ · ¯x
xT Q ¯x = xT ¯Q ¯x = xT (¯γ ¯x) = ¯γxT ¯x

as well as the following chains of equations:

Qx = γx
¯xT Qx = ¯xT (γx) = γ ¯xT x
xT Q ¯x = xT QT ¯x = ¯xT Qx = γ ¯xT x = γxT ¯x .

Thus ¯γxT ¯x = γxT ¯x, and since x (cid:8)= 0 implies xT ¯x (cid:8)= 0, ¯γ = γ , and so γ is
real.

Proposition 5 If Q is a real symmetric matrix, its eigenvectors correspond-
ing to diﬀerent eigenvalues are orthogonal.

Proof: Suppose

Then

Qx1 = γ1x1 and Qx2 = γ2x2 , γ1 (cid:8)= γ2 .

1 (γ2x2 ) = γ2xT
1 Qx2 = xT
1 x2 = (γ1x1 )T x2 = (Qx1 )T x2 = xT
γ1xT
1 x2 .
Since γ1 (cid:8)= γ2 , the above equality implies that xT
1 x2 = 0.

Proposition 6 If Q is a symmetric matrix, then Q has n (distinct) eigen-
vectors that form an orthonormal basis for (cid:3)n .

Proof: If all of the eigenvalues of Q are distinct, then we are done, as the
previous proposition provides the proof. If not, we construct eigenvectors

10

iteratively, as follows. Let u1 be a normalized (i.e., re-scaled so that its norm
is 1) eigenvector of Q with corresponding eigenvalue γ1 . Suppose we have k
mutually orthogonal normalized eigenvectors u1 , . . . , uk , with corresponding
eigenvalues γ1 , . . . , γk . We will now show how to construct a new eigenvector
uk+1 with eigenvalue γk+1 , such that uk+1 is orthogonal to each of the vectors
u1 , . . . , uk .
Let U = [u1 , . . . , uk ] ∈ (cid:3)n×k . Then QU = [γ1u1 , . . . , γk uk ].
Let V = [vk+1 , . . . , vn ] ∈ (cid:3)n×(n−k) be a matrix composed of any n − k
mutually orthogonal vectors such that the n vectors u1 , . . . , uk , vk+1 , . . . , vn
constitute an orthonormal basis for (cid:3)n . Then note that
U T V = 0

and

V T QU = V T [γ1u1 , . . . , γk uk ] = 0.
Let w be an eigenvector of V T QV ∈ (cid:3)(n−k)×(n−k) for some eigenvalue
γ , so that V T QV w = γw, and uk+1 = V w (assume w is normalized so that
uk+1 has norm 1). We now claim the following two statements are true:

(a) U T uk+1 = 0, so that uk+1 is orthogonal to all of the columns of U , and

(b) uk+1 is an eigenvector of Q, and γ is the corresponding eigenvalue of
Q.

Note that if (a) and (b) are true, we can keep adding orthogonal vectors
until k = n, completing the proof of the proposition.
To prove (a), simply note that U T uk+1 = U T V w = 0w = 0. To prove
(b), let d = Quk+1 − γuk+1 . We need to show that d = 0. Note that
d = QV w − γV w, and so V T d = V T QV w − γV T V w = V T QV w − γw = 0.
Therefore, d = U r for some r ∈ (cid:3)k , and so
r = U T U r = U T d = U T QV w − γU T V w = 0 − 0 = 0.
Therefore, d = 0, which completes the proof.

Proposition 7 If Q is SPSD, the eigenvalues of Q are nonnegative.

11

Proof: If γ is an eigenvalue of Q, Qx = γx for some x (cid:8)= 0. Then 0 ≤
xT Qx = xT (γx) = γxT x, whereby γ ≥ 0.

Proposition 8 If Q is symmetric, then Q = RDRT , where R is an or-
thonormal matrix, the columns of R are an orthonormal basis of eigenvec-
tors of Q, and D is a diagonal matrix of the corresponding eigenvalues of
Q.

⎛
⎞
Proof: Let R = [u1 , . . . , un ], where u1 , . . . , un are the n orthonormal eigen-
vectors of Q, and let
⎟⎠ ,
⎜⎝ γ1
. . .
0
γn
(cid:7)
where γ1 , . . . , γn are the corresponding eigenvalues. Then
0, if i (cid:8)= j
1, if i = j

(RT R)ij = uT
i uj =

D =

0

,

so RT R = I , i.e., RT = R

−1 .

Note that γiRT ui = γiei , i = 1, . . . , n (here, ei is the ith unit vector).
Therefore,

RT QR = RT Q[u1 , . . . , un ] = RT [γ1u1 , . . . , γnun ]
⎛
⎞
= [γ1e1 , . . . , γnen ]
⎜⎝ γ1
⎟⎠ = D .
0

. . .

γn

=

0

Thus Q = (RT )−1DR

−1 = RDRT .

Proposition 9 If Q is SPSD, then Q = M T M for some matrix M .

Proof: Q = RDRT = RD

1
2 D

1
2 RT = M T M , where M = D

1
2 RT .

12

Proposition 10 If Q is SPSD, then xT Qx = 0 implies Qx = 0.

Proof:
0 = xT Qx = xT M T M x = (M x)T (M x) = (cid:4)M x(cid:4)2 ⇒ M x = 0 ⇒ Qx = M T M x = 0.
Proposition 11 Suppose Q is symmetric. Then Q (cid:10) 0 and nonsingular if
and only if Q (cid:11) 0.

Proof:
(⇒) Suppose x (cid:8)= 0. Then xT Qx ≥ 0.
If xT Qx = 0, then Qx = 0,
which is a contradiction since Q is nonsingular. Thus xT Qx > 0, and so Q
is positive deﬁnite.
(⇐) Clearly, if Q (cid:11) 0, then Q (cid:10) 0. If Q is singular, then Qx = 0, x (cid:8)= 0
has a solution, whereby xT Qx = 0, x (cid:8)= 0, and so Q is not positive deﬁnite,
which is a contradiction.

4 Additional Properties of SPD Matrices

Proposition 12 If Q (cid:11) 0 (Q (cid:10) 0), then any principal submatrix of Q is
positive deﬁnite (positive semideﬁnite).

Proof: Follows directly.
Proposition 13 Suppose Q is symmetric. If Q (cid:11) 0 and
(cid:8)
(cid:9)

,

M =

Q c
cT
b
then M (cid:11) 0 if and only if b > cT Q
−1 c.
−1 c. Let x = (−cT Q
Proof: Suppose b ≤ cT Q
−1 , 1)T . Then
−1 c − 2cT Q
−1 c + b ≤ 0.
xT M x = cT Q

13

Thus M is not positive deﬁnite.

−1 c. Let x = (y , z ). Then xT M x = yT Qy +
Conversely, suppose b > cT Q
2z cT y + bz 2 . If x (cid:8)= 0 and z = 0, then xT M x = yT Qy > 0, since Q (cid:11) 0.
If z (cid:8)= 0, we can assume without loss of generality that z = 1, and so
xT M x = yT Qy + 2cT y + b. The value of y that minimizes this form is
y = −Q
−1 c, and at this point, yT Qy + 2cT y + b = −cT Q
−1 c + b > 0, and so
M is positive deﬁnite.
The kth leading principal minor of a matrix M is the determinant of the
submatrix of M corresponding to the ﬁrst k indices of columns and rows.

Proposition 14 Suppose Q is a symmetric matrix. Then Q is positive
deﬁnite if and only if al l leading principal minors of Q are positive.
Proof: If Q (cid:11) 0, then any leading principal submatrix of Q is a matrix M ,
(cid:8)
(cid:9)
where

Q =

M N
N T P

,

−1 (where R is or-
and M must be SPD. Therefore M = RDRT = RDR
thonormal and D is diagonal), and det(M ) = det(D) > 0.

Conversely, suppose all leading principal minors are positive. If n = 1,
then Q (cid:11) 0. If n > 1, by induction, suppose that the statement is true for
k = n − 1. Then for k = n,
(cid:9)
(cid:8)

,

Q =

M c
cT
b
where M ∈ (cid:3)(n−1)×(n−1) and M has all its principal minors positive, so
M (cid:11) 0. Therefore, M = T T T for some nonsingular T . Thus
(cid:8)
(cid:9)
T T T c
cT
b

.

Let

F =

(T T )−1
0
−cT (T T T )−1 1

(cid:9)

.

Q =
(cid:8)

14

(cid:9)

(cid:9)

(cid:8)

·

(cid:9)

(cid:8)
·
(cid:9)

Then

(cid:8)

F QF T =
(cid:8)

(T T )−1
T T T c
0
−cT (T T T )−1 1
(cid:8)
(cid:9)
cT
b
−1 −(T T T )−1 c
(T T )−1 c
·
0
I
T
0 b − cT (T T T )−1 c
b − cT (T T T )−1 c
=
0
0
1
Then det Q = b−cT (T T T )−1 c
> 0 implies b − cT (T T T )−1 c > 0, and so Q (cid:11) 0
det(F )2
from Proposition 13.

−1 −(T T T )−1 c
T
(cid:8)
0
1

=

T

(cid:9)

.

5 Quadratic Forms Exercises
1. Suppose that M (cid:11) 0. Show that M
−1 (cid:11) 0.
−1 exists and that M
2. Suppose that M (cid:10) 0. Show that there exists a matrix N satisfying
N (cid:10) 0 and N 2 := N N = M . Such a matrix N is called a “square
1
root” of M and is written as M
2 .
3. Let (cid:4)v(cid:4) denote the usual Euclidian norm of a vector, namely(cid:4)v(cid:4) :=
√
vT v . The operator norm of a matrix M is deﬁned as follows:
{(cid:4)M x(cid:4) | (cid:4)x(cid:4) = 1} .
(cid:4)M (cid:4) := maxx
Prove the following two propositions:
Proposition 1: If M is n × n and symmetric, then
{|λ| | λ is an eigenvalue of M } .
(cid:4)M (cid:4) = max
λ
Proposition 2: If M is m × n with m < n and M has rank m, then
(cid:10)
(cid:4)M (cid:4) =
λmax (M M T ) ,

where λmax (A) denotes the largest eigenvalue of a matrix A.

15

4. Let (cid:4)v(cid:4) denote the usual Euclidian norm of a vector, namely (cid:4)v(cid:4) :=
√
vT v . The operator norm of a matrix M is deﬁned as follows:
{(cid:4)M x(cid:4) | (cid:4)x(cid:4) = 1} .
(cid:4)M (cid:4) := maxx
Prove the following proposition:
Proposition: Suppose that M is a symmetric matrix. Then the
following are equivalent:
−1(cid:4) ≤ 1
(a) h > 0 satisﬁes (cid:4)M
h
(b) h > 0 satisﬁes (cid:4)M v(cid:4) ≥ h · (cid:4)v(cid:4) for any vector v
(c) h > 0 satisﬁes |λi (M )| ≥ h for every eigenvalue λi (M ) of M ,
i = 1, . . . , m.

5. Let Q (cid:10) 0 and let S := {x | xT Qx ≤ 1}. Prove that S is a closed
convex set.
6. Let Q (cid:10) 0 and let S := {x | xT Qx ≤ 1}. Let γi be a nonzero
eigenvalue of Q and let ui be a corresponding eigenvector normalized
so that (cid:4)ui(cid:4)2 = 1. Let ai := ui√
γi . Prove that ai ∈ S and −ai ∈ S .
7. Let Q (cid:11) 0 and consider the problem:
∗ = maximumx
(P) : z

cT x
xT Qx ≤ 1 .

s.t.

Prove that the unique optimal solution of (P) is:
(cid:11)
−1 c
= Q
cT Q−1 c
(cid:10)
with optimal ob jective function value
cT Q−1 c .

∗
x

=

∗

z

16

8. Let Q (cid:11) 0 and consider the problem:
∗ = maximumx
(P) : z

s.t.

cT x
xT Qx ≤ 1 .
For what values of c will it be true that the optimal solution of (P)
will be equal to c? (Hint: think eigenvectors.)
9. Let Q (cid:10) 0 and let S := {x | xT Qx ≤ 1}. Let the eigendecomposition
of Q be Q = RDRT where R is orthonormal and D is diagonal with
diagonal entries γ1 , . . . , γn . Prove that x ∈ S if and only if x = Rv for
some vector v satisfying
n(cid:12)
j=1

i ≤ 1 .
γiv2

10. Prove the following:
Diagonal Dominance Theorem: Suppose that M is symmetric and
that for each i = 1, . . . , n, we have:
(cid:12)
Mii ≥
j (cid:3)=i

|Mij | .

Then M is positive semideﬁnite. Furthermore, if the inequalities above
are all strict, then M is positive deﬁnite.
11. A function f (·) : (cid:3)n → (cid:3) is a norm if:
(i) f (x) ≥ 0 for any x, and f (x) = 0 if and only if x = 0
(ii) f (αx) = |α|f (x) for any x and any α ∈ (cid:3), and
(iii) f (x + y) ≤ f (x) + f (y).
(cid:11)
xtQx. Prove that fQ (x) is a norm if and only if Q
Deﬁne fQ (x) =
is positive deﬁnite.

12. If Q is positive semi-deﬁnite, under what conditions (on Q and c)
2 xtQx + ctx attain its minimum over all x ∈ (cid:3)n?, be
will f (x) = 1
unbounded over all x ∈ (cid:3)n?

13. Consider the problem to minimize f (x) = 1
2 xtQx + ctx sub ject to
Ax = b. When will this program have an optimal solution?, when
not?

17

14. Prove that if Q is symmetric and all its eigenvalues are nonnegative,
(cid:8)
(cid:9)
then Q is positive semi-deﬁnite.
2 3
15. Let Q =
. Note that γ1 = 1 and γ2 = 2 are the eigenvalues of
0 1
Q, but that xtQx < 0 for x = (2, −3)t . Why does this not contradict
(cid:13)n
(cid:13)p
the result of the previous exercise?
j=1 y2
16. A quadratic form of the type g(y) =
j +
j=p+1 dj yj + dn+1 is a
s eparable hybrid of a quadratic and linear form, as g(y) is quadratic in
the ﬁrst p components of y and linear (and separable) in the remaining
n − p components. Show that if f (x) = 1
2 xtQx + ctx where Q is
positive semi-deﬁnite, then there is an invertible linear transformation
y = T (x) = F x + g such that f (x) = g(y) and g(y) is a separable
hybrid, i.e., there is an index p, a nonsingular matrix F , a vector g
and constants dp , . . . , dn+1 such that
p(cid:12)
n(cid:12)
(F x + g)2
j +
j=1
j=p+1
17. An n×n matrix P is called a projection matrix if P T = P and P P = P .
Prove that if P is a pro jection matrix, then
a. I − P is a pro jection matrix.
b. P is positive semideﬁnite.
c. (cid:4)P x(cid:4) ≤ (cid:4)x(cid:4) for any x, where (cid:4) (cid:4) is the Euclidian norm.

dj (F x + g)j + dn+1 = f (x).

g(y) =

18. Let us denote the largest eigenvalue of a symmetric matrix M by
“λmax (M ).” Consider the program
∗ = maximumx xT M x
(Q) : z
(cid:4)x(cid:4) = 1 ,
∗ = λmax (M ).
where M is a symmetric matrix. Prove that z

s.t.

18

19. Let us denote the smallest eigenvalue of a symmetric matrix M by
“λmin (M ).” Consider the program
(P) : z∗ = minimumx xT M x
(cid:4)x(cid:4) = 1 ,
where M is a symmetric matrix. Prove that z∗ = λmin (M ).
(cid:14)
(cid:15)
20. Consider the matrix

s.t.

M =

A B
B T C
where A, B are symmetric matrices and A is nonsingular. Prove that
M is positive semi-deﬁnite if and only if C − B T A
−1B is positive
semi-deﬁnite.

,

