The  Steepest  Descent  Algorithm  for  Unconstrained 
Optimization 
and  a 
Bisection  Line-search  Method 

Robert   M.   Freund  

February,  2004  

2004 Massachusetts Institute of Technology.

1 

1  The  Algorithm 

The  problem  we  are  interested  in  solving  is: 

P  :  minimize  f (x) 
x ∈ (cid:3)n , 

s.t. 

where  f (x)  is  diﬀerentiable.  If  x = ¯x is  a  given  point,  f (x)  can  be  approxi-
mated  by  its  linear  expansion 
x) + ∇f ( ¯
x + d) ≈ f ( ¯
x)T d 
f ( ¯

if  d “small”,  i.e.,  if  (cid:6)d(cid:6)  is  small.  Now  notice  that  if  the  approximation  in 
the  above  expression  is  good,  then  we  want  to  choose  d  so  that  the  inner 
product ∇f ( ¯x)T d is as small as possible.  Let us normalize d so that (cid:6)d(cid:6) = 1. 
Then  among  all  directions  d with  norm  (cid:6)d(cid:6) = 1,  the  direction 
−∇f ( ¯x) 
d˜ = 
(cid:6)∇f ( ¯x)(cid:6) 
makes the smallest inner product with the gradient ∇f ( ¯x).  This fact follows 
(cid:2) 
(cid:1) 
from  the  following  inequalities: 
−∇f ( ¯
∇f ( ¯
x)T d ≥ −(cid:6)∇f ( ¯
x)(cid:6)(cid:6)d(cid:6) = ∇f ( ¯)T 
x) 
x)(cid:6) 
(cid:6)∇f ( ¯
x
For  this  reason  the  un-normalized  direction: 

= −∇f ( ¯)T  ˜
x d . 

d¯ = −∇f ( ¯x) 
is  called  the  direction   of   steepest   descent   at  the  point  ¯x.

Note  that  d¯ = −∇f ( ¯
x)  (cid:8)
x)  is  a  descent  direction  as  long  as  ∇f ( ¯ = 0.  To 
see  this,  simply  observe  that  d¯T ∇f ( ¯
x) =  −(∇f ( ¯
x))T ∇f ( ¯
x)  < 0 so long as 
x)  (cid:8)
∇f ( ¯ = 0. 

2 



A natural consequence of this is the following algorithm, called the steep (cid:173)
est   descent   algorithm. 

Steepest  Descent  Algorithm:  

Step  0.  Given  x0 , set  k  := 0 

Step  1.  dk  := −∇f (xk ).  If  dk  = 0,  then  stop. 

Step  2.  Solve  minα f (xk  + αdk )  for  the  stepsize  αk ,  perhaps  chosen  by 
an  exact  or  inexact  linesearch. 

Step  3.   Set  xk+1  ← xk  + αk dk , k ← k + 1.  Go  to  Step  1. 

Note  from Step 2 and the  fact that dk  = −∇f (xk )  is a descent direction, 
it  follows  that  f (xk+1 ) < f (xk ). 

2  Global  Convergence 

We  have  the  following  theorem: 
Convergence   Theorem:   Suppose  that  f (·) :  (cid:3)n  → (cid:3)  is  continuously 
diﬀerentiable on the set S  = {x ∈ (cid:3)n  | f (x) ≤ f (x0 )}, and that S  is a closed 
and bounded  set.  Then  every point  ¯x  that  is  a  cluster point  of  the  sequence 
{xk }  satisﬁes  ∇f ( ¯x) = 0. 
Proof:   The proof of this theorem is by contradiction.  By the Weierstrass 
Theorem, at least one cluster point of the sequence {x } must exist.  Let  ¯
k 
x be
any such cluster point.  Without loss of generality, assume that limk→∞ x = 
k
x)  (cid:8)
x,  but  that  ∇f ( ¯ =  0.  This  being  the  case,  there  is  a  value  of  α >  0 
¯
¯
x) −  f ( ¯
αd)  >  0,  where  d ¯ =  −∇f ( ¯

x + ¯ ¯ 
such  that  δ  :=  f ( ¯
x).  Then  also 
( ¯ αd) ∈ intS . 
x + ¯ ¯


3 

αd)  ∈  intS , and  (xk  + ¯
αdk )  → 
Now  limk→∞ dk  =  d¯.  Then  since  ( ¯
x + ¯ ¯	
x + ¯ ¯
( ¯	 αd),  for  k  suﬃciently  large  we  have 
αdk ) ≤ f ( ¯ αd) + 
x + ¯ ¯ 
f (x k  + ¯

x) − δ + 
= f ( ¯

x) − 
= f ( ¯

δ 
.
2 

δ 
2 

δ 
2 

However, 

x) < f (x k  + αk dk ) ≤ f (x k  + ¯
x) −
αdk ) ≤ f ( ¯
f ( ¯	
which  is  of  course  a  contradiction.  Thus  d ¯ = −∇f ( ¯x) = 0. 
q.e.d. 

δ 
, 
2 

3	 The  Rate  of  Convergence  for  the  Case  of  a  Quadratic 
Function 

In  this  section  we  explore  answers  to  the  question  of  how  fast  the  steepest 
descent  algorithm  converges.  We  say  that  an  algorithm  exhibits  linear   con(cid:173)
vergence   in  the  ob jective  function  values  if  there  is  a  constant  δ <  1 such 
that  for  all  k  suﬃciently  large,  the  iterates  xk  satisfy: 
k+1 ) − f (x 
∗
) 
f (x
f (xk ) − f (x ∗ ) 

≤ δ, 

∗
where x  is some optimal value of the problem P .  The above statement says 
that the optimality gap shrinks by at  least δ  at each  iteration.  Notice that  if 
δ = 0.1,  for  example,  then  the  iterates gain an  extra digit of accuracy  in  the 
optimal  ob jective  function  value  at  each  iteration.  If  δ  = 0.9,  for  example, 
then  the  iterates  gain  an  extra  digit  of  accuracy  in  the  optimal  ob jective 
function  value  every  22  iterations,  since  (0.9)22  ≈ 0.10. 
The  quantity  δ  above  is  called  the  convergence   constant.  We  would  like 
this  constant  to  be  smaller  rather  than  larger. 
We  will  show  now  that  the  steepest  descent  algorithm  exhibits  linear 
convergence,  but  that  the  convergence  constant  depends  very  much  on  the 
ratio  of  the  largest  to  the  smallest  eigenvalue  of  the Hessian matrix H (x) at 

4 

∗
the  optimal  solution  x = x  .  In  order  to  see  how  this  arises,  we  will  exam-
ine  the  case  where  the  ob jective  function  f (x)  is  itself  a  simple  quadratic 
function  of  the  form: 
1 x T Qx + q T x,
2 
where  Q  is  a  positive  deﬁnite  symmetric  matrix.  We  will  suppose  that  the 
eigenvalues  of  Q  are 

f (x) = 

A = a1  ≥ a2  ≥ . . . ≥ an  = a > 0, 
i.e,  A  and  a  are  the  largest  and  smallest  eigenvalues  of  Q. 

The  optimal  solution  of  P  is  easily  computed  as: 
∗ x  = −Q−1  q 
and  direct  substitution  shows  that  the  optimal  ob jective  function  value  is: 
) = − 
1  T Q−1
∗
f (x 
q 
2 

q . 

For  convenience,  let  x  denote  the  current  point  in  the  steepest  descent 
algorithm.  We  have: 

f (x) = 

1 x T Qx + q T x
2 

and  let d denote  the  current direction, which  is  the negative of  the gradient, 
i.e., 

d = −∇f (x) = −Qx − q . 

Now  let  us  compute  the  next  iterate  of  the  steepest  descent  algorithm. 
If  α  is  the  generic  step-length,  then 

f (x + αd) = 

1 
(x + αd)T Q(x + αd) + q T (x + αd) 
2

5 

=

1 
1 x T Qx + αdT Qx +  α2dT Qd + q x + αqT d
T
2 
2
= f (x) − αdT d +  α2dT Qd.
1 
2 

Optimizing  the  value  of  α  in  this  last  expression  yields 

dT d 
α =  dT Qd

, 

and  the  next  iterate  of  the  algorithm  then  is 

(cid:1) 
x  = x + αd = x + 

dT d 
d,
dT Qd 

and 

(cid:1) ) = f (x + αd) = f (x) − αdT d +
f (x 

1 α2dT Qd = f (x) − 
2

1 (dT d)2 
2  dT Qd 

.

Therefore, 

where 

(cid:1) ) − f (x 
∗ )
f (x 
f (x) − f (x ∗ ) 

= 

= 1 − 

= 1 − 

f (x) −  1 (dT  d)2  − f (x  )
∗
2  dT  Qd 
f (x) − f (x ∗ ) 
1 (dT  d)2 
2  dT  Qd 
2 qT Q−1 q
1 xT Qx + qT x +  1 
2
1 (dT  d)2 
2  dT  Qd 
2 (Qx + q)T Q−1 (Qx + q) 
1 
(dT d)2 
= 1 − 
(dT Qd)(dT Q−1d) 
= 1 −  β 
1
(dT Qd)(dT Q−1d)
(dT d)2 

β  = 

.

6 

In order for the convergence constant to be good, which will translate to fast 
linear  convergence,  we would  like  the  quantity  β  to  be  small.  The  following 
result  provides  an  upper  bound  on  the  value  of  β . 
Kantorovich   Inequality:   Let  A  and  a  be  the  largest  and  the  smallest 
eigenvalues  of  Q,  respectively.  Then 
β  ≤ 

. 

(A + a)2 
4Aa 

We will  prove  this  inequality  later.  For  now,  let  us  apply  this  inequality 
to  the  above  analysis.  Continuing,  we  have 
(cid:1) 
(cid:2)2 
A/a − 1 
A/a + 1 

(cid:1) ) − f (x  )
∗
f (x 
f (x) − f (x ∗ ) 

= 1 − ≤ 1 − 
1 
β 

(A − a)2 
(A + a)2 

=: δ .

4Aa 
(A + a)2 

=

= 

Note  by  deﬁnition  that  A/a  is  always  at  least  1.  If  A/a  is  small  (not 
much  bigger  than  1),  then  the  convergence  constant  δ  will  be much  smaller 
than  1.  However,  if  A/a  is  large,  then  the  convergence  constant  δ  will  be 
only  slightly  smaller  than  1.  Table  1  shows  some  sample  values.  Note  that 
the  number  of  iterations  needed  to  reduce  the  optimality  gap  by  a  factor  of 
10  grows  linearly  in  the  ratio  A/a. 

4  Examples 

4.1  Example   1:  Typical  Behavior 
Consider  the  function  f (x1 , x2 ) = 5x1  + x2  + 4x1x2  − 14x1  − 6x2  + 20.  This 
2
2 
∗ 
∗
∗
function  has  its  optimal  solution  at  x  = (x1 , x2 ) = (1, 1)  and  f (1, 1) = 10. 
(cid:4)  (cid:3) 
(cid:4) 
(cid:3) 
In  step  1  of  the  steepest  descent  algorithm,  we  need  to  compute 
4−  x
−10x
dk  = −∇f (x
k
k 
2  + 14 
k , x1
4−  x
−2x
1 
=

k
k 
1  + 6 
2 

k 
d

1
d
k 
2 

k ) =2

7


(cid:5) A/a−1 
(cid:6)2 
Upper  Bound  on 
δ = 
A/a+1 
0.0023 
0.25 
0.67 
0.96 
0.98 
0.99 

a 
A 
1.0 
1.1 
1.0 
3.0 
10.0 
1.0 
100.0  1.0 
200.0  1.0 
400.0  1.0 

Number  of  Iterations  to  Reduce 
the  Optimality  Gap  by  a  factor  of  10 
1 
2 
6 
58 
116 
231 

Table 1:  Sensitivity of Steepest Descent Convergence Rate to the Eigenvalue 
Ratio 

and in step 2 we  need  to  solve  αk  = arg minα h(α) =  f (xk  + αdk ).  In  this 
example we will be  able  to derive  an  analytic  expression  for αk .  Notice  that 

h(α) =  f (x	k  + αdk )

2 ) −
2 )2  + 4(x1  + αdk 
= 5(x1  + αdk 
k 
1 )2  + (x2  + αdk 
1 )(x2  + αdk 
k
k
k

−14(x1  + αdk 
1 ) − 6(x2  + αdk
k
k 
2 ) + 20, 
and  this  is  a  simple  quadratic  function  of  the  scalar  α.  It  is  minimized  at 

αk  = 

2 )2 
(dk 
1 )2  + (dk 
1 dk
2 )2  + 4dk 
1 )2  + (dk 
2(5(dk 
2 ) 

Using  the  steepest  descent  algorithm  to  minimize  f (x)  starting  from 
x1  = (x1 , x2 ) = (0, 10),  and  using  a  tolerance  of   = 10−6 ,  we  compute  the 
1
1
iterates  shown  in  Table  2  and  in  Figure  2: 
For a convex quadratic function f (x) =  1  xT Qx− cT x, the contours of the 
function values will be  shaped  like  ellipsoids,  and  the gradient vector ∇f (x) 
2
at  any  point  x  will  be  perpendicular  to  the  contour  line  passing  through  x, 
see  Figure  1. 

8 

||dk ||2 
αk 
xk 
xk 
kd1 
kd2
f (xk ) 
k 
2 
1
0.000000  10.000000  −26.000000  −14.000000  29.52964612  0.0866  60.000000 
1 
2  −2.252782 
1.379968  −2.562798 
2.91071234  2.1800  22.222576 
8.786963 
3.200064  −6.355739  −3.422321 
7.21856659  0.0866  12.987827 
0.755548 
3 
0.337335  −0.626480 
4 
0.204852 
2.903535 
0.71152803  2.1800  10.730379 
1.537809  −1.553670  −0.836592 
1.76458951  0.0866  10.178542 
0.940243 
5 
0.082462  −0.153144 
0.17393410  2.1800  10.043645 
0.805625 
6 
1.465322 
1.131468  −0.379797  −0.204506 
0.43135657  0.0866  10.010669 
0.985392 
7 
0.020158  −0.037436 
8 
0.952485 
1.113749 
0.04251845  2.1800  10.002608 
1.032138  −0.092842  −0.049992 
0.10544577  0.0866  10.000638 
0.996429 
9 
0.004928  −0.009151 
0.01039370  2.1800  10.000156 
0.988385 
10 
1.027806 
1.007856  −0.022695  −0.012221 
0.02577638  0.0866  10.000038 
0.999127 
11 
0.001205  −0.002237 
12 
0.997161 
1.006797 
0.00254076  2.1800  10.000009 
1.001920  −0.005548  −0.002987 
0.00630107  0.0866  10.000002 
0.999787 
13 
0.000294  −0.000547 
0.00062109  2.1800  10.000001 
0.999306 
14 
1.001662 
1.000469  −0.001356  −0.000730 
0.00154031  0.0866  10.000000 
0.999948 
15 
0.000072  −0.000134 
16 
0.999830 
1.000406 
0.00015183  2.1800  10.000000 
1.000115  −0.000332  −0.000179 
0.00037653  0.0866  10.000000 
0.999987 
17 
0.000018  −0.000033 
0.00003711  2.1800  10.000000 
0.999959 
18 
1.000099 
1.000028  −0.000081  −0.000044 
0.00009204  0.0866  10.000000 
0.999997 
19 
0.000004  −0.000008 
20 
0.999990 
1.000024 
0.00000907  2.1803  10.000000 
1.000007  −0.000020  −0.000011 
0.00002250  0.0866  10.000000 
0.999999 
21 
0.000001  −0.000002 
0.00000222  2.1817  10.000000 
0.999998 
22 
1.000006 
1.000002  −0.000005  −0.000003 
0.00000550  0.0866  10.000000 
1.000000 
23 
0.000000  −0.000000 
24 
0.999999 
1.000001 
0.00000054  0.0000  10.000000 

Table  2:  Iterations  of  the  example  in  Subsection  4.1. 

9 

5 x2+4 y2+3 x y+7 x+20

1200

1000

800

600

400

200

0
10

5

0

y

−5

−10

−10

−5

0

x

5

10

Figure 1: The contours of a convex quadratic function are ellipsoids.

4.2 Example 2: Hem-Stitching

Consider the function

f (x) =

1
2

where Q and c are given by:

xT Qx − cT x + 10
(cid:4)
(cid:3)

and

Q =

c =

20 5
2
5
(cid:3)
(cid:4)

14
6

10

10 

8 

6 

4 

2 

0 

−2 
−5 

0 

5 

Figure  2:  Behavior  of  the  steepest  descent  algorithm  for  the  example  of 
Subsection  4.1. 

(cid:5) 
(cid:6)2 
A
For  this  problem,  a  = 30.234,  and  so  the  linear  convergence  rate  δ  of 
A/a−1 
the steepest descent algorithm  is bounded above by δ =  A/a+1  = 0.8760. 

If  we  apply  the  steepest  descent  algorithm  to  minimize  f (x) starting 
from  x1  = (40, −100),  we  obtain  the  iteration  values  shown  in  Table  3  and 
shown  graphically  in  Figure  3. 

4.3  Example  3:  Rapid  Convergence 

Consider  the  function 

f (x) = 

1 x T Qx − c T x + 10 
2 

11 

||dk ||2 
xk 
f (xk ) 
αk 
xk 
k 
2 
1 
40.000000  −100.000000  286.06293014  0.0506  6050.000000 
1 
25.542693  −99.696700 
77.69702948  0.4509  3981.695128 
2 
26.277558  −64.668130  188.25191488  0.0506  2620.587793 
3 
16.763512  −64.468535 
51.13075844  0.4509  1724.872077 
4 
17.247111  −41.416980  123.88457127  0.0506  1135.420663 
5 
10.986120  −41.285630 
747.515255 
33.64806192  0.4509 
6 
11.304366  −26.115894 
492.242977 
81.52579489  0.0506 
7 
7.184142  −26.029455 
22.14307211  0.4509 
324.253734 
8 
7.393573  −16.046575 
213.703595 
53.65038732  0.0506 
9 
4.682141  −15.989692 
140.952906 
14.57188362  0.4509 
10 
3.066216 
1.79847660  0.4509 
0.948466 
0.460997 
20 
30  −0.059980 
3.038991 
0.22196980  0.4509 
0.965823 
40  −0.124280 
0.933828 
0.02739574  0.4509 
3.297005 
50  −0.132216 
0.933341 
0.00338121  0.4509 
3.328850 
60  −0.133195 
0.933333 
0.00041731  0.4509 
3.332780 
70  −0.133316 
3.333265 
0.00005151  0.4509 
0.933333 
80  −0.133331 
0.933333 
0.00000636  0.4509 
3.333325 
90  −0.133333 
3.333332 
0.00000078  0.0000 
0.933333 

f (xk )−f (x 
∗ ) 
f (xk−1 )−f (x ∗ ) 

0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658079 
0.658078 
0.658025 
0.654656 
0.000000 

Table  3:  Iteration  values  for  example  of  Subsection  4.2,  which  shows  hem-
stitching. 

where  Q  and  c  are  given  by: 

Q = 

and 

(cid:4) 

(cid:3) 
20  5 
5
16 
(cid:4) 
(cid:3)

(cid:1)
(cid:2)2 
A
For  this  problem,  a  = 1.8541,  and  so  the  linear  convergence  rate  δ  of 
−1  = 0.0896. 
A 
the  steepest  descent  algorithm  is  bounded  above  by  δ = 
a 
A 
+1 
a 

c = 

14 
6 

12 

Figure  3:  Hem-stitching  in  the  example  of  Subsection  4.2. 

If  we  apply  the  steepest  descent  algorithm  to  minimize  f (x) starting 
from  x1  = (40, −100),  we  obtain  the  iteration  values  shown  in  Table  4  and 
shown  graphically  in  Figure  4. 

Figure  4:  Rapid  convergence  in  the  example  of  Subsection  4.3. 

13 

||dk ||2 
xk 
f (xk ) 
αk 
xk 
2 
1 
40.000000  −100.000000  1434.79336491  0.0704  76050.000000 
−1.025060 
3591.615327 
385.96252652  0.0459 
19.867118 
−4.555081 
174.058930 
67.67315150  0.0704 
2.513241 
12.867208 
18.20422450  0.0459 
1.563658 
0.113150 
−0.053347 
0.745149 
3.19185713  0.0704 
5.264475 
4.905886 
0.85861649  0.0459 
0.166834 
0.700361 
4.888973 
0.15054644  0.0704 
0.158981 
0.661755 
0.659643 
0.169366 
0.04049732  0.0459 
4.888175 
4.888137 
0.00710064  0.0704 
0.168996 
0.657822 
4.888136 
0.00191009  0.0459 
0.169486 
0.657722 
4.888136 
0.00033491  0.0704 
0.169468 
0.657636 
0.657632 
0.169491 
0.00009009  0.0459 
4.888136 
4.888136 
0.00001580  0.0704 
0.169490 
0.657628 
4.888136 
0.00000425  0.0459 
0.169492 
0.657627 
0.657627 
0.169491 
0.00000075  0.0000 
4.888136 

k 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 

f (xk )−f (x 
∗ ) 
f (xk−1 )−f (x ∗ ) 

0.047166 
0.047166 
0.047166 
0.047166 
0.047166 
0.047166 
0.047166 
0.047166 
0.047166 
0.047166 
0.047161 
0.047068 
0.045002 
0.000000 

Table  4:  Iteration  values  for  the  example  in  Subsection  4.3,  which  exhibits 
rapid  convergence. 

Let 

4.4  Example  4:  A  nonquadratic  function 
(cid:7) 
(cid:7) 
4
4 
f (x) = x1  − 0.6x2  + 4x3  + 0.25x4  − 
log(xi ) − log(5 − 
xi )  . 
i=1 
i=1 

Table  5  shows  values  of  the  iterates  of  steepest  descent  applied  to  this 
function. 

4.5  Example   5:  An  analytic  example 

Suppose  that 

f (x) = 

1 x T Qx + q T x
2 

14 

||dk ||2 
f (xk ) 
xk 
xk 
xk 
xk 
k 
1 
2 
3 
4 
1.000000  1.000000  1.000000  1.000000  4.17402683  4.650000 
1 
0.802973  1.118216  0.211893  0.950743  1.06742574  2.276855 
2 
0.634501  1.710704  0.332224  1.121308  1.88344095  1.939389 
3 
0.616707  1.735137  0.205759  1.108079  0.46928947  1.797957 
4 
0.545466  1.972851  0.267358  1.054072  1.15992055  1.739209 
5 
0.543856  1.986648  0.204195  1.044882  0.31575186  1.698071 
6 
0.553037  2.121519  0.241186  0.991524  0.80724427  1.674964 
7 
0.547600  2.129461  0.202091  0.983563  0.23764416  1.657486 
8 
0.526273  2.205845  0.229776  0.938380  0.58321024  1.646479 
9 
0.525822  2.212935  0.202342  0.933770  0.18499125  1.637766 
10 
0.511244  2.406747  0.200739  0.841546  0.06214411  1.612480 
20 
0.503892  2.468138  0.200271  0.813922  0.02150306  1.609795 
30 
0.501351  2.489042  0.200095  0.804762  0.00743136  1.609480 
40 
0.500467  2.496222  0.200033  0.801639  0.00256658  1.609443 
50 
0.500161  2.498696  0.200011  0.800565  0.00088621  1.609439 
60 
0.500056  2.499550  0.200004  0.800195  0.00030597  1.609438 
70 
0.500019  2.499845  0.200001  0.800067  0.00010564  1.609438 
80 
0.500007  2.499946  0.200000  0.800023  0.00003647  1.609438 
90 
100  0.500002  2.499981  0.200000  0.800008  0.00001257  1.609438 
110  0.500001  2.499993  0.200000  0.800003  0.00000448  1.609438 
120  0.500000  2.499998  0.200000  0.800001  0.00000145  1.609438 
121  0.500000  2.499998  0.200000  0.800001  0.00000455  1.609438 
122  0.500000  2.499998  0.200000  0.800001  0.00000098  1.609438 

f (xk )−f (x 
∗ ) 
f (xk−1 )−f (x ∗ ) 

0.219505 
0.494371 
0.571354 
0.688369 
0.682999 
0.739299 
0.733266 
0.770919 
0.764765 
0.803953 
0.806953 
0.807905 
0.808228 
0.808338 
0.808376 
0.808381 
0.808271 
0.806288 
0.813620 
0.606005 
0.468218 
0.000000 

Table  5:  Iteration  values  for  the  example  of  Subsection  4.4. 

15 

. 

Then 

and 

where	

and  so 

∇f (x) = 

(cid:2)
(cid:1)
(cid:2) 
(cid:1)
+4  −2	
+2 
Q =  −2 +2 
q =  −2 
and 
(cid:1)
(cid:2) (cid:1) (cid:2) (cid:1)
(cid:2)
+4  −2 
+2 
x1  + 
−2 
−2 +2 
(cid:1) (cid:2) 
x2 
∗ 
0 
x	 =  1 
) = −1. 
∗
f (x 
√ 
√ 
Direct  computation  shows  that  the  eigenvalues  of  Q  are  A  = 3 +  5 and 
a = 3 −  5,  whereby  the  bound  on  the  convergence  constant  is 
(cid:1) 
(cid:2)2 
A/a − 1 
≤ 0.556. 
A/a + 1 

δ = 

Suppose  that  x0  = (0, 0).  Then  we  have: 
x 1  = (−0.4, 0.4),
and  the  even  numbered  iterates  satisfy 
f (x 2n ) = (1 − 0.2n )2  − 2 + 2(0.2)n 
x 2n  = (0, 1 − 0.2n ) 
and  so 

x 2  = (0, 0.8) 

and 

f (x 2n ) − f (x  ) = (0.2)2n . 
∗

Therefore,  starting  from the point x0  = (0, 0),  the optimality gap goes down 
by  a  factor  of  0.04  after  every  two  iterations  of  the  algorithm. 

 Remarks
5 Final
•	 The proof of  convergence  is basic  in  that  it  only  relies on  fundamental 
properties  of  the  continuity  of  the  gradient  and  on  the  closedness  and 
boundedness  of  the  level  sets  of  the  function  f (x). 
•	 The  analysis  of  the  rate  of  convergence  is  quite  elegant. 

16 

 
•	 Kantorovich won  the Nobel  Prize  in  Economics  in  1975  for  his  contri-
butions  to  the  application  of  linear  programming  to  economic  theory. 
•	 The  bound  on  the  rate  of  convergence  is   attained  in  practice  quite 
often,  which  is  too  bad. 
•	 The  ratio of  the  largest  to  the  smallest  eigenvalue of a matrix  is  called 
the  condition   number   of  the  matrix.  The  condition  number  plays  an 
extremely  important  role  in  the  theory  and  the  computation  of  quan-
tities  involving  the  matrix  Q. 
•	 What  about non-quadratic  functions?  Most  functions behave  as near-
quadratic  functions  in  a  neighborhood  of  the  optimal  solution.  The 
analysis  of  the  non-quadratic  case  gets  very  involved;  fortunately,  the 
key  intuition  is  obtained  by  analyzing  the  quadratic  case. 

6	 A  Bisection  Algorithm  for  a  Line-Search  of  a 
Convex  Function 

Suppose  that f (x)  is a continuously diﬀerentiable convex  function,  and  that 
we  seek  to  solve: 

x + αd¯),
α := arg min f ( ¯
¯
α 
where  ¯x is our current iterate, and d¯ is the current direction generated by an 
algorithm  that  seeks  to  minimize  f (x).  Suppose  further  that  d¯ is  a  descent 
direction  of  f (x) at  x = ¯,  namely: 
x

x + d¯) < f ( ¯
f ( ¯
x)  for  all   > 0  and  suﬃciently  small  . 

Let 

h(α) := f ( ¯x + αd¯), 
whereby h(α)  is a  convex  function  in  the  scalar variable α,  and our problem 
is  to  solve  for 

¯α = arg min h(α). 
α 
We  therefore  seek  a  value  ¯α for  which 
(cid:1) 
h ( ¯α) = 0. 

17 

It  is  elementary  to  show  that 
h  (α) = ∇f ( ¯
(cid:1) 
¯  d. 
x + αd)T  ¯ 
(cid:1) (0) < 0. 
Proposition  6.1  h 
q.e.d. 

Because  h(α)  is  a  convex  function  of  α, we also have: 

(cid:1) (α)  is   a   monotone   increasing   function   of   α. 
Proposition  6.2  h 
q.e.d. 

Figure  5  shows  an  example  of  convex  function  of  two  variables  to  be 
optimized.  Figure  6  shows  the  function  h(α)  obtained  by  restricting  the 
function of Figure 5 to the line shown in that ﬁgure.  Note from Figure 6 that 
(cid:1) (α)  will  be  a  monotonically 
h(α)  is  convex.  Therefore  its  ﬁrst  derivative  h 
increasing  function.  This  is  shown  in  Figure  7. 

(cid:1) (α)  is  a  monotonically  increasing  function,  we  can  approxi-
Because  h 
(cid:1) ( ¯
α) = 0, by a  suitable bisection 
α,  the point  that  satisﬁes h 
mately compute  ¯
(cid:1) ( ˆ
(cid:1) (0)  <  0 
α  that  h 
method.  Suppose  that  we  know  a  value  ˆ
α)  >  0.  Since  h 
(cid:1) ( ˆ
α  =  0+  ˆ is  a  suitable  test-point.  Note  the 
α
α)  >  0,  the  mid-value  ˜
and  h 
2
following: 
•  If  h 
(cid:1) ( ˜α) = 0,  we  are  done. 
•  If  h 
(cid:1) ( ˜
α  in  the  interval  (0, ˜
α) > 0,  we  can  now  bracket  ¯
α). 
•  If  h 
(cid:1) ( ˜
α) < 0,  we  can  now  bracket  ¯
α, ˆ
α  in  the  interval  ( ˜ α). 
This  leads  to  the  following  bisection   algorithm   for minimizing  h(α) = f ( ¯x + 
(cid:1) (α) ≈ 0. 
¯ αd)  by  solving  the  equation  h 

Step  0.  Set  k = 0.  Set  αl  := 0  and  αu  :=  ˆα. 
(cid:1) ( ˜
α =  αu+αl  and  compute  h 
α). 
Step   k.   Set  ˜
2 

18 

20

10

0

−10

−20
f(x1,x2)           
−30
−40

−50

−60

−70

−80

0

0.5

1.5

1

x2

2

2.5

3

−2

2

0

4
x1

Figure 5: A convex function to be optimized.

10

8

6

19

0 

−10 

−20 

h(α) 
−30 

−40 

−50 

−60 
−0.4 

−0.2 

0 

0.2 

α 

0.4 

0.6 

0.8 

1 

Figure  6:  The  1-dimensional  function  h(α).


1 

0.5 

0 

h′(α) 
−0.5 

−1 

−1.5 

−2 
−0.4 

−0.2 

0 

0.2 

α 
(cid:1) (α)  is  monotonically  increasing. 
Figure  7:  The  function  h 

0.4 

0.6 

0.8 

1 

20 


•	 If  h 
α. Set  k ← k + 1. 
(cid:1) ( ˜
α) > 0,  re-set  αu  :=  ˜
•	 If  h 
α. Set  k ← k + 1. 
(cid:1) ( ˜
α) < 0,  re-set  αl  :=  ˜
•	 If  h 
(cid:1) ( ˜α) = 0,  stop. 
Proposition  6.3  After   every   iteration   of   the   bisection   algorithm,   the   cur-
(cid:1) ( ¯
α) = 0.
rent   interval   [αl , αu ]  must   contain   a   point   α  such   that   h 
¯
Proposition  6.4  At  the  kth   iteration   of   the   bisection   algorithm,   the   length  
of   the   current   interval   [αl , αu ]  is   (cid:1) (cid:2)k1 
L = 
( ˆα).
2 
Proposition  6.5  A   value   of   α  such   that   |α − ¯α| ≤   can be found in at  
(cid:8) 
(cid:1) (cid:2)(cid:9)
most  
ˆα
log2 
 

steps   of   the   bisection   algorithm.  

(cid:1) 
6.1  Computing  α  for  which  h ( ˆ
α) > 0
ˆ	
(cid:1) ( ˆ
α for which h 
α) > 
Suppose that we do not have available a convenient value  ˆ
(cid:1) ( ˆ
α). 
α  and  compute  h 
0.  One way  to  proceed  is  to  pick  an  initial  “guess”  of  ˆ	
α)  ≤ 0,  then 
(cid:1) ( ˆ
(cid:1) ( ˆ
If  h 
α)  >  0,  then  proceed  to  the  bisection  algorithm;  if  h 
α ← 2 ˆ
re-set  ˆ
α  and  repeat  the  process. 

6.2  Stopping  Criteria  for  the  Bisection  Algorithm 

In practice, we need to run the bisection algorithm with a stopping criterion. 
Some  relevant  stopping  criteria  are: 
•	 Stop  after  a  ﬁxed  number  of  iterations.  That  is  stop  when  k  =  K , 
¯ 
¯
where K  speciﬁed  by  the  user. 
•	 Stop when the interval becomes small.  That is, stop when αu − αl  ≤ , 
where    is  speciﬁed  by  the  user. 
α)| ≤ ,
α)| becomes  small.  That  is,  stop  when  |h 
•	 Stop  when  |h 
(cid:1) ( ˜
(cid:1) ( ˜
where    is  speciﬁed  by  the  user. 

This  third  stopping  criterion  typically  yields  the  best  results  in  practice. 

21 

6.3 	 Modiﬁcation  of  the  Bisection  Algorithm  when  the  Do­
main  of  f (x)  is  Restricted 
The  discussion  and  analysis  of  the  bisection  algorithm  has  presumed  that 
our  optimization  problem  is 

P  :  minimize  f (x) 
x ∈ (cid:3) . 
n

s.t. 

Given  a  point  ¯x  and  a  direction  d¯,  the  line-search  problem  then  is 
LS  :  minimize  h(α) := f ( ¯x + αd¯) 
α ∈ (cid:3). 

s.t. 

Suppose instead that the domain of deﬁnition of f (x) is an open set X  ⊂ (cid:3) . 
n
Then  our  optimization  problem  is: 

P  :  minimize  f (x) 
x ∈ X, 

s.t. 

and  the  line-search  problem  then  is 
LS  :  minimize  h(α) := f ( ¯x + αd¯) 
x + αd ∈ X. 
¯
¯

s.t. 

In  this  case,  we  must  ensure  that  all  iterate  values  of  α  in  the  bisection  al-
x + αd ∈ X .  As an example,  consider the  following problem: 
¯ 
gorithm satisfy  ¯
(cid:10) 
ln(bi  − Aix) 
P  :  minimize  f (x) := − 
m 
i=1 
b − Ax > 0. 

s.t. 

22 

Here  the  domain  of  f (x) is  X  =  {x  ∈ (cid:3) | b − Ax  >  0}. Given  a  point 
n
¯x ∈ X  and  a  direction  d¯,  the  line-search  problem  is: 
(cid:10) 
ln(bi  − Ai ( ¯
x + αd¯) = − 
m 
x + αd¯)) 
LS :  minimize  h(α) := f ( ¯
i=1 

s.t. 

b − A( ¯x + αd¯) > 0. 

where

α := − min 
ˇ
¯
Ai d<0 

Standard  arithmetic  manipulation  can  be  used  to  establish  that 
b − A( ¯
x + αd¯) > 0  if  and  only  if  α < α <  ˆ
ˇ
α
(cid:11) 
(cid:12) 
(cid:11) 
bi  − Aix
bi  − Aix
¯
¯
−Aid 
¯ 
¯ 
Aid 
and  the  line-search  problem  then  is: 
(cid:10) 
LS :  minimize  h(α) := − 
m 
i=1 

ln(bi  − Ai ( ¯x + αd¯)) 

ˆ
and  α :=  min 
¯
Ai d>0 

(cid:12)


,

s.t. 

α < α <  ˆ
ˇ
α. 

23 

7  Proof  of  Kantorovich  Inequality 

Kantorovich   Inequality:   Let  A  and  a  be  the  largest  and  the  smallest 
eigenvalues  of  Q,  respectively.  Then 
(A + a)2 
β  ≤ 
4Aa 

.

Proof:   Let  Q  =  RDRT ,  and  then  Q−1  =  RD−1RT ,  where  R  =  RT  is 
an  orthonormal  matrix,  and  the  eigenvalues  of  Q  are 
0 < a = a1  ≤ a2  ≤ . . . ≤ an  = A, 

and, 

Then 

⎛ a1 
⎜  0  a2 
⎜ ⎜ ⎝ 
0 
. . . 
. . . 
0 
0 

⎞ 
0  ⎟ 
⎟ ⎟ ⎠ 
0 
. . . 
. . . 
. 
. . . 
. . . 
. . .  an 

D = 

β  = 

= 

(dT RDRT d)(dT RD−1RT d)
f T Df f T D−1f 
(dT RRT d)(dT RRT d) 
f T f f T f 
(cid:10) 
f .  Then  λi  ≥ 0 and 
n 
2 
f 
where  f  = RT d. Let  λi  =  f
λi  = 1,  and 
i 
(cid:5) (cid:6) 
T
(cid:10) 
i=1 
(cid:1) (cid:2)
n (cid:7) (cid:7) 
n 
⎞ . 
=  ⎛
1 
λi 
n 
ai
1 
1  ⎟ 
⎜ 
i=1
λiai 
λi 
⎝  n 
⎠ 
(cid:10) 
ai
i=1 
i=1 
λi ai 
i=1 

β  = 

The  largest  value  of  β  is  when  λ1  + λn  = 1,  see  the  illustration  in  Figure  8. 
Therefore, 

24 

β  ≤ 

1
1
λ1  a  + λn A  =
1 
λ1 a+λnA 

(λ1a + λnA)(λ1A + λna)  ≤ 
Aa 

( 1 A +  1  a)( 1  a +  1 A)
2
2
2
2
Aa 

=

(A + a)2 
4Aa 

. 

q.e.d. 

y 

y = 1/x 

Σλi(1/ai) 

1/(Σλiai) 

0 
0  a1  a2 a3 
=a 

Σλiai 

. . . 

x 

an 
=A 

Figure  8:  Illustration  for  proof  of  the  Kantorovich  Inequality. 

25 

8  Steepest  Descent  Exercises 

NOTE  ON  COMPUTATION  EXERCISES:  You may use any machine 
and  any  programming  language.  We  recommend,  however,  that  you  use 
MATLAB  on  Athena  or  on  your  own  computer.  (The  teaching  assistant 
is  prepared  to  help  you  get  started  in  MATLAB  on  Athena.)  Some  of  the 
speciﬁc  details  in  the  computation  exercises  are  purposely  left  to  your  own 
discretion.  For  example,  in  cases  where  you  must  use  a  line-search  routine, 
you  must  choose  your  own  tolerance  for  the  line-search.  Also,  you  must 
choose  your  own  stopping  criteria  for  your  computation.  Please  include 
your  code  or  pseudo-code  in  your  write-up. 

1.  (Steepest  Descent)  Suppose	 that  xk  and  xk+1  are  two  consecutive 
points  generated  by  the  steepest  descent  algorithm  with  exact  line-
search.  Show  that ∇f (xk )T ∇f (xk+1 ) = 0. 
2.  (Steepest  Descent)  Suppose  that  we  seek  to  minimize 
f (x1 , x2 ) = 5x  + 5x2  − x1x2  − 11x1  + 11x2  + 11.
2
2
1 

(a)  Find  a  point  satisfying  the  ﬁrst-order  necessary  conditions  for  a 
solution. 
(b)  Show  that  this  point  is  a  global  minimum  of  f (x). 
(c)  What  would  be  the  worst  rate  of  convergence  for  the  steepest 
descent  algorithm  for  this  problem? 
(d)  Starting  at  (x1 , x2 ) = (0, 0),  at  most  how  many  steepest  descent 
iterations  would  it  take  to  reduce  the  function  value  to  10−11? 

3.  (Steepest  Descent)  Suppose  we  seek  to  minimize 

where 

1 
f (x) =  x T H x + c x + 13,
T
2 
(cid:3)
(cid:4) 
(cid:3)
10  −9
4

and  c =  −15 
H  =  −9
10 

(cid:4)


. 

Implement  the  steepest  descent  algorithm  on  this  problem,  using  the 
following  starting  points: 

26 

•  x0  = (0, 0)T . 
•  x0  = (−0.4, 0)T . 
•  x0  = (10, 0)T . 
•  x0  = (11, 0)T . 
∗
As  it  turns  out,  the  optimal  solution  to  this  problem  is  x  = (5, 6)T , 
with  f (x  ) = −22.  What  linear  convergence  constants  do  you  observe

∗
for  each  of  the  above  starting  points?


4.  (Steepest  Descent)  Suppose  we  seek  to  minimize 

where 

1 
f (x) =  x T H x + c  x,
T
2 
⎛ 
⎞
⎛
⎞

⎟
⎜
⎟
⎜ 
10  −18
40  −1  ⎠  and  c = ⎝  −47  ⎠ . 
H  = ⎝  −18 
12 

2 
2  −1
−8 
3 

Implement  the  steepest  descent  algorithm  on  this  problem,  using  the

following  starting  points.

•  x0  = (0, 0, 0)T . 
•  x0  = (15.09, 7.66, −6.56)T . 
•  x0  = (11.77, 6.42, −4.28)T . 
•  x0  = (4.46, 2.25, 1.85)T . 
∗
As  it  turns out,  the optimal  solution  to  this problem  is x  = (4, 3, 1)T , 
with  f (x  ) =  −50.5.  What  linear  convergence  constants  do  you  ob
-
∗
serve  for  each  of  the  above  starting  points?


5.  (Steepest  Descent)  Suppose  that  we  seek  to  minimize  the  following 
function: 
f (x1 , x2 ) = −9x1−10x2+θ(− ln(100−x1−x2 )−ln(x1 )−ln(x2 )−ln(50−x1+x2 )), 

where  θ  is  a  given  parameter.  Note  that  the  domain  of  this  function

is  X  =  {(x1 , x2 )  |  x1  >  0, x2  >  0, x1  +  x2  <  100, x1  −  x2  <  50}.

Implement  the  steepest  descent  algorithm  for  this  problem,  using  the


27 

bisection  algorithm  for  your  line-search,  with  the  stopping  criterion 
that  |h 
(cid:1) ( ˜α)| ≤   = 10−6  .  Run  your  algorithm  for  θ  =  10  and  for 
θ = 100,  using  the  following  starting  points. 
• x0  = (8, 90)T . 
• x0  = (1, 40)T . 
• x0  = (15, 68.69)T . 
• x0  = (10, 20)T . 
What linear convergence constants do you observe for each of the above 
starting  points? 
Helpful  Hint:  Note  that  f (x1 , x2 )  is  only  deﬁned  on  the  domain 
X  = {(x1 , x2 )  | x1  > 0, x2  > 0, x1  + x2  < 100, x1  − x2  < 50}.  If you are 
x = ( ¯ x2 ) and you have computed a direction d ¯ = (d¯ 1 , d2 ), 
¯ 
at a point  ¯
x1 , ¯
then note  that a value of  the upper bound  ˆα  is eﬀectively given by  the 
largest  value  of  α  for  which  the  following  constraints  are  satisﬁed: 
x2+αd¯ 2  < 100, x1+αd1−x2−αd2  < 50. 
x1+αd1  > 0, x2+αd2  > 0, x1+αd¯ 1+ ¯
¯ 
¯ 
¯ 
¯
¯
¯
¯
¯
¯

The  largest  value  of  α  satisfying  these  conditions  can  easily  be  com-
puted  by  applying  appropriate  logic. 
Important   Note:   Please  keep  the  value  of  θ  as  a  speciﬁc  command 
in  your  code.  The  reason  for  this  is  that  later  in  the  semester, we will 
consider  an  algorithm  where  we  iteratively  change  the  value  of  θ . It 
will  then  be  useful  for  your  code  to  have  a  line  in  it  where  θ  can  be 
set  and  re-set. 

28 

