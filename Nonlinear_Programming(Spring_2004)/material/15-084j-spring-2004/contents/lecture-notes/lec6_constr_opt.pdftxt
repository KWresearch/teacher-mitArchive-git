Optimality Conditions  for Constrained 
Optimization Problems 

Robert M.  Freund 

February,  2004 

2004 Massachusetts Institute of Technology.

1 

1 

Introduction 

Recall  that a  constrained optimization problem  is a problem of  the  form 

(P)  minx f (x) 
g(x) ≤ 0 

s.t. 

h(x) = 0 
x ∈ X, 

where  X  is  an  open  set  and  g(x)) =  (g1 (x), . . . , gm (x))  : (cid:4)n  → (cid:4)m ,  h(x) = 
(h1 (x), . . . , hl (x)) : (cid:4)n  → (cid:4)l . Let  S  denote  the  feasible  region  of  (P),  i.e., 

S  := {x ∈ X  : g(x) ≤ 0, h(x) = 0}. 

Then  the  problem  (P)  can  be  written  as 

min f (x)  . 
x∈S 

Recall  that  ¯x  is  a  local  minimum  of  (P)  if  there  exists   >  0  such  that 
x) ≤ f (y) for all y ∈ S ∩ B ( ¯
f ( ¯
x, ).  Local, global minima and maxima, strict 
and  non-strict,  are  deﬁned  analogously. 
We  will  often  use  the  following  “shorthand”  notation: 
⎡
⎤ 
⎡
⎥
⎢ 
⎢ 
∇g1 (x)t 
∇h1 (x)t 
∇g(x) = ⎣ 
⎦  and ∇h(x) = ⎣ 
. 
. 
. 
. 
. 
.
∇gm (x)t 
∇hl (x)t 
i.e., ∇g(x) ∈ (cid:4)m×n  and ∇h(x) ∈ (cid:4)l×n  are Jacobian matrices, whose  ith  row 
is  the  transpose  of  the  corresponding  gradient. 

⎤ 
⎥
⎦ ,

2 

2  Necessary  Optimality  Conditions 

2.1  Geometric  Necessary  Conditions 
A set  C  ⊆ (cid:4)n  is  a  cone  if  for  every  x ∈ C ,  αx ∈ C  for  any  α > 0. 
A set  C  is  a  convex  cone  if  C  is  a  cone  and  C  is  a  convex  set. 
Suppose  ¯x ∈ S .  We  have  the  following  deﬁnitions: 
•  F0  := {d  : ∇f ( ¯x)td < 0} is  the  cone  of  “improving”  directions  of  f (x) 
at  ¯x. 
•  I  =  {i  :  gi ( ¯x) = 0}  is  the  set  of  indices  of  the  binding  inequality 
constraints  at  ¯x. 
•  G0  = {d  : ∇gi ( ¯x)td < 0  for  all  i ∈ I } is  the  cone  of  “inward”  pointing 
directions  for  the  binding  constrains  at  ¯x. 
•  H0  =  {d  :  ∇hi ( ¯x)td  = 0  for  all  i  = 1, . . . , l}  is  the  set  of  tangent 
directions  for  the  equality  constraints  at  ¯x. 
Theorem  1  Assume  that  h(x)  is  a  linear  function,  i.e.,  h(x) = Ax − b  for 
A ∈ (cid:4)l×n . If  ¯x  is  a  local  minimum  of  (P),  then  F0  ∩ G0  ∩ H0  = ∅. 
Proof:  Note  that  ∇hi ( ¯x) =  Ai· ,  i.e.,  H0  =  {d  :  Ad  = 0}.  Suppose  d  ∈ 
x + λd)  ≤  gi ( ¯
F0  ∩ G0  ∩ H0 .  Then  for  all  λ >  0  suﬃciently  small  gi ( ¯
x) =
0  for  all  i ∈ I  (for  i  (cid:10)
∈ I , since  λ  is  small,  gi ( ¯
x + λd) = 
x + λd) < 0),  and  h( ¯
x + λd  ∈ S  for  all  λ >  0  suﬃciently  small. 
x − b) + λAd =  0.  Therefore  ¯
(A ¯
On  the  other  hand,  for  all  suﬃciently  small  λ >  0,  f ( ¯
x).  This 
x + λd) < f ( ¯
contradicts  the  assumption  that  ¯x  is  a  local minimum  of  (P). 

The  following  is  the  extension  of Theorem  1  to  handle  general  nonlinear 
functions  hi (x), i = 1, . . . , l. 
x is a local minimum of (P) and the gradient vectors ∇hi ( ¯
Theorem  2  If  ¯
x), i = 
1, . . . , l  are  linearly  independent,  then  F0  ∩ G0  ∩ H0  = ∅. 

3 

Note  that  Theorem  2  is  essentially  saying  that  if  a  point  ¯x  is  (locally) 
optimal,  there  is  no  direction  d  which  is  an  improving  direction  (i.e.,  such 
x) for small λ >0),  and at the same time is also a feasible 
x+ λd) < f ( ¯
that f ( ¯
x + λd)  ≈
x) = 0  for  i ∈ I  and  h( ¯
x + λd)  ≤ gi ( ¯
direction  (i.e.,  such  that  gi ( ¯
0),  which  makes  sense  intuitively.  Observe,  however,  that  the  condition  in 
Theorem 2 is somewhat weaker than the above intuitive explanation:  indeed, 
we  can  have  a  direction  d which  is  an  improving  direction  but ∇f ( ¯x)td = 0 
and/or ∇g( ¯x)td = 0. 
The  proof  of  Theorem  2  is  rather  awkward  and  involved,  and  relies  on 
the  Implicit  Function  Theorem.  We  present  this  proof  at  the  end  of  this 
note,  in  Section  6. 

2.2  Separation  of  Convex  Sets 

We  will  shortly  attempt  to  restate  the  geometric  necessary  local  opti-
mality  conditions  (F0 ∩ G0 ∩ H0  = ∅)  into  a  constructive  and  “computable” 
algebraic  statement  about  the  gradients  of  the  ob jective  function  and  the 
constraints  functions.  The  vehicle  that  will  make  this  happen  involves  the 
separation  theory  of  convex  sets. 

•	 If p (cid:10)= 0 is a vector in (cid:4)n  and α is a scalar, H := {x ∈ (cid:4)n  : p x = α} is a 
t
hyperplane, and  H +  = {x ∈ (cid:4)n  : p x ≥ α},  H
−  = {x ∈ (cid:4) : ptx ≤ α}
t	
n
are  halfspaces. 
•	 Let  S  and  T  be  two  non-empty  sets  in  (cid:4) .  A  hyperplane  H  =  {x  : 
n
p x  =  α}  is  said  to  separate  S  and  T  if  p x  ≥  α  for  all  x  ∈  S  and 
t	
t
tp x ≤ α  for  all  x ∈ T ,  i.e.,  if  S  ⊆ H +  and  T  ⊆ H
− .  If,  in  addition, 
S ∪ T  (cid:10)⊂ H ,  then H  is  said  to  properly  separate  S  and  T . 
•	 H  is  said  to  strictly  separate  S  and  T  if  ptx > α  for  all  x  ∈  S  and 
ptx < α for  all  x ∈ T . 
•	 H  is  said  to  strongly  separate  S  and  T  if  for  some   > 0,  p x ≥ α +  
t
for  all  x ∈ S  and  p x ≤ α −  for  all  x ∈ T . 
t

4 

Theorem  3  Let S  be  a nonempty  closed  convex  set  in (cid:4)n ,  and  suppose  that 
∈ S .  Then  there  exists  p  (cid:10)
y  (cid:10)
= 0  and  α  such  that  H  = {x  : p x = α}  strongly 
t
separates  S  and  {y}. 

To  prove  the  theorem,  we  need  the  following  result: 

Theorem  4  Let  S  be  a  nonempty  closed  convex  set  in  (cid:4)n , and  y  (cid:10)∈  S . 
Then  there  exists  a  unique  point  x  ∈  S  with  minimum  distance  from  y .
¯
x  is  the minimizing  point  if  and  only  if  (y − ¯
x)t (x − ¯
x) ≤ 0  for 
Furthermore,  ¯
al l  x ∈ S . 

1 
(cid:2)
x + x 
( ¯
2

Proof:  Let  ˆx  be  an  arbitrary  point  in  S , and  let  S ¯ =  S ∩ {x  :  (cid:15)x − y(cid:15) ≤ 
(cid:15) ˆx − y(cid:15)}.  Then  S ¯  is  a  compact  set.  Let  f (x) = (cid:15)x − y(cid:15).  Then  f (x)  attains 
x ∈ S .  Note  that  ¯ = y .
x  (cid:10)
its minimum  over  the  set  S ¯  at  some  point  ¯
¯ 
(cid:2)  ∈  S  for  which  (cid:15)y − 
To  show  uniqueness,  suppose  that  there  is  some  x
(cid:2)(cid:15)
(cid:2) )  ∈  S .  But  by  the  triangle 
¯x(cid:15)  =  (cid:15)y  − x
.
 By  convexity  of  S ,
 1 
2 ( ¯x + x
inequality,  we  have: 
(cid:7) 
(cid:7) 
(cid:7) 
(cid:7) 
)(cid:7) ≤ (cid:15)y − ¯
(cid:7)y −
(cid:7) 
(cid:7)
x(cid:15) +  (cid:15)y − x 
(cid:2)(cid:15). 
1 
1
2
2 
If  strict  inequality holds, we  have  a  contradiction.  Therefore  equality holds, 
x = λ(y − x
and  we  must  have  y − ¯
(cid:2) )  for  some  λ. Since  (cid:15)y − ¯
(cid:2)(cid:15), 
x(cid:15) = (cid:15)y − x
= 1.  If  λ = −1,  then  y
|λ|

(cid:2) )  ∈ S ,  contradicting  the  assumption. 
1 
= 
2 ( ¯x + x
(cid:2)  = ¯
Hence  λ = 1,  whereby  x

x. 
Finally we need  to  establish  that  ¯x  is  the minimizing point  if  and  only  if 
x) ≤ 0  for  all  x ∈ S .  To  establish  suﬃciency,  note  that  for  any 
x)t (x − ¯
(y − ¯
x ∈ S , 
x) ≥ (cid:15) ¯
x− ¯x(cid:15)
x−y(cid:15) . 
x)t (y− ¯
2+(cid:15)y− ¯
x(cid:15)2−2(x− ¯
2 = (cid:15)(x− ¯
x)(cid:15)
x)−(y− ¯ 2
= (cid:15)
(cid:15)x−y(cid:15)
2 
Conversely,  assume  that  x  is  the  minimizing  point.  For  any  x  ∈  S ,  λx +
¯

5 

x ∈ S  for  any  λ ∈ [0, 1].  Also,  (cid:15)λx + (1 − λ) ¯
(1 − λ) ¯
x − y(cid:15) ≥ (cid:15)x − y(cid:15). Thus, 
¯
(cid:15)x − y(cid:15)2  ≤ (cid:15)λx + (1 − λ) ¯
x − y(cid:15)2 
¯
=  (cid:15)λ(x − ¯
x − y)(cid:15)
2
x) + ( ¯
x(cid:15)2  + 2λ(x − ¯ x − y) + (cid:15)x − y(cid:15) , 
=  λ2(cid:15)x − ¯
2
¯
x)t ( ¯
which  when  rearranged  yields: 

x(cid:15)2  ≥ 2λ(y − ¯
λ2(cid:15)x − ¯
x)t (x − ¯
x)  .
This implies that (y − x)t (x− x) ≤ 0 for any x ∈ S , since otherwise the above 
¯
¯
expression  can  be  invalidated  by  choosing  λ > 0  and  suﬃciently  small. 
Proof of Theorem 3:  Let  ¯x ∈ S  be the point minimizing the distance from 
x = y . Let p = y −x, α =  2 (y −x)t (y + ¯
the point y to the set S .  Note that  ¯ (cid:10)
1 
¯
¯
x), 
and   =  1 (cid:15)y − ¯
x(cid:15)2 .  Then  for  any  x ∈ S , (x − ¯
x)t (y − ¯
x) ≤ 0,  and  so 
2 
x t (y−x)+  (cid:15)y−x(cid:15)2− =
y y−  x x− = α−.
x)t x ≤ ¯
x t (y−x) =  ¯
p x = (y− ¯
1
1
1  t 
t 
t ¯
¯
¯
¯
¯
2
2 
2 
Therefore  p x ≤ α −   for  all  x ∈ S .  On  the  other  hand,  p y  = (y − ¯
x)ty  = 
t
t
α + ,  establishing  the  result. 

Corollary  5  If  S  is  a  closed  convex  set  in  (cid:4)n ,  then  S  is  the  intersection  of 
al l  halfspaces  that  contain  it. 

Theorem  6  Let  S  ∈ (cid:4)n  and  let  C  be  the  intersection  of  al l  halfspaces 
containing  S .  Then  C  is  the  smal lest  closed  convex  set  containing  S . 

Theorem  7  Suppose  S1  and  S2  are  disjoint  nonempty  closed  convex  sets 
and  S1  is  bounded.  Then  S1  and  S2  can  be  strongly  separated  by  a  hyper-
plane. 

Proof:  Let  T  =  {x  ∈ (cid:4) :  x  =  y − z ,  where  y  ∈ S1 , z  ∈ S2}.  Then  it  is 
n
easy  to  show  that  T  is  a  convex  set.  We  also  claim  that  T  is  a  closed  set. 

6 

To  see  this,  let  {xi}
⊂ T ,  and  suppose  ¯
x =  limi→∞ xi .  Then  xi  = yi  − zi 
∞ 
for  {yi}∞  ⊂  S1  and  {zi}∞  ⊂  S2 .  By  the  Weierstrass  Theorem,  some 
i=1 
subsequence  of  {yi}  converges  to  a  point  ¯
y ∈ S1 .  Then  zi  = yi − xi  → y¯ − ¯
i=1 
i=1 
x 
y − ¯
x is  a  limit  point  of  {zi}. Since  S2 
(over  this  subsequence),  so  that  ¯
z = ¯
y −  ¯
x ∈  T ,  proving  that  T  is  a  closed 
z  ∈  S2 ,  and  then  ¯
is  also  closed,  ¯
x = ¯
set. 
By  hypothesis,  S1  ∩ S2  =  ∅, so  0  (cid:10)∈  T . Since  T  is  convex  and  closed, 
there  exists  a  hyperplane  H  =  {x :  p x = ¯
α}  such  that  p x >  ¯
α  for  x ∈  T 
t
t
and  pt0 <  ¯
α (and  hence  ¯
α > 0).
Let  y ∈  S1  and  z ∈  S2 .  Then  x =  y − z  ∈  T , and  so  pt (y − z ) >  ¯α > 0 
for  any  y ∈ S1  and  z ∈ S2 . 
Let  α1  = inf {p y  :  y  ∈  S1}  and  α2  = sup{ptz  :  z  ∈  S2}  (note  that 
t
α  ≤  α1  − α2 );  deﬁne  α  =  2 (α1  + α2 ) and    =  α >  0.  Then  for  all 
1
1
0  <  ¯
¯
y ∈ S1  and  z ∈ S2  we  have 
2 
p t y ≥ α1  = 
1
2

(α1  − α2 ) ≥ α +  α = α + 
1 
1 
¯
2
2 

(α1  + α2 ) + 

and 

(α1  − α2 ) ≤ α −  α = α − .
(α1  + α2 ) − 
p t z ≤ α2  = 
1 
1 
1
¯
2
2 
2
Theorem  8  (Farkas’  Lemma)  Given  an m × n matrix A and  an n-vector 
c,  exactly  one  of  the  fol lowing  two  systems  has  a  solution: 
(i)  Ax ≤ 0, c x > 0 
t

(ii)  A y = c,  y ≥ 0. 
t

Proof:  First  note  that  both  systems  cannot  have  a  solution,  since  then  we 
would  have  0 < c x = y tAx ≤ 0. 
t
Suppose the system (ii) has no solution.  Let S = {x : x = A y  for  some  y ≥ 
t
0}.  Then  c (cid:10)∈ S .  S  is  easily  seen  to  be  a  convex  set.  Also,  S  is  a  closed  set. 
(For  an  exact  proof  of  this,  see Appendix B.3  of Nonlinear  Programming  by 
Dimitri  Bertsekas,  Athena  Scientiﬁc,  1999.)  Therefore  there  exist  p  and  α 
such  that  ctp > α and  pt (A y) = (Ap)ty ≤ α for  all  y ≥ 0. 
t

7 

If (Ap)i  > 0 for some i, one could set yi  suﬃciently large so that (Ap)ty > 
α,  a  contradiction.  Thus  Ap  ≤  0.  Taking  y  =  0,  we  also  have  that  α  ≥  0, 
and  so  c p > 0,  and  p  is  a  solution  of  (i). 
t

Lemma  9  (Key  Lemma)  Given  matrices  A, B , and  H  of  appropriate  di-
mensions,  exactly  one  of  the  two  fol lowing  systems  has  a  solution: 
(i)  Ax < 0, Bx ≤ 0, H x = 0 
¯

(ii)  Atu + B tv + H tw = 0, u ≥ 0, v ≥ 0, e u = 1. 
¯
t

Proof:  It  is  easy  to  show  that  both  (i)  and  (ii)  cannot  have  a  solution. 
Suppose  (i)  does  not  have  a  solution.  Then  the  system 
¯ Ax + eθ ≤ 0, θ > 0 
Bx ≤ 0 
H x ≤ 0 
−H x ≤ 0 

has  no  solution.  This  system  can  be  re-written  in  the  form 
⎡ 
⎤ 
(cid:8)  (cid:9) 
(cid:8)  (cid:9) 
⎢
⎥

⎢  B  0  ⎥ · 
¯ 
A e 
⎥

⎢
⎣  H  0  ⎦ 
x  ≤ 0,  (0, . . . , 0, 1) · 
x
θ 
θ
−H  0 
From  Farkas’  Lemma,  there  exists  a  vector  (u; v ; w1 ; w2 ) ≥ 0  such  that 
⎛ ⎞ 
⎤
⎡ 
⎛
⎞ 
⎟ ⎜  . ⎜  .  ⎟ 
⎟ 
⎢ 
⎥ ⎜ 
⎢  B  0  ⎥ ⎜  v  ⎟
0
¯ A e 
t 
1  ⎟ = ⎜  .  ⎟  . 
u 
⎢
⎥  · ⎜ 
⎜ ⎟ 
⎣  H  0  ⎦ ⎝  w ⎠ ⎝  0  ⎠ 
−H  0 
2
w
1 

> 0. 

This  can  be  rewritten  as 

¯ At u + B t v + H t (w 1  − w 2 ) = 0, 

e u = 1  . 
t

8 

Letting  w = w1  − w2  completes  the  proof  of  the  lemma. 

2.3  Algebraic  Necessary  Conditions 

Theorem  10  (Fritz  John  Necessary  Conditions)  Let  x  be  a  feasible 
¯
solution  of  (P).  If  x  is  a  local  minimum  of  (P),  then  there  exists  (u0 , u, v)
¯
(cid:16) 
(cid:16) 
such  that 
l 
m
vi∇hi ( ¯
ui∇gi ( ¯
u0∇f ( ¯
x) + 
x) = 0,
x) + 
i=1 
i=1 
u0 , u ≥ 0,  (u0 , u, v)  (cid:10)= 0, 
uigi ( ¯x) = 0, i = 1, . . . , m. 
(Note  that  the  ﬁrst  equation  can  be  rewritten  as 
x)t u + ∇h( ¯
u0∇f ( ¯
x) + ∇g( ¯
x)t v = 0  .) 

= 0 

Proof:  If the vectors ∇hi ( ¯
x) are linearly dependent,  then there exists v  (cid:10)
such  that ∇h( ¯x)tv = 0.  Setting  (u0 , u) = 0  establishes  the  result. 
Suppose  now  that  the  vectors  ∇hi ( ¯x)  are  linearly  independent.  Then 
we  can  apply  Theorem  2  and  assert  that  F0  ∩ G0  ∩ H0  =  ∅.  Assume  for 
simplicity  that  I  = {1, . . . , p}. Let 
⎡
⎤ 
⎤ 
⎡
x)t  ⎥ 
⎢ 
∇f ( ¯
⎢  ∇g1 ( ¯ ⎥ 
⎢
⎥
x)t 
⎥ , H  = ⎣ 
A = ⎢
⎦ . 
⎢ 
⎥ 
⎣ 
⎦ 
.. 
. 
∇gp ( ¯
x)t 

∇h1 ( ¯
x)t 
. 
..
∇hl ( ¯
x)t 

Then  there  is  no  d  that  satisﬁes  Ad  <  0,  H d  =  0.  From  the  Key  Lemma 
there  exists  (u0 , u1 , . . . , up ) and  (v1 , . . . , vl )  such  that 
(cid:16) 
(cid:16) 
l 
p
vi∇hi ( ¯
ui∇gi ( ¯
u0∇f ( ¯
x) + 
x) = 0,
x) + 
i=1 
i=1 

9 

with u0 + u1 + · · · + up  = 1 and (u0 , u1 , . . . , up ) ≥ 0.  Deﬁne up+1 , . . . , um  = 0. 
Then  (u0 , u)  ≥  0,  (u0 , u)  (cid:10)
=  0,  and  for  any  i,  either  gi ( ¯
x)  =  0,  or  ui  = 0. 
Finally, 

x)t u + ∇h( ¯
u0∇f ( ¯
x) + ∇g( ¯
x)t v = 0. 

Theorem  11  (Karush-Kuhn-Tucker  (KKT)  Necessary  Conditions) 
Let  x  be  a  feasible  solution  of  (P)  and  let  I  =  {i  :  gi ( ¯
x) = 0}.  Further, 
¯
suppose  that  ∇hi ( ¯
x)  for  i = 1, . . . , l  and  ∇gi ( ¯
x)  for  i ∈  I  are  linearly  inde-
pendent.  If  ¯x  is  a  local  minimum,  there  exists  (u, v)  such  that 
∇f ( ¯
x) + ∇g( ¯
x)t u + ∇h( ¯
x)t v = 0, 
u ≥ 0, 
uigi ( ¯x) = 0  , i = 1, . . . , m. 

Proof:  x must  satisfy  the  Fritz  John  conditions.  If  u0  > 0, we  can  redeﬁne 
¯
u ← u/u0  and  v ← v/u0 . If  u0  = 0,  then 
(cid:16) 
(cid:16) 
l 
vi∇hi ( ¯
ui∇gi ( ¯
x) + 
x) = 0  , 
i∈I
i=1 
and  so  the  above  gradients  are  linearly  dependent.  This  contradicts  the 
assumptions  of  the  theorem. 

Example  1  Consider  the  problem: 
min  6(x1  − 10)2  +4(x2  − 12.5)2 
+(x2  − 5)2 

s.t. 

2
x1 

2
x1 
(x1  − 6)2 

2
+3x2 

2
+x2 

In  this  problem,  we  have: 

10 

≤ 50 
≤ 200 
≤ 37 

f (x) = 6(x1  − 10)2  + 4(x2  − 12.5)2 

g1 (x) = x1  + (x2  − 5)2  − 50 
2 

g2 (x) = x  + 3x2  − 200
2 
2
1 

g3 (x) = (x1  − 6)2  + x2  − 37 
2 

We  also  have: 

∇f (x) = 

⎛
⎝

⎞
12(x1  − 10) 
⎠
8(x2  − 12.5) 
⎛
⎞
⎝
⎠
2x1 
2(x2  − 5) 
⎛
⎞
⎠
⎝

2x1 

∇g1 (x) = 

∇g2 (x) = 

6x2 

⎛
⎝

⎞
2(x1  − 6) 
⎠

2x2 

∇g3 (x) = 

Let  us  determine  whether  or  not  the  point  x  = ( ¯ x2 ) = (7, 6)  is  a 
¯
x1 , ¯
candidate  to  be  an  optimal  solution  to  this  problem. 

11 

We  ﬁrst  check  for  feasibility: 

g1 ( ¯x) = 0 ≤ 0 

g2 ( ¯x) = −43 < 0 

g3 ( ¯x) = 0 ≤ 0 
To  check  for  optimality,  we  compute  al l  gradients  at  ¯x: 
⎞ 
⎛
−36 
⎝
⎠
∇f (x) =  −52 ⎛ ⎞
14 ⎝ ⎠
∇g1 (x) = 
2 ⎛ ⎞
14 ⎝ ⎠
36 ⎛ ⎞
2 ⎝ ⎠
12 

∇g3 (x) = 

∇g2 (x) = 

We  next  check  to  see  if  the  gradients  “line  up”,  by  trying  to  solve  for 
u1  ≥ 0, u2  = 0, u3  ≥ 0  in  the  fol lowing  system: 
⎛ ⎞ 
⎞ ⎛ ⎞ 
⎛ 
⎛ ⎞ 
⎛ ⎞ 
−36
⎝
⎠ + ⎝ ⎠ u1  + ⎝ ⎠ u2  + ⎝ 
⎠ u3  = 
⎝ ⎠
14
14 
2 
0 
−52
0 
12 
36
2 
u ≥ 0
Notice  that  ¯
u1 , ¯ u3 ) = (2, 0, 4)  solves  this  system,  and  that  ¯
u = ( ¯ u2 , ¯
and  ¯
u2  = 0.  Therefore  x  is  a  candidate  to  be  an  optimal  solution  of  this 
¯
problem. 

12 

Example  2  Consider  the  problem  (P): 

s.t. 

(P)  : maxx  xT Qx 
(cid:15)x(cid:15) ≤ 1 
where  Q  is  symmetric.  This  is  equivalent  to: 
(P) : minx  −xT Qx 
x x  ≤ 1  . 
T

s.t. 

The  KKT  conditions  are: 

−2Qx + 2ux  = 0 
Tx x  ≤  1 
u  ≥  0 
u(1 − xT x) = 0  . 
One  solution  to  the  KKT  system  is  x  = 0, u  = 0,  with  objective  function 
value  xT Qx = 0.  Are  there  any  better  solutions  to  the  KKT  system? 
If  x  (cid:10)= 0  is  a  solution  of  the  KKT  system  together  with  some  value 
u  ,  then  x  is  an  eigenvector  of  Q  with  nonnegative  eigenvalue  u. Also, 
xT Qx = uxT x = u, and so the objective value of this solution is u.  Therefore 
the  solution  of  (P)  with  the  largest  objective  function  value  is  x  = 0  if  the 
largest  eigenvalue  of  Q  is  nonpositive.  If  the  largest  eigenvalue  of  Q  is 
positive,  then  the optimal objective value of (P)  is  the  largest eigenvalue, and 
the  optimal  solution  is  any  eigenvector  x  corresponding  to  this  eigenvalue, 
normalized  so  that  (cid:15)x(cid:15) = 1. 

Example  3  Consider  the  problem: 

13 

min  (x1  − 12)2  +(x2  + 6)2 
− 4.5x2  ≤ 6.5 
≤ 64 

2
2
x1  + 3x1  +x2 
(x1  − 9)2 

s.t.	

2
+x2 

8x1 

+4x2 

= 20 

In  this  problem,  we  have: 

f (x) = (x1  − 12)2  + (x2  + 6)2 

g1 (x) = x  + 3x1  + x2  − 4.5x2  − 6.5
2
2 
1 

g2 (x) = (x1  − 9)2  + x2  − 64 
2 

h1 (x) = 8x1  + 4x2  − 20 

Let  us  determine  whether  or  not  the  point  x  = ( ¯ x2 ) = (2, 1)  is  a 
x1 , ¯
¯
candidate  to  be  an  optimal  solution  to  this  problem. 

We  ﬁrst  check  for  feasibility: 

g1 ( ¯x) = 0 ≤ 0 

g2 ( ¯x) = −14 < 0 

h1 ( ¯x) = 0


To  check  for  optimality,  we  compute  al l  gradients  at  ¯
x: 

14 

∇f (x) = 

⎛
⎞ 
−20 
⎝
⎠
⎞
14 ⎛
7 ⎝
⎠
∇g1 (x) =  −2.5 
⎞ 
⎛
−14 
⎝
⎠
∇g2 (x) = 
2 ⎛ ⎞
8 ⎝ ⎠
4 

∇h1 (x) = 

We  next  check  to  see  if  the  gradients  “line  up”,  by  trying  to  solve  for 
u1  ≥ 0, u2  = 0, v1  in  the  fol lowing  system: 
⎛ ⎞ 
⎞ ⎛ 
⎛
⎛ ⎞ 
⎞ 
⎛
⎞ 
−20 
−14 
⎝
⎠ + ⎝
⎠ u1  + ⎝
⎠ u2  + ⎝ 
⎠ v1  = 
⎝ ⎠
7 
8 
0 
−2.5 
4
0 
14 
2 
Notice  that  ( ¯ v) = ( ¯ u2 , v¯1 ) = (4, 0, −1)  solves  this  system  and  that 
u1 , ¯
u, ¯
u  ≥ 0  and  ¯
u2  = 0.  Therefore  ¯
x  is  a  candidate  to  be  an  optimal  solution  of 
¯
this  problem. 

3  Generalizations  of  Convexity 

Suppose  X  is a convex set  in  (cid:4) .  The  function  f (x) :  X  → (cid:4) is  a 
n
quasiconvex  function  if  for  all  x, y ∈ X  and  for  all  λ ∈ [0, 1], 
f (λx + (1 − λ)y) ≤ max{f (x), f (y)}. 
f (x) is  quasiconcave  if  for  all  x, y ∈ X  and  for  all  λ ∈ [0, 1], 
f (λx + (1 − λ)y) ≥ min{f (x), f (y)}. 

15 

If  f (x) : X  → (cid:4),  then  the  level  sets  of  f (x)  are  the  sets 
Sα  = {x ∈ X  : f (x) ≤ α} 

for  each  α ∈ (cid:4). 

Proposition  12  If  f (x)  is  convex,  then  f (x)  is  quasiconvex. 
Proof:  If  f (x)  is  convex,  for  λ ∈ [0, 1], 
f (λx + (1 − λ)y) ≤ λf (x) + (1 − λ)f (y) ≤ max{f (x), f (y)}. 

Theorem  13  A  function  f (x)  is  quasiconvex  on  X  if  and only if  Sα  is  a 
convex  set  for  every  α ∈ (cid:4). 

Proof:  Suppose that f (x) is quasiconvex.  For any given value of α, suppose 
that  x, y ∈ Sα . 
Let z = λx+(1−λ)y for some λ ∈ [0, 1].  Then f (z ) ≤ max{f (x), f (y)} ≤
α, so  z  ∈ Sα ,  which  shows  that  Sα  is  a  convex  set. 
Conversely, suppose Sα  is a convex set for every α. Let x and y  be given, 
and  let  α = max{f (x), f (y)},  and  hence  x, y  ∈ Sα .  Then  for  any  λ ∈ [0, 1], 
f (λx + (1 − λ)y)  ≤  α  = max{f (x), f (y)}, and  so  f (x)  is  a  quasiconvex 
function. 

Corollary  14  If  f (x)  is  a  convex  function,  its  level  sets  are  convex  sets. 
Suppose X  is a convex set in (cid:4)n .  The diﬀerentiable function f (x) : X  → 
(cid:4) is  a  pseudoconvex  function  if  for  every  x, y ∈ X  the  following  holds: 
∇f (x)t (y − x) ≥ 0  ⇒  f (y) ≥ f (x)  . 

Theorem  15 

(i)  A  diﬀerentiable  convex  function  is  pseudoconvex. 

16 

(ii)  A  pseudoconvex  function  is  quasiconvex. 

Proof:  To  prove  the  ﬁrst  claim,  we  use  the  gradient  inequality:  if  f (x) 
is  convex  and  diﬀerentiable,  then  f (y)  ≥ f (x) + ∇f (x)t (y − x).  Hence,  if 
∇f (x)t (y − x) ≥ 0,  then  f (y) ≥ f (x),  and  so  f (x)  is  pseudoconvex. 
To  show  the  second  claim,  suppose  f (x)  is  pseudoconvex.  Let  x, y  and 
λ ∈ [0, 1] be given,  and  let z = λx + (1 − λ)y . If λ = 0 or λ = 1,  then f (z ) ≤ 
max{f (x), f (y)} trivially;  therefore,  assume  0 < λ < 1.  Let  d = y − x. 
If  ∇f (z )td ≥ 0,  then 

∇f (z )t (y − z ) = ∇f (z )t (λ(y − x)) = λ∇f (z )td ≥ 0, 
so  f (z ) ≤ f (y) ≤ max{f (x), f (y)}. 
On  the  other  hand,  if ∇f (z )td ≤ 0,  then 

∇f (z )t (x − z ) = ∇f (z )t (−(1 − λ)(y − x)) = −(1 − λ)∇f (z )td ≥ 0, 
so  f (z ) ≤ f (x) ≤ max{f (x), f (y)}. Thus  f (x)  is  quasiconvex. 
Incidentally,  we  deﬁne  a  diﬀerentiable  function  f (x) :  X  → (cid:4) to  be 
pseudoconcave  if  for  every  x, y ∈ X  the  following  holds: 
∇f (x)t (y − x) ≤ 0  ⇒  f (y) ≤ f (x)  . 

4  Suﬃcient  Conditions  for  Optimality 

Theorem  16  (KKT  Suﬃcient  Conditions)  Let  ¯x  be  a  feasible  solution 
of  (P),  and  suppose  ¯x  together  with  multipliers  (u, v)  satisﬁes 
∇f ( ¯
x) + ∇g( ¯
x)t u + ∇h( ¯
x)t v = 0, 
u ≥ 0, 

17 

uigi ( ¯x) = 0, i = 1, . . . , m. 
If  f (x)  is  a  pseudoconvex  function,  gi (x), i = 1, . . . , m  are  quasiconvex  func-
tions,  and  hi (x), i = 1, . . . , l  are  linear  functions,  then  ¯x  is  a  global  optimal 
solution  of  (P). 

Proof:  Because  each  gi (x)  is  quasiconvex,  the  level  sets 

Ci  := {x ∈ X  : gi (x) ≤ 0}, i = 1, . . . , m 

are  convex  sets.  Also,  because  each  hi (x)  is  linear,  the  sets 

Di  = {x ∈ X  : hi (x) = 0}, i = 1, . . . , l 

are  convex  sets.  Thus,  since  the  intersection  of  convex  sets  is  also  a  convex 
set,  the  feasible  region 

S  = {x ∈ X  : g(x) ≤ 0, h(x) = 0} 

is  a  convex  set. 
Let  I  =  {i  |  gi ( ¯
x) = 0}  denote  the  index  of  active  constraints  at  ¯
x. Let 
x  ∈  S  be  any  point  diﬀerent  from  ¯
x.  Then  λx + (1 − λ) ¯
x  is  feasible  for  all 
λ ∈ (0, 1).  Thus  for  i ∈ I  we  have 
x + λ(x − ¯
gi (λx + (1 − λ) ¯
x)) ≤ 0 = gi ( ¯
x)
x) = gi ( ¯
for  any  λ  ∈  (0, 1),  and  since  the  value  of  gi (·)  does  not  increase  by  moving 
in  the  direction  x − ¯
x, we must have  ∇gi ( ¯
x)t (x − ¯
x) ≤ 0  for  all  i ∈ I . 
x)t (x −  ¯
x + λ(x −  ¯
x))  =  0,  and  so  ∇hi ( ¯
Similarly,  ∇hi ( ¯
x)  =  0  for  all 
i = 1, . . . , l. 

(cid:18)
(cid:17)
Thus,  from  the  KKT  conditions, 
∇f ( ¯
(x − ¯
x)t u + ∇h( ¯
x)t (x − ¯
x) ≥ 0, 
x) = − ∇g( ¯
x)t v 
and  by  pseudoconvexity,  f (x) ≥ f ( ¯x)  for  any  feasible  x. 

t 

18 

The  program 

(P)  minx f (x) 
g(x) ≤ 0 

s.t. 

h(x) = 0 
x ∈ X 
is  called  a  convex  program  if  f (x),  gi (x), i = 1, . . . , m  are  convex  functions, 
hi (x), i = 1 . . . , l  are  linear  functions,  and X  is  an  open  convex  set. 

Corollary  17  The KKT conditions are  suﬃcient  for optimality of a convex 
program. 

Example  4  Continuing  Example  1,  note  that  f (x), g1 (x), g2 (x), and  g3 (x) 
are  al l  convex  functions.  Therefore  the  problem  is  a  convex  optimization 
problem,  and  the  KKT  conditions  are  necessary  and  suﬃcient.  Therefore 
¯x = (7, 6)  is  the  global  minimum. 

Example  5  Continuing Example  3,  note  that  f (x), g1 (x), g2 (x)  are  al l  con-
vex  functions  and  that  h1 (x)  is  a  linear  function.  Therefore  the  problem  is 
a  convex  optimization  problem,  and  the  KKT  conditions  are  necessary  and 
suﬃcient.  Therefore  ¯x = (2, 1)  is  the  global  minimum. 

5  Constraint  Qualiﬁcations 

Recall  that  the  statement  of  the  KKT  necessary  conditions  established 
herein has the form “if ¯x is a local minimum of (P) and (some requirement for 
the  constraints)  then  the  KKT  conditions must  hold  at  ¯x.”  This  additional 
requirement  for  the  constraints  that  enables  us  to  proceed with  the  proof  of 
the  KKT  conditions  is  called  a  constraint  qualiﬁcation. 

In  (Theorem  11)  we  established  the  following  constraint  qualiﬁcation: 

19 

Linear Independence Constraint Qualiﬁcation:  The gradients ∇gi ( ¯x), i ∈ 
I ,  ∇hi ( ¯x), i = 1, . . . , l  are  linearly  independent. 
We  will  now  establish  two  other  useful  constraint  qualiﬁcations.  Before 
doing  so  we  have  the  following  important  deﬁnition: 

Deﬁnition  5.1  A point  x  is  cal led  a  Slater point  if  x  satisﬁes  g(x) < 0  and 
h(x) = 0,  that  is,  x  is  feasible  and  satisﬁes  al l  inequalities  strictly. 

Theorem  18  (Slater  condition)  Suppose  that  gi (x),
i  = 1, . . . , m  are 
pseudoconvex,  hi (x), i  = 1, . . . , l  are  linear,  and  ∇hi (x), i  = 1, . . . , l  are 
linearly  independent,  and  (P)  has  a  Slater  point.  Then  the  KKT  conditions 
are  necessary  to  characterize  an  optimal  solution. 

Proof:  Let  ¯x be a  local minimum.  The Fritz-John  conditions  are necessary 
for  this problem, whereby  there must  exist  (u0 , u, v)  (cid:10)= 0  such  that  (u0 , u) ≥ 
0 and 

x)t u + ∇h( ¯
u0∇f ( ¯
x) + ∇g( ¯
x)t v = 0, uigi ( ¯
x) = 0.

If  u0  >  0,  dividing  through  by  u0  demonstrates  KKT  conditions.  Now 
suppose  u0  = 0.  Let  x0  be  Slater  point,  and  deﬁne  d  :=  x0  −  ¯x.  Then  for 
each  i ∈ I , 0 = gi ( ¯x) > gi (x0 ),  and  by  the  pseudo-convexity  of  gi (·) we have 
∇gi ( ¯
x)td < 0.  Also,  since  hi (x), i = 1, . . . , l  are  linear,  dt∇h( ¯
x) = 0.  Thus, 

x)t u + ∇h( ¯
0 = 0td = (∇g( ¯
x)t v)td < 0, 
unless  ui  = 0  for  all  i  ∈  I .  But  if  this  is  true,  then  we  would  have  v  (cid:10)= 0 
and  ∇h( ¯x)tv  =  0,  violating  the  linear  independence  assumption.  This  is  a 
contradiction,  and  so  u0  > 0. 

Theorem  19  (Linear  constraints)  If  al l  constraints  are  linear,  the KKT 
conditions  are  necessary  to  characterize  an  optimal  solution. 

Proof:  Our  problem  is 

20 

(P)  minx f (x) 
s.t.  Ax ≤ b 

M x = g . 

Suppose  ¯x  is  a  local  optimum.  Without  loss  of  generality,  we  can  partition 
the  constraints  Ax  ≤  b  into  groups  AI x  ≤  bI  and  AI¯x  ≤  b ¯ such  that 
x < bI¯.  Then  at  x,  the  set  {d  :  AI d  ≤  0, M d  = 0  }  is 
I 
AI x  =  bI  and  AI¯¯
¯
¯
precisely  the  set  of  feasible  directions.  Thus,  in  particular,  for  every  d  as 
above, ∇f ( ¯x)td ≥ 0,  for otherwise d would be a  feasible descent direction at 
¯x,  violating  its  local  optimality.  Therefore,  the  linear  system 
∇f ( ¯x)td < 0, AI d ≤ 0, M d = 0 
has  no  solution.  From  the  Key  Lemma,  there  exists  (u, v , w)  satisfying 
u = 1,  v ≥ 0,  and  ∇f ( ¯x)u + AI
T v + M T w = 0  which  are  precisely  the  KKT 
conditions. 

5.1  Second-Order  Optimality  Conditions 

To  describe  the  second  order  conditions  for  optimality,  we  will  deﬁne 
the  following  function,  known  as  the  Lagrangian  function,  or  simply  the 
Lagrangian: 
(cid:16) 
(cid:16) 
l 
m
L(x, u, v) = f (x) + 
vihi (x) = f (x) + u g(x) + v th(x). 
uigi (x) + 
t
i=1 
i=1 
Using  the Lagrangian, we  can,  for  example,  re-write  the  gradient  conditions 
of  the  KKT  necessary  conditions  as  follows: 
∇xL( ¯x, u, v) = 0, 
since  ∇xL(x, u, v) = ∇f (x) + ∇g(x)tu + ∇h(x)tv . 
(cid:19)
(cid:19) 
Also, note that ∇2  L(x, u, v) = ∇2f (x)+
i=1 ui∇2gi (x)+
i=1 vi∇2hi (x).
m
l
Here  we  use  the  standard  notation:  ∇2 q(x)  denotes  the  Hessian  of  the 
xx

(1) 

21 

function  q(x),  and  ∇2  L(x, u, v)  denotes  the  submatrix  of  the  Hessian  of 
xx
L(x, u, v) corresponding to the partial derivatives with respect to the x vari-
ables  only. 

Theorem  20  (KKT  second  order  necessary  conditions)  Suppose  ¯x is 
a  local minimum  of  (P),  and ∇gi ( ¯
x), i ∈ I  and ∇hi ( ¯
x), i = 1, . . . , l  are  lin-
early  independent.  Then  ¯x  must  satisfy  the  KKT  conditions.  Furthermore, 
every  d  that  satisﬁes: 

must  also  satisfy 

∇gi ( ¯x)td  ≤ 0,
∇hi ( ¯x)td  = 0,

i ∈ I , 
i = 1 . . . , l 

dt∇xxL( ¯x, u, v)d ≥ 0  . 

Theorem  21  (KKT  second  order  suﬃcient  conditions)  Suppose the point 
¯x  ∈  S  together  with  multipliers  (u, v)  satisﬁes  the  KKT  conditions.  Let 
I +  =  {i ∈  I  : ui  >  0}  and  I 0  =  {i ∈  I  : ui  = 0}.  Additional ly,  suppose  that 
every  d  (cid:10)= 0  that  satisﬁes 
∇gi ( ¯x)td  = 0,
∇gi ( ¯x)td  ≤ 0,
∇hi ( ¯x)td  = 0,

i ∈ I + , 
i ∈ I 0 , 
i = 1 . . . , l 

also  satisﬁes 

dt∇2  L( ¯x, u, v)d > 0  .
xx

Then  ¯x  is  a  strict  local  minimum  of  (P). 

22 

6  A  Proof  of  Theorem  2 

The  proof  of  Theorem  2  relies  on  the  Implicit  Function  Theorem.  To moti-
vate  the  Implicit  Function  Theorem,  consider  a  system  of  linear  functions: 

h(x) := Ax − b 

and  suppose  that  we  are  interested  in  solving 

h(x) = Ax − b = 0  . 
Let us  assume  that A ∈ (cid:4)l×n  has  full  row  rank  (i.e.,  its  rows are  linearly 
independent).  Then  we  can  partition  columns  of  A  and  elements  of  x  as 
follows:  A  = [B  |  N ],  x  = (y ; z ),  so  that  B  ∈ (cid:4)l×l  is  non-singular,  and 
h(x) = B y + N z − b. 
−1 b−B
−1N z .  Then for any z , h(s(z ), z ) = B s(z )+N z − b = 
Let s(z ) = B
0,  i.e.,  x = (s(z ), z )  solves h(x) = 0.  This  idea  of  “invertability”  of  a  system 
of  equations  is  generalized  (although  only  locally)  by  the  following  version 
of  the  Implicit Function Theorem, where we will  preserve  the  notation  used 
above: 

Theorem  22  (Implicit  Function  Theorem)  Let  h(x) :  (cid:4)n  → (cid:4)l  and 
y1 , . . . , y¯l , z¯1 , . . . , z¯n−l ) = ( ¯ z )  satisfy: 
x = ( ¯
¯
y , ¯

1.  h( ¯x) = 0 

2.  h(x)  is  continuously  diﬀerentiable  in  a  neighborhood  of  ¯x 
3.  The  l × l  Jacobian  matrix ⎡  ∂h1 ( ¯x) 
⎤ 
⎥ ⎥ 
⎢ ⎢ 
⎣ 
⎦ 
∂ y1 
. . . 
∂hl ( ¯x) 
∂ y1 

· · ·  ∂h1 ( ¯x) 
∂ yl 
. . . 
. . . 
· · · 
∂hl ( ¯x) 
∂ yl 

is  nonsingular. 

23 


Then  there  exists   >  0  along  with  functions  s(z ) = (s1 (z ), . . . , sl (z ))  such 
that  for  al l  z  ∈ B ( ¯z , ),  h(s(z ), z ) = 0  and  sk (z )  are  continuously  diﬀeren-
tiable.  Moreover,  for  al l  i = 1, . . . , m  and  j  = 1, . . . , n − l  we  have: 
l (cid:16)  ∂hi (y , z )  ∂ sk (z ) 
· 
∂ yk 
∂ zj 
k=1 

∂hi (y , z ) 
∂ zj

= 0  . 

+ 

Proof of Theorem 2:  Let  A = ∇h( ¯x) ∈ (cid:4)l×n .  Then  A  has  full  row  rank, 
and its columns (along with corresponding elements of  ¯x) can be re-arranged 
so  that  A = [B  | N ] and  ¯
y ; ¯
x = ( ¯ z ),  where  B  is  non-singular.  Let  z  lie  in  a 
small  neighborhood  of  ¯z .  Then,  from  the  Implicit  Function  Theorem,  there 
exists  s(z )  such  that  h(s(z ), z ) = 0. 
Suppose  that  d  ∈  F0  ∩ G0  ∩ H0 ,  and  let  us  write  d  = (q ; p).  Then 
0 =  Ad  =  B q + N p,  whereby  q  =  −B
−1N p. Let  z (θ) =  z¯ + θp,  y(θ) = 
s(z (θ)) = s( ¯z + θp),  and  x(θ) = (y(θ), z (θ)).  We  will  derive  a  contradiction 
by  showing  that  d  is  an  improving  feasible  direction,  i.e.,  for  small  θ >  0, 
x(θ)  is  feasible  and  f (x(θ)) < f ( ¯x). 

To  show  feasibility  of  x(θ),  note  that  for  θ >  0  suﬃciently  small,  it 
follows  from  the  Implicit  Function  Theorem  that: 

h(x(θ)) = h(s(z (θ)), z (θ)) = 0  . 

Furthermore,  for  i = 1, . . . , l  we  have: 
(cid:16)  ∂hi (s(z (θ)), z (θ))  ∂ sk (z (θ))  n−l  ∂hi (s(z (θ)), z (θ))  ∂ zk (θ)
(cid:16) 
l 
· 
· 
+ 
∂ θ 
∂ θ 
∂ yk 
∂ zk 
k=1 
k=1 

∂hi (x(θ)) 
∂ θ 

0 = 

= 

. 

∂ zk (θ)  =  pk .  The  above  equation  system 
Let  rk  =  ∂ sk (z (θ)) ,  and  recall  that 
can  then  be  re-written  as  0  =  B r + N p, or  r  =  −B
−1N p  =  q .  Therefore, 
∂ θ 
∂ θ 
∂xk (θ)  = dk  for  k = 1, . . . , n.
∂ θ 
For  i ∈ I , 

24 

(cid:20) (cid:20) 
gi (x(θ))  =  gi ( ¯x) + θ  ∂ gi (x(θ)) (cid:20)  + |θ |αi (θ)
(cid:20) 
(cid:19) 
∂ gi (x(θ))  ∂xk (θ) (cid:20)
∂ θ 
θ=0 
(cid:20) 
· 
n 
=  θ 
k=1 
∂ θ 
xk 
θ=0 
=  θ∇gi ( ¯x)td + |θ |αi (θ), 
where  αi (θ) → 0 as  θ → 0.  Hence  gi (x(θ)) < 0  for  all  i = 1, . . . , m  for  θ > 0 
suﬃciently  small,  and  therefore,  x(θ)  is  feasible  for  any  θ >  0  suﬃciently 
small. 

On  the  other  hand, 

x)td + |θ |α(θ) < f ( ¯
x) + θ∇f ( ¯	
x)
f (x(θ)) = f ( ¯

for  θ >  0  suﬃciently  small,  which  contradicts  the  local  optimality  of  x.¯
Therefore  no  such  d  can  exist,  and  the  theorem  is  proved. 

7  Constrained  Optimization  Exercises 
(cid:21) 
1.  Suppose  that	 f (x) and  gi (x), i  = 1, . . . , m  are  convex  real-valued 
functions  over  (cid:4)n ,  and  that  X  ⊂ (cid:4)n  is  a  closed  and  bounded  con-
vex  set.  Let  I  = (s, z ) ∈ (cid:4)m+1  :  there  exists  an  x  ∈ X  for  which 
g(x) ≤ s,  f (x) ≤ z}.  Prove  that  I  is  a  closed  convex  set. 
2.  Suppose  that	 f (x) and  gi (x), i  = 1, . . . , m  are  convex  real-valued 
functions  over  (cid:4)n ,  and  that  X  ⊂ (cid:4)n  is  a  closed  and  bounded  convex 
set.  Consider  the  perturbation  function: 
∗
z  (y) = minimumx 

i = 1, . . . , m 

s.t. 

f (x) 
gi (x)  ≤  yi ,
x ∈ X . 
•	 Prove  that  z  (·)  is  a  convex  function. 
∗

25 

s.t. 

•  Show  that  y1  ≤ y2  implies  that  z  (y1 ) ≥ z  (y2 ). 
∗
∗
3.  Consider  the  program 
(P)  :  z  = minimumx  (cid:15)c − x(cid:15) 
∗
(cid:15)x(cid:15) = α , 
where  α  is  a  given  nonnegative  scalar.  What  are  the  necessary  opti-
mality  conditions  for  this problem?  Use  these  conditions  to  show  that 
∗ z  =  |(cid:15)c(cid:15) −α |.  What  is  the  optimal  solution  x 
∗? 
4.  Let  S1  and  S2  be  convex  sets  in  (cid:4) .  Recall  the  deﬁnition  of  strong 
n
separation  of  convex  sets  in  the  notes,  and  show  that  there  exists  a 
hyperplane  that  strongly  separates  S1  and  S2  if  and  only  if 
inf {(cid:15)x1  − x2(cid:15) |x 1  ∈ S1 , x2  ∈ S2}  >  0  . 
5.  Consider  S  = {x ∈ (cid:4) | x1 + x2 
≤ 1}.  Represent  S  as  the  intersection 
2
2 
2
of  a  collection  of  half-spaces.  Find  the  half-spaces  explicitly. 
6.  Let  C  be  a  nonempty  set  in  (cid:4)n .  Show  that  C  is  a  convex  cone  if  and 
only  if  x1 , x2  ∈ C  implies  that  λ1x1  + λ2x2  ∈ C  whenever  λ1 , λ2  ≥ 0 
and  λ1  + λ2  (cid:10)= 0. 
7.  Let  S  be  a  nonempty  convex  set  in  (cid:4)n  and  let  f (·) :  S  → (cid:4).  Show 
that  f (·)  is  a  convex  function  on  S  if  and  only  if  for  any  integer  k ≥ 2 
⎛
⎞ 
the  following  holds  true: 
(cid:16) 
(cid:16) 
1 x  , . . . , x k  ∈ S  ⇒  f ⎝ 
λj xj ⎠ ≤ 
k 
k
λj f (xj ) 
(cid:19) 
j=1 
j=1 
whenever  λ1 , . . . , λk  satisfy  λ1 , . . . , λk  ≥ 0 and 
k 
λj  = 1.

j=1

8.  Let  f1 (·), . . . , fk (·) :  (cid:4)n  → (cid:4)  be  convex  functions,  and  consider  the 
function  f (·)  deﬁned  by: 
f (x) := max{f1 (x), . . . , fk (x)} . 
Prove  that  f (·)  is  a  convex  function.  State  and  prove  a  similar  result 
for  concave  functions. 

26 

9.  Let  f1 (·), . . . , fk (·) :  (cid:4)n  → (cid:4)  be  convex  functions,  and  consider  the 
function  f (·)  deﬁned  by: 
f (x) := α1f1 (x) + · · · + αk fk (x)  , 
where  α1 , . . . , αk  > 0.  Prove  that  f (·)  is  a  convex  function.  State  and 
prove  a  similar  result  for  concave  functions. 

10.  Consider  the  following  problem: 
minimumx  (x1  − 4)2  + (x2  − 6)2 
−x1  + x2 
2

s.t. 

≥  0 
≤  4 

x2 
x ∈ (cid:4)2  . 
Write a necessary condition for optimality and verify that it is satisﬁed 
by  the  point  (2, 4).  Is  this  the  optimal  point?  Why  or  why  not? 
11.  Consider  the  problem  to  minimize	 f (x)  sub ject  to  x  ∈  S  where  S 
is  a  convex  set  in  (cid:4)n  and  f (·)  is  a  diﬀerentiable  convex  function  on 
S .	 Prove  that  ¯x  is  an  optimal  solution  of  this  problem  if  and  only  if 
x) ≥ 0  for  every  x ∈ S .
x)t (x − ¯
∇f ( ¯
12.  Consider  the  following  problem: 
3x1  − x2  + x3 
2
x1  + x2  + x3  ≤  0 
−x1  + 2x2  + x2  = 0
3 
x ∈ (cid:4)3  . 
•  Write  down  the  KKT  optimality  conditions. 
•  Argue  why  this  problem  is  unbounded. 

maximizex 

s.t. 

27 

(cid:18)2 
(cid:17)
13.  Consider  the  following  problem: 
x1  − 9  + (x2  − 2)2 
4 
x2  − x2
1 

minimizex 

s.t. 

≥  0 
≤  6 

x1  + x2 
x1  ≥ 0 
x2  ≥ 0 
x ∈ (cid:4)2  . 
(cid:17) 
(cid:18) 
•  Write down  the KKT optimality  conditions and verify  that  these 
x =  2 , 9  .
3
conditions  are  satisﬁed  at  the  point  ¯
4 
•  Present  a  graphical  interpretation  of  the  KKT  conditions  at  ¯x. 
•  Show  that  ¯x  is  the  optimal  solution  of  the  problem. 
14.  Let  f (·) : (cid:4)n  → (cid:4),  gi (·) : (cid:4)n  → (cid:4),  i = 1, . . . , m,  be  convex  functions. 
Consider  the  problem  to  minimize  f (x)  sub ject  to  gi (x)  ≤  0 for  i  = 
1, . . . , m, and suppose that the optimal ob jective value of this problem 
∗
∗
is  z  and  is  attained  at  some  feasible  point  x  . Let  M  be  a  proper 
subset of {1, . . . , m} and suppose that ˆx solves the problem to minimize 
f (x)  sub ject  to  gi (x)  ≤  0 for  i  ∈  M . Let  V  :=  {i  |  gi ( ˆx)  >  0}. If 
z  > f ( ˆx),  show  that  gi (x  )  =  0  for  some  i  ∈ V .  (This  shows  that  if 
∗
∗
an  unconstrained  minimum  of  f (·)  is  infeasible  and  has  an  ob jective 
∗
value  that  is  less  than  z  ,  then  any  constrained  minimum  lies  on  the 
boundary  of  the  feasible  region.) 
15.  Consider  the  following  problem,  where  c  (cid:10)= 0  is  a  vector  in  (cid:4)n : 
cT d 
minimized 
s.t. 
dtd 
d ∈ (cid:4) . 
n
•  Show  that  d ¯ := −  c 
is  a  KKT  point  of  this  problem.  Further-
(cid:6)c(cid:6)2 
more,  show  that  d ¯ is  indeed  the  unique  optimal  solution. 

≤  1 

28 

•	 How  is  this  result  related  to  the  deﬁnition  of  the  direction  of 
steepest  descent  in  the  steepest  descent  algorithm? 

16.  Consider  the  following  problem,  where  b  and  aj , cj , j  = 1, . . . , n  are 
positive  constants: 
(cid:19) 
n  cj
xj 
j=1 
(cid:19) 
n 
aj xj  =  b 
j=1 

minimizex 

s.t.	

≥  0, j  = 1, . . . , n 

xj 
x ∈ (cid:4) . 
n

Write  down  the KKT  optimality  conditions,  and  solve  for  the  point  ¯
x 
that  solves  this  problem. 
17.  Let c ∈ (cid:4)n , b ∈ (cid:4)m , A ∈ (cid:4)m×n , and H  ∈ (cid:4)n×n .  Consider the following 
two  problems: 

P1  :  minimizex  c x +  1 xT H x
t
2 

s.t. 

Ax 
x ∈ (cid:4)
n

≤  b 

and


P2  :  minimizeu  htu +  1 uT Gu

2 

≥  0 

s.t. 

u 
u ∈ (cid:4) , 
m
−1AT  and h  := AH
−1 c+b.  Investigate the relationship 
where G := AH
between  the  KKT  conditions  of  these  two  problems. 
18.  Consider  the  problem  to  minimize  f (x)  sub ject  to  Ax  ≤  b.  Suppose 
¯
¯
x  is  a  feasible  solution  such  that  Aβ x =  bβ  and  Aη x < bη  where 
that  ¯

29 

β , η are  a  partition  of  the  rows  of A.  Assuming  that Aβ  has  full  rank, 
the matrix P  that pro jects any vector onto the nullspace of Aβ  is given 
by: 
P  = I − AT [Aβ AT ]
−1Aβ  .
β 
β
•	 Let  d ¯ =  −P ∇f ( ¯
x).  Show  that  if  d ¯ (cid:10)
= 0  then  d ¯ is  an  improving 
x + λd ¯ is  feasible  and  f ( ¯
x + λd¯) < f ( ¯
direction,  that  is,  ¯	
x)  for  all 
λ > 0  and  suﬃciently  small. 
•  Suppose  that  d ¯ =  0  and  that  u  :=  −AT [Aβ AT ]−1Aβ ∇f ( ¯x)  ≥ 0.
β 
β
Show  that  ¯x  is  a  KKT  point. 
•	 Show  that  d ¯ is  a  positive  multiple  of  the  optimal  solution  of  the 
following  problem: 

0
0 
≤  1 

minimized  ∇f ( ¯x)T d 
s.t. 
Aβ d 
dT d 
d ∈ (cid:4)n  . 
•	 Suppose  that  A  =  −I  and  b  =  0,  that  is,  the  constraints  are  of 
the  form  “x  ≥  0”.  Develop  a  simple  way  to  construct  d ¯ in  this 
case. 
19.  Consider  the  problem  to minimize  f (x)  sub ject  to  x ∈ X  and  gi (x) ≤ 
x  be  a  feasible  point,  and  let  I  := {i  | gi ( ¯
x) = 0}. 
0, i = 1, . . . , m. Let  ¯
Suppose  that  X  is  an  open  set  and  gi (x), i = 1, . . . , m  are  continuous 
functions,  and  let  J  :=  {i  |  gi (·)  is  pseudoconcave}.  Furthermore, 
(cid:22)	
(cid:23) 
suppose  that 
d  | ∇gi ( ¯	
x)td < 0 for  i ∈ I  \ J
x)td ≤ 0 for  i ∈ J,  ∇gi ( ¯

is nonempty.  Show that this condition is suﬃcient to validate the KKT 
conditions  at  ¯x.  (This  is  called  the  “Arrow-Hurwicz-Uzawa  constraint 
qualiﬁcation.”) 

30 

