Introduction  to  Semideﬁnite  Programming   (SDP) 

Robert   M.   Freund  

March,   2004  

2004 Massachusetts Institute of Technology.

1 

1 

Introduction 

Semideﬁnite programming (SDP ) is the most exciting development in math-
ematical  programming  in  the  1990’s.  SDP  has  applications  in  such  diverse 
ﬁelds  as  traditional  convex  constrained  optimization,  control  theory,  and 
combinatorial  optimization.  Because  SDP  is  solvable  via  interior  point 
methods,  most  of  these  applications  can  usually  be  solved  very  eﬃciently 
in  practice  as  well  as  in  theory. 

2  Review  of  Linear  Programming 

Consider  the  linear  programming  problem  in  standard  form: 

LP  :  minimize  c · x 
ai  · x = bi ,
x ∈ (cid:3)+ . 
n

s.t. 

i = 1, . . . , m 

(cid:1) n 
Here  x  is  a  vector  of  n  variables,  and  we  write  “c · x”  for  the  inner-product 
j=1  cj xj ”,  etc. 
“

Also,  (cid:3)+  := {x ∈ (cid:3) | x ≥ 0}, and  we  call  (cid:3)+  the  nonnegative  orthant. 
n
n 
n
In  fact, (cid:3)n  is a closed convex   cone,   where K  is called a closed a convex cone 
+ 
if K  satisﬁes  the  following  two  conditions: 

•  If  x, w ∈ K ,  then  αx + βw ∈ K  for  all  nonnegative  scalars  α  and  β . 
•  K  is  a  closed  set. 

2 

In  words,  LP  is  the  following  problem: 

“Minimize   the   linear   function   c · x,   subject   to   the   condition   that   x must   solve  
m  given   equations   ai  · x = bi , i = 1, . . . , m,   and   that   x  must   lie   in   the   closed  
convex  cone  K  = (cid:3)+ .”  
n

s.t.	

yiai  + s = c 

yi bi 

LD  :  maximize 

We  will  write  the  standard  linear  programming  dual  problem  as: 
(cid:1) 
m 
(cid:1) 
i=1 
m 
i=1 
s ∈ (cid:3)+ . 
n
(cid:1) 
(cid:1) 
Given  a  feasible  solution  x  of  LP  and  a  feasible  solution  (y , s) of  LD ,  the 
i=1  yi bi  = (c −
· x = s · x ≥ 0, because 
duality gap is simply c · x − 
m
m
i=1  yiai ) 
x ≥ 0 and  s ≥ 0.  We  know  from  LP  duality  theory  that  so  long  as  the  pri-
mal  problem  LP  is  feasible  and  has  bounded  optimal  ob jective  value,  then 
the primal and  the dual both attain  their optima with no duality gap.  That 
(cid:1) m 
∗
∗	
∗
is,  there  exists  x  and  (y  , s  )  feasible  for  the  primal  and  dual,  respectively, 
such  that  c · x  − 
bi  = s  · x 
∗
∗
∗ 
∗  = 0. 
i=1  yi 

3  Facts  about  Matrices and  the  Semideﬁnite  Cone 
If X  is  an  n × n  matrix,  then X  is  a  positive  semideﬁnite  (psd)  matrix  if 

v T X v ≥ 0  for  any  v ∈ (cid:3)n . 

3 

If X  is  an  n × n  matrix,  then X  is  a  positive  deﬁnite  (pd)  matrix  if 

v T X v > 0  for  any  v ∈ (cid:3)n , v  (cid:5)= 0. 

Let  S n  denote  the  set  of  symmetric  n × n  matrices, and  let  nS+ denote

the  set  of  positive  semideﬁnite  (psd)  n × n  symmetric  matrices.  Similarly 
let  S n  denote  the  set  of  positive  deﬁnite  (pd)  n × n  symmetric matrices. 
++ 

Let  X  and  Y  be  any  symmetric  matrices.  We  write  “X  (cid:6) 0”  to  denote 
that  X  is  symmetric  and  positive  semideﬁnite,  and  we  write  “X  (cid:6)  Y ” to 
denote  that  X − Y  (cid:6) 0.  We  write  “X  (cid:7) 0”  to  denote  that  X  is  symmetric 
and  positive  deﬁnite,  etc. 
{X  ∈  S n 
nS+  = 
Remark   1  
n × (n + 1)/2. 
dimension  

X  (cid:6)  0}  is   a   closed   convex   cone   in   (cid:3) 2  of
n

|


To  see  why  this  remark  is  true,  suppose  that 
α, β  ≥ 0.  For  any  v ∈ (cid:3)n , we have:

v T (αX + βW )v = αvT X v + β vT W v ≥ 0,


n∈
X, W S+ .  Pick  any  scalars


∈
αX βW S+ This  shows  that  nS+ is  a  cone.  It  is  also  straight-
n
. 
whereby 
+ 
forward  to  show  that  nS+ is  a  closed  set. 

Recall  the  following  properties  of  symmetric  matrices: 
•	 If  X  ∈  S n ,  then  X  =  QDQT  for  some  orthonormal  matrix  Q  and 
some  diagonal  matrix  D .  (Recall  that  Q  is  orthonormal  means  that 

4 

Q−1  = QT ,  and  that D  is diagonal means  that  the  oﬀ-diagonal  entries 
of D  are  all  zeros.) 
•	 If  X  =  QDQT  as  above,  then  the  columns  of  Q  form  a  set  of  n 
orthogonal eigenvectors of X , whose eigenvalues are the corresponding 
diagonal  entries  of D . 
•	 X  (cid:6)  0  if  and  only  if  X  =  QDQT  where  the  eigenvalues  (i.e.,  the 
diagonal  entries  of D)  are  all  nonnegative. 
•	 X  (cid:7)  0  if  and  only  if  X  =  QDQT  where  the  eigenvalues  (i.e.,  the 
diagonal  entries  of D)  are  all  positive. 
•	 If X  (cid:6) 0 and  if Xii  = 0,  then Xij  = Xj i  = 0  for  all  j  = 1, . . . , n. 
•	 Consider  the  matrix M  deﬁned  as  follows: 
(cid:3) 
(cid:2) 
v 
P
, 
Tv
d

M  = 

where  P  (cid:7)  0,  v  is  a  vector,  and  d  is  a  scalar.  Then  M  (cid:7)  0 if and 
only  if  d − v
T P −1
v > 0. 

4  Semideﬁnite  Programming 
Let X  ∈ S n .  We  can  think  of X  as  a matrix,  or  equivalently,  as  an  array  of 
n2  components  of  the  form  (x11 , . . . , xnn ).  We  can  also  just  think  of  X  as 
an  ob ject  (a  vector)  in  the  space  S n .  All  three  diﬀerent  equivalent  ways  of 
looking  at X  will  be  useful. 

What  will  a  linear  function  of  X  look  like?  If  C (X )  is  a  linear  function 
of X ,  then  C (X )  can  be  written  as  C • X ,  where 

5 

C • X  := 

(cid:4) (cid:4) 
n
n 
Cij Xij . 
i=1  j=1 

If  X  is  a  symmetric  matrix,  there  is  no  loss  of  generality  in  assuming  that 
the  matrix  C  is  also  symmetric.  With  this  notation,  we  are  now  ready  to 
deﬁne  a  semideﬁnite  program.  A  semideﬁnite  program  (SDP )  is  an  opti-
mization  problem  of  the  form: 

SDP  :  minimize  C • X 
Ai  • X  = bi  , i = 1, . . . , m, 
X  (cid:6) 0, 

s.t. 

Notice  that  in  an  SDP  that  the  variable  is  the  matrix  X ,  but  it  might 
be  helpful  to  think  of  X  as  an  array  of  n2  numbers  or  simply  as  a  vector 
in  S n .  The  ob jective  function  is  the  linear  function  C • X  and  there  are  m 
linear  equations  that  X  must  satisfy,  namely  Ai  • X  =  bi 
, i  = 1, . . . , m. 
The variable X  also must  lie  in the  (closed convex) cone of positive semidef-
inite  symmetric  matrices  S n 
+ .  Note  that  the  data  for  SDP  consists  of  the 
symmetric  matrix  C  (which  is  the  data  for  the  ob jective  function)  and  the 
m  symmetric  matrices  A1 , . . . , Am ,  and  the  m−vector  b,  which  form  the  m 
linear  equations. 

Let  us  see  an  example  of  an  SDP  for  n  =  3  and  m  =  2.  Deﬁne  the 
following matrices: 
⎛
⎞
⎛
⎞ 
⎛
⎞
0 ⎠ ,  and  C  = ⎝ 2 9 0 
A2  = ⎝ 2
A1  = ⎝ 0 3 7 
⎠
⎠
8 
0
1 2 3 
1 0 1 
1 7 5 
3 0 7 
8
4 

2
6
0

,

, 

6 

and  b1  = 11  and  b2  = 19.  Then  the  variable  X  will  be  the  3 × 3  symmetric 
matrix: 
⎛ 
⎞ 
X  = ⎝ x21  x22  x23  ⎠ , 
x11  x12  x13 
x31  x32  x33 

and  so,  for  example, 

C • X  =  x11  + 2x12  + 3x13  + 2x21  + 9x22  + 0x23  + 3x31  + 0x32  + 7x33 

=  x11  + 4x12  + 6x13  + 9x22  + 0x23  + 7x33 . 

since,  in  particular,  X  is  symmetric.  Therefore  the  SDP  can  be  written 
as: 

SDP  :  minimize 

x11  + 4x12  + 6x13  + 9x22  + 0x23  + 7x33 

s.t. 

x11  + 0x12  + 2x13  + 3x22  + 14x23  + 5x33  = 11 
0x11  + 4x12  + 16x13  + 6x22  + 0x23  + 4x33  = 19 
⎛
⎞ 
X  = ⎝ x21  x22  x23  ⎠ (cid:6) 0. 
x11  x12  x13 
x31  x32  x33 

Notice  that  SDP  looks  remarkably  similar  to  a  linear  program.  However, 
the standard LP  constraint that x must  lie  in the nonnegative orthant  is re-
placed by  the  constraint  that  the  variable X  must  lie  in  the  cone  of positive 
semideﬁnite matrices.  Just as “x ≥ 0”  states  that each of  the n components 

7 

of  x  must  be  nonnegative,  it  may  be  helpful  to  think  of  “X  (cid:6) 0”  as  stating 
that  each  of  the  n  eigenvalues   of  X  must  be  nonnegative. 

It  is  easy  to  see  that  a  linear  program  LP  is  a  special  instance  of  an 
SDP .  To  see  one  way  of  doing  this,  suppose  that  (c, a1 , . . . , am , b1 , . . . , bm ) 
comprise  the  data  for  LP .  Then  deﬁne: 
⎞ 
⎛
⎞	
⎛
⎜  0 
⎜  0 
⎟
⎟
⎟	
⎟
⎜
⎜
. . .  0 
0 
. . . 
c1 
ai1 
.  ⎟ , i = 1, . . . , m,  and  C  = ⎜  .
.  ⎟ . 
Ai  = ⎜	
⎝  . 
⎝  ..	
..  ⎠
..  ⎠
. . .  0 
0	
. . . 
.
. 
. 
. .
. .
.
0
0
. . .  cn 
. . .  ain 

0 
ai2 
.
. 
. 
0 

0 
c2 
.
. 
. 
0 

Then  LP  can  be  written  as: 

SDP  :	 minimize  C • X 
Ai  • X  = bi  , i = 1, . . . , m, 
Xij  = 0,
i = 1, . . . , n,  j  = i + 1, . . . , n, 
X  (cid:6) 0, 

s.t.	

with  the  association  that 

⎛	
⎜  0 
⎜	
x1 
X  = ⎜  .
⎝  .. 
0

⎞ 
⎟
⎟
0 
. . . 
.  ⎟ .
..  ⎠
0 
. . . 
. 
. .
. . .  xn 

0 
x2 
.
. 
. 
0 

Of  course,  in  practice  one  would  never  want  to  convert  an  instance  of  LP 
into  an  instance  of  SDP .  The  above  construction  merely  shows  that  SDP 

8 

includes  linear  programming  as  a  special  case. 

5  Semideﬁnite  Programming  Duality 

The dual problem of SDP  is deﬁned  (or derived  from ﬁrst principles)  to be: 
(cid:1) 
m 
(cid:1) 
i=1 
m 
i=1 
S  (cid:6) 0. 

SDD  :  maximize 

yi bi 

s.t.	

yiAi  + S  = C 

(cid:1) 
One convenient way of thinking about this problem is as follows.  Given mul-
m
tipliers y1 , . . . , ym , the ob jective is to maximize the linear function 
i=1  yi bi . 
The  constraints  of  SDD  state  that  the  matrix  S  deﬁned  as 
(cid:4) 
m 
S  = C − 
yiAi 
i=1 

must  be  positive  semideﬁnite.  That  is, 
(cid:4) 
m 
yiAi  (cid:6) 0. 
C − 
i=1 

We  illustrate  this  construction  with  the  example  presented  earlier.  The 
dual  problem  is: 

9 

SDD  :  maximize  11y1  + 19y2 
⎛ 
⎞
⎛ 
⎞
⎛ 
y1  ⎝ 0 3 7 ⎠ + y2  ⎝ 2 6 0 ⎠ + S  = ⎝ 2
1 0 1 
0 2 8 
1
1 7 5 
8 0 4 
3
S  (cid:6) 0, 

s.t.	

⎞
3 ⎠
0 
7 

2
9
0

which  we  can  rewrite  in  the  following  form: 

SDD  :  maximize 

s.t. 

11y1  + 19y2 
⎛ 
⎜ ⎝  2 − 0y1  − 2y2  9 − 3y1  − 6y2  0 − 7y1  − 0y2 
1 − 1y1  − 0y2  2 − 0y1  − 2y2  3 − 1y1  − 8y2 
3 − 1y1  − 8y2  0 − 7y1  − 0y2  7 − 5y1  − 4y2 

⎞ 
⎟ ⎠ (cid:6) 0. 

It  is  often  easier  to  “see” and  to work with  a  semideﬁnite program when 
it  is presented  in  the  format  of  the  dual SDD ,  since  the  variables  are  the m 
multipliers  y1 , . . . , ym . 

As  in  linear  programming,  we  can  switch  from  one  format  of  SDP  (pri-
mal  or  dual)  to  any  other  format  with  great  ease,  and  there  is  no  loss  of 
generality  in  assuming  a  particular  speciﬁc  format  for  the  primal  or  the 
dual. 

The  following  proposition  states  that  weak  duality  must  hold  for  the 
primal  and  dual  of  SDP : 

10 

(cid:1) 
Proposition  5.1  Given   a   feasible   solution   X  of   SDP  and   a   feasible   solu (cid:173)
(cid:1) 
tion   (y , S )  of   SDD,   the   duality   gap   is   C  • X  − 
=  S  • X  ≥  0. If  
m
i=1  yi bi 
C • X − 
m
i=1  yi bi  = 0,   then   X  and  (y , S )  are   each   optimal   solutions   to   SDP 
and  SDD,   respectively,   and   furthermore,   SX  = 0. 

In  order  to  prove Proposition  5.1,  it will  be  convenient  to work with  the 
trace   of  a matrix,  deﬁned  below: 

(cid:4) 
n 
trace(M ) =  Mj j . 
j=1 

Simple  arithmetic  can  be  used  to  establish  the  following  two  elementary 
identities: 
•  trace(M N ) = trace(N M ) 
•  A • B = trace(AT B ) 

Proof  of  Proposition  5.1.  For  the  ﬁrst  part  of  the  proposition,  we 
must  show  that  if  S  (cid:6)  0 and  X  (cid:6)  0,  then  S • X  ≥  0.  Let  S  = P DP T  and 
X  = QEQT  where P , Q are orthonormal matrices and D , E  are nonnegative 
diagonal  matrices.  We  have: 

S • X  = trace(S T X ) = trace(SX ) = trace(P DP T QEQT ) 

(cid:4) 
n 
= trace(DP T QEQT P ) =  Dj j (P T QEQT P )j j  ≥ 0, 
j=1 

where  the  last  inequality  follows  from  the  fact  that  all Dj j  ≥ 0  and  the  fact 

11 

that  the diagonal of  the  symmetric positive  semideﬁnite matrix P T QEQT P 
must  be  nonnegative. 

To prove the second part of the proposition, suppose that trace(SX ) = 0. 
Then  from  the  above  equalities,  we  have 
(cid:4) 
n 
Dj j (P T QEQT P )j j  = 0. 
j=1 

However,  this  implies  that  for  each  j  = 1, . . . , n,  either  Dj j  =  0  or  the 
(P T QEQT P )j j  =  0.  Furthermore,  the  latter  case  implies  that  the  j th   row 
of  P T QEQT P  is  all  zeros.  Therefore  DP T QEQT P  = 0, and  so  SX  = 
P DP T QEQT  = 0. 
q.e.d. 

Unlike the case of linear programming, we cannot assert that either SDP 
or  SDD  will  attain  their  respective  optima,  and/or  that  there  will  be  no 
duality  gap,  unless  certain  regularity  conditions  hold.  One  such  regularity 
condition  which  ensures  that  strong  duality  will  prevail  is  a  version  of  the 
“Slater  condition”,  summarized  in  the  following  theorem  which  we  will  not 
prove: 

∗
∗
Theorem  5.1   Let  zP  and  zD  denote   the   optimal   objective   function   values  
of   SDP  and  SDD,   respectively.   Suppose   that   there   exists   a   feasible   solution  
X  of   SDP  such   that   X  (cid:7)  0,   and   that   there   exists   a   feasible   solution   ( ˆ
ˆ
ˆ
y , Sˆ)
of   SDD  such   that   Sˆ (cid:7)  0.   Then   both   SDP  and  SDD  attain   their   optimal  
∗
∗
values,   and   zP  = zD . 

12 

6	 Key  Properties  of  Linear  Programming  that  do 
not  extend  to  SDP  

The  following  summarizes  some  of  the  more  important  properties  of  linear 
programming  that  do  not  extend  to  SDP : 

•	 There may  be  a  ﬁnite  or  inﬁnite  duality  gap.  The  primal  and/or  dual 
may  or  may  not  attain  their  optima.  As  noted  above  in  Theorem 
5.1,  both  programs  will  attain  their  common  optimal  value  if  both 
programs have feasible solutions in the interior of the semideﬁnite cone. 
•	 There  is  no  ﬁnite  algorithm  for  solving  SDP .  There  is  a  simplex 
algorithm,  but  it  is  not  a  ﬁnite  algorithm.  There  is  no  direct  analog 
of  a  “basic  feasible  solution”  for  SDP . 
•	 Given rational data, the feasible region may have no rational solutions. 
The  optimal  solution  may  not  have  rational  components  or  rational 
eigenvalues. 
•	 Given  rational data whose binary  encoding  is  size L,  the norms of any 
feasible  and/or  optimal  solutions  may  exceed  22L  (or  worse). 
•	 Given  rational data whose binary  encoding  is  size L,  the norms of any 
feasible  and/or  optimal  solutions  may  be  less  than  2−2L  (or  worse). 

7	 SDP  in  Combinatorial  Optimization 

SDP  has  wide  applicability  in  combinatorial  optimization.  A  number  of 
N P −hard combinatorial optimization problems have convex relaxations that 
are  semideﬁnite  programs.  In  many  instances,  the  SDP  relaxation  is  very 
tight  in practice,  and  in  certain  instances  in particular,  the optimal  solution 
to  the  SDP  relaxation  can  be  converted  to  a  feasible  solution  for  the  origi-
nal  problem  with  provably  good  ob jective  value.  An  example  of  the  use  of 
SDP  in  combinatorial  optimization  is  given  below. 

13 

7.1  An  SDP  Relaxation  of  the   MAX  CUT  Problem 
Let  G  be  an  undirected  graph  with  nodes  N  =  {1, . . . , n},  and  edge  set  E . 
Let  wij  =  wj i  be  the  weight  on  edge  (i, j ),  for  (i, j )  ∈  E .  We  assume  that 
wij  ≥ 0  for  all  (i, j ) ∈ E .  The MAX  CUT  problem  is  to  determine  a  subset 
S  of  the  nodes  N  for  which  the  sum  of  the  weights  of  the  edges  that  cross 
from  S  to  its  complement  S  is maximized  (where  ( S ¯  := N  \ S ) . 
¯

We  can  formulate MAX  CUT  as  an  integer  program  as  follows.  Let  xj  = 1 
for  j  ∈ S  and  xj  = −1 for  j  ∈ S¯.  Then  our  formulation  is: 
(cid:1)  (cid:1) 
n
n 
i=1  j=1 
xj  ∈ {−1, 1},

M AX C U T  :  maximizex 

j  = 1, . . . , n. 

1
4 

wij (1 − xixj ) 

s.t. 

Now  let 

whereby 

Y  = xx T , 

Yij  = xixj 

i = 1, . . . , n,  j  = 1, . . . , n. 

Also  let  W  be  the  matrix  whose  (i, j )th   element  is  wij  for  i  = 1, . . . , n 
and  j  = 1, . . . , n.  Then MAX  CUT  can  be  equivalently  formulated  as: 

14 

M AX C U T  :  maximizeY ,x 

s.t. 

1
4 

(cid:1)  (cid:1) 
n
n 
i=1  j=1 
xj  ∈ {−1, 1},

wij  − W  • Y

j  = 1, . . . , n 

Y  = xxT . 

Notice  in  this  problem  that  the  ﬁrst  set  of  constraints  are  equivalent  to 
Yj j  = 1, j  = 1, . . . , n.  We  therefore  obtain: 
(cid:1)  (cid:1) 
n
n 
i=1  j=1 

M AX C U T  :  maximizeY ,x 

wij  − W  • Y

1
4 

s.t. 

Yj j  = 1,

j  = 1, . . . , n 

Y  = xxT . 

Last  of  all,  notice  that  the  matrix  Y  =  xxT  is  a  symmetric  rank-1  posi-
tive  semideﬁnite  matrix.  If  we  relax  this  condition  by  removing  the  rank-
1  restriction,  we  obtain  the  following  relaxtion  of  MAX  CUT,  which  is  a 
semideﬁnite  program: 
(cid:1)  (cid:1) 
n
n 
i=1  j=1 

RELAX  :  maximizeY 

wij  − W  • Y 

1
4 

s.t. 

Yj j  = 1,
Y  (cid:6) 0. 

j  = 1, . . . , n 

It  is  therefore  easy  to  see  that  RELAX  provides  an  upper  bound  on MAX-
CUT,  i.e., 

15 

M AX C U T  ≤ RELAX. 

As  it  turns  out,  one  can  also  prove  without  too  much  eﬀort  that: 

0.87856  RELAX  ≤ M AX C U T  ≤ RELAX. 

This  is  an  impressive  result,  in  that  it  states  that  the  value  of  the  semideﬁ-
nite  relaxation  is  guaranteed  to  be  no more  than  12%  higher  than  the  value 
of  N P -hard  problem MAX  CUT. 

8  SDP  in  Convex  Optimization 

As  stated  above,  SDP  has  very  wide  applications  in  convex  optimization. 
The types of constraints that can be modeled in the SDP  framework include: 
linear  inequalities,  convex  quadratic  inequalities,  lower  bounds  on  matrix 
norms,  lower  bounds  on  determinants  of  symmetric  positive  semideﬁnite 
matrices,  lower bounds on  the geometric mean of a nonnegative vector, plus 
many  others.  Using  these  and  other  constructions,  the  following  problems 
(among  many  others)  can  be  cast  in  the  form  of  a  semideﬁnite  program: 
linear  programming,  optimizing  a  convex  quadratic  form  sub ject  to  convex 
quadratic  inequality  constraints, minimizing  the  volume  of  an  ellipsoid  that 
covers  a  given  set  of  points  and  ellipsoids,  maximizing  the  volume  of  an 
ellipsoid  that  is  contained  in  a  given  polytope,  plus  a  variety  of  maximum 
eigenvalue  and  minimum  eigenvalue  problems.  In  the  subsections  below  we 
demonstrate  how  some  important  problems  in  convex  optimization  can  be 
re-formulated  as  instances  of  SDP . 

16 

8.1 	 SDP  for  Convex  Quadratically  Constrained  Quadratic 
Programming, Part  I 

A  convex  quadratically  constrained  quadratic  program  is  a  problem  of  the 
form: 

QCQP  :  minimize  xT Q0x + q0  x + c0

T
x

x + ci  ≤ 0  , i = 1, . . . , m, 
s.t. 

xT Qix + qi 
T

where  the  Q0  (cid:6) 0 and  Qi  (cid:6) 0,

i = 1, . . . , m.  This  problem  is  the  same  as: 

QCQP  :  minimize  θ

x, θ

xT Q0x + q0  x + c0  − θ ≤ 0 
T
s.t. 
x + ci  ≤ 0  , i = 1, . . . , m. 
xT Qix + qi 
T

We  can  factor  each  Qi  into 

T Mi 
Qi  = Mi

for  some  matrix Mi .  Then  note  the  equivalence: 
(cid:11)	
(cid:12) 

(cid:6) 0  ⇔  x Qix + qi  x + ci  ≤ 0. 
T 
T

I
xT M T 
i 

Mix 
−ci  − qi
T x 

In  this  way  we  can  write  QCQP  as: 

17 

QCQP  :  minimize  θ

x, θ

(cid:11)	
s.t. 
(cid:11)	

(cid:12) 

(cid:6) 0

(cid:6) 0  , i = 1, . . . , m. 

I
M0x 
T  −c0  − q0  x + θ 
xT M0 
T
(cid:12) 

Mix 
I
T  −ci  − qi
xT Mi
T x 

Notice  in  the  above  formulation  that  the  variables  are  θ  and  x  and  that 
all matrix  coeﬃcients  are  linear  functions  of  θ  and  x. 

8.2 	 SDP  for  Convex  Quadratically  Constrained  Quadratic 
Programming, Part  II 

As  it  turns  out,  there  is  an  alternative  way  to  formulate  QCQP  as  a  semi-
deﬁnite  program.  We  begin  with  the  following  elementary  proposition. 
Proposition  8.1  Given   a   vector   x ∈ (cid:3)k  and   a   matrix   W  ∈ (cid:3)k×k ,   then  
(cid:12) 
(cid:11) 
if  and  only  if  W  (cid:6) xx T . 
(cid:6) 0	

T
1  x
x W 

Using this proposition, it is straightforward to show that QCQP  is equiv-
alent  to  the  following  semi-deﬁnite  program: 

18 

QCQP  :  minimize  θ

x, W, θ

(cid:11) 
s.t. 
(cid:11) 
(cid:11) 

(cid:12)  (cid:11) 
1  T 
• 
2 q0 
Q0 
(cid:12)  (cid:11) 
•

(cid:12) 
T  ≤ 0
1  x
x W 
(cid:12) 

T
1  x
x W 

≤ 0  , i = 1, . . . , m

c0  − θ 
1 
2 q0 

1  T 
ci 
2 qi 
1 
2 qi  Qi 
(cid:12) 

T
1  x
x W 

(cid:6) 0  . 

Notice  in  this  formulation  that  there  are  now  (n+1)(n+2)  variables,  but  that 
2 
the constraints are all linear inequalities as opposed to semi-deﬁnite inequal-
ities. 

8.3  SDP  for  the  Smallest  Circumscrib ed  Ellipsoid  Problem 
A  given matrix R (cid:7) 0 and  a given  point  z  can  be  used  to  deﬁne  an  ellipsoid 
in  (cid:3)n : 

ER,z  := {y  | (y − z )T R(y − z ) ≤ 1}. 

One  can  prove  that  the  volume  of  ER,z  is  proportional  to  det(R−1 ). 

Suppose  we  are  given  a  convex  set X  ∈ (cid:3)n  described  as  the  convex  hull 

19 

of k points c1 , . . . , ck .  We would like to ﬁnd an ellipsoid circumscribing these 
k  points  that  has minimum  volume.  Our  problem  can  be written  in  the  fol-
lowing  form: 

M C P  :  minimize  vol  (ER,z ) 
R, z 
ci  ∈ ER,z ,
s.t. 

i = 1, . . . , k , 

which  is  equivalent  to: 

M C P  :  minimize  − ln(det(R))

R, z

(ci  − z )T R(ci  − z ) ≤ 1,
s.t. 
R (cid:7) 0, 

i = 1, . . . , k 

Now  factor  R  =  M 2  where  M  (cid:7)  0  (that  is,  M  is  a  square  root  of  R), 
and  now M C P  becomes: 

M C P  :  minimize  − ln(det(M 2 ))

M , z

(ci  − z )T M T M (ci  − z ) ≤ 1,
s.t. 
M  (cid:7) 0. 

i = 1, . . . , k , 

Next  notice  the  equivalence: 
(cid:11) 

I 
(M ci  − M z )T 

M ci  − M z 
1 

(cid:12) 

(cid:6) 0  ⇔ 

(ci  − z )T M T M (ci  − z ) ≤ 1 

20 

In  this  way  we  can  write M C P  as:


M C P  :  minimize  −2 ln(det(M )) 
(cid:11) 
M , z 

s.t. 

I 
(M ci  − M z )T 
M  (cid:7) 0. 

(cid:12) 

M ci  − M z 
1 

(cid:6) 0,

i = 1, . . . , k , 

Last  of  all, make  the  substitution  y = M z  to  obtain: 

M C P  :  minimize  −2 ln(det(M ))

(cid:11) 
M , y


s.t. 

I 
(M ci  − y)T 
M  (cid:7) 0. 

(cid:12) 

M ci  − y
1 

(cid:6) 0,

i = 1, . . . , k , 

Notice  that  this  last  program  involves  semideﬁnite  constraints  where  all  of 
the  matrix  coeﬃcients  are  linear  functions  of  the  variables M  and  y . How-
ever,  the  ob jective  function  is not  a  linear  function.  It  is possible  to  convert 
this problem  further  into a genuine  instance of SDP , because there  is a way 
to  convert  constraints  of  the  form 

− ln(det(X )) ≤ θ 

to  a  semideﬁnite  system.  Nevertheless,  this  is  not  necessary,  either  from 
a  theoretical or a practical viewpoint, because  it  turns out  that  the  function 
f (X ) = − ln(det(X )) is extremely well-behaved and is very easy to optimize 
(both  in  theory  and  in  practice). 

21 

Finally,  note  that  after  solving  the  formulation  of  M C P  above,  we  can 
recover  the matrix R  and  the center z  of  the optimal ellipsoid by computing 
R = M 2  and  z = M −1 y . 

8.4  SDP  for   the  Largest  Inscribed  Ellipsoid  Problem 
Recall  that  a  given matrix  R (cid:7) 0  and  a  given  point  z  can  be  used  to  deﬁne 
an  ellipsoid  in  (cid:3)n : 

ER,z  := {x  | (x − z )T R(x − z ) ≤ 1}, 

and  that  the  volume  of  ER,z  is  proportional  to  det(R−1 ). 

Suppose  we  are  given  a  convex  set X  ∈ (cid:3)n  described  as  the  intersection 
of  k  halfspaces  {x  | (ai )T x ≤ bi}, i = 1, . . . , k ,  that  is, 

X  = {x  | Ax ≤ b} 

where  the  ith  row  of  the  matrix  A  consists  of  the  entries  of  the  vector 
ai , i  = 1, . . . , k .  We  would  like  to  ﬁnd  an  ellipsoid  inscribed  in  X  of  maxi-
mum  volume.  Our  problem  can  be  written  in  the  following  form: 

M I P  :  maximize  vol  (ER,z ) 
R, z 
ER,z  ⊂ X, 
s.t. 

22 


which  is  equivalent  to: 


M I P  :  maximize  det(R−1 ) 
R, z 
ER,z  ⊂ {x  | (ai )T x ≤ bi},
s.t. 
R (cid:7) 0, 

i = 1, . . . , k 

which  is  equivalent  to: 

M I P  :  maximize 
R, z

s.t.	

ln(det(R−1 ))

maxx{ai 
x  | (x − z )T R(x − z ) ≤ 1} ≤ bi ,
T
R (cid:7) 0. 

i = 1, . . . , k 

For  a  given  i  = 1, . . . , k ,  the  solution  to  the  optimization  problem  in  the 
ith  constraint  is 

∗ x  = z + (cid:13) 
R−1ai
R−1ai 
Tai 

with  optimal  ob jective  function  value 
(cid:13) 
aT
i 

T ai  z +	

R−1ai  , 

and  so M I P  can  be  rewritten  as: 

23 

M I P  :  maximize 
R, z

s.t. 

ln(det(R−1 ))

(cid:13) 
z +  aT R−1ai  ≤ bi ,
T
ai 
i 
R (cid:7) 0. 

i = 1, . . . , k

Now  factor  R−1  = M 2  where  M  (cid:7)  0  (that  is,  M  is  a  square  root  of  R−1 ), 
and  now M I P  becomes: 

M I P  :  maximize 
M , z

s.t. 

ln(det(M 2 ))

(cid:13) 
M T M ai  ≤ bi ,
T
T
z +  ai 
ai 
M  (cid:7) 0, 

i = 1, . . . , k 

which  we  can  re-write  as: 

M I P  :  maximize  2 ln(det(M ))

M , z

M T M ai  ≤ (bi  − ai 
T
T
s.t. 
ai 
bi  − ai 
z ≥ 0,
T
i = 1, . . . , k 
M  (cid:7) 0. 

z )2 ,

i = 1, . . . , k 

Next  notice  the  equivalence: 
(cid:11) 

(bi  − ai 
T 
z )I 
(M ai )T 

M ai 
(bi  − ai 
T

z ) 

(cid:12)

(cid:6) 0  ⇔ 

⎧
⎪  ai
⎨
T M T M ai  ≤ (bi  − ai 
T
⎪ 
⎩ 
and
z ≥ 0. 
bi  − ai 
T 

⎫ 
z )2  ⎪
⎬ 
⎪ 
⎭ 

24 

In  this  way  we  can  write M I P  as:


M I P  :  minimize  −2 ln(det(M )) 
(cid:11) 
M , z 
(bi  − aT 
z )I 
i 
(M ai )T 
M  (cid:7) 0. 

s.t. 

(cid:12) 

M ai 
(bi  − aT 
z ) 
i 

(cid:6) 0,

i = 1, . . . , k , 

Notice  that  this  last  program  involves  semideﬁnite  constraints  where  all  of 
the  matrix  coeﬃcients  are  linear  functions  of  the  variables M  and  z . How-
ever,  the  ob jective  function  is  not  a  linear  function.  It  is  possible,  but  not 
necessary in practice, to convert this problem further into a genuine instance 
of  SDP ,  because  there  is  a  way  to  convert  constraints  of  the  form 

− ln(det(X )) ≤ θ 

to  a  semideﬁnite  system.  Such  a  conversion  is  not  necessary,  either  from 
a  theoretical or a practical viewpoint, because  it  turns out  that  the  function 
f (X ) = − ln(det(X )) is extremely well-behaved and is very easy to optimize 
(both  in  theory  and  in  practice). 

Finally,  note  that  after  solving  the  formulation  of  M I P  above,  we  can 
recover  the  matrix  R  of  the  optimal  ellipsoid  by  computing 
R = M −2  . 

25 

8.5  SDP  for  Eigenvalue  Optimization 

There  are  many  types  of  eigenvalue  optimization  problems  that  can  be  for-
mualated  as  SDP s.  A  typical  eigenvalue  optimization  problem  is  to  create 
a  matrix 

(cid:4) 
k 
S  := B − 
wiAi 
i=1 

given symmetric data matrices B and Ai , i = 1, . . . , k , using weights w1 , . . . , wk , 
in  such  a  way  to  minimize  the  diﬀerence  between  the  largest  and  smallest 
eigenvalue  of  S .  This  problem  can  be  written  down  as: 

EOP  :  minimize  λmax  (S ) − λmin (S ) 
k (cid:1) 
w, S 
S  = B −  wiAi , 
i=1 

s.t. 

where  λmin (S ) and  λmax  (S )  denote  the  smallest  and  the  largest  eigenvalue 
of S ,  respectively.  We  now  show  how  to  convert  this  problem  into  an SDP . 

Recall  that  S  can  be  factored  into  S  = QDQT  where  Q  is  an  orthonor-
mal  matrix  (i.e.,  Q−1  =  QT ) and  D  is  a  diagonal  matrix  consisting  of  the 
eigenvalues  of  S .  The  conditions: 

λI  (cid:11) S  (cid:11) µI 

can  be  rewritten  as: 

Q(λI )QT  (cid:11) QDQT  (cid:11) Q(µI )QT . 

26 

After premultiplying  the above QT  and postmultiplying Q,  these  conditions 
become: 

λI  (cid:11) D (cid:11) µI 

which  are  equivalent  to: 

λ ≤ λmin (S ) and  λmax  (S ) ≤ µ. 

Therefore  EOP  can  be  written  as: 

EOP  :  minimize  µ − λ 
w, S, µ, λ 

s.t. 

k (cid:1) 
S  = B −  wiAi 
i=1 
λI  (cid:11) S  (cid:11) µI . 

This  last  problem  is  a  semideﬁnite  program. 

Using  constructs  such  as  those  shown  above,  very  many  other  types  of 
eigenvalue  optimization  problems  can  be  formulated  as  SDP s. 

27 

9  SDP  in  Control  Theory 

A variety of control and system problems can be cast and solved as instances 
of  SDP .  However,  this  topic  is  beyond  the  scope  of  these  notes. 

10 

Interior-point  Methods  for  SDP 

At  the  heart  of  an  interior-point  method  is  a  barrier  function  that  exerts 
a  repelling  force  from  the  boundary  of  the  feasible  region.  For  SDP , we 
need  a  barrier  function  whose  values  approach  +∞  as  points  X  approach 
the  boundary  of  the  semideﬁnite  cone  S n 
+ . 

Let  X  ∈  S n 
+ .  Then  X  will  have  n  eigenvalues,  say  λ1 (X ), . . . , λn (X ) 
(possibly  counting  multiplicities).  We  can  characterize  the  interior  of  the 
semideﬁnite  cone  as  follows: 

intS n  = {X  ∈ S n  |  λ1 (X ) > 0, . . . , λn (X ) > 0}.
+ 

A  natural  barrier  function  to  use  to  repel  X  from  the  boundary  of  S n 
+ 
then  is 

(cid:4) 
(cid:21) 
n
n 
ln(λi (X )) = − ln( 
− 
j=1 
j=1 

λi (X )) = − ln(det(X )). 

Consider  the  logarithmic  barrier  problem  BSDP (θ)  parameterized  by 
the  positive  barrier  parameter  θ : 

28 

BSDP (θ)  :  minimize  C • X − θ ln(det(X )) 
Ai  • X  = bi  , i = 1, . . . , m, 
X  (cid:7) 0. 

s.t.	

Let  fθ (X )  denote  the  ob jective  function  of  BSDP (θ).  Then  it  is  not  too 
diﬃcult  to  derive: 

∇fθ (X ) = C − θX −1  ,	

and  so  the  Karush-Kuhn-Tucker  conditions  for  BSDP (θ)  are: 
⎧ ⎪  Ai  • X  = bi  , i = 1, . . . , m, 
⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨  X  (cid:7) 0,	
⎪ ⎪ ⎪ ⎪ 
⎪ ⎪ ⎪ ⎩  C − θX −1  = 
(cid:1) 
m 
i=1 

yiAi . 

(1) 

(2) 

Because  X  is  symmetric,  we  can  factorize  X  into  X  =  LLT .  We  then 
can  deﬁne 

which  implies 

S  = θX −1  = θL−T L−1  , 

1 LT SL = I ,
θ 

29 

and  we  can  rewrite  the  Karush-Kuhn-Tucker  conditions  as: 
⎧ ⎪  Ai  • X  = bi  , i = 1, . . . , m, 
⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪  X  (cid:7) 0, X  = LLT 
⎪ ⎨ 
(cid:1) 
m ⎪ ⎪ ⎪ 
⎪ ⎪ ⎪  i=1 
⎪ ⎪ ⎪ ⎪ ⎪ ⎩  I −  1 LT SL = 0.
yiAi  + S  = C 
θ 

(3)

From the equations of (3)  it  follows that  if (X, y , S )  is a solution of (3),  then 
X  is  feasible  for  SDP , (y , S ) is feasible for  SDD ,  and  the  resulting  duality 
gap  is 

S • X  = 

(cid:4) (cid:4) 
n
n
Sij Xij  =
i=1  j=1 

(cid:4) 
n 
(SX )j j  = 
j=1 

(cid:4) 
n 
θ = nθ . 
j=1 

This  suggests  that  we  try  solving  BSDP (θ)  for  a  variety  of  values  of  θ 
as  θ → 0. 

However, we cannot usually solve (3) exactly, because the fourth equation 
group is not  linear in the variables.  We will instead deﬁne a “β -approximate 
solution”  of  the  Karush-Kuhn-Tucker  conditions  (3).  Before  doing  so,  we 
(cid:22) 
introduce  the  following  norm  on matrices,  called  the  Frobenius  norm: 
(cid:23)  n  n
(cid:23)(cid:4) (cid:4) 
(cid:15)M (cid:15)  :=  M  • M  = (cid:24) 
√ 
M 2 
ij 
i=1  j=1 

.

For some important properties of the Frobenius norm, see the last subsection 

30 

of  this  section.  A  “β -approximate  solution”  of  BSDP (θ) is deﬁned as any 
solution  (X, y , S ) of  ⎧ ⎪  Ai  • X  = bi  , i = 1, . . . , m, 
⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪  X  (cid:7) 0, X  = LLT 
⎪ ⎨ 
(cid:1) 
m ⎪ ⎪ ⎪ 
⎪ ⎪ ⎪  i=1 
⎪ ⎪ ⎪ ⎪ ⎪ ⎩  (cid:15)I − 1 LT SL(cid:15) ≤ β .
yiAi  + S  = C 
θ 

(4)

Lemma   10.1   If  ( ¯  y , S )  is   a   β -approximate   solution   of   BSDP (θ)  and 
X , ¯ ¯
¯ 
y , S¯)  is   feasible   for   SDD, and  the  
β <  1,   then   X  is   feasible   for   SDP ,  ( ¯
duality   gap   satisﬁes:  
(cid:4) 
m 
yi bi  = X  • S ¯ ≤ nθ(1 + β ). 
nθ(1 − β ) ≤ C • X − 
¯ 
i=1 

(5) 

Proof:   Primal  feasibility  is  obvious.  To  prove  dual  feasibility,  we  need 
to  show  that  S ¯ (cid:6) 0.  To  see  this,  deﬁne 

R = I − 
¯1 LT  ¯ ¯ 
SL 
θ 
and  note  that  (cid:15)R(cid:15) ≤ β < 1.  Rearranging  (6),  we  obtain 
L−1  (cid:7) 0
S  = θL−T (I − R) ¯ 
¯
¯ 
because (cid:15)R(cid:15) < 1  implies that I − R (cid:7) 0.  We also have X • S  = trace( ¯ 
X S¯) =
¯ 
¯
LLT S¯) = trace( ¯  SL) =  θtrace(I − R) =  θ(n − trace(R)).  However, 
trace( ¯ ¯ 
LT  ¯ ¯ 
√ 
|trace(R)| ≤  n(cid:15)R(cid:15) ≤nβ ,  whereby  we  obtain 

(6)

nθ(1 − β ) ≤ X  • S ¯ ≤ nθ(1 + β ). 
¯ 

31 

q.e.d. 

10.1  The  Algorithm 

Based  on  the  analysis  just  presented,  we  are  motivated  to  develop  the  fol-
lowing  algorithm: 

Step  0  .  Initialization.  Data  is  (X 0 , y0 , S 0 , θ0 ).  k  =  0.  Assume  that 
(X 0 , y0 , S 0 ) is a β -approximate solution of BSDP (θ0 ) for some known value 
of  β  that  satisﬁes  β < 1. 

X , ¯ ¯
Step  1.  Set  Current  values.  ( ¯  y , S ) = (X k , yk , S k ),  θ = θk . 
(cid:1)  =  αθ  for  some  α  ∈  (0, 1).  In  fact,  it  will  be 
Step  2.  Shrink  θ.  Set  θ 
appropriate  to  set 

√ 
β − β 
α = 1 − √  √ 
β +  n 

Step  3.   Compute  Newton  Direction  and  Multipliers.  Compute 
(cid:1) 
(cid:1)  for  BSDP (θ  ) at  X  =  X  by  factoring  X  =  L ¯  and 
¯ 
¯
¯ LT
the  Newton  step  D 
⎧ ⎪ 
solving  the  following  system  of  equations  in  the  variables  (D , y): 
(cid:1) 
⎪  C − θ 
⎨ 
(cid:1) X −1  + θ 
(cid:1) X −1DX −1  = 
m 
¯
¯ 
¯ 
⎪ ⎪ ⎩  Ai  • D = 0,
i=1 
i = 1, . . . , m. 
(cid:1) 
(cid:1) , y  ). 
Denote  the  solution  to  this  system  by  (D 

yiAi

(7) 

Step  4.  Update  All  Values. 

(cid:1) 
(cid:1) 
¯ 
X  = X + D 

32 

(cid:4) 
m 
S  = C − 
(cid:1) 
(cid:1) 
yi Ai 
i=1 

(cid:1) 
(cid:1) 
(cid:1) , y  , S  ). 
Step  5.   Reset  Counter  and  Continue.  (X k+1 , yk+1 , S k+1 ) = (X 
(cid:1) .  k ← k + 1.  Go  to  Step  1. 
θk+1  = θ 

Figure  1  shows  a  picture  of  the  algorithm: 

θ = 0 

θ = 1/10 

θ = 10 

θ = 100 

Figure  1:  A  conceptual  picture  of  the  interior-point  algorithm. 

Some  of  the  unresolved  issues  regarding  this  algorithm  include: 
•  how  to  set  the  fractional  decrease  parameter  α 
•	 the  derivation  of  the  Newton  step D 
(cid:1)  and  the  multipliers  y


(cid:1) 

33 


•	 whether or not successive iterative values (X k , yk , S k ) are β -approximate 
solutions  to  BSDP (θk ),  and 
•	 how  to  get  the  method  started  in  the  ﬁrst  place. 

10.2	 The  Newton  Step 

¯
Suppose  that X  is  a  feasible  solution  to  BSDP (θ): 

BSDP (θ)  :  minimize  C • X − θ ln(det(X )) 
Ai  • X  = bi  , i = 1, . . . , m, 
X  (cid:7) 0. 

s.t.	

Let  us  denote  the  ob jective  function  of  BSDP (θ) by  fθ (X ),  i.e., 

fθ (X ) = C • X − θ ln(det(X )). 

Then  we  can  derive: 

X ) = C − θ  ¯ 
∇fθ ( ¯ 
X −1

¯
and  the  quadratic  approximation  of  BSDP (θ) at  X  =  X  can  be  derived 
as: 
minimize  fθ ( ¯ X ) + (C − X −1 ) • (X − X ) +  1 θX −1 (X − X ) • X −1 (X − X )
¯
¯
¯
¯
¯
¯ 
2
X 

s.t.	

Ai  • X  = bi ,

i = 1, . . . , m. 

34 

Letting D = X − X ,  this  is  equivalent  to: 
¯
minimize  (C − θX −1 ) • D +  1 θX −1D • X −1D
¯
¯
¯ 
2
D 

s.t. 

Ai  • D = 0,

i = 1, . . . , m. 

The  solution  to  this  program  will  be  the  Newton  direction.  The  Karush-
Kuhn-Tucker  conditions  for  this  program  are  necessary  and  suﬃcient,  and 
are: 
⎧ ⎪ 
(cid:1) 
⎪  C − θX −1  + θX −1DX −1  = 
⎨ 
m 
¯ 
¯
¯ 
⎪ ⎪ ⎩  Ai  • D = 0,
i=1 
(cid:1)  and  y  denote  the 
(cid:1) 
These  equations  are  called  the Normal   Equations. Let D 
solution to the Normal Equations.  Note in particular from the ﬁrst equation 
(cid:1) 
(cid:1) 
(cid:1) , y  )  is  the  (unique)  so-
in  (8)  that D  must  be  symmetric.  Suppose  that  (D 
lution  of  the  Normal  Equations  (8).  We  obtain  the  new  value  of  the  primal 
variable X  by  taking  the  Newton  step,  i.e., 

i = 1, . . . , m. 

yiAi

(8) 

(cid:1) 
(cid:1) 
¯ 
X  = X + D . 

(cid:1) 
We  can  produce  new  values  of  the  dual  variables  (y , S )  by  setting  the  new 
(cid:1)  and  by  setting  S  =  C  − 
m 
(cid:1) 
(cid:1) 
y Ai .  Using  (8),  then,  we 
value  of  y  to  be  y 
i=1 
have  that 

S  = θX −1  − θX −1D 
(cid:1) X −1  . 
(cid:1) 
¯ 
¯
¯ 

(9) 

35 

We  have  the  following  very  powerful  convergence  theorem  which  demon-
strates  the  quadratic  convergence  of  Newton’s  method  for  this  problem, 
with an explicit guarantee of the range in which quadratic convergence takes 
place. 

Theorem  10.1   (Explicit  Quadratic  Convergence  of  Newton’s   Method).  
Suppose   that   ( ¯  y , S )  is   a   β -approximate   solution   of   BSDP (θ)  and  β < 1. 
X , ¯ ¯
(cid:1) , y  )  be   the   solution   to   the   Normal   Equations   (8),   and   let  
(cid:1) 
Let  (D 

and 

(cid:1) 
(cid:1) 
¯ 
X  = X + D 
S  = θX −1  − θX −1D 
X −1
(cid:1) 
(cid:1) 
¯ 
¯
¯
. 
(cid:1) , y  , S  )  is   a   β 2 -approximate   solution   of   BSDP (θ). 
(cid:1) 
(cid:1) 
Then   (X 

¯
Proof:   Our  current  point X  satisﬁes: 
Ai  • X  = bi , i = 1, . . . , m, X  = L ¯  (cid:7) 0 
¯ LT
¯
¯ 
(cid:4) 
m 
y¯iAi  + S ¯ = C 
i=1 
(cid:15)I − 
SL(cid:15) ≤ β < 1. 
¯1 LT  ¯ ¯
θ 
(cid:1)  and  multipliers  y  satisfy: 
(cid:1) 
Furthermore  the  Newton  direction D 
Ai  • D  = 0, i = 1, . . . , m 
(cid:1) (cid:4) 
m 
(cid:1) 
(cid:1) 
yi Ai  + S  = C 
i=1 
L−1D 
L−T ) ¯ 
(cid:1) 
(cid:1) 
(cid:1)  ¯ 
¯
¯ 
X  = X + D  = L(I + ¯ 
LT
S  = θX −1  − θX −1D 
(cid:1)  ¯ = θL−T (I − ¯ 
L−1
L−T ) ¯ 
X −1 
L−1D 
(cid:1) 
(cid:1)  ¯ 
¯ 
¯
¯ 
. 
We  will  ﬁrst  show  that  (cid:15)L−1D L−T (cid:15) ≤  β .  It  turns  out  that  this  is  the 
(cid:1)  ¯ 
¯
crucial  fact  from  which  everything  will  follow  nicely.  To  prove  this,  note 
(cid:4) 
(cid:4) 
(cid:4) 
that 
m 
m
m 
yi Ai  + θL−T (I − ¯ 
L−1 
L−T ) ¯ 
L−1D 
(cid:1)  ¯ 
(cid:1) 
(cid:1) 
(cid:1)
y¯iAi  + S ¯ = C  = 
¯ 
yi Ai  + S  = 
i=1

i=1 
i=1 

.


36 

(cid:1)  yields: 
Taking  the  inner  product  with D 
S • D  = θL−T  ¯ 
• D  − θL−T  ¯ 
(cid:1)  ¯  L−1
¯  L−1D 
¯  L−1 
L−T  ¯ 
(cid:1) 
(cid:1) 
¯

• D , 
(cid:1) 

which  we  can  rewrite  as: 
L−T  − θL−1D 
(cid:1)  ¯ = θI  • L−1D 
¯  SL • L−1D 
L−T 
L−T 
(cid:1)  ¯ 
(cid:1)  ¯
¯ 
¯ 
LT  ¯ ¯  ¯ 
(cid:3) 
(cid:2) 
which  we  ﬁnally  rewrite  as: 
L−T (cid:15)2  =  I −  ¯  SL  • L−1D 
(cid:15)L−1D 
1 LT  ¯ ¯ 
L−T
(cid:1)  ¯
(cid:1)  ¯ 
¯ 
¯ 
θ 

. 

• L−1D 
L−T , 
(cid:1)  ¯
¯ 

Invoking  the  Cauchy-Schwartz  inequality  we  obtain: 
¯  SL(cid:15)(cid:15)L−1D L−T (cid:15) ≤ β (cid:15)L−1D 
L−T (cid:15)2  ≤ (cid:15)I − 
L−T (cid:15),
(cid:15)L−1D 
1 LT  ¯ ¯  ¯ 
(cid:1)  ¯
(cid:1)  ¯
(cid:1)  ¯
¯ 
¯ 
θ 
from  which  we  see  that  (cid:15)L−1D L−T (cid:15) ≤β . 
(cid:1)  ¯ 
¯

It  therefore  follows  that 
LT  (cid:7) 0
L−T ) ¯ 
L−1D 
(cid:1)  ¯
(cid:1) 
¯ 
X  = L(I + ¯ 

and 

L−1  (cid:7) 0,
S  = θL−T (I − ¯ 
L−T ) ¯

L−1D 
(cid:1)  ¯ 
(cid:1) 
¯ 
since  (cid:15)L−1D L−T (cid:15) ≤β <  1,  which  guarantees  that  I ± ¯ 
L−T  (cid:7) 0. 
L−1D 
(cid:1)  ¯
(cid:1)  ¯ 
¯


Next,  factorize 

I + L−1D 
L−T
(cid:1)  ¯ = M 2  , 
¯ 
(where M  = M T )  and  note  that 
(cid:1) 
(cid:1) 
(cid:1) (L  )T 
¯ 
¯
X  = LM M LT  = L 
(cid:1)  = LM .  Then  note  that 
¯
where  we  deﬁne  L 
(cid:1) )T S (cid:2)L  = I −  M LT S  LM 
I − 
1 
1
(cid:1) 
(cid:1)  ¯ 
¯ 
(L 
θ 
θ
1 M LT (θL−T (I − ¯ 
L−T ) ¯  LM  = I − M (I − ¯ 
L−T )M 
L−1D 
L−1 ) ¯ 
L−1D 
(cid:1)  ¯ 
(cid:1)  ¯
¯
¯ 
θ 

= I − 

37 

L−T )M  = I −M M+M (M M−I )M  = (I −M M)(I −M M) 
= I −M M+M ( ¯ 
L−1D 
(cid:1)  ¯ 
L−1D 
L−T )( ¯ 
L−1D 
L−T ). 
(cid:1)  ¯
(cid:1)  ¯ 
= ( ¯ 
From  this  we  next  have: 
(cid:15)I − 
L−T )(cid:15) ≤ (cid:15)( ¯ 
(L  )T S (cid:2)L (cid:15) = (cid:15)( ¯ 
L−T )(cid:15)2  ≤ β 2  . 
1
L−1D 
L−1D 
L−1D 
L−T )( ¯ 
(cid:1)  ¯
(cid:1)  ¯ 
(cid:1) 
(cid:1) 
(cid:1)  ¯ 
θ 
(cid:1) 
(cid:1) 
(cid:1) , y  , S  ) is a  β 2 -approximate  solution  of  BSDP (θ). 
This  shows  that  (X 
q.e.d. 

10.3  Complexity  Analysis  of  the  Algorithm 

Theorem  10.2   (Relaxation  Theorem).  Suppose   that   ( ¯  y , S )  is   a   β -
X , ¯ ¯
approximate   solution   of   BSDP (θ)  and  β < 1. Let  
√ 
β − β 
α = 1 − √  √ 
β +  n 
√ 
(cid:1) 
(cid:1) 
and   let   θ  = αθ.   Then   ( ¯  y , S )  is   a  
X , ¯ ¯
β -approximate   solution   of   BSDP (θ  ). 
(cid:1) 
Proof:   The  triplet  ( ¯  y , S )  satisﬁes  Ai  • X  =  bi , i = 1, . . . , m, X  (cid:7) 0,  and 
¯
¯ 
X , ¯ ¯
m 
y¯iAi  + S ¯ = C ,  and  so  it  remains  to  show  that 
(cid:25) 
(cid:25) (cid:25)  (cid:26) 
(cid:25) 1 LT  ¯ ¯
i=1 
(cid:25)  ¯  SL − I (cid:25) ≤  β ,
(cid:25) θ (cid:1) 
(cid:25) 
(cid:25) 
(cid:25) 
(cid:25) 
(cid:25) 
(cid:25) 
(cid:2) 
(cid:3)  (cid:2) 
¯ ¯
¯ 
(cid:25) 
(cid:25) 1 LT  ¯ ¯ 
(cid:25)  1  LT  ¯ ¯ 
(cid:25) 1
(cid:25) 
where  X  = LLT . We  have 
(cid:25) 
¯  SL − I (cid:25) = (cid:25) 
(cid:25) 
(cid:25)
(cid:25) αθ 
(cid:25)
(cid:25) θ (cid:1) 
(cid:25) α 
(cid:25)
¯  SL − I  −  1 − 
¯  SL − I 
1 LT  ¯ ¯
= 
(cid:2)  (cid:3) (cid:25) 
(cid:25) 
(cid:27) 
(cid:27) 
1  (cid:25) 1 LT  ¯ ¯
(cid:27) 1 − α (cid:27)
(cid:25)
θ
(cid:25)  ¯  SL − I (cid:25) + (cid:27)
(cid:27) (cid:15)I (cid:15) 
(cid:25) θ 
(cid:27)  α 
(cid:25)
(cid:27)
≤ 
α (cid:2)
(cid:3) 
√ 
β +  n  √ 
1 − α  √ 
−  n 
≤  + 
β 
(cid:26)  √
√  (cid:26) 
n  = 
α 
α
α 
=  β +  n −  n =  β . 

(cid:25) 
(cid:3) 
(cid:25)
(cid:25) 
(cid:25)
1 
I 
α 

38 

q.e.d. 

Theorem  10.3   (Convergence  Theorem).  Suppose   that   (X 0 , y0 , S 0 )  is   a  
β -approximate   solution   of   BSDP (θ0 ) and  β < 1.   Then   for   al l   k = 1, 2, 3, ..., 
(X k , yk , S k )  is   a   β -approximate   solution   of   BSDP (θk ). 

Proof:   By induction, suppose that the theorem is true for iterates 0, 1, 2, ..., k . 

Then  (X k , yk , S k ) is a  β -approximate  solution  of  BSDP (θk ). 
√ 
From  the  Relaxation  Theorem,  (X k , yk , S k ) is a  β -approximate  solution 
of  BSDP (θk+1 )  where  θk+1  = αθk . 

From the Quadratic Convergence Theorem, (X k+1 , yk+1 , S k+1 ) is a β -approximate 
solution  of  BSDP (θk+1 ). 

Therefore,  by  induction,  the  theorem  is  true  for  all  values  of  k . 
q.e.d. 

Figure  2  shows  a  better  picture  of  the  algorithm: 

Theorem  10.4   (Complexity  Theorem).  Suppose   that   (X 0 , y0 , S 0 )  is   a  
1
β  =  4 -approximate   solution   of   BSDP (θ0 ).   In   order   to   obtain   primal   and  
dual   feasible   solutions   (X k , yk , S k ) with   a   duality   gap   of   at   most   ,  one  needs 
to   run   the   algorithm   for   at   most  
(cid:12)(cid:29) 
(cid:28) 
(cid:11) 
√ 
1.25 X 0  • S 0 
k = 6  n ln 
0.75 
 

iterations.  

39 

θ = 80 

x^ 

x~ 

θ = 90 

_ 
x

θ = 100 

Figure  2:  Another  picture  of  the  interior-point  algorithm. 

40 


Proof:   Let  k  be  as  deﬁned  above.  Note  that 
√ 
= 1 − (cid:30) 
4  (cid:31)  = 1 −
1  − 1 
−√ 
α = 1 − √ 
β 
β
√ 
2 
+ 
β
n 
1
+ 
n
2 
(cid:3)k
(cid:2)
θk  ≤  1 − √ 
1 
6 n 

Therefore 

θ0  .

√ 
1
2 + 4
n 

≤ 1 −

√ 
1 
n

. 

6

(1.25nθ0 )

(cid:3) 

(cid:2)
(cid:3)k
This  implies  that 
m (cid:4) 
i  = X k  • S k  ≤ θk n(1 + β ) ≤  1 − √ 
C • X k  − 
1 
biy k
(cid:12) 
(cid:11) 
6 n 
(cid:2)
(cid:3)k
i=1 
X 0  • S 0 
≤  1 − √ 
1 
(1.25n)
, 
0.75n
6 n 
(cid:12) 
(cid:11) 
(cid:2)
(cid:3)
(cid:2) 
from  (5).  Taking  logarithms,  we  obtain 
(cid:4) 
m 
k  ≤ k ln  1 − √  + ln 
ln  C • X k  − 
X 0  • S 0 
1.25
1
biyi
(cid:2) 
(cid:3) 
0.75 
6 n 
i=1 
−k 
≤ √  + ln 
X 0  • S 0 
1.25 
(cid:12) 
(cid:11) 
(cid:2) 
(cid:3) 
6 n 
0.75 
1.25 X 0  • S 0 
≤ − ln 
X 0  • S 0  = ln().
1.25 
+ ln 
(cid:1) 
0.75 
0.75 
 
Therefore  C • X k  − 
≤ . 
m 
i=1 
q.e.d. 

biyk
i 

10.4  How  to  Start  the  Method  from  a  Strictly  Feasible  Point 

The algorithm and its performance relies on having a starting point (X 0 , y0 , S 0 ) 
that  is  a  β -approximate  solution  of  the  problem BSDP (θ0 ).  In  this  subsec-
tion,  we  show  how  to  obtain  such  a  starting  point,  given  a  positive  deﬁnite 
feasible  solution X 0  of  SDP . 

41 

We  suppose  that  we  are  given  a  target  value  θ0  of  the  barrier  param-
eter,  and  we  are  given  X  =  X 0  that  is  feasible  for  BSDP (θ0 ),  that  is, 
Ai  • X 0  =  bi , i = 1, . . . , m, and  X 0  (cid:7) 0.  We  will  attempt  to  approximately 
solve  BSDP (θ0 ) starting at  X  =  X 0 ,  using  the  Newton  direction  at  each 
iteration.  The  formal  statement  of  the  algorithm  is  as  follows: 

Step  0  .  Initialization.  Data  is (X 0 , θ0 ).  k = 0.  Assume that X 0  satisﬁes 
Ai  • X 0  = bi , i = 1, . . . , m, X 0  (cid:7) 0. 
¯ LT
¯
Step  1.  Set  Current  values.  X  = X k .  Factor  X  = L ¯ . 
¯ 

Step  2.   Compute  Newton  Direction  and  Multipliers.  Compute  the 
(cid:1)  for  BSDP (θ0 ) at  X  =  X  by  solving  the  following  system 
¯
Newton  step  D 
of  equations  in  the  variables  (D , y): 
⎧ ⎪ 
(cid:1) 
⎪  C − θ0  ¯ 
⎨ 
X −1  + θ0  ¯
X −1DX −1  = 
m 
¯ 
⎪ ⎪ ⎩  Ai  • D = 0,
i=1 

i = 1, . . . , m. 

yiAi

(10) 

(cid:1) 
(cid:1) , y  ).  Set 
Denote  the  solution  to  this  system  by  (D 
(cid:4) 
m 
S  = C − 
(cid:1) 
(cid:1) 
yiAi . 
i=1 

Step  3.  Test  the  Current  Point.  If  (cid:15)L−1D L−T (cid:15) ≤  1 
(cid:1)  ¯ 
¯
4 ,  stop.  In  this 
¯ 
case, X  is  a  1 
4 -approximate  solution  of BSDP (θ0 ),  along with  the  dual  val-
(cid:1) 
(cid:1) , S  ). 
ues  (y 
Step  4.  Update  Primal   Point.  

(cid:1) 
(cid:1) 
¯ 
X  = X + αD 

42 

where 

α = 

0.2 
(cid:15)L−1D (cid:1)  ¯ 
L−T (cid:15)
¯ 
(cid:1) 
Alternatively,  α  can  be  computed  by  a  line-search  of  fθ0 ( ¯ X + αD  ). 
←  X 
(cid:1) ,  k  ←  k  + 1. 

Step  5.  Reset  Counter  and  Continue. 
Go  to  Step  1. 

X k+1

.

The  following  proposition  validates  Step  3  of  the  algorithm: 

(cid:1) , y  )  is   the   solution   of   the   Normal   equa (cid:173)
(cid:1) 
Proposition   10.1  Suppose   that   (D 
¯
tions   (10)   for   the   point   X  for   the   given   value   θ0  of   the   barrier   parameter,  
and   that  
¯(cid:15)L−1D L−T (cid:15) ≤ 
1
(cid:1)  ¯ 
.

4 
¯

Then   X  is   a   1 
4 -approximate   solution   of   BSDP (θ0 ). 
(cid:1) 
m 
Proof:   We must  exhibit  values  (y , S )  that  satisfy 
(cid:25) (cid:25) (cid:25) (cid:25) ≤ 
(cid:25) (cid:25) (cid:25) (cid:25) I − 
i=1 
¯ LT S ¯ L 
(cid:1) 
(cid:1) 
(cid:1) 
Let (D 
) solve the Normal equations (10), and let S 
, y 
we  have  from  (10)  that 
L = I − θ0 
1  LT (θ0 ( ¯  L−1−L−T  ¯ 
I −  LT S 
1 
L = L−1D 
L−T  ¯ 
L−T  ¯
¯  L−1D 
(cid:1)  ¯  L−1 )) ¯ 
L−T ,
(cid:1)  ¯ 
(cid:1)  ¯
¯ 
¯ 
¯ 
θ0 
whereby 

m (cid:1) 
= C − 
i=1 

yiAi  + S  = C  and 

(cid:1)
iAi .  Then 
y 

1 
. 
4 

1 
θ0 

L(cid:15) = (cid:15)L−1D L−T (cid:15) ≤
(cid:15)I −  LT S 
1
(cid:1)  ¯ 
(cid:1)  ¯ 
¯ 
¯
θ0 

1
. 
4 

q.e.d. 

The  next  proposition  shows  that  whenever  the  algorithm  proceeds  to 
Step  4,  then  the  ob jective  function  fθ0 (X )  decreases  by  at  least  0.025θ0 : 

43 

Proposition  10.2  Suppose   that   X  satisﬁes   Ai  • X  =  bi , i = 1, . . . , m, and  
¯
¯
¯ X  (cid:7)  0.   Suppose   that   (D 
(cid:1) 
(cid:1) , y  )  is   the   solution   of   the   Normal   equations   (10)  
¯
for   the   point   X  for   a   given   value   θ0  of   the   barrier   parameter,   and   that  
¯(cid:15)L−1D L−T (cid:15) > 
1
(cid:1)  ¯ 
. 
4 
Then   for   al l   γ  ∈ [0, 1),  
(cid:11) 
(cid:2) 
(cid:3) 
D  ≤ fθ0 ( ¯ 
X ) + θ0  −γ (cid:15)L−1D L−T (cid:15) + 
γ 
(cid:1) 
(cid:1)  ¯ 
¯ 
¯ 
fθ0  X +  (cid:15)L−1D (cid:1)  ¯ 
L−T (cid:15) 
¯ 
(cid:3) 
(cid:2) 
D  ≤ fθ0 ( ¯ 
X ) − 0.025θ0  . 
0.2 
(cid:1)
¯ 
fθ0  X +  (cid:15)L−1D (cid:1)  ¯ 
L−T (cid:15) 
¯ 

(cid:12) 
γ 2 
2(1 − γ ) 
.

In   particular,  

(11)

In order to prove this proposition, we will need two powerful  facts about the 
logarithm  function: 

Fact  1.  Suppose  that  |x| ≤ δ < 1.  Then 
ln(1 + x) ≥ x − 

2x
2(1 − δ) 
.

Proof:   We  have: 
ln(1 + x) =  x − x2  +  x − x + . . .
4
3
2
3
4 
≥  x − |x|2  − |x|3  − |x|4  − . . .
2
3
4 
≥  x − |x|2  − |x|3  − |x|4  − . . .
2   
! 
2
2
2 
1 + |x| + |x|2  + |x|3  + . . .
=  x − x
2 
=  x − 
2 
x
2(1−|x|) 
≥  x − 2(1−δ) . 
2x

44 

q.e.d. 

Fact  2.  Suppose  that  R ∈ S n  and  that  (cid:15)R(cid:15) ≤γ <  1.  Then 
γ 2 
ln(det(I + R)) ≥ I  • R − 
2(1 − γ ) 
.

" (cid:1) 
Proof:   Factor  R  =  QDQT  where  Q  is  orthonormal  and  D  is  a  diagonal 
matrix  of  the  eigenvalues  of  R.  Then  ﬁrst  note  that  (cid:15)R(cid:15) = 
n 
D2  . We 
j j
j=1 
then  have: 
ln(det(I + QDQT )) 
(cid:1)  (cid:2) 
ln(det(I + D)) 
(cid:3) 
n 
n (cid:1) 
ln(1 + Dj j ) 
j=1 
Dj j  − 2(1−γ )
D2 
j j
j=1 
(cid:3)R(cid:3)2 
I  • D − 2(1−γ ) 
I  • QT RQ − 2(1−γ ) 
γ 2 
I  • R − 2(1−γ ) 
γ 2 

ln(det(I + R))  = 
= 
= 
≥ 
= 
≥ 
= 

q.e.d. 

Proof  of  Proposition   10.2.   Let 

and  notice  that 

α = 

γ 
¯(cid:15)L−1D (cid:1)  ¯ 
L−T (cid:15) ,

¯(cid:15)αL−1D L−T (cid:15) = γ . 
(cid:1)  ¯ 

45 

(cid:30) 
(cid:31) 
(cid:31) 
(cid:30) 
Then 
(cid:1) 
(cid:1) 
¯ 
¯
γ 
D  =  fθ0  X + αD 
fθ0  X +  (cid:3) ¯
(cid:1)  ¯L−T  (cid:3) 
L−1D
=  C • X + αC • D  − θ0  ln(det( ¯ 
L(I + αL−1D 
L−T ) ¯ 
(cid:1)  ¯ 
(cid:1) 
¯ 
¯ 
LT ))
(cid:1)  − θ0  ln(det(I + αL−1D L−T )) 
X )) + αC • D 
=  C • X − θ0  ln(det( ¯
(cid:1)  ¯ 
¯ 
¯ 
X ) + αC • D  − θ0αI  • L−1D 
≤  fθ0 ( ¯ 
L−T  + θ0 
(cid:1)  ¯ 
(cid:1) 
¯ 
(cid:1)  − θ0αL−T  ¯ 
X ) + αC • D 
• D  + θ0 
¯  L−1
(cid:1) 
=  fθ0 ( ¯ 
! 
 
X −1 • D 
X ) + α C − θ0  ¯ 
(cid:1)  + θ0 
=  fθ0 ( ¯ 

γ 2 
2(1−γ ) 

γ 2 
2(1−γ )

γ 2 
2(1−γ ) . 

(cid:1) , y  )  solve  the  Normal  equations: 
(cid:1) 
Now,  (D 
⎧ 
(cid:1) X −1  (cid:1) 
⎪ 
⎪  C − θ0  ¯
⎨ 
X −1D 
X −1  + θ0  ¯ 
m 
¯ = 
⎪ ⎪ ⎩ 
i=1 
Ai  • D  = 0,
(cid:1) 

i = 1, . . . , m. 

(cid:1) 
yi 
Ai 

(12) 

(cid:1) 
Taking  the  inner  product  of  both  sides  of  the  ﬁrst  equation  above  with  D 
and  rearranging  yields: 
X −1D 
(cid:1)
θ0  ¯ 

X −1 ) • D 
• X −1D  = −(C − θ0  ¯
(cid:1) 
(cid:1) . 
¯ 

Substituting  this  in  our  inequality  above  yields: 

46 

(cid:31) 
(cid:30) 
D  ≤  fθ0 ( ¯
X ) − αθ0  ¯ 
X −1D 
(cid:1) 
(cid:1) 
¯ 
γ 
fθ0  X +  (cid:3) ¯
(cid:1)  ¯L−T  (cid:3) 
L−1D

• X −1D 
(cid:1)  + θ0 
¯ 

γ 2
2(1−γ )

γ 2 
2(1−γ ) 

L−T  • L−1D 
X ) − αθ0  ¯ 
L−1D 
L−T  + θ0 
(cid:1)  ¯ 
(cid:1)  ¯
=  fθ0 ( ¯ 
¯ 
L−T (cid:15)2  + θ0 
=  fθ0 ( ¯ X ) − αθ0(cid:15)L−1D 
(cid:1)  ¯ 
¯ 
L−T (cid:15) + θ0 
=  fθ0 ( ¯ X ) − θ0γ (cid:15)L−1D 
(cid:1)  ¯ 
¯ 
γ 2
(cid:30) 
2(1−γ ) 
=  fθ0 ( ¯ X ) + θ0  −γ (cid:15)L−1D 
(cid:1)  ¯ L−T (cid:15) +  2(1−γ ) 
¯ 
γ 2

(cid:31) 
. 

γ 2
2(1−γ ) 

Subsituting  γ  = 0.2  and  and  (cid:15)L−1D 
(cid:1)  ¯ L−T (cid:15) >  4  yields  the  ﬁnal  result. 
¯
1
q.e.d. 

Last  of  all,  we  prove  a  bound  on  the  number  of  iterations  that  the  algo-
rithm  will  need  in  order  to  ﬁnd  a  1 
4 -approximate  solution  of  BSDP (θ0 ): 
Proposition  10.3  Suppose   that   X 0  satisﬁes   Ai • X 0  = bi , i = 1, . . . , m, and  
X 0  (cid:7)  0. Let  θ0  be   given   and   let   fθ0  be   the   optimal   objective   function   value  
∗
of   BSDP (θ0 ).   Then   the   algorithm   initiated   at   X 0  wil l  ﬁnd  a  1 
4 -approximate  
(cid:28) 
(cid:29) 
solution   of   BSDP (θ0 )  in   at   most  
fθ0 (X 0 ) − fθ0 
∗
0.025θ0 

k = 

iterations.  

Proof:   This  follows  immediately  from  (11).  Each  iteration  that  is  not  a  1 -4 
approximate  solution  decreases  the  ob jective  function  fθ0 (X ) of BSDP (θ0 ) 
(cid:28) 
(cid:29) 
by  at  least  0.025θ0  .  Therefore,  there  cannot  be more  than 
fθ0 (X 0 ) − fθ0 
∗
0.025θ0 

47 

iterations  that  are  not  1 
4 -approximate  solutions  of  BSDP (θ0 ). 
q.e.d. 

10.5  Some  Properties  of  the  Frobenius  Norm 
Proposition  10.4  If  M  ∈ S n ,   then  
" (cid:1) 
1.   (cid:15)M (cid:15)  =
n 
(λj (M ))2  ,  where   λ1 (M ), λ2 (M ), . . . , λn (M )  is   an   enu-
j=1 
meration   of   the   n  eigenvalues   of   M . 
2.   If   λ  is   any   eigenvalue   of   M ,   then   |λ| ≤ (cid:15)M (cid:15).

√

3.   |trace(M )| ≤  n(cid:15)M (cid:15). 
4.   If   (cid:15)M (cid:15) < 1,   then   I + M  (cid:7) 0. 

= 

Proof:   We  can  factorize  M  =  QDQT  where  Q  is  orthonormal  and  D  is  a 
(cid:13) 
(cid:13) 
diagonal  matrix  of  the  eigenvalues  of M .  Then 
√ 
(cid:15)M (cid:15) =  M  • M  =  QDQT  • QDQT  = 
(cid:22) (cid:23) (cid:4) 
trace(QDQT QDQT ) 
(cid:23)  n 
(cid:13) 
(cid:13)
trace(DD) = (cid:24) 
(λj (M ))2  . 
trace(QT QDQT QD) = 
j=1 
This  proves  the  ﬁrst  two  assertions.  To  prove  the  third  assertion,  note  that 
(cid:22) 
trace(M ) = trace(QDQT ) = trace(QT QD) 
(cid:23)  n
n (cid:4) 
√  (cid:23)(cid:4) 
λj (M ) ≤  n(cid:24) 
√ 
(λj (M ))2  =  n(cid:15)M (cid:15). 
j=1 
j=1 
(cid:1)  be  an  eigenvalue  of  I  + M .  Then 
To  prove  the  fourth  assertion,  let  λ 
(cid:1) 
λ  = 1 + λ  where  λ  is  an  eigenvalue  of M .  However,  from  the  second  asser-
(cid:1)  = 1 + λ ≥ 1 − (cid:15)M (cid:15) > 0,  and  so M  (cid:7) 0. 
tion,  λ 
q.e.d. 

= trace(D) = 

48 

Proof:   We  have 

Proposition  10.5  If  A, B  ∈ S n ,   then   (cid:15)AB(cid:15) ≤ (cid:15)A(cid:15)(cid:15)B(cid:15). 
(cid:22) 
(cid:23) (cid:23) (cid:4) (cid:4)  (cid:4) 
(cid:12)2 
(cid:11)
(cid:23)  n
(cid:15)AB(cid:15) = (cid:24) 
n 
n 
Aik Bkj 
(cid:22) (cid:23) (cid:4) (cid:4)  (cid:4) 
(cid:11)(cid:11)
(cid:12) (cid:11) 
(cid:12)(cid:12) 
(cid:23)  n  n
i=1  j=1 
k=1 
(cid:4) 
(cid:24) 
n 
n 
≤ 
B 2
A2 
(cid:22) (cid:23) (cid:23) 
kj 
ik 
⎞ 
⎛
(cid:12) 
(cid:11) 
i=1  j=1 
k=1 
k=1 
(cid:23)  n
(cid:4) (cid:4) 
(cid:4) (cid:4) 
B 2  ⎠ = (cid:15)A(cid:15)(cid:15)B(cid:15).
A2  ⎝ 
= (cid:24) 
n 
n
n 
ik 
kj 
j=1  k=1 
i=1  k=1 

q.e.d. 

11	

Issues  in  Solving  SDP   using  the  Ellipsoid  Al­
gorithm 

To  see  how  the  ellipsoid  algorithm  is  used  to  solve  a  semideﬁnite  program, 
assume  for  convenience  that  the  format  of  the  problem  is  that  of  the  dual 
problem  SDD .  Then  the  feasible  region  of  the  problem  can  be  written  as: 
$ 
#	
(cid:4) 
m 
F  = (y1 , . . . , ym ) ∈ (cid:3) | C − 
yiAi  (cid:6) 0 
m
, 
i=1 
(cid:1) 
m
i=1  yi bi . 

Note  that  F  is  just  a  convex  re-

and  the  ob jective  function  is 
gion  in  (cid:3) . 
m

Recall that at any iteration of the ellipsoid algorithm, the set of solutions 
of SDD  is known to lie in the current ellipsoid, and the center of the current 

49 

(cid:1) 
(cid:1) 
y  ∈ F ,  then  we  perform  an  optimality 
ellipsoid  is,  say,  ¯
y  = ( ¯
y1 , . . . , y¯m ).  If  ¯
i=1  yi bi  ≥ 
m
m
cut  of  the  form 
i=1  y¯i bi ,  and  use  standard  formulas  to  update 
the  ellipsoid  and  its  new  center.  If  ¯ ∈ F ,  then  we  perform  a  feasibility  cut 
y /
h ∈ (cid:3)m  such  that  ¯
hT y > hT y¯ for  all  y ∈ F . 
by  computing  a  vector  ¯ 
¯ 

There  are  four  issues  that  must  be  resolved  in  order  to  implement  the 
above  version  of  the  ellipsoid  algorithm  to  solve  SDD : 
(cid:1) 
1.  Testing  if  y¯ ∈  F .  This  is  done  by  computing  the  matrix  S ¯ =  C  − 
y¯iAi . If  S ¯  (cid:6)  0,  then  y¯ ∈  F .  Testing  if  the  matrix  S ¯  (cid:6)  0 can 
m 
i=1 
be  done  by  computing  an  exact  Cholesky  factorization  of  S¯,  which 
takes O(n3 )  operations,  assuming  that  computing  square  roots  can be 
performed  (exactly)  in  one  operation. 
2.  Computing  a  feasibility  cut.	 As  above,  testing  if  S ¯  (cid:6) 0 can  be  com-
puted  in O(n3 ) operations.  If S ¯  (cid:5)(cid:6) 0,  then again assuming  exact arith-
v  such  that  ¯T  ¯ v <  0 in  O(n3 )  oper-
v S ¯
metic,  we  can  ﬁnd  an  n-vector  ¯
ations  as  well.  Then  the  feasiblity  cut  vector  ¯ h  is  computed  by  the 
formula: 

¯ 
hi  = ¯T Aiv ,  i = 1, . . . , m, 
¯
v 

T ¯	
y h = 

(cid:12) 
(cid:11) 
whose  computation  requires  O(mn2 )  operations.  Notice  that  for  any 
y ∈ F ,  that 
(cid:4) 
(cid:4)	
m	
m 
yiv¯T Aiv¯ = ¯
T v
yiAi  v¯
(cid:11) 
(cid:12) 
(cid:4) 
(cid:4) 
i=1 
i=1 
m
m 
y¯iv¯T Aiv¯ = ¯T ¯ 
y¯iAi  v¯ = 
y  h, 
i=1 
i=1 
thereby  showing  ¯	
y .
h  indeed  provides  a  feasibility  cut  for  F  at  y = ¯

≤ v¯T C ¯	
T
v < v¯

3.  Starting the ellipsoid algorithm.  We need to determine an upper bound 
∗
R  on  the  distance  of  some  optimal  solution  y  from  the  origin.  This 

50 

cannot  be done by examining the input length of the data, as is the case 
in  linear  programming.  One  needs  to  know  some  special  information 
about  the  speciﬁc  problem  at  hand  in  order  to  determine  R  before 
solving  the  semideﬁnite  program. 

4.  Stopping  the  ellipsoid  algorithm.  Suppose  that  we  seek  an  -optimal 
solution of SDD .  In order to prove a complexity bound on the number 
of  iterations  needed  to  ﬁnd  an  -optimal  solution,  we  need  to  know 
beforehand  the  radius  r  of  a  Euclidean  ball  that  is  contained  in  the 
set  of  -optimal  solutions  of  SDD .  The  value  of  r  also  cannot  be 
determined  by  examining  the  input  length  of  the  data,  as  is  the  case 
in  linear  programming.  One  needs  to  know  some  special  information 
about  the  speciﬁc  problem  at  hand  in  order  to  determine  r  before 
solving  the  semideﬁnite  program. 

12  Current  Research  in  SDP 

There  are  many  very  active  research  areas  in  semideﬁnite  programming  in 
nonlinear (convex) programming, in combinatorial optimization, and in con-
trol  theory.  In  the  area  of  convex  analysis,  recent  research  topics  include 
the geometry and the boundary structure of SDP  feasible regions (including 
notions  of  degeneracy)  and  research  related  to  the  computational  complex-
ity  of  SDP  such  as  decidability  questions,  certiﬁcates  of  infeasibility,  and 
duality  theory.  In  the  area  of  combinatorial  optimization,  there  has  been 
much research on the practical and the theoretical use of SDP  relaxations of 
hard  combinatorial  optimization  problems.  As  regards  interior  point  meth-
ods,  there  are  a  host  of  research  issues,  mostly  involving  the  development 
of  diﬀerent  interior  point  algorithms  and  their  properties,  including  rates  of 
convergence,  performance  guarantees,  etc. 

13  Computational  State  of  the  Art  of  SDP 

Because SDP  has so many applications, and because interior point methods 
show  so much  promise,  perhaps  the most  exciting  area  of  research  on  SDP 
has to do with computation and implementation of interior point algorithms 

51 

for  solving  SDP .  Much  research  has  focused  on  the  practical  eﬃciency  of 
interior  point  methods  for  SDP .  However,  in  the  research  to  date,  com-
putational  issues  have  arisen  that  are  much  more  complex  than  those  for 
linear  programming,  and  these  computational  issues  are  only  beginning  to 
be well-understood.  They probably stem from a variety of factors,  including 
the fact that SDP  is not guaranteed to have strictly complementary optimal 
solutions  (as  is  the  case  in  linear  programming).  Finally,  because  SDP  is 
such  a  new  ﬁeld,  there  is  no  representative  suite  of  practical  problems  on 
which  to  test  algorithms,  i.e.,  there  is  no  equivalent  version  of  the  netlib 
suite  of  industrial  linear  programming  problems. 

A  good  website  for  semideﬁnite  programming  is: 

http://www.zib.de/helmberg/semidef.html. 

14  Exercises 
(cid:1) 
1.  For  a  (square) matrix M  ∈ IRn×n ,  deﬁne  trace(M ) =  Mj j ,  and  for 
n 
j=1 
two  matrices  A, B  ∈ IRk×l  deﬁne 
(cid:4) (cid:4) 
k
l 
Aij Bij  . 
i=1  j=1 

A • B  := 

Prove  that: 
(a)  A • B = trace(AT B ). 
(b)  trace(M N ) = trace(N M ). 
2.  Let S k×k  denote the cone of positive semi-deﬁnite symmetric matrices, 
(cid:30)
(cid:31)∗ 
+ 
namely  S k×k  =  {X  ∈ S k×k  | vT X v  ≥ 0  for  all  v  ∈ (cid:3) }.  Considering 
n
+ 
S k×k  as  a  cone,  prove  that 
= S n×n ,  thus  showing  that  S k×k 
S k×k
+ 
+
+
+ 
is  self-dual. 

52 

3.  Consider  the  problem: 
(P) :  minimized  dT Qd 

dT M d = 1  .

s.t. 
If M  (cid:7) 0,  show  that  (P)  is  equivalent  to:

(S)  :  minimizeX  Q • X 
M  • X  = 1 
X  (cid:6) 0  .


s.t. 

What  is  the  SDP  dual  of  (S)? 


4.  Prove  that 

if  and  only  if 

X  (cid:6) xx T 
(cid:3) 
(cid:2) 
X x  (cid:6) 0  . 
xT 
1 
5.  Let K  := {X  ∈ S k×k  |  X  (cid:6) 0}  and  deﬁne 
∗ K  := {S  ∈ S k×k  |  S • X  ≥ 0  for  all X  (cid:6) 0}  . 
∗
Prove  that K = K . 

6.  Let  λmin  denote  the  smallest  eigenvalue  of  the  symmetric  matrix  Q. 
Show that the following three optimization problems each have optimal 
ob jective  function  value  equal  to  λmin  . 

(P1) :  minimized  dT Qd 

s.t. 

dT I d = 1  . 

(P2) :  maximizeλ  λ 
Q (cid:6) λI  . 

s.t. 

53 

s.t.	

(P3) :  minimizeX  Q • X 
I  • X  = 1 
X  (cid:6) 0  . 
7.  Suppose  that  Q (cid:6) 0.  Prove  the  following: 
T x  Qx + q T x + c ≤ 0

if  and  only  if  there  exists W  for  which

(cid:12)  (cid:11) 
(cid:11) 
(cid:12) 
• 

≤ 0 and 

(cid:11) 

1 
T 
c 
2 q

1 
2 q Q

1  xT	
x W	

(cid:12) 

(cid:6) 0  .

T
1  x
x W 

8.  Consider  the  matrix 

Hint:  use  the  property  shown  in  Exercise  4. 
(cid:2)
(cid:3) 
A B 
B T  C 
where  A, B  are  symmetric  matrices  and  A  is  nonsingular.  Prove  that 
M  (cid:6) 0  if  and  only  if  C − B T A−1B (cid:6) 0. 

M  = 

,

54 

