18.335 Problem Set 3 Solutions 
Problem 1:  SVD and low-rank approximations (5+10+10+10 pts) 
(a)	 A = Qˆ Rˆ , where the columns of Qˆ are orthonormal and hence Qˆ ∗Qˆ = I . Therefore, A∗A = ( Qˆ Rˆ )∗ (Qˆ Rˆ ) = 
Rˆ ∗ (Qˆ ∗Qˆ )Rˆ = Rˆ ∗Rˆ .  But the singular values of A and Rˆ are the square roots of the nonzero eigenvalues 
of  A∗A  and  Rˆ ∗Rˆ ,  respectively,  and  since  these  two matrices  are  the  same  the  singular  values  are  the 
same. Q.E.D. 

(b)  It is sufﬁcient to show that the reduced SVD AVˆ = Uˆ Σˆ is real, since the remaining columns of U  and 
V  are  formed  as  a basis  for  the orthogonal  complement of  the  columns of Uˆ and Vˆ ,  and  if  the  latter 
are real then their complement is obviously also real.  Furthermore, it is sufﬁcient to show that Uˆ can 
be chosen real, since A∗ui/σi  = vi  for each column ui  of Uˆ and vi  of Uˆ , and A∗  is real.  The columns 
ui  are eigenvectors of A∗A = B, which  is a  real-symmetric matrix,  i.e.  Bui  = σi 
2ui .  Suppose  that  the 
ui  are  not  real.  Then  the  real  and  imaginary  parts  of  ui  are  themselves  eigenvectors with  eigenvalue 
2  (proof:  take  the  real  and  imaginary parts of Bui  = σi 
2 ui ,  since B  and σi 
2  are  real).  Hence,  taking 
σi 
either  the  real or  imaginary parts of  the complex ui  (whichever  is nonzero) and normalizing  them  to 
unit length, we obtain a new purely real Uˆ . Q.E.D.1 
(c)  We just need to show that, for any A ∈ Cm×n  with rank < n and for any ε > 0, we can ﬁnd a full-rank 
matrix B with �A − B�2  < ε .  Form  the SVD A = U ΣV ∗  with  singular values σ1 , . . . , σr  where  r < n 
is  the  rank of A.  Let B = U Σ˜ V ∗  where Σ˜ is  the same as Σ except  that  it has n − r  additional nonzero 
singular values σk>r  = ε /2.  From equation 5.4 in the book, �B − A�2  = σr+1  = ε /2 < ε , noting that 
A = Br  in the notation of the book. 

(d)  Take any grayscale photograph (either one of your own, or off the web).  Scale it down to be no more 
than 1500 × 1500 (but not necessarily square), and read it into Matlab as a matrix A with the imread 
command (type “doc  imread” for instructions). 

(i)  This  is  plotted  on  a  semilog  scale  in Fig  1,  showing  that  the  singular  values σi  decrease  faster 
than exponentially with i. 
(ii)  Figure 2 shows an image of a handsome fellow, both at full resolution (200 singular values), and 
using only 16 and 8 singular values. Even with just 8 singular values (4% of the data), the image 
is still at least somewhat recognizable.  If the image were larger (this one is only 282 × 200) then 
it would probably compress even more. 

Problem 2: QR and orthogonal bases (10+10+(5+5+5) pts) 
(a)  If  A = QR,  then  B = RQ = Q∗AQ = Q−1AQ  is  a  similarity  transformation,  and  hence  has  the  same 
eigenvalues  as  shown  in  the  book.  Numerically  (and  as  explained  in  class  and  in  lecture  28),  doing 
this  repeatedly  for  a  Hermitian  A  (the  unshifted  QR  algorithm)  converges  to  a  diagonal  matrix  Λ 
of  the  eigenvalues  in  descending  order.  To  get  the  eigenvectors,  we  observe  that  if  the  Q  matrices 
· · ·
1AQ1Q2 · · · 
from each step are Q1 , Q2 , and so on,  then we are computing  Q∗ 
2Q∗ 
= Λ, or A = QΛQ∗ 
where  Q = Q1Q2 · · · .  By  comparison  to  the  formula  for  diagonalizing  A,  the  columns  of  Q  are  the 
eigenvectors. 

(b)  The easiest way to approach this problem is probably to look at the explicit construction of Rˆ via the 
Gram-Schmidt  algorithms.  By  inspection,  ri j  = q∗
i v j  is  zero  if  i  is  even  and  j  is  odd  or  vice-versa. 

1 There is a slight wrinkle if there are repeated eigenvalues, e.g. σ1  = σ2 , because the real or imaginary parts of u1  and u2  might not 
be orthogonal. However, taken together, the real and imaginary parts of any multiple eigenvalues must span the same space, and hence 
we can ﬁnd a real orthonormal basis with Gram-Schmidt or whatever. 

1 

Figure 1: Distribution of the singular values σi  in the image of Fig. 2, showing that they decrease faster than 
exponetially with i. 

Figure 2: Left:  full resolution image (albeit JPEG-compressed). Middle:  16% of the singular values. Right: 
4% of the singular values. 

2 

020406080100120140160180200100101102103104105index i of singular value σisingular values σiBecause of this, Rˆ will have a checkerboard pattern of nonzero values: ⎞
⎛ 
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ 
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 
× 
×
×
×
× 
×
×
×
×
×
× 
×
×
× 
Rˆ = 
× 
×
. 
× 
×
×  × 
(c)  Trefethen, problem 10.4: 
(i)  e.g.  consider  θ  = π /2 (c = 0,  s = 1):  Je1  = −e2  and  Je2  = e1 , while Fe1  = e2  and Fe2  = e1 . 
J  rotates  clockwise  in  the  plane  by  θ .  F  is  easier  to  interpret  if  we  write  it  as  J  multiplied 
on  the  right  by  [−1, 0; 0, 1]:  i.e.,  F  corresponds  to  a  mirror  reﬂection  through  the  y  (e2 )  axis 
followed by clockwise rotation by θ .  More subtly, F  corresponds to reﬂection through a mirror 
plane  corresponding  to  the  y  axis  rotated  clockwise  by  θ /2.  That  is,  let  c2  = cos(θ /2)  and 
� 
� 
� � 
� 
�  � 
� � 
� � 
� 
2 − s2
s2  = cos(θ /2), in which case (recalling the identities c2
2  = c, 2s2 c2  = s): 
−s2  = 
c−  2
c2  −s2 
−1 0 
s2 
= F,
= 
s2 
s2 
c2 
c2 
c2 
0
1 

c2 
s2 
−s2  c2 
which shows that F  is reﬂection through the y axis rotated by θ /2. 
(ii)  The key thing is to focus on how we perform elimination under a single column of A, which we 
� 
�  � 
� 
then  repeat  for  each  column.  For  Householder,  this  is  done  by  a  single  Householder  rotation. 
Here, since we are using 2 × 2 rotations, we have  to eliminate under a column one number at a 
�2 
�x
a
into J x = 
time:  given 2-component vector x = 
, where J  is clockwise rotation 
b 
0
by θ  = tan−1 (b/a) [or, on a computer, atan2(b, a)].  Then we just do this working “bottom-up” 
from  the  column:  rotate  the  bottom  two  rows  to  introduce  one  zero,  then  the  next  two  rows  to 
introduce a second zero, etc. 
(iii)  The ﬂops to compute the J matrix itself are asymptotically irrelevant, because once J is computed 
it is applied to many columns (all columns from the current one to the right). To multiply J by a 
single 2-component vector requires 4 multiplications and 2 additions, or 6 ﬂops.  That is, 6 ﬂops 
per row per column of the matrix.  In contrast, Householder requires each column x to be rotated 
via x = x − 2v(v∗ x).  If x has m components, v∗ x requires m multiplications and m − 1 additions, 
multiplication  by  2v  requires  m  more  multiplications,  and  then  subtraction  from  x  requires  m 
more additions, for 4m − 1 ﬂops overall. That is, asymptotically 4 ﬂops per row per column. The 
6 ﬂops of Givens is 50% more than the 4 of Householder. 

c2 
s2 

−c 
s

s
c 

Problem 3:  Schur ﬁne (10 + 15 points) 
(a)  First,  let  us  show  that  T  is  normal:  substituting  A = QT Q∗  into  AA∗  = A∗A  yields QT Q∗QT ∗Q∗  = 
QT ∗Q∗QT Q∗  and hence (cancelling the Qs) T T ∗  = T ∗T . 
The  (1,1)  entry  of  T ∗T  is  the  squared  L2  norm  (� · �2
2 )  of  the  ﬁrst  column  of  T ,  i.e.  |t1,1 |2  since 
T  is  upper  triangular,  and  the  (1,1)  entry  of  T T ∗  is  the  squared  L2  norm  of  the  ﬁrst  row  of  T ,  i.e. 
∑i |t1,i |2 .  For  these  to  be  equal,  we  must  obviously  have  t1,i  = 0  for  i > 1,  i.e.  that  the  ﬁrst  row  is 
diagonal. 

3 

x satisﬁes 

We proceed by  induction.  Suppose  that  the ﬁrst  j − 1  rows of T  are diagonal,  and we want  to prove 
this of row  j.  The ( j, j) entry of T ∗T  is the squared norm of the  j-th column, i.e.  ∑i≤ j |ti, j |2 , but this 
is  just  |t j, j |2  since ti, j  = 
0  for  i <  j  by  induction.  The  ( j, j) entry of T T ∗  is  the  squared norm of  the 
j-th  row,  i.e.  ∑i≥ j |t j,i | .  For  this  to  equal  |t j, j |2 , we must  have t j,i  = 0  for  i >  j ,  and  hence  the  j-th 
2 
row is diagonal. Q.E.D. 
(b)  The eigenvalues are the roots of det(T − λ I ) = ∏i (ti,i − λ ) = 0—since T  is upper-triangular, the roots 
are obviously therefore λ  = ti,i  for i = 1, . . . , m.  To get the eigenvector for a given λ  = ti,i ,  it sufﬁces 
to compute the eigenvector x of T , since the corresponding eigenvector of A is Qx. 
⎞

⎞ ⎛
⎛ 
0 = (T − ti,i I )x = ⎝ 
0  v∗  ⎠ ⎝  α  ⎠ ,

x1

T1  u B 
T2 
x2

where we have broken up T − ti,i I into the ﬁrst i − 1 rows (T1 u B), the i-th row (which has a zero on the 
diagonal), and the last m − i rows T2 ; similarly, we have broken up x into the ﬁrst i − 1 rows x1 , the i-th 
row α , and the last m − i rows x2 . Here, T1  ∈ C(i−1)×(i−1)  and T2  ∈ C(m−i)×(m−i)  are upper-triangular, 
and are non-singular because by assumption there are no repeated eigenvalues and hence no other t j, j 
equals  ti,i .  u ∈ Ci−1 ,  v ∈ Cm−i ,  and B ∈ C(i−1)×(m−i)  come  from  the  upper  triangle  of  T  and  can  be 
anything.  Taking the last m − i rows of the above equation, we have T2 x2  = 0, and hence x2  = 0 since 
T2  is invertible. Furthermore, we can scale x arbitrarily, so we set α = 1. The ﬁrst i − 1 rows then give 
us the equation T1 x1 + u = 0, which leads to an upper-triangular system T1 x1  = −u that we can solve 
for x1 . 

Now,  let  us  count  the  number  of  operations.  For  the  i-th  eigenvalue  ti,i ,  to  solve  for  x1  requires 
∼ (i − 1)2 ∼ i2  ﬂops to do backsubstitution on an (i − 1) × (i − 1) system T1 x1 = −u. Then to compute 
the  eigenvector Qx  of  A  (exploiting  the m − i  zeros  in  x)  requires ∼ 2mi  ﬂops.  Adding  these  up  for 
−
i=1 i2  ∼ m3/3, and 2m ∑m
1 i ∼ m3 , and hence the overall cost is ∼  3
i = 1 . . . m, we obtain ∑m
4 m3  ﬂops 
i=
0
(K = 4/3). 

4 

MIT OpenCourseWare
http://ocw.mit.edu 

18.335J / 6.337J Introduction to Numerical Methods

 
Fal l 2010 
 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

