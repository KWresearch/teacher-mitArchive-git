18.335 Problem Set 2 Solutions 
Problem 1: Floating-point 
(a)  The smallest  integer  that cannot be exactly  represented  is n = β t + 1  (for base-β  with a t -digit man­

tissa).  You might  be  tempted  to  think  that  β t  cannot  be  represented,  since  a  t -digit  number,  at  ﬁrst

glance,  only  goes  up  to  β t  − 1  (e.g.  three  base-10  digits  can  only  represent  up  to  999,  not  1000).

· 
However, β t  can be represented by β t−1  β 1 , where the β 1  is absorbed in the exponent.


In IEEE single and double precision, β = 2 and t = 24 and 53, respectively, giving 224 + 1 = 16, 777, 217 
and 253 + 1 = 9, 007, 199, 254, 740, 993. 

Evidence that n = 253 + 1 is not exactly represented but that numbers less than that are can be found 
by  looking at  the  last  few decimal digits as we  increment  the numbers.  e.g.  the  last 3 decimal digits 
of m in Matlab are returned by rem(m,1000).  rem(2^53-2,1000)=990, rem(2^53-1,1000)=991, 
rem(2^53,1000)=992, rem(2^53+1,1000)=992, rem(2^53+2,1000)=994, rem(2^53+3,1000)=996, 
and  rem(2^53+4,1000)=996.  That  is,  incrementing  up  to  n − 1  increments  the  last  digit  as  ex­
pected,  while  going  from  n − 1  to  n  the  last  digit  (and  indeed,  the  whole  number)  doesn’t  change, 
and  after  that  the  last  digit  increments  in  steps  of  2.  In  particular,  n + 1  and  n + 3  are  both  exactly 
represented,  because  they  are  even  numbers:  a  factor  of  two  can  be  pulled  into  the  exponent,  since 
·
· 
253 + 2 = (252 + 1)  2 and 253 + 4 = (252 + 2)  2, and hence the signiﬁcand is still exactly represented. 
(b)  What we want  to  show,  for  a  function g(x) with  a  convergent Taylor  series  at  x = 0,  that g(O(ε )) =

g(0) + g� (0)O(ε ).  [We must also assume g� (0) =� 0, otherwise it is obviously false.] The ﬁrst thing we

need to do is to write down precisely what this means. We know what it means for a function  f (ε ) to

be O(ε ):  it means  that,  for  ε  sufﬁciently  small  (0 ≤ ε  < δ  for  some δ ),  then  | f (ε )| < C1 ε  for  some

C1  > 0.  Then, by g(O(ε )), we mean g( f (ε ))  for any  f (ε ) ∈ O(ε ); we wish  to show  that  f (ε ) being

O(ε ) implies that

g( f (ε )) = g(0) + g� (0)h(ε ) 

for some h(ε ) that is also O(ε ). 

Since g(x) has a convergent Taylor series, we can explicitly write 
∞
∑ g(n) (0)
1
f (ε )n .
h(ε ) =  f (ε ) + 
g�
(0) 
n!
n=2 
��� 
���
⎡
⎤ 
But since | f (ε )| < C1 ε  for some C1  (and for sufﬁciently small ε ), it immediately follows that 
h(ε ) < C1 ε ⎣1 +
⎦
g(n+1) (0)
|
| 
n ε n
C1 
,
(n + 1)!

|g�

∞
∑1
| 
(0)
n=1

which  is  clearly  < 2C1 ε  for  sufﬁciently  small  ε  (and  indeed,  is  < C2 ε  for  any C2  > C1 ),  since  the 
→
summation  of  ε n  must  go  to  zero  as  ε 
0  [if  it  doesn’t,  it  is  trivial  to  show  that  the  Taylor  series 
won’t converge to a function with the correct derivative g� (0) at ε = 0]. 

Problem 2: Addition 
(a)  We  can  prove  this  by  induction  on  n.  For  the  base  case  of  n = 2,  f˜(x) = (0 ⊕ x1 ) ⊕ x2  = x1 ⊕ x2  = 
(x1 + x2 )(1 + ε2 ) for |ε2 | ≤ εmachine is a consequence of the correct rounding of ⊕ (0 ⊕ x1  must equal 
x1 , and x1 ⊕ x2  must be within εmachine of the exact result). 

1 

Figure 1:  Error | f˜(x) −  f (x)|/ ∑i |xi | for random x ∈ [0, 1)n , where  f˜ is computed by a simple loop in single 
precision,  averaged  over  100  random  x  vectors,  as  a  function  of  n.  Notice  that  it ﬁts  very well  to ≈ 1.2 ×
√
10−8√
n, matching the expected 
n growth for random errors. 

Now  for  the  inductive  step.  Suppose  ˜sn−1  = (x1 + x2 ) ∏n−1 
i=3  xi ∏n−1 
k=2 (1 + εk ) + ∑n−1 
k=i  (1 + εk ).  Then 
s˜n  = s˜n−1 ⊕ xn  = ( s˜n−1 + xn )(1 + εn ) where |εn | < εmachine  is guaranteed by ﬂoating-point addition. 
The result follows by inspection:  the previous terms are all multiplied by (1 + εn ), and we add a new 
term xn (1 + εn ). 

· · ·
(1 + εn ) = 1 + ∑n
k=i εk + (products of ε ) = 
(b)  This  is  trivial:  just  multiplying  out  the  terms  (1 + εi )
|
| ≤ 
1 + δi , where the products of εk  terms are O(ε 2 
δi
machine ), and hence (by the triangle inequality) 
k=i |εk | 
machine ) ≤ (n − i + 1)εmachine + O(ε 2 
∑n 
+ O(ε 2 
machine ).
(c)  We have:  f˜(x) =  f (x) + (x1 + x2 )δ2 + ∑n
i=3 xi δi , and hence (by the triangle inequality): 

n 
x1 |δ2 ∑  xi δ i
f˜(x) −  f (x)
| ≤ |
| 
|
| + 
|
| |
i=2 

|. 

|
| ≤ nεmachine + O(ε 2 
But  δi
machine )  for  all  i,  from  the  previous  part,  and  hence 
i=1 |
|
nεmachine ∑n
xi .
(d)  For uniform random εk , since δi  is the sum of (n − i + 1) random variables with variance ∼ εmachine , it 
i=1 |xi | � 
nO(εmachine ). Hence | f˜(x) −  f (x)| = O �√
follows from the usual properties of random walks that the mean |δi | has magnitude ∼ √
n − i + 1O(εmachine ) ≤
√
nεmachine ∑n
. 
(e)  Results of the suggested numerical experiment are plotted in ﬁgure 1.  For each n, I averaged the error

| f˜(x) −  f (x)|/ ∑i |xi | over 100 runs to reduce the variance.


f˜(x) −  f (x)
| 
| ≤ 

2 

10210310410510610−710−610−5input length nmean error abs(loopsum(x)−sum(x)) / sum(abs(x))  error averaged over 100 x1.2×10−8 sqrt(n)Problem 3: Addition, another way 
(a)  Suppose n = 2m  with m ≥ 1. We will ﬁrst show that 
n
m 
f˜(x) = ∑
 xi ∏
 (1 + εi,k ) 
i=1 
k=1 
where |εi,k | ≤ εmachine . We prove the above relationship by induction.  For n = 2 it follows from the 
deﬁnition  of  ﬂoating-point  arithmetic.  Now,  suppose  it  is  true  for  n  and we wish  to  prove  it  for  2n. 
The sum of  2n  number  is  ﬁrst  summing  the  two  halves  recursively  (which  has  the  above  bound  for 
� 
� 
each half since they are of length n) and then adding the two sums, for a total result of 
2n
n
m 
m 
f˜(x ∈ R2n ) =  ∑
 xi ∏
 (1 + εi,k ) +  ∑
 xi ∏
 (1 + εi,k ) (1 + ε ) 
i=1 
i=n+1 
k=1 
k=1 
for |ε | < εmachine . The result follows by inspection, with εi,m+1  = ε . 
k=1 (1 + εi,k ) = 1 + δi with |δi | ≤ mεmachine + O(ε 2 
Then, we use the result from problem 2 that ∏m
machine ). 
Since m = log2 (n), the desired result follows immediately. 
� 
√
(b)  As in problem 2, our δi  factor is now a sum of random εi,k  values and grows as 
m. That is, we expect 
log2 nO(εmachine ) ∑i |xi |. 
that the average error grows as 
(c)  Just enlarge  the base case.  Instead of  recursively dividing  the problem  in  two until n < 2, divide  the 
problem in two until n < N  for some N , at which point we sum the < N numbers with a simple loop as 
in problem 2.  A  little arithmetic  reveals  that  this produces ∼ 2n/N  function calls—this  is negligible 
compared  to  the  n − 1  additions  required  as  long  as  N  is  sufﬁciently  large  (say,  N  = 200),  and  the 
efﬁciency should be roughly that of a simple loop. 

Using a  simple  loop has error bounds  that grow as N  as you  showed above,  but N  is  just a constant, 
so this doesn’t change the overall logarithmic nature of the error growth with n.  A more careful anal­
ysis  analogous  to  above  reveals  that  the  worst-case  error  grows  as  [N + log2 (n/N )]εmachine ∑i |xi |. 
Asymptotically,  this  is  not  only  log2 (n)εmachine ∑i |xi |  error  growth,  but  with  the  same  asymptotic 
constant factor! 

(d)  Instead  of  “if  (n  <  2),”  just  do  “if  (n  <  200)”.  To  keep  everything  in  single  precision,  one  should, 
strictly speaking, call loopsum instead of the built-in function sum (which uses at least double preci­
sion, and probably uses extended precision). 

The  logarithmic  error  growth  is  actually  so  slow  that  it  is  practically  impossible  to  see  the  errors 
growing at all.  In an attempt to see it more clearly, I wrote a C program to implement the same func­
tion (orders of magnitude quicker than doing recursion in Matlab), and went up to n = 109  or so.  As 
in problem 2, I averaged over 100 random x to reduce the variance.  The results are plotted in ﬁgure 2 
for  two cases:  N = 1 (trivial base case) and N = 200 (large base case, much faster).  Asymptotically, 
the error is growing extremely slowly with n, as expected, although it is hard to see even a logarithmic 
growth; it looks pretty ﬂat. There are also a few surprises. 

First, we see that the errors are oscillating, at a constant rate on a semilog scale.  In fact, the period of 
the oscillations corresponds  to powers of  two—the error decreases as a power of  two  is approached, 
and  then  jumps  up  again  when  n  exceeds  a  power  of  2.  Intuitively,  what  is  happening  is  this:  the 
reason  for  the  slow  error  growth  is  that  we  are  recursively  dividing  x  into  equal-sized  chunks,  and 
are  therefore adding quantities with nearly equal magnitudes on average  (which minimized  roundoff 
error), but when n is not a power of two some of the chunks are unequal in size and the error increases. 

3 

Figure 2: Error | f˜(x) −  f (x)|/ ∑i |xi | for random xi  ∈ [0, 1)n , averaged over 100 x vectors, for  f˜ computed in 
single precision by  recursively dividing  the sum  in  two halves until n < N ,  at which point a simple  loop  is 
employed. Results for N = 1 and N = 200 base cases are shown. 

√
√
Second,  for  the  N  = 200  base  case,  the  errors  initially  increase  much  faster—as 
n,  in  fact,  and 
then  come  back  down  for  n � N .  Obviously,  for  n < N  the  errors must  increase  as 
n  as  in  prob­
lem  2,  since  for  this  case  we  do  no  recursion  and  just  sum  via  a  loop.  However,  when  n � N ,  the 
logarithmic  terms  in  the  error  dominate  over  the  O(N )  term,  and  the  error  approaches  the  error  for 
N = 1 with the same constant factor, as predicted above! 

However, predicting the exact functional dependence is clearly quite difﬁcult! 
(e)  An m × m matrix multiplication is just a bunch of length-m dot products. The only error accumulation 
in a dot product will occur in the summation, so the error growth with m should be basically the same 
as in our analysis of the corresponding summation algorithm. 

√
If you use the simple 3-loop row-column algorithm, you are doing the summation(s) via simple loops, 
and the errors should thus grow as O(εmachine
m) on average as above.  The cache-oblivious algo­
√
rithm, on  the other hand,  corresponds  to  recursively dividing each dot product  in  two,  and hence  the 
errors should grow as O(εmachine
log m) as above. 

In  most  cases,  however,  m  isn’t  large  enough  for  people  to  care  about  this  difference  in  accuracy 
for matrix multiplies. 

Problem 4:  Stability 
(a)  Trefethen,  exercise  15.1.	 In  the  following,  I  abbreviate  εmachine  =  εm ,  and  I  use  the  fact  (from 
problem  1)  that  we  can  replace  any  g(O(ε ))  with  g(0) + g� (0)O(ε ).  I  also  assume  that  ﬂ(x)  is 

4 

10110210310410510610710810900.20.40.60.811.21.41.61.82x 10−7input length nmean error / sum |xi|  n = 1 base casen = 200 base casedeterministic—by  a  stretch  of  Trefethen’s  deﬁnitions,  it  could  conceivably  be  nondeterministic  in 
which case one of the answers changes as noted below, but this seems crazy to me (and doesn’t corre­
spond to any real machine). 
(i)  Backward  stable.	 x ⊕ x = ﬂ(x) ⊕ ﬂ(x) = [x(1 + ε1 ) + x(1 + ε1 )](1 + ε2 ) = 2 ˜x  for  |εi | ≤ εm  and 
x˜ = x(1 + ε1 + ε2 + 2ε1 ε2 ) = x[1 + O(εm )]. 
(ii)  Backward  stable.	 x ⊗ x = ﬂ(x) ⊗ ﬂ(x) = [x(1 + ε1 ) × x(1 + ε1 )](1 + ε2 ) = x˜2  for  |εi | ≤ εm  and 
√
1 + ε2  = x[1 + O(εm )]. 
x˜ = x(1 + ε1 )
(iii)  Stable but not backwards stable.  x � x = [ﬂ(x)/ ﬂ(x)](1 + ε ) = 1 + ε  (not including x = 0 or ∞, 
which give NaN). This is actually forwards stable, but there is no  ˜x such that  ˜x/x˜ =� 1 so it is not 
backwards stable.  (Under the stronger assumption of correctly rounded arithmetic, this will give 
exactly 1, however.) 
(iv)  Backwards stable.  x � x = [ﬂ(x) − ﬂ(x)](1 + ε ) = 0. This is the correct answer for  ˜x = x.  (In the 
crazy case where ﬂ is not deterministic, then it might give a nonzero answer, in which case it is 
unstable.) 
(v)  Unstable.  It  is  deﬁnitely  not  backwards  stable,  because  there  is  no  data  (and  hence  no way  to 
choose  ˜x  to  match  the  output).  To  be  stable,  it  would  have  to  be  forwards  stable,  but  it  isn’t 
6  ⊕ · · ·  summed 
because  the errors decrease more slowly  than O(εm ).  More explicitly, 1 ⊕ 1
2  ⊕ 1
· · ·	
· · · 
from  left  to  right  will  give  ((1 +  1 )(1 + ε1 ) +  1 )(1 + ε2 ) = e +  3 ε1 +  10 ε2 + 
dropping
2
6	
2
6
terms of O(ε 2 ), where the coefﬁcients of the εk  factors converge to e.  The number of terms is n 
where n satisﬁes n! ≈ 1/εm , which is a function that grows very slowly with 1/εm , and hence the 
error from the additions alone is bounded above by ≈ nεm .  The key point is that the errors grow 
at  least as  fast as nεm  (not even counting errors  from  truncation of  the series,  approximation of 
1/k!, etcetera), which is not O(εm ) because n grows slowly with decreasing εm . 
(vi)  Stable.  As  in  (e),  it  is  not  backwards  stable,  so  the  only  thing  is  to  check  forwards  stability. 
Again,  there will  be  n  terms  in  the  series, where  n  is  a  slowly  growing  function  of  1/εm  (n! ≈
1/εm ). However, the summation errors no longer grow as n.  From right to left, we are summing 
1)! ⊕ · · · ⊕ 1. But this gives (( n
2)! )(1 + εn−2 ) · · · ,and the linear 
! ⊕ (n−1
1)! )(1 + εn−1 ) + (n−1
1
1
! + (n−1
n
�����
����� 
� 
� 
terms in the εk  are then bounded by 
n − 1 
1n−
1n−
n  1 
j 
1 
≤ εm 
≈ εm e = O(εm ).
∑ 
∑

∑ 
εk 
j! 
j! 
n!
j! 
k=1  j=k 
k=1 
The key point is that the coefﬁcients of the εk  coefﬁcients grow smaller and smaller with k, rather 
than approaching e as for left-to-right summation, and the sum of the coefﬁcients converges. The 
truncation error is of O(εm ), and we assume 1/k! can also be calculated to within O(εm ), e.g.  via 
Stirling’s approximation for  large k, so  the overall error  is O(εm ) and  the algorithm  is forwards 
stable. 
(vii)  Unstable.  Not backwards stable since no data, but what about forwards stability?  The problem 
is the squaring of the sine function. Suppose x = π − δ  and x� = x(1 + εm ) for some small δ  > 0. 
Then  sin(x) sin(x� ) ≈ δ (δ − εmπ ) + O(δ 2 ).  In  exact  arithmetic,  this  goes  to  zero  for  δ  = 0, 
√
εm ),then  sin(x) sin(x� ) = O(εm ), 
i.e.  x = π .  However,  it  goes  to  zero  too  rapidly:  if  δ  = O(
and an O(εm ) ﬂoating-point error in computing sin will cause the product to pass through zero. 
√
εm ), which is too slow to be considered stable. 
Therefore, this procedure only ﬁnds π  to O(
(b)  Trefethen, exercise 16.1.  Since stability under all norms is equivalent, we are free to choose � · � to be 
the L2  norm  (and  the corresponding  induced norm  for matrices),  for convenience,  since  that norm  is 
preserved by unitary matrices. 

1n−
∑ 
+ 
j=1 

n 
∑

j=k 

= εm

5 

ble.  That  is, we  need  to ﬁnd  a  δ A with  �δ A� = �A�O(εmachine )  such  that  �QA = Q(A + δ A). 
(i)  First,  we  need  to  show  that multiplication  of  A  by  a  single  unitary matrix Q  is  backwards  sta­
Since �Qδ A� = �δ A�, however,  this is equivalent to showing � �QA − QA� = �A�O(εmachine ). 
It  is  sufﬁcient  to  look  at  the  error  in  the  i j-th  element  of  QA,  i.e. 
the  error  in  computing 
∑k qik ak j .  Assuming  we  do  this  sum  by  a  straightforward  loop,  the  analysis  is  exactly  the 
same  as  in  problem  2,  except  that  there  is  an  additional  (1 + ε )  factor  in  each  term  for  the 
error in the product qik ak j  [or (1 + 2ε ) if we include the rounding of qik  to  ˜qik  = ﬂ(qik )]. Hence, 
the  error  in  the  i j-th  element  is bounded by mO(εmachine ) ∑k |qik ak j |,  and  (using  the unitarity 
of  Q,  which  implies  that  |qik | ≤ 1,  and  the  equivalence  of  norms)  this  in  turn  is  bounded  by 
errors in the individual elements of QA, again using norm equivalence, we obtain � �QA − QA� ≤ 
mO(εmachine ) ∑k |ak j | ≤ mO(εmachine ) ∑k j |ak j | ≤ mO(εmachine )�A�.  Summing m2  of these 
( �
O(1) ∑i j | QA − QA)i j | ≤ m3O(εmachine )�A�.  Thus,  we  have  proved  backwards  stability  for 
multiplying by one unitary matrix (with a very pessimistic m3  coefﬁcient, but that doesn’t matter 
here). 

Now,  we  will  show  by  induction  that  multiplying  by  k  unitary  matrices  is  backwards  stable. 
Suppose  we  have  proved  it  for  k,  and  want  to  prove  for  k + 1.  That  is,  consider  QQ1 · · · Qk A. 
for  some  �δ Ak � = O(εmachine )�A�.  Also,  from  above,  �QB = Q(B + δ B)  for  some  �δ B� = 
� 
By assumption, Q1 · · · Qk A is backwards stable, and hence B = Q1
· · · Qk A = Q1 · · · Qk (A + δ Ak ) 
Qk A = �
O(εmachine )�B�.  Furthermore,  �B� = �Q1 · · · Qk (A + δ Ak )� = �A + δ Ak � ≤ �A� + �δ Ak � = 
QQ�
�A�[1 + O(εmachine )]. Hence, 
1 · · ·
QB = Q[Q1 · · ·
= QQ1 · · ·
Qk (A + δ Ak ) + δ B] 
Qk (A +
δ A) where δ A = δ Ak + [Q1 · · · Qk ]−1 δ B and �δ A� ≤ �δ Ak � + �δ B� = O(εmachine )�A�. Q.E.D. 
in  exact  arithmetic,  but  after  ﬂoating-point  errors  it  is  unlikely  that  �X A will  be  exactly  rank  1. 
(ii)  Consider X A, where X  is some rank-1 matrix xy∗  and A has rank > 1. The product X A has rank 1 
�
Hence  it  is not backwards stable, because X A˜ will be rank 1 regardless of A˜ , and  thus  is =� X A. 
(See also example 15.2 in the text.) 

6 

MIT OpenCourseWare
http://ocw.mit.edu 

18.335J / 6.337J Introduction to Numerical Methods

 
Fal l 2010 
 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

