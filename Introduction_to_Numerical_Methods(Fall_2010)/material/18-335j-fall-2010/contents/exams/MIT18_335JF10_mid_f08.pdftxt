18.335 Midterm 
You have two hours. All problems have equal weight, but some problems may be harder than others, so try 
not to spend too much time on one problem. 

Problem 1:  Schur, backsubstitution, complexity (20 points) 
You  are  given matrices  A  (m × m),  B  (n × n),  and C  (m × n),  and want  to  solve  for  an  unknown  matrix  X 
(m × n) solving: 
AX − X B = C. 
We will do this using the Schur decompositions of A and B.  (Recall that any square matrix S can be factorized 
in the Schur form S = QU Q∗  for some unitary matrix Q and some upper-triangular matrix U .) 
B , show how to transform AX − X B = C 
(a)	 Given the Schur decompositions A = QAUAQ∗ 
A  and B = QBUBQ∗
into  new  equations  A�X � − X �B�  = C � ,  where  A�  and  B�  are  upper  triangular  and  X �  is  the  new m × n 
matrix of unknowns.  That  is,  express, A� , B� ,  and C �  in  terms of A, B,  and C  (or  their Schur  factors), 
and show how to get X  back from X � . 
(b)  Given the upper-triangular system A�X � − X �B�  = C �  from (a), give an algorithm to ﬁnd the last row of 
X � .  (Hint:  look at the last row of the equation.) 
(c)  Suppose you have computed all rows >  j of X � , give an algorithm to compute the  j-th row. 

(d)  The  combination  of  the  previous  three  parts  yields  a  backsubstitution-like  algorithm  to  solve  for  X . 
What  is  the  complexity  of  this  algorithm  (not  including  the  cost  of  computing  the  Schur  factors)? 
That is, give the leading terms in m, n for the operation count; don’t worry about constant factors [ for 
example, O(m7 ) would be ﬁne; you don’t need to specify that it is ∼  9  m7  ]. 
15

Problem 2:  Stability (20 points) 
If  a method  for  solving  Ax = b  is  backwards  stable,  what  does  that  imply  about  the  size  of  the  computed 
�
(ﬂoating-point) residual  ˜r = b − Ax˜ from the computed  ˜x?  i.e.  relate �r˜� to O(εmachine ) and any norms or 
condition numbers of A and/or b that you might need. 

Problem 3: Conjugate gradient (20 points) 
The conjugate-gradient (CG) algorithm to solve a positive-deﬁnite Hermitian system of equations Ax = b is 
given below. 

x0  = 0, r0  = b, d0  = r0 

for n = 1, 2, . . . 
∗−1 rn−1 
rn
∗−1 Adn−1 
dn

αn  = 

xn  = xn−1 + αndn−1 
rn  = rn−1 − αnAdn−1 
∗
rn
dn  = rn + dn−1 
r∗ 
n−
1

r
n
n−1
r

1 

Now, suppose instead of A being positive-deﬁnite, it is only positive-semideﬁnite—the eigenvalues λ  are 
nonnegative, but some are zero.  A (non-unique) solution to Ax = b still exists if b is in the column space of 
A, that is if b is in the span of the positive-λ  eigenvectors.  The following questions concern the application 
CG to this situation. 

(a)  Let the eigenvalues and (orthonormal) eigenvectors of A be denoted by Aq j  = λ j q j  ( j = 1, . . . , m), and 
suppose  that only  the ﬁrst k  eigenvalues λ1,...,k  are zero.  If we expand xn  in  the basis of  these eigen­
vectors q j  at each step, what does each step of CG do to the coefﬁcients of these ﬁrst k eigenvectors? 
i.e.  do these zero-λ  eigenvector components, grow, shrink, or stay the same in magnitude when we go 
from xn−1  to xn ? 

(b)  From the previous part, conclude whether the CG algorithm above will converge.  If not, explain what 
the residual �rn� does (how fast it grows, if it grows).  If it converges, explain what it converges to and 
what determines  the  rate of convergence  (it can’t be κ (A) since  that  is  inﬁnite!).  (Hint:  relate CG  to 
CG on the linear operator A restricted to the subspace of nonzero-λ  eigenvectors). 

(c)  Does  it matter  if we  choose  the  initial  guess  x0  = 0?  i.e.  what  happens  if we  choose  some  random 
initial guess x0  (and correspondingly set r0  = b − Ax0 )? 
(d)  Suggest an iterative algorithm to ﬁnd a vector in the null space of A.  (Hint:  think about b = 0.) 

Problem 4: Rayleigh quotients (20 points) 
� 
� 
Recall  that,  for a Hermitian matrix A,  the Rayleigh quotient  is  the function r(x) = x∗Ax/x∗ x.  Furthermore, 
we showed that, for any x, r(x) is between the minimum and maximum eigenvalues of A. 
B b
Let A = 
be a Hermitian matrix (for some Hermitian matrix B, some vector b, and some real 
b∗ 
γ 
number γ ).  Show that the smallest eigenvalue of B is ≥ smallest eigenvalue of A. 
� 
� 
Problem 5: Norms and SVDs (20 points) 
with �B�2  < 1.  Show that κ (A) = �A�2�A−1�2  =  1+�B�2 .
B
I 
Let A =  B
∗ 
1−�B�2 
I 
Hint:  Given  the  SVD  B = U ΣV ∗  of  B,  construct  the  SVD  of  A.  Recall  that  the  induced  norm  � · �2 
of  a matrix  is  simply  the  largest  singular  value,  and  hence  the  condition  number  is  the  ratio  of  the  largest 
to  smallest  singular  values.  (It  might  help  you  to  think  about  the  case  where  B  is  just  a  number,  to  get 
you started—what are  the eigenvectors and eigenvalues of A  in  that case?  Then for  the general case, make 
eigenvectors of A out of U  and V .) 

Problem 6: Least-squares problems (20 points) 
√
If W  is positive-deﬁnite n × n, and we deﬁne the norm �x�W  = 
x∗W x for x ∈ Cn , propose an accurate (no 
stability  proof  required),  reasonably  efﬁcient  algorithm  to  solve  the  weighted-least-squares  problem:  ﬁnd 
x ∈ Cn  to minimize �Ax − b�W  for any m × n matrix A with full column rank. 

2 

MIT OpenCourseWare
http://ocw.mit.edu 

18.335J / 6.337J Introduction to Numerical Methods 
Fall 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

