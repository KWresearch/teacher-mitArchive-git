18.335 Midterm Solutions, Fall 2010 
Problem 1:  SVD Stability (20 points) 
Consider  the  problem  of  computing  the  SVD A = U ΣV ∗  from  a matrix A  (the  input).  In  this  case, we  are 
computing  the  function  f (A) = (U , Σ,V ):  the outputs are of  the SVD are 3 matrices,  i.e.  a  triple  (U , Σ,V ) 
and not just the product U ΣV ∗ . 
(a)  In ﬂoating-point, we compute  f˜(A) = ( U˜ , Σ˜ ,V˜ ).  If  this were backwards stable,  there would be some

δ A with �δ A� = �A�O(εmachine ) such that  f (A + δ A) = ( U˜ , Σ˜ ,V˜ ), i.e.  (U˜ , Σ˜ ,V˜ ) are the exact SVD

of A + δ A. Note that this is a much stronger statement than simply that A + δ A = U˜ Σ˜ V˜ ∗ !

(b)  For  it  to be backwards stable,  (U˜ , Σ˜ ,V˜ ) would have  to be  the exact SVD of something, which would

mean  that U˜ and  V˜ would  have  to  be  exactly  unitary,  which  is  extremely  unlikely  under  roundoff

errors.

(c)  If the exact SVD of A + δ A is  f (A + δ A) = (U � , Σ� ,V � ), then stability would mean that, in addition to 
�δ A� = �A�O(εmachine ), we would also have to have �(U � , Σ� ,V � ) − (U˜ , Σ˜ ,V˜ )� = �(U , Σ,V )�O(εmachine ), 
for any norm �(U , Σ,V )� on triples (U , Σ,V ). For example, we could use �(U , Σ,V )� = max(�U �, �Σ�, �V �) 
for any matrix norm.  e.g.  for the L2  induced norm, �U � = �V � = 1 and �Σ� = �A�, so we would have 
max(�U � − U˜ �, �Σ� − Σ˜ �, �V − V˜ �) = �A�O(εmachine ), 

or equivalently


�U � − U˜ � = �A�O(εmachine ),

�Σ� − Σ˜ � = �A�O(εmachine ),

�V � − V˜ � = �A�O(εmachine ).

It is tempting to instead put �U �O(εmachine ), �Σ�O(εmachine ), and �V �O(εmachine ) on the right-
hand  sides,  but  this  appears  to  be  a much  stronger  condition  (which may well  be  true  in  SVD  algo­
rithms, but is not what was given). 

Problem 2: Least squares (20 points) 

Suppose that we want to solve the weighted least-squares problem

min �B−1 (Ax − b)�2 
x 
where B (m × m) is a nonsingular square matrix and A (m × n) has full column rank. This can be solved a bit 
trivially because we can write it down as an ordinary least squares problem 
min �A� x − b� )�2 
x 
for A�  = B−1A and b�  = B−1 b. 
A∗ � 
B−1 �∗ B−1Ax = A∗ � 
B−1 �∗ B−1 x. 
(a)  The normal equations are A�∗A� x = A�∗ b�  from ordinary least-squares, i.e. 
(b)  We  can  solve  it  exactly  as  for  the  ordinary  least-squares  problem  in  A�  and  b� .  First,  compute  A�  =

U −1L−1A  by  backsubstitution,  where  B = LU  e.g.  by  Gaussian  elimination.  Then  QR  factorize

A�  = QR  e.g.  by Householder,  then  solve  R∗ x = Q∗b�  = Q∗U −1L−1 b.  As  in  ordinary QR  for  least-

squares,  all  of  the  squareing  of  A�  has  been  cancelled  analytically,  and  both  the  right  and  left-hand

sides of this equation are multiplications of things with κ (R) = κ (B−1A) and κ (B−1 ), respectively.


1 

I should mention that there are even more efﬁcient/accurate ways to solve this problem than doing ordinary 
least-squares  with  B−1A.  LAPACK  provides  something  called  a  “generalized  QR”  factorization  A = QR, 
B = QT Z  for this, where Z  is also unitary and T  is upper triangular, to avoid the necessity of a separate LU 
factorization of B. 

Problem 3: Eigenvalues (20 points) 
· · · 
(a)  Suppose our initial guess is expanded in the eigenvectors qi  as x1 = c1q1 + c2q2 + 
(where for a ran­
n q2 + · · · )/� · · · �.  If |λ1 | = |λ2 |, 
dom x1  all the ci  are �= 0 in general), in which case xn = (c1λ1 
nq1 + c2λ2 
then  the q1  and q2  terms will grow at  the  same  rate,  and  it will never be dominated by q1  alone—xn 
will “bounce around”  in a  two-dimensional  subspace  spanned by q1  and q2 .  Note  that  the algorithm 
will not  in general converge:  for example, if λ2  = −λ1 , then the relative signs of the q1  and q2  terms 
will oscillate.  (More generally, λ2  = eiφ λ1  for some phase φ , and  the q2  term will rotate  in phase by 
φ  relative to q1  on each step.) 

n (c1q1 + c2 q2 ) + O(|λ3/λ1 |n )]/� · · · �,  and  xn  therefore  becomes 
However,  if  λ1  = λ2 ,  then  xn  = [λ1 
parallel to c1q1 + c2q2  as n → ∞ (assuming |λ3 | < |λ1 |).  But both q1  and q2  are eigenvectors with the 
same eigenvalue λ1 , so c1q1 + c2q2  is also an eigenvector of λ1 .  Therefore, this is not a problem:  we 
still get an eigenvector of λ1 .  Equivalently, all of the eigenvectors for λ  = λ1  form a subspace which 
is magniﬁed relative to other eigenvalues by multiplying repeatedly by A, as long as other eigenvalues 
= λ1  have smaller magnitude. 
(b)  Applying Lanczos to (A − µ I )2  is computationally easy because we just need to multiply by (A − µ I ) 
twice in each step (cheap if A is sparse, cost ∼ #nonzeros).  However, we have squared the condition 
number  of  A − µ I ,  and  thus  we  square  the  rate  at  which  the  largest-|λ |  eigenvalues  appear  in  the 
Krylov space, correspondingly slowing the rate (~ doubling the number of iterations) at which we get 
the  smallest  eigenvalue  (the  one  closest  to  µ ),  and  exacerbating  problems  with  roundoff  errors  and 
ghost eigenvalues. 
On the other hand, (A − µ I )−1  does not square any condition numbers, since the desired eigenvalue is 
now the largest-magnitude one, there should be no problem with ghost eigenvalues or roundoff (from 
homework)  and  Lanczos  should  converge  well.  However,  we  now  have  to  multiply  by  (A − µ I )−1 
at  each  step,  which  means  that  on  each  step  we  need  to  solve  a  linear  system  with  A − µ I .  If  A  is 
amenable to sparse-direct factorization, then we can do this once and re-use it throughout the Lanczos 
process (only somewhat slower than multiplying by A repeatedly, since the sparse-direct factorization 
generally loses some sparsity).  If sparse-direct solvers are impractical and we have to use an iterative 
solver like GMRES etcetera, then we have the expense of repeating this iterative process on each step 
of Lanczos. 

Because  of  these  tradeoffs,  neither  method  is  ideal  for  computing  interior  the  eigenvalue  closest  to 
µ—computing interior eigenvalues is tricky. 

2 

�
MIT OpenCourseWare
http://ocw.mit.edu 

18.335J / 6.337J Introduction to Numerical Methods 
Fall 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

