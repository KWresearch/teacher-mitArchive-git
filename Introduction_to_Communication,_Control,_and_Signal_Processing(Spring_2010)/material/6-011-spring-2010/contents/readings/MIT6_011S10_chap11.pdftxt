C H A P T E R 

11 

Wiener  Filtering 

INTRODUCTION 

In this chapter we will consider the use of LTI systems in order to perform minimum 
mean-square-error  (MMSE)  estimation  of  a WSS  random  process  of  interest,  given 
measurements  of  another  related  process.  The  measurements  are  applied  to  the 
input  of  the  LTI  system,  and  the  system  is  designed  to  produce  as  its  output  the 
MMSE  estimate  of  the  process  of  interest. 

We  ﬁrst  develop  the  results  in  discrete  time,  and  for  convenience  assume  (unless 
otherwise stated) that the processes we deal with are zero-mean.  We will then show 
that  exactly  analogous  results  apply  in  continuous  time,  although  their  derivation 
is  slightly  diﬀerent  in  certain  parts. 

Our  problem  in  the  DT  case  may  be  stated  in  terms  of  Figure  11.1. 

Here  x[n]  is  a  WSS  random  process  that  we  have  measurements  of.  We  want 
to  determine  the  unit  sample  response  or  frequency  response  of  the  above  LTI 
system such that the ﬁlter output yb[n]  is the minimum-mean-square-error  (MMSE) 
estimate  of  some  “target”  process  y [n]  that  is  jointly WSS  with  x[n].  Deﬁning  the 
error  e[n]  as 
Δ 
e[n] = yb[n] − y [n] , 
(11.1) 
we  wish  to  carry  out  the  following  minimization: 
min ǫ = E {e 2 [n]}  .	
]· 
h[
The resulting ﬁlter h[n] is called the Wiener ﬁlter for estimation of y [n] from x[n]. 

(11.2) 

In  some  contexts  it  is  appropriate  or  convenient  to  restrict  the  ﬁlter  to  be  an 
FIR  (ﬁnite-duration  impulse  response)  ﬁlter  of  length  N ,  e.g.  h[n]  =  0  except  in 
the  interval  0  ≤  n  ≤  N  − 1.  In  other  contexts  the  ﬁlter  impulse  response  can 
be  of  inﬁnite  duration  and  may  either  be  restricted  to  be  causal  or  allowed  to 
be  noncausal.  In  the  next  section  we  discuss  the  FIR  and  general  noncausal  IIR 

x[n] 

�  LTI  h[n] 

�	 yb[n] =  estimate 
y [n] =  target  process 
FIGURE  11.1  DT  LTI  ﬁlter  for  linear MMSE  estimation. 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010	

195 

196  Chapter  11 

Wiener  Filtering 

(inﬁnite-duration  impulse  response)  cases.  A  later  section  deals  with  the  more 
involved  case  where  the  ﬁlter  is  IIR  but  restricted  to  be  causal. 

If x[n] = y [n] + v [n] where y [n] is a signal and v [n] is noise (both random processes), 
then  the  above  estimation  problem  is  called  a  ﬁltering  problem.  If  y [n] = x[n + n0 ] 
with  n0  positive,  and  if  h[n]  is  restricted  to  be  causal,  then  we  have  a  prediction 
problem.  Both  ﬁt  within  the  same  general  framework,  but  the  solution  under  the 
restriction  that  h[n]  be  causal  is  more  subtle. 

11.1  NONCAUSAL  DT  WIENER  FILTER 

(11.3) 

To determine the optimal choice for h[n] in (11.2), we ﬁrst expand the error criterion 
in  (11.2): 
h[k ]x[n − k ] − y [n] !2 
ǫ = E  
Ã
+∞X 
. 
 
 
k −∞=
The  impulse  response  values  that  minimize  ǫ  can  then  be  obtained  by  setting 
∂ ǫ 
=  0  for  all  values  of  m  for  which  h[m]  is  not  restricted  to  be  zero  (or 
∂h[m]

otherwise  pre-speciﬁed):

 
= E  
! 
ÃX 
h[k ]x[n − k ] − y [n]  x[n − m] 
2 
 
 
k 
} 
{z
|
e[n] 
The  above  equation  implies  that 
E {e[n]x[n − m]} = 0,  or 
Rex [m] = 0,  for  all m  for  which  h[m]  can  be  freely  chosen. 

∂ ǫ 
∂h[m] 

= 0  . 

(11.4) 

(11.5) 

You  may  recognize  the  above  equation  (or  constraint)  on  the  relation  between  the 
input  and  the  error  as  the  familiar  orthogonality  principle:  for  the  optimal  ﬁlter, 
the  error  is  orthogonal  to  al l  the  data  that  is  used  to  form  the  estimate.  Under  our 
assumption  of  zero-mean  x[n],  orthogonality  is  equivalent  to  uncorrelatedness.  As 
we  will  show  shortly,  the  orthogonality  principle  also  applies  in  continuous  time. 

Note  that 

Rex [m] = E {e[n]x[n − m]} 
= E {(yb[n] − y [n])x[n − m]} 
= R [m] − Ryx [m]  . 
b
yx
Therefore,  an  alternative  way  of  stating  the  orthogonality  principle  (11.5)  is  that 
Ryx [m] = Ryx [m]  for  all  appropriate m . 
b
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

(11.6) 

(11.7) 

Section  11.1 

Noncausal  DT Wiener  Filter  197 

or  equivalently, 

h[k ]Rxx [m − k ] = Ryx [m] 

In other words,  for the optimal system,  the cross-correlation between the  input and 
output  of  the  estimator  equals  the  cross-correlation  between  the  input  and  target 
output. 
To  actually  ﬁnd  the  impulse  response  values,  observe  that  since  yb[n]  is  obtained 
by  ﬁltering  x[n]  through  an  LTI  system  with  impulse  response  h[n],  the  following 
relationship  applies: 
Ryx [m] = h[m] ∗ Rxx [m]  . 
(11.8) 
b
Combining  this  with  the  alternative  statement  of  the  orthogonality  condition,  we 
can  write 
h[m] ∗ Rxx [m] = Ryx [m]  , 
X 
k 
Equation  (11.10)  represents  a  set  of  linear  equations  to  be  solved  for  the  impulse 
response  values.  If  the  ﬁlter  is  FIR  of  length N ,  then  there  are N  equations  in  the 
N  unrestricted  values  of  h[n].  For  instance,  suppose  that  h[n]  is  restricted  to  be 
zero  except  for n ∈  [0, N − 1].  The  condition  (11.10)  then yields  as many  equations 
as  unknowns,  which  can  be  arranged  in  the  following  matrix  form,  which  you may 
recognize  as  the  appropriate  form  of  the  normal  equations  for  LMMSE  estimation, 
which  we  introduced  in  Chapter  8: 
Ryx [0]   
h[0]    
Rxx [1 − N ]    
 
Rxx [0] 
Rxx [−1] 
· · · 
Ryx [1]   .
h[1]   = 
· · ·  Rxx [2 − N ]   

Rxx [0] 
Rxx [1] 

 
 

. 
. 
. 
. 

 
 

.
.
. 
. 
. 
. 
.
.
 
  
  
 
. 
. 
. 
. 
.
.
Ryx [N  − 1] 
h[N  − 1] 
Rxx [N  − 1]  Rxx [N  − 2] 
Rxx [0] 
· · · 
(11.11) 
These  equations  can  now be  solved  for  the  impulse  response  values.  Because  of  the 
particular  structure  of  these  equations,  there  are  eﬃcient  methods  for  solving  for 
the  unknown  parameters,  but  further  discussion  of  these  methods  is  beyond  the 
scope  of  our  course. 

(11.9) 

(11.10) 

In  the  case  of  an  IIR  ﬁlter,  equation  (11.10)  must  hold  for  an  inﬁnite  number  of 
values  of  m  and,  therefore,  cannot  simply  be  solved  by  the  methods  used  for  a 
ﬁnite  number  of  linear  equations.  However,  if  h[n]  is  not  restricted  to  be  causal  or 
FIR,  the  equation  (11.10)  must  hold  for  all  values  of  m  from  −∞  to  +∞,  so  the 
z-transform  can  be  applied  to  equation  (11.10)  to  obtain 

H (z )Sxx (z ) = Syx (z ) 

(11.12) 

The  optimal  transfer  function,  i.e.  the  transfer  function  of  the  resulting  (Wiener) 
ﬁlter,  is  then 

H (z ) = Syx (z )/Sxx (z ) 

(11.13) 

If  either  of  the  correlation  functions  involved  in  this  calculation  does  not  possess 
a  z-transform  but  if  both  possess  Fourier  transforms,  then  the  calculation  can  be 
carried  out  in  the  Fourier  transform  domain. 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

198  Chapter  11 

Wiener  Filtering 

Note  the  similarity  between  the  above  expression  for  the  optimal  ﬁlter  and  the 
expression we  obtained  in Chapters  5  and  7  for  the  gain  σY X /σXX  that multiplies 
a  zero-mean  random  variable X  to  produce  the LMMSE  estimator  for  a  zero-mean 
random  variables  Y .  In  eﬀect,  by  going  to  the  transform  domain  or  frequency 
domain, we  have  decoupled  the  design  into  a  problem  that —  at  each  frequency — 
is  as  simple  as  the  one  we  solved  in  the  earlier  chapters. 

As  we  will  see  shortly,  in  continuous  time  the  results  are  exactly  the  same: 
Ryx (τ ) = Ryx (τ ), 
b
h(τ ) ∗ Rxx (τ ) = Ryx (τ ), 
H (s)Sxx (s) = Syx (s),  and 

(11.14) 

(11.15) 
(11.16) 

H (s) = Syx (s)/Sxx (s) 

(11.17) 

The  mean-square-error  corresponding  to  the  optimum  ﬁlter,  i.e.  the  minimum 
MSE,  can  be  determined  by  straightforward  computation.  We  leave  you  to  show 
that 

(11.18) 

Ree [m] = Ryy [m] − R [m] = Ryy [m] − h[m] ∗ Rxy [m] 
b
yy
where  h[m]  is  the  impulse  response  of  the  optimal  ﬁlter.  The  MMSE  is  then  just 
Ree [0].  It  is  illuminating  to  rewrite  this  in  the  frequency  domain,  but  dropping  the 
argument  ejΩ  on  the power  spectra S (ejΩ )  and  frequency  response H (ejΩ ) below 
∗∗
to  avoid  notational  clutter: 
1  Z  π 
2π 
−π 
1  Z  π 
(Syy  − H Sxy ) dΩ 
2π 
−π 
1  Z  π 
Syy Sxx ´ 
Syy ³1 − 
SyxSxy
dΩ 
2π 
−π 
1  Z  π 
Syy ³1 − ρyxρyx ∗  ´ 
dΩ  . 
2π 
−π 

MMSE = Ree [0] = 

(11.19) 

See dΩ 

= 

= 

= 

The  function  ρyx (ejΩ )  deﬁned  by 

ρyx (ejΩ ) = 

Syx (ejΩ ) 
pSyy (ejΩ )Sxx
(ejΩ )
evidently plays the role of a frequency-domain correlation coeﬃcient (compare with 
our  earlier  deﬁnition  of  the  correlation  coeﬃcient  between  two  random  variables). 
This function is sometimes referred to as the coherence function of the two processes. 
Again, note the similarity of this expression to the expression σY Y  (1− ρ2 
) that we 
Y X 
obtained  in  a  previous  lecture  for  the  (minimum) mean-square-error  after  LMMSE 

(11.20) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  11.1 

Noncausal  DT Wiener  Filter  199 

estimation  of  a  random  variable  Y  using  measurements  of  a  random  variable X . 

EXAMPLE  11.1 

Signal  Estimation  in  Noise  (Filtering) 

Consider  a  situation  in which  x[n],  the  sum  of  a  target  process  y [n]  and  noise  v [n], 
is  observed: 

(11.21) 
x[n] = y [n] + v [n] . 
We  would  like  to  estimate  y [n]  from  our  observations  of  x[n].  Assume  that  the 
signal  and  noise  are  uncorrelated,  i.e.  Rvy [m] = 0.  Then 

H (ejΩ ) = 

(11.22) 
(11.23) 

Rxx [m] = Ryy [m] + Rvv [m], 
Ryx [m] = Ryy [m], 
Syy (ejΩ ) 
Syy (ejΩ ) + Svv (ejΩ ) 
At  values  of  Ω  for  which  the  signal  power  is  much  greater  than  the  noise  power, 
H (ejΩ )  ≈  1.  Where  the  noise  power  is  much  greater  than  the  signal  power, 
H (ejΩ ) ≈ 0.  For  example,  when 
Syy (ejΩ ) = (1 + e−jΩ )(1 + ejΩ ) = 2(1 + cos Ω) 
and  the  noise  is  white,  the  optimal  ﬁlter  will  be  a  low-pass  ﬁlter  with  a  frequency 
response  that  is appropriately  shaped,  shown  in Figure 11.2.  Note  that  the ﬁlter  in 

. 

(11.24) 

(11.25) 

4 

3.5


3


2.5


2


1.5


1


0.5


0


S  (ejΩ
)
yy

− π 

− π/2 

H(ejΩ
) 

0 
Ω 

S  (ejΩ
)
vv

π/2 

π 

FIGURE  11.2  Optimal  ﬁlter  frequency  response,  H (ejΩ ),  input  signal  PSD  signal, 
Syy (ejΩ ),  and  PSD  of  white  noise,  Svv (ejΩ ). 

this  case  must  have  an  impulse  response  that  is  an  even  function  of  time,  since  its 
frequency  response  is  a  real  –  and  hence  even  –  function  of  frequency. 

Figure  11.3  shows  a  simulation  example  of  such  a  ﬁlter  in  action  (though  for  a 
diﬀerent  Syy (ejΩ ).  The  top  plot  is  the  PSD  of  the  signal  of  interest;  the  middle 
plot  shows  both  the  signal  s[n]  and  the measured  signal  x[n];  and  the  bottom  plot 
compares  the  estimate  of  s[n]  with  s[n]  itself. 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

200  Chapter  11 

Wiener  Filtering 

 
y
t
i
s
n
e
d
 
l
a
)
B
r
t
c
d
e
(
p
s
 
r
e
w
o
P

30
25
20
15
10
5
0
-5
-10
-0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5
Frequency
Power spectral density of AR(1) process

Syy

10
8
6
4
2
0
-2
-4
-6
-8
-10
0

Data x
Signal y

5

10

15 20 25
30 35
Sample number, n
(a) Signal and Data

40

45 50

10
8
6
4
2
0
-2
-4
-6
-8
-10

0

5

Signal estimate y 
True signal y

45 50

10

15 20 25
30 35
Sample number, n
(b) Signal and Signal Estimate

40

Wiener Filtering Example

Image by MIT OpenCourseWare, adapted from Fundamentals of Statistical
Signal Processing: Estimation Theory, Steven Kay. Prentice Hall, 1993.

FIGURE 11.3  Wiener ﬁltering example.  (From S.M. Kay, Fundamentals of Statistical

Signal Processing:  Estimation Theory, Prentice Hall, 1993.  Figures 11.9 and 11.10.)

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  11.1 

Noncausal  DT Wiener  Filter  201 

EXAMPLE  11.2 

Prediction 

Suppose  we  wish  to  predict  the measured  process  n0  steps  ahead,  so 

y [n] = x[n + n0 ]  . 

Then 

Ryx [m] = Rxx [m + n0 ] 
so  the  optimum  ﬁlter  has  system  function 

H (z ) = z n0  . 

(11.26) 

(11.27) 

(11.28) 

This  is  of  course  not  surprising:  since  we’re  allowing  the  ﬁlter  to  be  noncausal, 
prediction  is  not  a  diﬃcult  problem!  Causal  prediction  is  much  more  challenging 
and  interesting,  and  we  will  examine  it  later  in  this  chapter. 

EXAMPLE  11.3 

Deblurring  (or  Deconvolution) 

x[n] 

�  G(z ) 

v [n] 
�L� �  H (z ) 
�  xb[n] 
r[n] 
ξ [n] 

Known,  stable  system 
Wiener  ﬁlter

FIGURE  11.4  Wiener  ﬁltering  of  a  blurred  and  noisy  signal. 

In  the  Figure  11.4,  r[n]  is  a  ﬁltered  or  “blurred”  version  of  the  signal  of  interest, 
x[n], while v [n]  is additive noise that  is uncorrelated with x[n].  We wish to design a 
ﬁlter that will deblur the noisy measured signal ξ [n] and produce an estimate of the 
input  signal  x[n].  Note  that  in  the  absence  of  the  additive  noise,  the  inverse  ﬁlter 
1/G(z )  will  recover  the  input  exactly.  However,  this  is  not  a  good  solution  when 
noise  is  present,  because  the  inverse  ﬁlter  accentuates  precisely  those  frequencies 
where  the  measurement  power  is  small  relative  to  that  of  the  noise.  We  shall 
therefore  design  a Wiener  ﬁlter  to  produce  an  estimate  of  the  signal  x[n]. 

We  have  shown  that  the  cross-correlation  between  the  measured  signal,  which  is 
the  input  to  the Wiener  ﬁlter,  and  the  estimate  produced  at  its  output  is  equal  to 
the  cross-correlation  between  the  measurement  process  and  the  target  process.  In 
the  transform  domain,  the  statement  of  this  condition  is 
Sxξ (z ) = Sxξ (z ) 
b
Sξξ (z )H (z ) = S (z ) = Sxξ (z )  . 
bxξ 
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

(11.29) 

or 

(11.30) 

202  Chapter  11 

Wiener  Filtering 

We  also  know  that 

Sξξ (z ) = Svv (z ) + Sxx (z )G(z )G(1/z ) 
Sxξ (z ) = Sxr (z ) 
= Sxx (z )G(1/z ), 

(11.31) 
(11.32) 
(11.33) 

where we have (in the ﬁrst equality above) used the fact that Svr (z ) = G(1/z )Svx (z ) = 
0.  We  can  now  write 

H (z ) = 

Sxx (z )G(1/z )
Svv (z ) + Sxx (z )G(z )G(1/z ) 

. 

(11.34) 

We  leave  you  to  check  that  this  system  function  assumes  reasonable  values  in  the 
limiting cases where the noise power is very small, or very large.  It is also interesting 
to  verify  that  the  same  overall  ﬁlter  is  obtained  if  we  ﬁrst  ﬁnd  an MMSE  estimate 
rb[n]  from  ξ [n]  (as  in  Example  11.1),  and  then  pass  rb[n]  through  the  inverse  ﬁlter 
1/G(z ). 

EXAMPLE  11.4 

“De-Multiplication” 

A message  s[n]  is  transmitted  over  a multiplicative  channel  (e.g.  a  fading  channel) 
so  that  the  received  signal  r[n]  is 

r[n] = s[n]f [n]  . 

(11.35) 

Suppose  s[n]  and  f [n]  are  zero  mean  and  independent.  We  wish  to  estimate  s[n] 
from  r[n]  using  a Wiener  ﬁlter. 

Again,  we  have 

. 

Rsr [m] = Rsr [m] 
b
= h[m] ∗  Rrr [m] 
|  {z  } 
Rss [m]Rf f [m] 
But  we  also  know  that  Rsr [m] = 0.  Therefore  h[m] = 0.  This  example  emphasizes 
that  the  optimality  of  a  ﬁlter  satisfying  certain  constraints  and  minimizing  some 
criterion  does  not  necessarily  make  the  ﬁlter  a  good  one.  The  constraints  on  the 
ﬁlter  and  the  criterion  have  to  be  relevant  and  appropriate  for  the  intended  task. 
For  instance,  if f [n] was known  to be  i.i.d.  and +1 or −1 at  each  time,  then  simply 
squaring  the  received  signal  r[n]  at  any  time would have  at  least  given us  the  value 
of  s2 [n],  which would  seem  to  be more  valuable  information  than what  the Wiener 
ﬁlter  produces  in  this  case. 

(11.36) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

11.2  NONCAUSAL  CT  WIENER  FILTER 

Section  11.2 

Noncausal  CT Wiener  Filter  203 

In the previous discussion we derived and  illustrated the discrete-time Wiener ﬁlter 
for  the FIR  and  noncausal  IIR  cases.  In  this  section we  derive  the  continuous-time 
counterpart  of  the  result  for  the  noncausal  IIR  Wiener  ﬁlter.  The  DT  derivation 
involved  taking  derivatives  with  respect  to  a  (countable)  set  of  parameters  h[m], 
but  in  the CT  case  the  impulse  response  that we  seek  to  compute  is  a CT  function 
h(t),  so  the  DT  derivation  cannot  be  directly  copied.  However,  you  will  see  that 
the results take the same form as  in the DT case;  furthermore,  the derivation below 
has  a  natural  DT  counterpart,  which  provides  an  alternate  route  to  the  results  in 
the  preceding  section. 

Our  problem  is  again  stated  in  terms  of  Figure  11.5. 

Estimator 

x(t) 

�  h(t), H (jω) 

�	 yb(t) =  estimate 
y(t) =  target  process 
FIGURE  11.5  CT  LTI  ﬁlter  for  linear MMSE  estimation. 

Let  x(t)  be  a  (zero-mean)  WSS  random  process  that  we  have  measurements  of. 
We want to determine the  impulse response or  frequency response of the above LTI 
system such that the ﬁlter output yb(t) is the LMMSE estimate of some (zero-mean) 
“target”  process  y(t)  that  is  jointly WSS  with  x(t).  We  can  again  write 
Δ 
e(t) = y(t) − yb(t) 
min ǫ = E {e 2 (t)} . 
h(	 )· 
Assuming  the ﬁlter  is  stable  (or at  least has a well-deﬁned  frequency  response),  the 
process  yb(t)  is  jointly WSS  with  x(t).  Furthermore, 
E [yb(t + τ )y(t)] = h(τ ) ∗ Rxy (τ ) = Rbyy (τ )  , 
The  quantity  we  want  to  minimize  can  again  be  written  as 
ǫ = E {e 2 (t)} = Ree (0)  ,	

(11.37) 

(11.38) 

(11.39) 

where  the error autocorrelation  function Ree (τ )  is — using the deﬁnition  in  (11.37) 
—  evidently  given  by 

Ree (τ ) = Ryy (τ ) + R y (τ ) − R  y (τ ) − Ryy (τ )  . 
byb
yb
b
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

(11.40) 

204  Chapter  11 

Wiener  Filtering 

Thus 

. 

= 

= 

dω 

ǫ = E {e 2 (t)} = Ree (0) = 

1  Z 
∞ 
See (jω) dω 
2π 
−∞ 
1  Z 
∞  ³Syy (jω) + S y (jω) − S  y (jω) − Syy (jω)´ 
byb
yb
b
2π 
−∞ 
1  Z 
∞ 
(Syy  + HH ∗Sxx  − H ∗Syx  − H Sxy ) dω  ,  (11.41) 
2π 
−∞ 
where  we  have  dropped  the  argument  jω  from  the  PSDs  in  the  last  line  above  for 
notational simplicity, and have used H ∗  to denote the complex conjugate of H (jω), 
namely H (−jω).  The  expression  in  this  last  line  is  obtained  by  using  the  fact  that 
x(t) and yb(t) are the WSS input and output, respectively, of a ﬁlter whose frequency 
response  is  H (jω).  Note  also  that  because  Ryx (τ ) = Rxy (−τ )  we  have 
Syx  = Syx (jω) = Sxy (−jω) = S ∗ 
. 
(11.42) 
xy 
Our  task  is  now  to  choose  H (jω)  to  minimize  the  integral  in  (11.41).  We  can  do 
this  by  minimizing  the  integrand  for  each  ω .  The  ﬁrst  term  in  the  integrand  does 
not  involve  or  depend  on H ,  so  in  eﬀect  we  need  to  minimize 
HH ∗Sxx  − H ∗Syx  − H Sxy  = HH ∗Sxx  − H ∗Syx  − H S ∗ 
yx 
If all  the quantities  in  this  equation were  real,  this minimization would be  straight­
forward.  Even  with  a  complex H  and  Syx ,  however,  the  minimization  is  not  hard. 
The  key  to  the  minimization  is  an  elementary  technique  referred  to  as  completing 
the  square.  For  this,  we  write  the  quantity  in  (11.43)  in  terms  of  the  squared 
magnitude  of  a  term  that  is  linear  in  H .  This  leads  to  the  following  rewriting  of 
(11.43): 
Syx ∗ ´ 
³HpSxx  − √Sxx ´³H ∗pSxx  − √Sxx  − 
S ∗
Syx yx 
Syx 
(11.44) 
. 
Sxx 
In writing √Sxx , we have made use of the fact that Sxx (jω) is real and nonnegative. 
We have also  felt  free to divide by pSxx (jω) because  for any ω  where this quantity 
is 0 it can be shown that Syx (jω) = 0 also.  The optimal choice of H (jω) is therefore 
arbitrary  at  such  ω ,  as  evident  from  (11.43).  We  thus  only  need  to  compute  the 
optimal  H  at  frequencies  where pSxx (jω) > 0. 
Notice  that  the  second  term  in  parentheses  in  (11.44)  is  the  complex  conjugate 
of  the  ﬁrst  term,  so  the  product  of  these  two  terms  in  parentheses  is  real  and 
nonnegative.  Also,  the  last  term  does  not  involve  H  at  all.  To  cause  the  terms 
in  parentheses  to  vanish  and  their  product  to  thereby  become  0,  which  is  the  best 
we  can  do,  we  evidently  must  choose  as  follows  (assuming  there  are  no  additional 
constraints  such  as  causality  on  the  estimator): 

(11.43) 

Syx (jω)
Sxx (jω) 
This expression has the same form as in the DT case.  The formula for H (jω) causes 
it  to  inherit  the  symmetry  properties  of  Syx (jω),  so  H (jω)  has  a  real  part  that  is 

H (jω) = 

(11.45) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  11.3 

Causal Wiener  Filtering  205 

even  in  ω ,  and  an  imaginary  part  that  is  odd  in  ω .  Its  inverse  transform  is  thus  a 
real  impulse  response  h(t),  and  the  expression  in  (11.45)  is  the  frequency  response 
of  the  optimum  (Wiener)  ﬁlter. 

See dω 

MMSE = Ree (0) = 

With  the  choice  of  optimum  ﬁlter  frequency  response  in  (11.45),  the mean-square­
error  expression  in  (11.41)  reduces  (just  as  in  the  DT  case)  to: 
1  Z 
∞ 
2π 
−∞ 
1  Z 
∞ 
(Syy  − H Sxy ) dω 
2π 
−∞ 
1  Z 
SyxSxy ´ 
Syy ³1 − 
∞ 
dω 
2π
Syy Sxx
−∞ 
1  Z 
∞ 
Syy (1 − ρρ∗ ) dω 
2π 
−∞ 
where  the  function  ρ(jω)  is  deﬁned  by 
Syx (jω)
pSyy (jω)Sxx (jω) 
and  evidently  plays  the  role  of  a  (complex)  frequency-by-frequency  correlation  co­
eﬃcient, analogous to that played by the correlation coeﬃcient of random variables 
Y  and X . 

ρ(jω) = 

(11.47) 

(11.46) 

= 

= 

= 

11.2.1  Orthogonality  Property 

Rearranging  the  equation  for  the  optimal Wiener  ﬁlter,  we  ﬁnd 

H Sxx  = Syx 

(11.48) 

or 

(11.49) 

or  equivalently 

Syx  = Syx  , 
b
(11.50) 
Ryx (τ ) = Ryx (τ )  for  all  τ . 
b
Again,  for  the  optimal  system,  the  cross-correlation  between  the  input  and  output 
of  the  estimator  equals  the  cross-correlation  between  the  input  and  target  output. 
Yet another way to state the above result is via the following orthogonality property: 
Rex (τ ) = R (τ ) − Ryx (τ ) = 0  for  all  τ . 
b
yx
In  other  words,  for  the  optimal  system,  the  error  is  orthogonal  to  the  data. 
11.3  CAUSAL  WIENER  FILTERING 

(11.51) 

In  the  preceding  discussion  we  developed  the Wiener  ﬁlter  with  no  restrictions  on 
the  ﬁlter  frequency  response  H (jω).  This  allowed  us  to  minimize  a  frequency-
domain  integral by  choosing H (jω)  at  each ω  to minimize  the  integrand.  However, 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

206  Chapter  11 

Wiener  Filtering 

if we constrain the ﬁlter to be causal,  then the frequency response cannot be chosen 
arbitrarily at each  frequency,  so the previous approach needs to be modiﬁed.  It can 
be  shown  that  for  a  causal  system  the  real  part  of  H (jω)  can  be  determined  from 
the  imaginary  part,  and  vice  versa,  using  what  is  known  as  a  Hilbert  transform. 
This shows that H (jω)  is constrained  in the causal case.  (We shall not need to deal 
explicitly  with  the  particular  constraint  relating  the  real  and  imaginary  parts  of 
H (jω),  so we will not pursue  the Hilbert  transform  connection here.)  The develop­
ment of the Wiener ﬁlter in the causal case is therefore subtler than the unrestricted 
case,  but  you  know  enough  now  to  be  able  to  follow  the  argument. 

Recall  our  problem,  described  in  terms  of  Figure  11.6. 
Estimator 

x(t) 

�  h(t), H (jω) 

�  yb(t) =  estimate 
y(t) =  target  process 
FIGURE  11.6  Representation  of  LMMSE  estimation  using  an  LTI  system. 

The  input  x(t)  is  a  (zero-mean) WSS  random  process  that  we  have  measurements 
of,  and  we  want  to  determine  the  impulse  response  or  frequency  response  of  the 
above  LTI  system  such  that  the  ﬁlter  output  yb(t)  is  the  LMMSE  estimate  of  some 
(zero-mean)  “target”  process  y(t)  that  is  jointly WSS  with  x(t): 
Δ 
e(t) = y(t) − yb(t) 
min ǫ = E {e 2 (t)}  . 
(11.52) 
h( )· 
We  shall  now  require,  however,  that  the  ﬁlter  be  causal.  This  is  essential  in,  for 
example,  the  problem  of  prediction,  where  y(t) = x(t + t0 )  with  t0  > 0. 
We  have  already  seen  that  the  quantity  we  want  to  minimize  can  be  written  as 
1  Z 
∞ 
See (jω) dω 
2π 
−∞ 
1  Z 
∞  ³Syy (jω) + S (jω) − S  (jω) − S (jω)´ 
dω 
byb
yb
b
y 
2π 
y
yy
−∞ 
1  Z 
∞ 
(Syy  + HH ∗Sxx  − H ∗Syx  − H Sxy ) dω  (11.53) 
2π 
−∞ 
1  Z 
1  Z 
SyxS ∗ ´ 
∞  ¯¯¯HpSxx  − 
¯¯¯  dω +
∞  ³Syy  − 
Syx  2 
yx 
dω  . 
√Sxx 
2π 
Sxx
2π
−∞ 
−∞ 
(11.54) 
The  last  equality was  the  result  of  “completing  the  square”  on  the  integrand  in  the 
preceding  integral.  In  the case where H  is unrestricted, we can  set  the ﬁrst  integral 
of  the  last  equation  to  0  by  choosing 

ǫ = E {e 2 (t)} = Ree (0) = 

= 

= 

= 

H (jω) = 

Syx (jω)
Sxx (jω) 

(11.55) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  11.3 

Causal Wiener  Filtering  207 

at  each  frequency.  The  second  integral  of  the  last  equation  is  unaﬀected  by  our 
choice  of  H ,  and  determines  the MMSE. 
If  the Wiener  ﬁlter  is  required  to  be  causal,  then  we  have  to  deal  with  the  integral 
π  Z 
∞  ¯¯¯HpSxx  − √Sxx ¯¯¯  dω 
1 
Syx  2 
2
−∞ 
as  a  whole  when  we  minimize  it,  because  causality  imposes  constraints  on  H (jω) 
that  prevent  it  being  chosen  freely  at  each  ω .  (Because  of  the  Hilbert  transform 
relationship mentioned  earlier, we  could  for  instance  choose  the  real  part  of H (jω) 
freely,  but  then  the  imaginary  part  would  be  totally  determined.)  We  therefore 
have  to  proceed more  carefully. 

(11.56) 

or 

(11.57) 

(11.58) 

Sxx  = Mxx

Note ﬁrst that the expression we obtained for the integrand in (11.56) by completing 
the  square  is  actually not quite  as  general  as we might have made  it.  Since we may 
need to use all the ﬂexibility available to us when we tackle the constrained problem, 
we  should  explore  how  generally  we  can  complete  the  square.  Speciﬁcally,  instead 
of  using  the  real  square  root  √Sxx  of  the  PSD  Sxx ,  we  could  choose  a  complex 
square  root Mxx ,  deﬁned  by  the  requirement  that 
M ∗ 
(jω) = Mxx (jω)Mxx (−jω)  , 
Sxx
xx 
and  correspondingly  rewrite  the  criterion  in  (11.56)  as 
1  Z 
∞  ¯¯¯HMxx  − 
Syx  ¯¯¯  dω  , 
2
M ∗
2π
−∞ 
xx 
which  is  easily  veriﬁed  to  be  the  same  criterion,  although  written  diﬀerently.  The 
quantity Mxx (jω)  is termed a spectral  factor of Sxx (jω) or a modeling ﬁlter  for the 
process x.  The reason  for the  latter name  is that passing (zero-mean) unit-variance 
white noise through a ﬁlter with frequency response Mxx (jω) will produce a process 
with  the  PSD  Sxx (jω),  so  we  can  model  the  process  x  as  being  the  result  of  such 
a  ﬁltering  operation.  Note  that  the  real  square  root  √Sxx (jω)  we  used  earlier  is  a 
special  case  of  a  spectral  factor,  but  others  exist.  In  fact, multiplying √Sxx (jω) by 
an  all-pass  frequency  response  A(jω)  will  yield  a  modeling  ﬁlter: 
A(jω) pSxx (jω) = Mxx (jω)  ,
A(jω)A(−jω) = 1 . 
Conversely,  it  is  easy  to  show  that  the  frequency  response  of  any  modeling  ﬁlter 
can  be  written  as  the  product  of  an  all-pass  frequency  response  and √Sxx (jω). 
It  turns  out  that  under  fairly  mild  conditions  (which  we  shall  not  go  into  here)  a 
PSD is guaranteed to have a spectral factor that is the frequency response of a stable 
and  causal  system,  and whose  inverse  is also  the  frequency  response of a  stable  and 
causal system.  (To simplify how we talk about such factors, we shall adopt an abuse 
of  terminology  that  is  common when  talking  about Fourier  transforms,  referring  to 
the  factor  itself  —  rather  than  the  system  whose  frequency  response  is  this  factor 
—  as  being  stable  and  causal,  with  a  stable  and  causal  inverse.)  For  instance,  if 

(11.59) 

Sxx (jω) = 

ω2  + 9 
ω2  + 4 

, 

(11.60) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

208  Chapter  11 

Wiener  Filtering 

then  the  required  factor  is 

. 

Mxx (jω) = 

jω + 3 
jω + 2 
We shall limit ourselves entirely to Sxx  that have such a spectral factor, and assume 
for  the  rest  of  the  derivation  that  the  Mxx  introduced  in  the  criterion  (11.58)  is 
such  a  factor.  (Keep  in  mind  that  wherever  we  ask  for  a  stable  system  here,  we 
can  actually make  do with  a  system with  a well-deﬁned  frequency  response,  even  if 
it’s not BIBO  stable,  except  that  our  results may  then need  to be  interpreted more 
carefully.) 

(11.61) 

With  these  understandings,  it  is  evident  that  the  term  HMxx  in  the  integrand  in 
(11.58) is causal, as it is the cascade of two causal terms.  The other term, Syx/M ∗ ,xx
is  generally  not  causal,  but  we  may  separate  its  causal  part  out,  denoting  the 
transform of its causal part by [Syx/M ∗ ]+ , and the transform of its anti-causal part 
xx
by  [Syx /M ∗ ]
(In  the DT  case,  the  latter would  actually denote  the  transform  of 
xx − . 
the  strictly  anti-causal part,  i.e.,  at  times −1  and  earlier;  the  value  at  time  0 would 
be  retained  with  the  causal  part.) 

Now  consider  rewriting  (11.58)  in  the  time  domain,  using  Parseval’s  theorem.  If 
we denote  the  inverse  transform operation by I { · },  then  the  result  is  the  following 
rewriting  of  our  criterion: 
Z 
∞  ¯¯¯I {HMxx } − I {[Syx/M ∗  ]+  − I {[Syx/M ∗  ]− } ¯¯¯  dt 
2
xx
xx
−∞ 
Since  the  term  I {HMxx}  is  causal  (i.e.,  zero  for  negative  time),  the  best  we  can 
do  with  it,  as  far  as  minimizing  this  integral  is  concerned,  is  to  cancel  out  all  of 
In  other  words,  our  best  choice  is 
xx ]+ }. 
/M ∗
I {[Syx

(11.62) 

or 

HMxx 

= [Syx /M ∗  ]+  , 
xx
h  Syx (jω)  i
1 
Mxx (jω)  Mxx (−jω)  + 
Note  that  the  stability  and  causality  of  the  inverse  of Mxx  guarantee  that  this  last 
step  preserves  stability  and  causality,  respectively,  of  the  solution. 

H (jω) = 

(11.63) 

(11.64) 

. 

The  expression  in  (11.64)  is  the  solution  of  the Wiener  ﬁltering  problem  under  the 
causality  constraint.  It  is  also  evident  now  that  the  MMSE  is  larger  than  in  the 
unconstrained  (noncausal)  case  by  the  amount 
1  Z 
∞  ¯¯¯h Syx  i  ¯¯¯  dω  . 
2 
M ∗xx
2π
− 
−∞ 

ΔMMSE = 

(11.65) 

EXAMPLE  11.5 

DT  Prediction 

Although  the  preceding  results  were  developed  for  the  CT  case,  exactly  analogous 
expressions  with  obvious  modiﬁcations  (namely,  using  the  DTFT  instead  of  the 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  11.3 

Causal Wiener  Filtering  209 

CTFT,  with  integrals  from  −π  to  π  rather  than  −∞  to ∞,  etc.)  apply  to  the  DT 
case. 

Consider a process x[n]  that  is  the  result of passing  (zero-mean) white noise of unit 
variance  through  a  (modeling)  ﬁlter  with  frequency  response 
Mxx (ejΩ ) = α0  + α1 e−jΩ  , 

(11.66) 

where  both  α0  and  α1  are  assumed  nonzero.  This  ﬁlter  is  stable  and  causal,  and 
if  α1 < α0 then  the  inverse  is  stable  and  causal  too.  We  assume  this  condition 
| 
|
|
|
holds.  (If  it  doesn’t,  we  can  always  ﬁnd  another  modeling  ﬁlter  for  which  it  does, 
by  multiplying  the  present  ﬁlter  by  an  appropriate  allpass  ﬁlter.) 

Suppose we want to do causal one-step prediction for this process, so y [n] = x[n+ 1]. 
Then  Ryx [m] = Rxx [m + 1],  so 
Syx  = ejΩSxx  = ejΩMxxM ∗  . 
xx 
h Syx  i 
M ∗ +xx 
and  so  the  optimum  ﬁlter,  according  to  (11.64),  has  frequency  response 
α1 
H (ejΩ ) = 
α0  + α1 e−jΩ 
The  associated  MMSE  is  evaluated  by  the  expression  in  (11.65),  and  turns  out  to 
2  that  would  have 
be  simply  α2
0  (which  can  be  compared  with  the  value  of  α2
0  + α1
been  obtained  if  we  estimated  x[n + 1]  by  just  its mean  value,  namely  zero). 

= [ejΩMxx ]+  = α1  , 

(11.67) 

(11.68) 

Thus 

. 

(11.69) 

11.3.1  Dealing  with  Nonzero Means 

We  have  so  far  considered  the  case  where  both  x  and  y  have  zero  means  (and  the 
practical  consequence  has  been  that  we  haven’t  had  to  worry  about  their  PSDs 
having  impulses  at  the  origin).  If  their means  are nonzero,  then we  can do  a better 
job  of  estimating  y(t)  if we  allow  ourselves  to  adjust  the  estimates produced by  the 
LTI  system,  by  adding  appropriate  constants  (to  make  an  aﬃne  estimator).  For 
this, we  can ﬁrst  consider  the problem  of  estimating  y − µy  from x − µx ,  illustrated 
in  Figure  11.7 

Estimator 

x(t) − µx 

�  h(t), H (jω) 

�  yb(t) − µy  =  estimate 
y(t) − µy  =  target  process 
FIGURE  11.7  Wiener  ﬁltering  with  non-zero  means. 

Denoting  the  transforms  of  the  covariances  Cxx (τ )  and  Cyx (τ )  by  Dxx (jω)  and 
Dyx (jω)  respectively  (these  transforms  are  sometimes  referred  to  as  covariance 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

210  Chapter  11 

Wiener  Filtering 

PSDs),  the  optimal  unconstrained  Wiener  ﬁlter  for  our  task  will  evidently  have  a 
frequency  response  given  by 

H (jω) = 

Dyx (jω)
Dxx (jω) 

. 

(11.70) 

We can  then add µy  to  the output of  this ﬁlter  to get our LMMSE estimate of y(t). 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

MIT OpenCourseWare
http://ocw.mit.edu 

6.011 Introduction to Communication, Control, and Signal Processing 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

