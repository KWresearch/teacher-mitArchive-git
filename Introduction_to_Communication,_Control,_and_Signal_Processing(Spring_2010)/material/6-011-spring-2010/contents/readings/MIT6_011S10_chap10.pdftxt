C H A P T E R 

10 

Power  Spectral Density 

INTRODUCTION 

Understanding how  the  strength  of  a  signal  is distributed  in  the  frequency domain, 
relative  to  the  strengths  of  other  ambient  signals,  is  central  to  the  design  of  any 
LTI  ﬁlter  intended  to  extract  or  suppress  the  signal.  We  know  this well  in  the  case 
of  deterministic  signals,  and  it  turns  out  to  be  just  as  true  in  the  case  of  random 
signals.  For  instance,  if  a  measured  waveform  is  an  audio  signal  (modeled  as  a 
random  process  since  the  speciﬁc  audio  signal  isn’t  known)  with  additive  distur­
bance  signals,  you  might  want  to  build  a  lowpass  LTI  ﬁlter  to  extract  the  audio 
and  suppress  the  disturbance  signals.  We  would  need  to  decide  where  to  place  the 
cutoﬀ  frequency  of  the  ﬁlter. 

There  are  two  immediate  challenges  we  confront  in  trying  to  ﬁnd  an  appropriate 
frequency-domain  description  for  a WSS  random  process.  First,  individual  sample 
functions  typically  don’t have  transforms  that  are  ordinary, well-behaved  functions 
of  frequency;  rather,  their  transforms  are  only  deﬁned  in  the  sense  of  generalized 
functions.  Second,  since  the  particular  sample  function  is  determined  as  the  out­
come of a probabilistic experiment,  its  features will actually be random,  so we have 
to  search  for  features  of  the  transforms  that  are  representative  of  the  whole  class 
of  sample  functions,  i.e.,  of  the  random  process  as  a  whole. 

It  turns  out  that  the  key  is  to  focus  on  the  expected  power  in  the  signal.  This  is  a 
measure  of  signal  strength  that  meshes  nicely  with  the  second-moment  characteri­
zations  we  have  for WSS  processes,  as  we  show  in  this  chapter.  For  a  process  that 
is  second-order  ergodic,  this  will  also  correspond  to  the  time  average  power  in  any 
realization.  We  introduce  the  discussion  using  the  case  of  CT WSS  processes,  but 
the  DT  case  follows  very  similarly. 

10.1	 EXPECTED  INSTANTANEOUS  POWER  AND  POWER  SPECTRAL 
DENSITY 

Motivated  by  situations  in  which  x(t)  is  the  voltage  across  (or  current  through)  a 
unit  resistor, we  refer  to  x2 (t)  as  the  instantaneous  power  in  the  signal  x(t).  When 
x(t)  is WSS,  the  expected  instantaneous  power  is  given  by 

1  Z 
2π 
−∞ 
c	
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

E [x 2 (t)] = Rxx (0) = 

∞ 

Sxx (jω) dω  , 

(10.1) 

183

184  Chapter  10 

Power  Spectral  Density 

where  Sxx (jω)  is  the  CTFT  of  the  autocorrelation  function  Rxx (τ ).  Furthermore, 
when  x(t)  is  ergodic  in  correlation,  so  that  time  averages  and  ensemble  averages 
are  equal  in  correlation  computations,  then  (10.1)  also  represents  the  time-average 
power  in  any  ensemble  member.  Note  that  since  Rxx (τ ) =  Rxx (−τ ),  we  know 
Sxx (jω)  is  always  real  and  even  in  ω ;  a  simpler  notation  such  as  Pxx (ω)  might 
therefore  have  been more  appropriate  for  it,  but  we  shall  stick  to  Sxx (jω)  to  avoid 
a  proliferation  of  notational  conventions,  and  to  keep  apparent  the  fact  that  this 
quantity  is  the  Fourier  transform  of  Rxx (τ ). 
The  integral  above  suggests  that  we  might  be  able  to  consider  the  expected  (in­
stantaneous)  power  (or,  assuming  the  process  is  ergodic,  the  time-average  power) 
in  a  frequency  band  of  width  dω  to  be  given  by  (1/2π)Sxx (jω) dω .  To  examine 
this  thought  further,  consider  extracting  a  band  of  frequency  components  of  x(t) 
by  passing  x(t)  through  an  ideal  bandpass  ﬁlter,  shown  in  Figure  10.1. 

x(t) 

�  H (jω) 

�  y(t) 

�Δ� 

H (jω) 
� 
1 

� Δ �

−ω0 
FIGURE  10.1  Ideal bandpass ﬁlter  to  extract a band of  frequencies  from  input, x(t). 

ω0 

� 
ω

Because  of  the  way  we  are  obtaining  y(t)  from  x(t),  the  expected  power  in  the 
output  y(t)  can  be  interpreted  as  the  expected  power  that  x(t)  has  in  the  selected 
passband.  Using  the  fact  that 

(10.2) 

E {y 2 (t)} = Ryy (0) = 
Thus 

Syy (jω) = |H (jω)|2Sxx (jω)  , 
we  see  that  this  expected  power  can  be  computed  as 
1  Z  +∞ 
1  Z 
Syy (jω) dω = 
2π 
2π
−∞ 
passband 
1  Z 
Sxx (jω) dω 
2π  passband 
is  indeed  the  expected  power  of  x(t)  in  the  passband.  It  is  therefore  reasonable  to 
call Sxx (jω)  the power  spectral  density  (PSD)  of x(t).  Note  that  the  instanta­
neous power of y(t), and hence the expected instantaneous power E [y2 (t)], is always 
nonnegative,  no  matter  how  narrow  the  passband,  It  follows  that,  in  addition  to 
being  real  and  even  in  ω ,  the  PSD  is  always  nonnegative,  Sxx (jω)  ≥  0  for  all  ω . 
While  the PSD Sxx (jω)  is  the Fourier  transform  of  the  autocorrelation  function,  it 

Sxx (jω) dω  . 

(10.3) 

(10.4) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section 10.2 

Einstein-Wiener-Khinchin Theorem on Expected Time-Averaged Power  185 

is  useful  to  have  a  name  for  the  Laplace  transform  of  the  autocorrelation  function; 
we  shall  refer  to  Sxx (s)  as  the  complex  PSD. 
Exactly  parallel  results  apply  for  the  DT  case,  leading  to  the  conclusion  that 
Sxx (ejΩ )  is  the  power  spectral  density  of  x[n]. 

10.2	 EINSTEIN-WIENER-KHINCHIN  THEOREM  ON  EXPECTED  TIME­
AVERAGED  POWER 

The previous  section deﬁned  the PSD as  the  transform of  the autocorrelation  func­
tion,  and  provided  an  interpretation  of  this  transform.  We  now  develop  an  alter­
native  route  to  the  PSD.  Consider  a  random  realization  x(t)  of  a  WSS  process. 
We  have  already  mentioned  the  diﬃculties  with  trying  to  take  the  CTFT  of  x(t) 
directly,  so  we  proceed  indirectly.  Let  xT (t)  be  the  signal  obtained  by  windowing 
x(t),  so  it  equals  x(t)  in  the  interval  (−T , T )  but  is  0  outside  this  interval.  Thus 
xT (t) = wT (t) x(t)  ,	
(10.5) 

(10.7) 

where we deﬁne  the window  function wT (t)  to be 1  for  t < T  and 0 otherwise.  Let 
| |
XT (jω) denote the Fourier transform of xT (t); note that because the signal xT (t) is 
nonzero  only  over  the  ﬁnite  interval  (−T , T ),  its  Fourier  transform  is  typically well 
deﬁned.  We  know  that  the  energy  spectral  density  (ESD)  S xx (jω)  of  xT (t)  is 
given  by 

wT (α)wT (α − τ )x(α)x(α − τ ) dα ⇔ 

S xx (jω) = |XT (jω)|2	
(10.6) 
and that this ESD is actually the Fourier transform of xT (τ )∗x←T  (τ ), where x←T  (t) = 
xT (−t).  We  thus  have  the  CTFT  pair 
Z 
∞ 
wT (α)wT (α − τ )x(α)x(α − τ ) dα ⇔ |XT (jω)|2  , 
xT (τ ) ∗ x←T  (τ ) = 
−∞ 
or,  dividing  both  sides  by  2T  (which  is  valid,  since  scaling  a  signal  by  a  constant 
scales  its  Fourier  transform  by  the  same  amount), 
1  Z 
2T
−∞ 
The quantity on the right is what we deﬁned (for the DT case) as the periodogram 
of  the  ﬁnite-length  signal  xT (t). 
Because  the  Fourier  transform  operation  is  linear,  the  Fourier  transform  of  the 
expected  value  of  a  signal  is  the  expected  value  of  the  Fourier  transform.  We 
may  therefore  take  expectations  of  both  sides  in  the  preceding  equation.  Since 
E [x(α)x(α − τ )] = Rxx (τ ),  we  conclude  that 
1 
E [|XT (jω)| 2 ]  ,	
Rxx (τ )Λ(τ ) ⇔ 
2T
where  Λ(τ )  is  a  triangular  pulse  of  height  1  at  the  origin  and  decaying  to  0  at 
|τ |  = 2T ,  the  result  of  carrying  out  the  convolution  wT  ∗ wT← (τ )  and  dividing  by 
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

(10.9) 

∞	

1 
2 
2T  |XT (jω)|

. 

(10.8) 

186  Chapter  10 

Power  Spectral  Density 

2T .  Now  taking  the  limit  as  T  goes  to ∞,  we  arrive  at  the  result

1

E [|XT (jω)| 2 ]  . 
Rxx ⇔ Sxx
(jω) =  lim 
(τ )
T →∞  2T
This  is  the  Einstein-Wiener-Khinchin  theorem  (proved  by  Wiener,  and  inde­
pendently  by  Khinchin,  in  the  early  1930’s,  but  —  as  only  recently  recognized  — 
stated  by  Einstein  in  1914). 

(10.10) 

The  result  is  important  to  us  because  it  underlies  a  basic  method  for  estimating 
Sxx (jω):  with  a  given  T ,  compute  the  periodogram  for  several  realizations  of  the 
random  process  (i.e.,  in  several  independent  experiments),  and  average  the  results. 
Increasing  the  number  of  realizations  over  which  the  averaging  is  done  will  reduce 
the  noise  in  the  estimate,  while  repeating  the  entire  procedure  for  larger  T  will 
improve  the  frequency  resolution  of  the  estimate. 

10.2.1  System  Identiﬁcation  Using  Random  Processes  as  Input 

Consider  the  problem  of  determining  or  “identifying”  the  impulse  response  h[n] 
of  a  stable  LTI  system  from  measurements  of  the  input  x[n]  and  output  y [n],  as 
indicated  in  Figure  10.2. 

x[n] 

� 

h[n] 

�  y [n] 

FIGURE  10.2  System  with  impulse  response  h[n]  to  be  determined. 

The  most  straightforward  approach  is  to  choose  the  input  to  be  a  unit  impulse 
x[n] =  δ [n],  and  to  measure  the  corresponding  output  y [n],  which  by  deﬁnition  is 
the  impulse  response.  It  is  often  the  case  in  practice,  however,  that we  do  not wish 
to —  or  are  unable  to —  pick  this  simple  input. 

For instance, to obtain a reliable estimate of the impulse response in the presence of 
measurement errors, we may wish  to use a more “energetic”  input,  one  that excites 
the  system  more  strongly.  There  are  generally  limits  to  the  amplitude  we  can  use 
on  the  input  signal,  so  to  get  more  energy  we  have  to  cause  the  input  to  act  over 
a  longer  time.  We  could  then  compute  h[n]  by  evaluating  the  inverse  transform 
of  H (ejΩ ),  which  in  turn  could  be  determined  as  the  ratio  Y (ejΩ )/X (ejΩ ).  Care 
has  to  be  taken,  however,  to  ensure  that  X (ejΩ )  =  0  for  any  Ω;  in  other  words, 
the  input  has  to  be  suﬃciently  “rich”.  In  particular,  the  input  cannot  be  just  a 
ﬁnite  linear combination of sinusoids (unless the LTI system  is such that knowledge 
of  its  frequency  response  at  a  ﬁnite  number  of  frequencies  serves  to  determine  the 
frequency  response  at  all  frequencies  —  which  would  be  the  case  with  a  lumped 
system,  i.e.,  a  ﬁnite-order  system,  except  that  one  would  need  to  know  an  upper 
bound  on  the  order  of  the  system  so  as  to  have  a  suﬃcient  number  of  sinusoids 
combined  in  the  input). 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

6
Section 10.2 

Einstein-Wiener-Khinchin Theorem on Expected Time-Averaged Power  187 

The  above  constraints  might  suggest  using  a  randomly  generated  input  signal.  For 
instance, suppose we let the input be a Bernoul li process, with x[n] for each n taking 
the  value  +1  or  −1  with  equal  probability,  independently  of  the  values  taken  at 
other  times.  This  process  is  (strict- and)  wide-sense  stationary,  with  mean  value 
0  and  autocorrelation  function  Rxx [m] =  δ [m].  The  corresponding  power  spectral 
density  Sxx (ejΩ )  is  ﬂat  at  the  value  1  over  the  entire  frequency  range  Ω ∈  [−π , π ]; 
evidently  the  expected  power  of  x[n]  is  distributed  evenly  over  all  frequencies.  A 
process  with  ﬂat  power  spectrum  is  referred  to  as  a  white  process  (a  term  that 
is motivated  by  the  rough  notion  that white  light  contains  all  visible  frequencies  in 
equal  amounts);  a  process  that  is  not  white  is  termed  colored. 

Now  consider what  the DTFT X (ejΩ ) might  look  like  for a  typical  sample  function 
of  a  Bernoulli  process.  A  typical  sample  function  is  not  absolutely  summable  or 
square  summable,  and  so  does  not  fall  into  either  of  the  categories  for  which  we 
know  that  there  are  nicely  behaved  DTFTs.  We  might  expect  that  the  DTFT 
exists  in  some  generalized-function  sense  (since  the  sample  functions  are  bounded, 
and  therefore  do  not  grow  faster  than  polynomially  with  n  for  large  n ),  and  this 
|
|
is  indeed  the  case,  but  it  is not  a  simple  generalized  function;  not  even  as  “nice”  as 
the  impulses  or  impulse  trains  or  doublets  that  we  are  familiar  with. 

When  the  input  x[n]  is  a  Bernoulli  process,  the  output  y [n]  will  also  be  a  WSS 
random  process,  and  Y (ejΩ )  will  again  not  be  a  pleasant  transform  to  deal  with. 
However,  recall  that 

Ryx [m] = h[m] ∗ Rxx [m]  , 
so if we can estimate the cross-correlation of the input and output, we can determine 
the  impulse  response  (for  this  case  where  Rxx [m] =  δ [m])  as  h[m] =  Ryx [m].  For 
a  more  general  random  process  at  the  input,  with  a  more  general  Rxx [m],  we  can 
solve  for  H (ejΩ )  by  taking  the  Fourier  transform  of  (10.11),  obtaining 

(10.11) 

H (ejΩ ) = 

Syx (ejΩ ) 
Sxx (ejΩ ) 

. 

(10.12) 

If  the  input  is not accessible,  and only  its  autocorrelation  (or  equivalently  its PSD) 
is  known,  then  we  can  still  determine  the  magnitude  of  the  frequency  response,  as 
long  as  we  can  estimate  the  autocorrelation  (or  PSD)  of  the  output.  In  this  case, 
we  have 

Syy (ejΩ ) 
2 
|H (ejΩ )| = 
Sxx (ejΩ ) 
Given  additional  constraints  or  knowledge  about  the  system,  one  can  often  deter­
mine a lot more (or even everything) about H (ejω ) from knowledge of its magnitude. 

. 

(10.13) 

10.2.2 

Invoking  Ergodicity 

How does one estimate Ryx [m] and/or Rxx [m] in an example such as the one above? 
The  usual  procedure  is  to  assume  (or  prove)  that  the  signals  x  and  y  are  ergodic. 
What  ergodicity  permits  —  as  we  have  noted  earlier  —  is  the  replacement  of  an 
expectation  or  ensemble  average  by  a  time  average,  when  computing  the  expected 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

188  Chapter  10 

Power  Spectral  Density 

lim 
N→∞ 

E {x[n]} = 

value of various  functions of  random variables associated with a  stationary  random 
process.  Thus  a WSS  process  x[n]  would  be  called  mean-ergodic  if 
1  X 
N
2N  + 1 
k=−N 
(The  convergence  on  the  right  hand  side  involves  a  sequence  of  random  variables, 
so there are subtleties involved  in deﬁning it precisely, but we bypass these issues in 
6.011.)  Similarly, for a pair of jointly-correlation-ergodic processes, we could replace 
the  cross-correlation  E {y [n + m]x[n]}  by  the  time  average  of  y [n + m]x[n]. 
What ergodicity generally requires is that values taken by a typical sample function 
over  time  be  representative  of  the  values  taken  across  the  ensemble.  Intuitively, 
what  this  requires  is  that  the  correlation  between  samples  taken  at  diﬀerent  times 
falls  oﬀ  fast  enough.  For  instance,  a  suﬃcient  condition  for  a  WSS  process  x[n] 
with  ﬁnite  variance  to  be  mean-ergodic  turns  out  to  be  that  its  autocovariance 
function  Cxx [m]  tends  to  0  as  |m|  tends  to  ∞,  which  is  the  case  with  most  of  the 
examples  we  deal  with  in  these  notes.  A  more  precise  (necessary  and  suﬃcient) 
condition  for  mean-ergodicity  is  that  the  time-averaged  autocovariance  function 
Cxx [m],  weighted  by  a  triangular  window,  be  0: 
1 − |m|  ¶ 
1  X µ 
L
Cxx [m] = 0 . 
lim 
L→∞  2L + 1 
L + 1 
m=−L 
A  similar  statement  holds  in  the  CT  case.  More  stringent  conditions  (involving 
fourth  moments  rather  than  just  second  moments)  are  needed  to  ensure  that  a 
process  is  second-order  ergodic;  however,  these  conditions are  typically  satisﬁed  for 
the  processes  we  consider,  where  the  correlations  decay  exponentially  with  lag. 

x[k ]  . 

(10.14) 

(10.15) 

10.2.3  Modeling  Filters  and Whitening  Filters 

There  are  various  detection  and  estimation  problems  that  are  relatively  easy  to 
formulate,  solve,  and  analyze  when  some  random  process  that  is  involved  in  the 
problem — for instance, the set of measurements — is white,  i.e., has a ﬂat spectral 
density.  When  the  process  is  colored  rather  than  white,  the  easier  results  from  the 
white  case  can  still  often  be  invoked  in  some  appropriate  way  if: 

(a)	 the  colored  process  is  the  result  of  passing  a white  process  through  some  LTI 
modeling  or  shaping  ﬁlter,  which  shapes  the  white  process  at  the  input  into 
one  that  has  the  spectral  characteristics  of  the  given  colored  process  at  the 
output;  or 
(b)	 the colored process is transformable into a white process by passing it through 
an  LTI  whitening  ﬁlter,  which  ﬂattens  out  the  spectral  characteristics  of  the 
colored  process  presented  at  the  input  into  those  of  the  white  noise  obtained 
at  the  output. 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section 10.2 

Einstein-Wiener-Khinchin Theorem on Expected Time-Averaged Power  189 

Thus,  a modeling  or  shaping ﬁlter  is one  that  converts  a white process  to  some  col­
ored  process, while  a whitening  ﬁlter  converts  a  colored  process  to  a white  process. 

An  important  result  that  follows  from  thinking  in  terms  of  modeling  ﬁlters  is  the 
following  (stated  and  justiﬁed  rather  informally  here  —  a  more  careful  treatment 
is  beyond  our  scope): 

Key  Fact:  A  real  function Rxx [m]  is  the  autocorrelation  function  of  a  real-valued 
WSS  random  process  if  and  only  if  its  transform  Sxx (ejΩ )  is  real,  even  and  non­
negative.  The  transform  in  this  case  is  the  PSD  of  the  process. 

The necessity of  these  conditions on  the  transform of  the  candidate autocorrelation 
function  follows  from  properties  we  have  already  established  for  autocorrelation 
functions  and  PSDs. 

To argue that  these conditions are also suﬃcient,  suppose Sxx (ejΩ ) has  these prop­
erties,  and  assume  for  simplicity  that  it  has  no  impulsive  part.  Then  it  has  a 
real  and  even  square  root,  which  we  may  denote  by  pSxx (ejΩ ).  Now  construct  a 
(possibly  noncausal)  modeling  ﬁlter  whose  frequency  response  H (ejΩ )  equals  this 
square  root;  the  unit-sample  reponse  of  this  ﬁlter  is  found  by  inverse-transforming 
H (ejΩ ) =  pSxx (ejΩ ).  If  we  then  apply  to  the  input  of  this  ﬁlter  a  (zero-mean) 
unit-variance white noise process,  e.g.,  a Bernoulli process  that has  equal probabil­
ities  of  taking +1  and −1  at  each DT  instant  independently  of  every  other  instant, 
then  the  output  will  be  a  WSS  process  with  PSD  given  by  |H (ejΩ )|2  =  Sxx (ejΩ ), 
and  hence  with  the  speciﬁed  autocorrelation  function. 

If  the  transform  Sxx (ejΩ )  had  an  impulse  at  the  origin,  we  could  capture  this  by 
adding an appropriate constant (determined by the impulse strength) to the output 
of  a  modeling  ﬁlter,  constructed  as  above  by  using  only  the  non-impulsive  part  of 
the transform.  For a pair of  impulses at  frequencies Ω = ±Ωo  = 0  in the transform, 
we could  similarly add a  term of  the  form A cos(Ωon + Θ), where A  is deterministic 
(and  determined  by  the  impulse  strength)  and  Θ  is  independent  of  all  other  other 
variables,  and  uniform  in  [0, 2π ]. 

Similar  statements  can  be made  in  the  CT  case. 

We illustrate below the logic involved in designing a whitening ﬁlter for a particular 
example;  the  logic  for  a  modeling  ﬁlter  is  similar  (actually,  inverse)  to  this. 

Consider  the  following  discrete-time  system  shown  in  Figure  10.3. 

x[n] 

� 

h[n] 

�  w[n] 

FIGURE  10.3  A  discrete-time  whitening  ﬁlter. 

Suppose  that  x[n]  is  a  process  with  autocorrelation  function  Rxx [m]  and  PSD 
Sxx (ejΩ ),  i.e.,  Sxx (ejΩ ) =  F {Rxx [m]}.  We  would  like  w[n]  to  be  a  white  noise 
output  with  variance  σ2  .w

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

6
190  Chapter  10 

Power  Spectral  Density 

We  know  that 

or, 

Sww (ejΩ ) = |H (ejΩ )|2 Sxx (ejΩ ) 
σ2 
w 
|H (ejΩ )|2  = 
. 
ejΩ ) 
Sxx (
This  then  tells  us  what  the  squared  magnitude  of  the  frequency  response  of  the 
LTI  system  must  be  to  obtain  a  white  noise  output  with  variance  σ2  .  If  we  have 
w
Sxx (ejΩ ) available as a  rational  function of ejΩ  (or  can model  it  that way),  then we 
can  obtain  H (ejΩ )  by  appropriate  factorization  of  |H (ejΩ )|2 . 

(10.16) 

(10.17) 

EXAMPLE  10.1  Whitening  ﬁlter 

Suppose  that 

5
4  − cos(Ω). 
Then,  to  whiten  x(t),  we  require  a  stable  LTI  ﬁlter  for  which 

Sxx (ejΩ ) = 

|H (ejΩ )|2  = 

1 
ejΩ )(1 −

(1 − 

1
2

, 

1 e−jΩ )
2 

(10.18) 

(10.19) 

or  equivalently, 

1 
1 z−1 ) 
(1 −  1
z )(1 −
2 
2
The ﬁlter is constrained to be stable  in order to produce a WSS output.  One choice 
of  H (z )  that  results  in  a  causal  ﬁlter  is 

H (z )H (1/z ) = 

(10.20) 

. 

H (z ) = 

1 
1 
1 −  2 z−1 
with  region  of  convergence  (ROC)  given  by  |z | >  1 .  This  system  function  could  be 
2 
multiplied by the system function A(z ) of any al lpass system, i.e., a system function 
satisfying  A(z )A(z−1 )  =  1,  and  still  produce  the  same  whitening  action,  because 
|A(ejΩ )|2  = 1. 

(10.21) 

, 

10.3  SAMPLING  OF  BANDLIMITED  RANDOM  PROCESSES 

A  WSS  random  process  is  termed  band limited  if  its  PSD  is  bandlimited,  i.e.,  is 
zero  for  frequencies  outside  some  ﬁnite  band.  For  deterministic  signals  that  are 
bandlimited,  we  can  sample  at  or  above  the  Nyquist  rate  and  recover  the  signal 
exactly.  We  examine  here  whether  we  can  do  the  same  with  bandlimited  random 
processes. 

In the discussion of sampling and DT processing of CT signals in your prior courses, 
the  derivations  and  discussion  rely  heavily  on  picturing  the  eﬀect  in  the  frequency 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  10.3 

Sampling  of  Bandlimited  Random  Processes  191 

domain,  i.e.,  tracking  the  Fourier  transform  of  the  continuous-time  signal  through 
the  C/D  (sampling)  and  D/C  (reconstruction)  process.  While  the  arguments  can 
alternatively  be  carried  out  directly  in  the  time  domain,  for  deterministic  ﬁnite-
energy  signals  the  frequency  domain  development  seems more  conceptually  clear. 

As  you  might  expect,  results  similar  to  the  deterministic  case  hold  for  the  re­
construction  of  bandlimited  random  processes  from  samples.  However,  since  these 
stochastic signals do not possess Fourier transforms except in the generalized sense, 
we  carry  out  the  development  for  random  processes  directly  in  the  time  domain. 
An  essentially  parallel  argument  could  have  been  used  in  the  time  domain  for  de­
terministic signals (by examining the total energy in the reconstruction error rather 
than  the  expected  instantaneous  power  in  the  reconstruction  error,  which  is  what 
we  focus  on  below). 

The basic sampling and bandlimited reconstruction process should be familiar from 
your  prior  studies  in  signals  and  systems,  and  is  depicted  in  Figure  10.4  below. 
In  this  ﬁgure  we  have  explicitly  used  bold  upper-case  symbols  for  the  signals  to 
underscore  that  they  are  random  processes. 

Xc (t) 

� 

C/D 

� 
T 

�

X[n] = Xc (nT ) 

X[n] 

� 

D/C 

� 
T 

� Yc (t) = P+∞  X[n] sinc( t−T
nT  )
n=−∞ 
where  sinc x =  sinπx 
πx 

FIGURE  10.4  C/D  and  D/C  for  random  processes. 

π , then 
For the deterministic case, we know that if xc (t) is bandlimited to less than  T
with  the  D/C  reconstruction  deﬁned  as 
yc (t) = X 
t − nT 
x[n] sinc( 
T 
n 
it  follows  that  yc (t) = xc (t).  In  the  case  of  random  processes,  what  we  show  below 
is  that,  under  the  condition  that  Sxc xc (jω),  the  power  spectral  density  of Xc (t),  is 
bandlimited  to  less  that  π ,  the  mean  square  value  of  the  error  between  Xc (t)  and 
T 
Yc (t)  is  zero;  i.e.,  if 

(10.22) 

) 

Sxc xc (jω) = 0 

|w| ≥ 

π 
T

, 

(10.23) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

192  Chapter  10 

Power  Spectral  Density 

then 

E  △
= E {[Xc (t) − Yc (t)]2 } = 0  . 
This,  in  eﬀect,  says  that  there  is  “zero  power”  in  the  error.  (An  alternative  proof 
to  the  one  below  is  outlined  in  Problem  13  at  the  end  of  this  chapter.) 

(10.24) 

To develop the above result, we expand the error and use the deﬁnitions of the C/D 
(or  sampling)  and  D/C  (or  ideal  bandlimited  interpolation)  operations  in  Figure 
10.4  to  obtain 

(t)Xc

(10.25) 

(t)}  . 

E  = E {X2 
2 (t)} − 2E {Yc
c (t)} + E {Yc 
We  ﬁrst  consider  the  last  term,  E {Yc (t)Xc (t)}: 
E {Yc (t)Xc (t)} = E {  X 
+∞
t − nT 
Xc (nT ) sinc( 
T 
n=−∞ 
=  X 
+∞
nT  − t 
Rxc xc (nT  − t) sinc( 
) 
T 
n=−∞ 
where,  in  the  last  expression,  we  have  invoked  the  symmetry  of  sinc(.)  to  change 
the  sign  of  its  argument  from  the  expression  that  precedes  it. 

) Xc (t)}

(10.26) 

(10.27) 

Equation  (10.26)  can  be  evaluated  using Parseval’s  relation  in  discrete  time, which 
states  that 
1  Z  π
X 
+∞
v [n]w[n] = 
2π 
−π 
n=∞ 
To  apply  Parseval’s  relation,  note  that  Rxc xc (nT  − t)  can  be  viewed  as  the  result 
of  the  C/D  or  sampling  process  depicted  in  Figure  10.5,  in  which  the  input  is 
considered  to  be  a  function  of  the  variable  τ : 

V (ejΩ )W ∗ (ejΩ )dΩ 

(10.28) 

Rxc xc (τ  − t) 

� 

C/D 

� Rxc xc (nT  − t) 

� 
T 
FIGURE  10.5  C/D  applied  to  Rxc xc (τ  − t). 

The  CTFT  (in  the  variable  τ )  of  Rxc xc (τ  − t)  is  e−jωtSxc xc (jω),  and  since  this  is 
bandlimited  to  ω <  π ,  the  DTFT  of  its  sampled  version  Rxc xc (nT  − t)  is 
| 
|
T
Ω 
1 
−jΩt
e  T  Sxc xc (j  ) 
T 
T

(10.29) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  10.3 

Sampling  of  Bandlimited  Random  Processes  193 

= 

Sxc xc (jω)dω 

. Consequently, 

E {Yc (t)Xc (t)} = 

t − mT 
) sinc( 
T 
t − mT 
) sinc( 
T 

Ωt 
e −j
in  the  interval  |Ω| < π .  Similarly,  the DTFT  of  sinc( nT −t )  is 
T 
T 
π 
under  the  condition  that  Sxc xc (jω)  is  bandlimited  to  ω <  T  ,
| 
|
1  Z  π 
jΩ 
Sxc xc ( 
)dΩ 
2πT 
T
−π 
1  Z  (π/T ) 
2π 
−(π/T ) 
2 (t)} 
= Rxc xc (0) = E {Xc 
Next,  we  expand  the  middle  term  in  equation  (10.25): 
2 (t)} = E {X X 
t − nT 
Xc (nT )Xc (mT ) sinc( 
E {Yc 
T
n m 
= X X 
t − mT 
Rxc xc (nT  − mT ) sinc( 
T
n m 
With  the  substitution  n − m = r ,  we  can  express  10.31  as 
2 (t)} = X 
Rxc xc (rT ) X 
t − mT 
t − mT  − rT 
) sinc( 
sinc( 
E {Yc 
T 
T
m 
r
Using  the  identity 
X 
sinc(n − θ1 )sinc(n − θ2 ) = sinc(θ2  − θ1 )  , 
n 
which  again  comes  from  Parseval’s  theorem  (see  Problem  12  at  the  end  of  this 
chapter),  we  have 
2 (t)} = X 
(rT ) sinc(r)
E {Yc 
Rxc xc 
r 
= Rxc xc (0) = E {X2 
c } 
since  sinc(r)  =  1  if  r  =  0  and  zero  otherwise.  Substituting  10.31  and  10.34  into 
10.25,  we  obtain  the  result  that  E  = 0,  as  desired. 

)  . 

(10.32) 

)}

) . 

(10.31) 

(10.30) 

(10.33) 

(10.34) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

194

 

 
Chapter 10

 

Power  Spectral  Density 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010

c

MIT OpenCourseWare
http://ocw.mit.edu 

6.011 Introduction to Communication, Control, and Signal Processing 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

