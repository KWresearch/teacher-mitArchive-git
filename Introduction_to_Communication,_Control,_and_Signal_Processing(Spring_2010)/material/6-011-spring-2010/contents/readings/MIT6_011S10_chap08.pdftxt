C H A P T E R 

8 

Estimation with Minimum Mean 
Square  Error 

INTRODUCTION 

A  recurring  theme  in  this  text  and  in  much  of  communication,  control  and  signal 
processing  is  that  of  making  systematic  estimates,  predictions  or  decisions  about 
some  set  of  quantities,  based  on  information  obtained  from measurements  of  other 
quantities.  This  process  is  commonly  referred  to  as  inference.  Typically,  inferring 
the desired  information  from  the measurements  involves  incorporating models  that 
represent  our  prior  knowledge  or  beliefs  about  how  the measurements  relate  to  the 
quantities  of  interest. 

Inference  about  continuous  random  variables  and  ultimately  about  random  pro­
cesses  is  the  topic  of  this  chapter  and  several  that  follow.  One  key  step  is  the 
introduction  of  an  error  criterion  that  measures,  in  a  probabilistic  sense,  the  error 
between  the  desired  quantity  and  our  estimate  of  it.  Throughout  our  discussion 
in  this  and  the  related  subsequent  chapters,  we  focus  primarily  on  choosing  our 
estimate  to  minimize  the  expected  or  mean  value  of  the  square  of  the  error,  re­
ferred  to  as  a  minimum  mean-square-error  (MMSE)  criterion.  In  Section  8.1  we 
consider  the  MMSE  estimate  without  imposing  any  constraint  on  the  form  that 
the  estimator  takes.  In  Section  8.3 we  restrict  the  estimate  to  be  a  linear  combina­
tion  of  the measurements,  a  form  of  estimation  that we  refer  to  as  linear minimum 
mean-square-error  (LMMSE)  estimation. 

Later  in  the  text we  turn  from  inference  problems  for  continuous  random  variables 
to  inference  problems  for  discrete  random  quantities,  which  may  be  numerically 
speciﬁed or may be non-numerical.  In the latter case especially, the various possible 
outcomes  associated  with  the  random  quantity  are  often  termed  hypotheses,  and 
the  inference  task  in  this  setting  is  then  referred  to  as  hypothesis  testing,  i.e.,  the 
task of deciding which hypothesis applies, given measurements or observations.  The 
MMSE criterion may not be meaningful in such hypothesis testing problems, but we 
can  for  instance aim to minimize  the probability of an  incorrect  inference regarding 
which  hypothesis  actually  applies. 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

139

140  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

8.1  ESTIMATION  OF  A  CONTINUOUS  RANDOM  VARIABLE 

from  which 

To  begin  the  discussion,  let  us  assume  that  we  are  interested  in  a  random  variable 
Y  and  we  would  like  to  estimate  its  value,  knowing  only  its  probability  density 
function.  We  will  then  broaden  the  discussion  to  estimation  when  we  have  a  mea­
surement  or  observation  of  another  random  variable  X ,  together  with  the  joint 
probability  density  function  of X  and  Y . 
Based  only  on  knowledge  of  the  PDF  of  Y ,  we  wish  to  obtain  an  estimate  of  Y 
—  which  we  denote  as  yb —  so  as  to  minimize  the  mean  square  error  between  the 
actual  outcome  of  the  experiment  and  our  estimate  yb.  Speciﬁcally,  we  choose  yb to 
minimize 
Z 
(y − yb)2 fY  (y) dy  . 
E [(Y  − yb)2 ] = 
(8.1) 
Diﬀerentiating  (8.1)  with  respect  to  yb and  equating  the  result  to  zero,  we  obtain 
Z 
(y − yb)fY  (y) dy = 0 
− 2
(8.2) 
or 
Z 
Z 
yfY  (y) dy = 
yfY  (y) dy 
b
y = E [Y ]  . 
b

The  second  derivative  of  E [(Y  − yb)2 ]  with  respect  to  yb is

Z 
fY  (y) dy = 2  , 
2 
(8.5) 
which  is  positive,  so  (8.4)  does  indeed  deﬁne  the  minimizing  value  of  yb.  Hence  the 
MMSE  estimate  of  Y  in  this  case  is  simply  its mean  value,  E [Y ]. 
The  associated  error —  the  actual MMSE —  is  found  by  evaluating  the  expression 
in  (8.1)  with  yb =  E [Y ].  We  conclude  that  the  MMSE  is  just  the  variance  of  Y , 
2  : 
namely  σY 
min E [(Y  − yb)2 ] = E [(Y  − E [Y ])2 ] = σ2  . 
(8.6) 
Y 
In  a  similar  manner,  it  is  possible  to  show  that  the  median  of  Y ,  which  has  half 
the  probability  mass  of  Y  below  it  and  the  other  half  above,  is  the  value  of  yb that 
minimizes  the  mean  absolute  deviation,  E [ |Y  − yb| ].  Also,  the  mode  of  Y ,  which 
is  the  value  of  y  at  which  the  PDF  fY  (y)  is  largest,  turns  out  to  minimize  the 
expected  value  of  an  all-or-none  cost  function,  i.e.,  a  cost  that  is  unity  when  the 
error  is  outside  of  a  vanishingly  small  tolerance  band,  and  is  zero  within  the  band. 
We will  not  be  pursuing  these  alternative  error metrics  further,  but  it  is  important 
to  be  aware  that  our  choice  of  mean  square  error,  while  convenient,  is  only  one  of 
many  possible  error metrics. 

(8.3) 

(8.4) 

The  insights  from  the  simple  problem  leading  to  (8.4)  and  (8.6)  carry  over  directly 
to  the  case  in which we have  additional  information  in  the  form  of  the measured  or 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  8.1 

Estimation  of  a  Continuous  Random  Variable  141 

observed  value  x  of  a  random  variable  X  that  is  related  somehow  to  Y .  The  only 
change  from  the  previous  discussion  is  that,  given  the  additional  measurement, 
we  work  with  the  conditional  or  a  posteriori  density  fY |X (y |x),  rather  than  the 
unconditioned  density  fY  (y),  and  now  our  aim  is  to  minimize 
Z 
E [{Y  − yb(x)}2 |X  = x] = 
{y − yb(x)}2 fY |X (y |x) dy  . 
(8.7) 
We  have  introduced  the  notation  yb(x)  for  our  estimate  to  show  that  in  general  it 
will  depend  on  the  speciﬁc  value  x.  Exactly  the  same  calculations  as  in  the  case  of 
no  measurements  then  show  that 
y(x) = E [Y X  = x] , 
(8.8) 
|
b
the  conditional  expectation  of  Y ,  given  X  = x.  The  associated MMSE  is  the  vari­
ance  σ2 
of  the  conditional  density  fY |X (y |x),  i.e.,  the  MMSE  is  the  conditional 
Y |X 
variance.  Thus,  the  only  change  from  the  case  of  no  measurements  is  that  we  now 
condition  on  the  obtained  measurement. 

Going  a  further  step,  if  we  have  multiple  measurements,  say  X1  =  x1 , X2  = 
, XL  = xL ,  then  we  work  with  the  a  posteriori  density 
x2 , 
· · · 
fY  | X1 ,X2 ,··· ,XL (y | x1 , x2 , · · ·  , xL )  . 
Apart  from  this  modiﬁcation,  there  is  no  change  in  the  structure  of  the  solutions. 
Thus,  without  further  calculation,  we  can  state  the  following: 

(8.9) 

The MMSE  estimate  of  Y , 
given  X1  = x1 , 
, XL  = xL ,
· · · 
is  the  conditional  expectation  of  Y : 

(8.10) 

, XL  = xL ]

y(x1 , . . . , xL ) = E [Y X1  = x1 , 
| 
· · · 
b
For  notational  convenience,  we  can  arrange  the  measured  random  variables  into  a 
column  vector  X,  and  the  corresponding  measurements  into  the  column  vector  x. 
The dependence of the MMSE estimate on the measurements can now be  indicated 
by  the  notation  yb(x),  with 
Z 
∞ 
yb(x) = 
−∞ 
The minimum mean  square  error  (or MMSE)  for  the  given  value  of X  is  again  the 
2 
conditional  variance,  i.e.,  the  variance  σY 
|X  of  the  conditional  density  fY |X (y | x). 

y fY |X (y | X = x) dy = E [ Y  | X = x ]  . 

(8.11) 

EXAMPLE  8.1 

MMSE  Estimate  for  Discrete  Random  Variables 

A discrete-time discrete-amplitude  sequence  s[n]  is  stored  on  a noisy medium.  The 
retrieved  sequence  is  r[n].  Suppose  at  some particular  time  instant n = n0  we have 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

142  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

s[n0 ]  and  r[n0 ]  modeled  as  random  variables,  which  we  shall  simply  denote  by  S 
and  R  respectively.  From  prior  measurements,  we  have  determined  that  S  and  R 
have  the  joint  probability  mass  function  (PMF)  shown  in  Figure  8.1. 

r 

1 

-1 

-1 

1 

s

FIGURE  8.1  Joint  PMF  of  S  and  R. 
Based  on  receiving  the  value  R  =  1,  we  would  like  to  make  an  MMSE  estimate  sb
of  S .  From  (8.10),  sb = E (S |R = 1),  which  can  be  determined  from  the  conditional 
PMF  PS |R (s|R = 1),  which  in  turn  we  can  obtain  as 
PR,S (R = 1, s)
PS |R (s|R = 1) = 
(8.12) 
. 
PR (R = 1) 

From  Figure  8.1, 

(8.13) 

2 
7 

0 
1/7 
1/7 

and 

Consequently, 

s = −1 
s = 0 
s = +1 

PR (1) = 
PR,S (1, s) =  
 
½ 
1/2 
s = 0 
PS |R (s|R = 1) = 
1/2 
s = +1 
Thus,  the  MMSE  estimate  is  sb =  1 .  Note  that  although  this  estimate  minimizes 
2 
the  mean  square  error,  we  have  not  constrained  it  to  take  account  of  the  fact  that 
S  can  only  have  the  discrete  values  of  +1,  0  or  −1.  In  a  later  chapter  we  will 
return  to  this  example  and  consider  it  from  the  perspective  of  hypothesis  testing, 
i.e.,  determining which  of  the  three  known  possible  values will  result  in minimizing 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  8.1 

Estimation  of  a  Continuous  Random  Variable  143 

a  suitable  error  criterion. 

EXAMPLE  8.2 

MMSE  Estimate  of  Signal  in  Additive  Noise 

A  discrete-time  sequence  s[n]  is  transmitted  over  a  noisy  channel  and  retrieved. 
The  received  sequence  r[n]  is  modeled  as  r[n] =  s[n] + w[n]  where  w[n]  represents 
the  noise.  At  a  particular  time  instant  n =  n0 ,  suppose  r[n0 ],  s[n0 ]  and  w[n0 ]  are 
random  variables,  which  we  denote  as  R,  S  and  W  respectively.  We  assume  that 
1
1
S  and  W  are  independent,  that  W  is  uniformly  distributed  between  + and  −
, 
2
2
and  S  is  uniformly  distributed  between  −1  and  +1.  The  speciﬁc  received  value  is 
1
R = ,  and  we  want  the MMSE  estimate  sb for  S .  From  (8.10), 
4
1 
) 
s = E (S R = 
|
b
4
1
which  can  be  determined  from  fS |R (s R =|
4 ): 
1
1 
|
fR|S ( s)fS (s)
4
fS |R (s|R = 
) = 
1
4 ) 
4
fR (

(8.15) 

(8.14) 

. 

We  evaluate  separately  the  numerator  and  denominator  terms  in  (8.15).  The  PDF 
s)  is  identical  in  shape  to  the PDF  of W ,  but with  the mean  shifted  to  s,  as 
fR|S (r

|
1
4 |s)  is  as  shown  in  Figure  8.3,
indicated  in  Figure  8.2  below.

Consequently,  fR|S (
and  fR|S ( 1
s)fS (s)  is  shown  in  Figure  8.4.
4 |

1 

fR|S (r|s) 

− 1
2 + s 
FIGURE  8.2  Conditional  PDF  of  R  given  S ,  fR|S (r|s). 

+ 1
2 + s 

r 

1
1
)  we  divide  Figure  8.4  by  fR ( ),  which  can  easily  be  ob­
To  obtain  fS |R (s R|
=
4
4
tained  by  evaluating  the  convolution  of  the  PDF’s  of  S  and  W 
at  the  argument 
1
1
)  must  have  total  area  of  unity  and  it  is  the 
More  simply,  since  fS |R (s R|
. 
=
4
4
1
same  as Figure  8.4 but  scaled by fR (
4 ), we  can  easily  obtain  it by  just normalizing 
Figure  8.4  to  have  an  area  of  1.  The  resulting  value  for  sb is  the  mean  associated 
1
),  which  will  be 
with  the  PDF  fS |R (s R =|
4
1 
s = 
. 
(8.16) 
b
4 
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

144  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

1 

3
4 

Plot  of  fR|S (

1
4 |s). 

0 

− 1
4
FIGURE  8.3 

1 
2 

− 1
4
FIGURE  8.4 

0 

3
4

1
Plot  of  fR|S ( s)fS (s). 
4 |

s 

s 

The  associated MMSE  is  the  variance  of  this  PDF,  namely

1 
12 .

EXAMPLE  8.3 

MMSE  Estimate  for  Bivariate  Gaussian  Random  Variables 

Two  random  variables X  and  Y  are  said  to  have  a  bivariate Gaussian  joint PDF  if 
the joint density of the centered (i.e.  zero-mean) and normalized (i.e.  unit-variance) 
random  variables 

V  = 

is  given  by 

X − µX 
σX 

,  W  = 

Y  − µY 
σY 

(8.17) 

fV ,W (v , w) = 

½ 
2 ) ¾ 
1 
(v2 − 2ρvw + w
exp  − 
2πp1 − ρ2 
2(1 − ρ2 ) 
Here µX  and µY  are the means of X  and Y  respectively, and σX , σY  are the respec­
tive  standard  deviations  of  X  and  Y .  The  number  ρ  is  the  correlation  coeﬃcient 
of  X  and  Y ,  and  is  deﬁned  by 

. 

(8.18) 

ρ = 

σX Y 
σX σY 

,  with  σX Y  = E [X Y ] − µX µY 

(8.19) 

where  σX Y  is  the  covariance  of  X  and  Y . 
Now,  consider  yb(x),  the  MMSE  estimate  of  Y  given  X  =  x,  when  X  and  Y  are 
bivariate  Gaussian  random  variables.  From  (8.10), 
| 
y(x) = E [Y X  = x] 
(8.20) 
b
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  8.2 

From  Estimates  to  an  Estimator  145 

(8.21) 

or,  in  terms  of  the  zero-mean  normalized  random  variables  V  and W , 
x − µX ¸ 
· 
y(x) = E  (σY W  + µY  )  V  =
| 
b
σX 
µX ¸ 
= σY E · 
x − 
+ µY  . 
W  | V  = 
σX 
It  is  straightforward  to  show  with  some  computation  that  fW |V  (w v)  is  Gaussian 
| 
with  mean  ρv ,  and  variance  1 − ρ2 ,  from  which  it  follows  that 
x − µX ¸ 
· 
· 
x − µX ¸
E W V  =
. 
= ρ
| 
σX 
σX 
Combining  (8.21)  and  (8.22), 
y(x) = E [ Y  X = x ]
| 
b
σY 
= µY  + ρ 
(x − µX ) 
σX 
The  MMSE  estimate  in  the  case  of  bivariate  Gaussian  variables  has  a  nice  linear 
(or  more  correctly,  aﬃne,  i.e.,  linear  plus  a  constant)  form. 
The minimum mean square error is the variance of the conditional PDF fY |X (y |X = 
x): 
E [ (Y  − yb(x))2  | X = x ] = σY 
2  (1 − ρ2 )  . 
(8.24) 
2  is  the mean  square  error  in  Y  in  the  absence  of  any  additional  infor­
Note  that  σY 
mation.  Equation (8.24) shows what the residual mean square error is after we have 
a  measurement  of  X .  It  is  evident  and  intuitively  reasonable  that  the  larger  the 
magnitude  of  the  correlation  coeﬃcient  between X  and  Y ,  the  smaller  the  residual 
mean  square  error. 

(8.22) 

(8.23) 

8.2  FROM  ESTIMATES  TO  AN  ESTIMATOR 

The  MMSE  estimate  in  (8.8)  is  based  on  knowing  the  speciﬁc  value  x  that  the 
random variable X  takes.  While X  is a random variable,  the speciﬁc value x  is not, 
and  consequently  yb(x)  is  also  not  a  random  variable. 
As we move forward in the discussion,  it is important to draw a distinction between 
the estimate of a random variable and the procedure by which we form the estimate. 
This  is  completely  analogous  to  the  distinction  between  the  value  of  a  function  at 
a  point  and  the  function  itself.  We  will  refer  to  the  procedure  or  function  that 
produces  the  estimate  as  the  estimator. 

For instance, in Example 8.1 we determined the MMSE estimate of S  for the speciﬁc 
value  of  R  =  1.  We  could  more  generally  determine  an  estimate  of  S  for  each  of 
the  possible  values  of  R,  i.e.,  −1, 0, and + 1.  We  could  then  have  a  tabulation  of 
these  results  available  in  advance,  so  that  when  we  retrieve  a  speciﬁc  value  of  R 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

146  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

we  can  look  up  the  MMSE  estimate.  Such  a  table  or  more  generally  a  function 
of  R  would  correspond  to  what  we  term  the  MMSE  estimator.  The  input  to  the 
table  or  estimator  would  be  the  speciﬁc  retrieved  value  and  the  output  would  be 
the  estimate  associated  with  that  retrieved  value. 
We  have  already  introduced  the  notation  yb(x)  to  denote  the  estimate  of  Y  given 
X  =  x.  The  function  yb(  )  determines  the  corresponding  estimator,  which  we 
· 
will  denote  by  yb(X ),  or  more  simply  by  just  Yb ,  if  it  is  understood  what  random 
variable  the  estimator  is  operating  on.  Note  that  the  estimator  Yb =  yb(X )  is a 
random  variable.  We  have  already  seen  that  the  MMSE  estimate  yb(x)  is  given  by 
the conditional mean, E [Y X  = x], which suggests yet another natural notation  for 
|
the MMSE  estimator: 
Yb = yb(X ) = E [Y |X ]  . 
(8.25) 
Note  that  E [Y X ]  denotes  a  random  variable,  not  a  number. 
|
The preceding discussion applies essentially unchanged to the case where we observe 
several  random  variables,  assembled  in  the  vector X.  The MMSE  estimator  in  this 
case  is  denoted  by 
Yb = yb(X) = E [Y |X]  . 
Perhaps  not  surprisingly,  the MMSE  estimator  for  Y  given X minimizes  the mean 
square  error,  averaged  over  all  Y  and  X.  This  is  because  the  MMSE  estimator 
minimizes  the mean  square  error  for  each  particular  value  x  of X.  More  formally, 
[Y  − yb(X)]2  | X ´´ 
[Y  − yb(X)]2 ´ 
= EX ³ 
EY |X ³ 
EY ,X ³ 
= Z 
[Y  − yb(x)]2  | X = x ´ 
∞ ³ 
EY |X ³ 
fX (x) dx  . 
−∞ 
(The  subscripts  on  the  expectation  operators  are  used  to  indicate  explicitly  which 
densities  are  involved  in  computing  the  associated  expectations;  the  densities  and 
integration  are  multivariate  when  X  is  not  a  scalar.)  Because  the  estimate  yb(x) 
is  chosen  to  minimize  the  inner  expectation  EY |X  for  each  value  x  of  X,  it  also 
minimizes  the  outer  expectation  EX ,  since  fX (X)  is  nonnegative. 

(8.27) 

(8.26) 

EXAMPLE  8.4 

MMSE  Estimator  for  Bivariate  Gaussian  Random  Variables 

We  have  already,  in  Example  8.3,  constructed  the MMSE  estimate  of  one  member 
of a pair of bivariate Gaussian random variables, given a measurement of the other. 
Using the same notation as  in that example,  it  is evident that the MMSE estimator 
is  simply  obtained  on  replacing  x  by X  in  (8.23): 
σY
Yb = yb(X ) = µY  + ρ
(X − µX )  . 
σX 
The conditional MMSE given X  = x was  found  in the earlier example to be σ2  (1 −Y 
ρ2 ), which did not depend on the value of x, so the MMSE of the estimator, averaged 

(8.28) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  8.2 

From  Estimates  to  an  Estimator  147 

over  all  X ,  ends  up  still  being  σ2  (1 − ρ2 ). 
Y 

EXAMPLE  8.5 

MMSE  Estimator  for  Signal  in  Additive  Noise 

Suppose the random variable X  is a noisy measurement of the angular position Y  of 
an antenna,  so X  = Y  + W , where W  denotes  the additive noise.  Assume  the noise 
is  independent  of  the  angular  position,  i.e.,  Y  and  W  are  independent  random 
variables,  with  Y  uniformly  distributed  in  the  interval  [−1, 1]  and  W  uniformly 
distributed in the interval [−2, 2].  (Note that the setup in this example is essentially 
the  same  as  in  Example  8.2,  though  the  context,  notation  and  parameters  are 
diﬀerent.) 
Given  that  X  =  x,  we  would  like  to  determine  the  MMSE  estimate  yb(x),  the 
resulting  mean  square  error,  and  the  overall  mean  square  error  averaged  over  all 
possible values x that the random variable X  can take.  Since yb(x) is the conditional 
expectation  of  Y  given  X  =  x,  we  need  to  determine  fY |X (y |x).  For  this,  we  ﬁrst 
determine  the  joint  density  of  Y  and  W ,  and  from  this  the  required  conditional 
density. 
From  the  independence  of  Y  and W : 

fY ,W (y , w) = fY  (y)fW (w) = 

−2

− 2 ≤ w ≤ 2, −1 ≤ y ≤ 1 
otherwise


2 

� 
w 

 1 
8

 
0 
�y 
1 

0 

−1 

FIGURE  8.5  Joint  PDF  of  Y  and W  for  Example  8.5. 

Conditioned  on  Y  =  y ,  X  is  the  same  as  y + W ,  uniformly  distributed  over  the 
interval  [y − 2, y + 2].  Now 
1
fX,Y  (x, y) = fX |Y  (x|y)fY  (y) = ( 
4

1 
)( 
) = 
2

1 
8 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

148  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

for  −1 ≤ y ≤ 1,  y − 2 ≤ x ≤ y + 2,  and  zero  otherwise.  The  joint  PDF  is  therefore 
uniform  over  the  parallelogram  shown  in  the  Figure  8.6. 

�y 
1


� 
x


�

−3 

−2 

−1

0 

1

2

3


−1 

FIGURE  8.6  Joint  PDF  of X  and  Y  and  plot  of  the MMSE  estimator  of  Y  from X

for  Example  8.5.


�y 

�y 

�y 

�y 

�y 

�y 

�y 

1


0 

−1 

1


� 

1


1
2


1
2


1
2


fY |X (y  | −3) 

fY |X (y  | −1) 

fY |X (y  |  1) 

fY |X (y  |  3) 

fY |X (y  | −2) 

fY |X (y  |  0) 

fY |X (y  |  2) 

FIGURE  8.7  Conditional  PDF  fY |X  for  various  realizations  of X  for  Example  8.5. 
Given  X  =  x,  the  conditional  PDF  fY |X  is  uniform  on  the  corresponding  vertical 
section  of  the  parallelogram: 
 
 
1

3 − x 
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010

c

− 3 ≤ x ≤ −1 , −1 ≤ y ≤ x + 2

− 1 ≤ x ≤ 1 , −1 ≤ y ≤ 1 
1 ≤ x ≤ 3 , x − 2 ≤ y ≤ 1


(8.29)

1

3 + x

1

2


fY |X (y , x) = 

(8.30)

X  = x]

(8.31) 

1
2

1

+  x 
2

0 
1

x

2 

+

X  = x] = 

| 

(3 + x)2 
12


1

3

(3 − x)2

12


− 3 ≤ x < −1 
− 1 ≤ x < 1 
1 ≤ x ≤ 3


Section  8.2 
From  Estimates  to  an  Estimator  149 
The  MMSE  estimate  yb(x)  is  the  conditional  mean  of  Y  given  X  =  x,  and  the 
conditional mean  is  the midpoint of  the  corresponding vertical  section of  the paral­
lelogram.  The conditional mean  is displayed as  the heavy  line on  the parallelogram 
in  the  second  plot.  In  analytical  form, 
 
− 3 ≤ x < −1 
− 1 ≤ x < 1

y(x) = E [Y 
b
 
1
1 ≤ x ≤ 3

− 2
The  minimum  mean  square  error  associated  with  this  estimate  is  the  variance  of 
the  uniform  distribution  in  eq.  (8.29),  speciﬁcally: 
 
E [{Y  − yb(x)}2  | 
 
Equation  (8.31)  speciﬁes  the  mean  square  error  that  results  for  any  speciﬁc  value 
x  of  the measurement  of X .  Since  the measurement  is  a  random  variable,  it  is  also 
of  interest  to  know what  the mean  square  error  is,  averaged  over  all possible  values 
of  the  measurement,  i.e.  over  the  random  variable  X .  To  determine  this,  we  ﬁrst 
determine  the  marginal  PDF  of X : 
 
This  could  also  be  found  by  convolution,  fX  =  fY  ∗  fW ,  since  Y  and  W  are 
statistically  independent.  Then, 
Z∞ 
E [(Y  − yb(x))2  | X  = x]fX (x)dx 
EX [EY |X {(Y  − yb(x)}2  | X  = x]] = 
−∞ 
Z 
Z 
Z 
−1 
3

1
(3 + x)2 
(3 − x)2 
3 + x

3 − x

1
1

( 
= 
)dx + (  )(  )dx + ( 
)( 
)( 
12

8

3
 4

12

8

−3 
1 
−1
1

= 
4


− 3 ≤ x < −1 
− 1 ≤ x < 1

1 ≤ x ≤ 3

otherwise


fX,Y  (x, y) 
fY |X (y  | x) 

3 + x

8


1

4

3 − x 
8

0 

fX (x) = 

= 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010

c

)dx 

150  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

Compare this with the mean square error if we just estimated Y  by its mean, namely 
2  : 
0.  The mean  square  error  would  then  be  the  variance  σY 
[1 − (−1)]2 
12 

1 
3 

= 

σ2 
Y 

=

,

so  the mean  square  error  is  indeed  reduced  by  allowing  ourselves  to  use  knowledge 
of  X  and  of  the  probabilistic  relation  between  Y  and  X . 

8.2.1	 Orthogonality 

A  further  important  property  of  the  MMSE  estimator  is  that  the  residual  error 
Y  − yb(X)  is  orthogonal  to  any  function  h(X)  of  the measured  random  variables: 
EY ,X [{Y  − yb(X)}h(X)] = 0 ,	
(8.32) 
where  the  expectation  is  computed over  the  joint density of Y  and X.  Rearranging 
this,  we  have  the  equivalent  condition 
EY ,X [yb(X)h(X)] = EY ,X [Y h(X)]  , 
i.e.,  the MMSE  estimator  has  the  same  correlation  as  Y  does  with  any  function  of 
X .  In  particular,  choosing  h(X) = 1,  we  ﬁnd  that 
EY ,X [yb(X)] = EY  [Y ]  .	
The  latter  property  results  in  the  estimator  being  referred  to  as  unbiased:  its 
expected  value  equals  the  expected  value  of  the  random  variable  being  estimated. 
We  can  invoked  the  unbiasedness  property  to  interpret  (8.32)  as  stating  that  the 
estimation  error  of  the  MMSE  estimator  is  uncorrelated  with  any  function  of  the 
random  variables  used  to  construct  the  estimator. 

(8.33) 

(8.34) 

The proof of the correlation matching property in (8.33) is in the following sequence 
of  equalities: 

EY ,X [yb(X)h(X)]  = EX [EY |X [Y |X]h(X)] 
= EX [EY |X [Y h(X)|X]] 
= EY ,X [Y h(X)]  . 
Rearranging  the  ﬁnal  result  here, we  obtain  the  orthogonality  condition  in  (8.32). 

(8.36)

(8.37)


(8.35)


8.3	 LINEAR  MINIMUM  MEAN  SQUARE  ERROR  ESTIMATION 

In  general,  the  conditional  expectation  E (Y  X)  required  for  the  MMSE  estimator 
|
developed in the preceding sections is diﬃcult to determine, because the conditional 
density  fY |X (y |x)  is  not  easily  determined.  A  useful  and  widely  used  compromise 
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  8.3 

Linear  Minimum  Mean  Square  Error  Estimation  151 

is  to  restrict  the  estimator  to  be  a  ﬁxed  linear  (or  actually  aﬃne,  i.e.,  linear  plus 
a  constant)  function  of  the  measured  random  variables,  and  to  choose  the  linear 
relationship  so  as  to  minimize  the  mean  square  error.  The  resulting  estimator  is 
called  the  linear minimum mean  square  error  (LMMSE)  estimator.  We  begin  with 
the  simplest  case. 

(8.38) 

(8.39) 

Suppose  we  wish  to  construct  an  estimator  for  the  random  variable  Y  in  terms  of 
another  random  variable X ,  restricting  our  estimator  to  be  of  the  form 
Ybℓ  = ybℓ (X ) = aX + b , 
where  a  and  b  are  to  be  determined  so  as  to  minimize  the mean  square  error 
EY ,X [(Y  − Ybℓ )2 ] = EY ,X [{Y  − (aX + b)} 2 ]  . 
Note  that  the  expectation  is  taken  over  the  joint  density  of  Y  and  X ;  the  linear 
estimator  is  picked  to  be  optimum when  averaged  over  all  possible  combinations  of 
Y  and X  that may  occur.  We  have  accordingly  used  subscripts  on  the  expectation 
operations  in  (8.39)  to  make  explicit  for  now  the  variables  whose  joint  density  the 
expectation  is  being  computed  over;  we  shall  eventually  drop  the  subscripts. 
Once  the  optimum  a  and  b  have  been  chosen  in  this  manner,  the  estimate  of  Y , 
given  a  particular  x,  is  just  ybℓ (x) =  ax + b,  computed  with  the  already  designed 
values  of  a  and  b.  Thus,  in  the  LMMSE  case  we  construct  an  optimal  linear 
estimator,  and  for  any  particular  x  this  estimator  generates  an  estimate  that  is 
not  claimed  to  have  any  individual  optimality  property.  This  is  in  contrast  to  the 
MMSE  case  considered  in  the  previous  sections,  where  we  obtained  an  optimal 
MMSE  estimate  for  each  x,  namely  E [Y X  = x],  that  minimized  the  mean  square 
|
error  conditioned  on  X  =  x.  The  distinction  can  be  summarized  as  follows:  in 
the unrestricted MMSE  case,  the  optimal  estimator  is  obtained by  joining  together 
all  the  individual  optimal  estimates,  whereas  in  the  LMMSE  case  the  (generally 
non-optimal)  individual  estimates  are  obtained  by  simply  evaluating  the  optimal 
linear  estimator. 

We  turn  now  to  minimizing  the  expression  in  (8.39),  by  diﬀerentiating  it  with 
respect  to  the  parameters  a  and  b,  and  setting  each  of  the  derivatives  to  0.  (Con­
sideration  of  the  second  derivatives  will  show  that  we  do  indeed  ﬁnd  minimizing 
values  in  this  fashion,  but  we  omit  the  demonstration.)  First  diﬀerentiating  (8.39) 
with  respect  to  b,  taking  the  derivative  inside  the  integral  that  corresponds  to  the 
expectation  operation,  and  then  setting  the  result  to  0,  we  conclude  that 

or  equivalently 

EY ,X [Y  − (aX + b)] = 0 , 
E [Y ] = E [aX + b] = E [Ybℓ ]  , 
from  which  we  deduce  that 
b = µY  − aµX  , 
where  µY  =  E [Y ] =  EY ,X [Y ]  and  µX  =  E [X ] =  EY ,X [X ].  The  optimum  value  of 
b  speciﬁed  in  (8.42)  in  eﬀect  serves  to make  the  linear  estimator  unbiased,  i.e.,  the 

(8.41) 

(8.40) 

(8.42) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

152  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

expected value of  the estimator becomes equal  to  the expected value of  the  random 
variable  we  are  trying  to  estimate,  as  (8.41)  shows. 

Using  (8.42)  to  substitute  for  b  in  (8.38),  it  follows  that 
Ybℓ  = µY  + a(X − µX )  . 
In  other  words,  to  the  expected  value  µY  of  the  random  variable  Y  that  we  are 
estimating,  the  optimal  linear  estimator  adds  a  suitable  multiple  of  the  diﬀerence 
X  − µX  between  the  measured  random  variable  and  its  expected  value.  We  turn 
now  to  ﬁnding  the  optimum  value  of  this  multiple,  a. 

(8.43) 

(8.44) 

where 

First  rewrite  the  error  criterion  (8.39)  as 
E [{(Y  − µY  ) − (Ybℓ  − µY  )}2 ] = E [( Ye − aXe )2 ]  , 
Ye = Y  − µY 
and  Xe = X − µX  , 
(8.45) 
and  where  we  have  invoked  (8.43)  to  obtain  the  second  equality  in  (8.44).  Now 
taking  the  derivative  of  the  error  criterion  in  (8.44)  with  respect  to  a,  and  setting 
the  result  to  0,  we  ﬁnd 
E [( Ye − aXe )Xe ] = 0 . 
(8.46) 
Rearranging  this,  and  recalling  that  E [Ye Xe ] =  σY X ,  i.e.,  the  covariance  of  Y  and 
X ,  and  that  E [Xe 2 ] = σ2  ,  we  obtain 
X 
σY X 
σY 
a =
= ρY X 
, 
(8.47) 
σ2 
σX 
X 
where  ρY X  —  which  we  shall  simply  write  as  ρ  when  it  is  clear  from  context  what 
variables  are  involved —  denotes  the  correlation  coeﬃcient  between  Y  and X . 

It  is  also  enlightening  to  understand  the  above  expression  for  a  in  terms  of  the 
vector-space  picture  for  random  variables  developed  in  the  previous  chapter. 

eY  − a eX  = Y  −  bYℓ 

eY

aXe
FIGURE  8.8  Expression  for  a  from  Eq.  (8.47)  illustrated  in  vector  space. 

eX 

The  expression  (8.44)  for  the  error  criterion  shows  that we  are  looking  for  a  vector 
aXe , which  lies  along  the  vector Xe ,  such  that  the  squared  length  of  the  error  vector 
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  8.3 
Linear  Minimum  Mean  Square  Error  Estimation  153 
Ye − aXe is minimum.  It follows from familiar geometric reasoning that the optimum 
choice of aXe must be the orthogonal pro jection of Ye on Xe , and that this pro jection 
is 
<  e X > 
Y ,  e
e X >  e
a e
X .	
(8.48) 
X  = 
< X ,  e
Here,  as  in  the  previous  chapter,  <  U, V  >  denotes  the  inner  product  of  the  vec­
tors  U  and  V ,  and  in  the  case  where  the  “vectors”  are  random  variables,  denotes 
E [U V ].  Our  expression  for  a  in  (8.47)  follows  immediately.  Figure  8.8  shows  the 
construction  associated  with  the  requisite  calculations.  Recall  from  the  previous 
chapter  that  the  correlation  coeﬃcient  ρ  denotes  the  cosine  of  the  angle  between 
the  vectors  Ye and Xe . 
The  preceding  pro jection  operation  implies  that  the  error  Ye − aXe ,  which  can  also 
be  written  as  Y  − Ybℓ ,  must  be  orthogonal  to  Xe = X − µX .  This  is  precisely  what 
(8.46)  says.  In  addition,  invoking  the  unbiasedness  of  Ybℓ  shows  that  Y  − Ybℓ  must 
be  orthogonal  to  µX  (or  any  other  constant),  so  Y  − Ybℓ  is  therefore  orthogonal  to 
X	 itself: 
E [(Y  − Ybℓ )X ] = 0 . 
(8.49) 
In  other  words,  the  optimal  LMMSE  estimator  is  unbiased  and  such  that  the  esti­
mation  error  is  orthogonal  to  the  random  variable  on which  the  estimator  is based. 
(Note that the statement in the case of the MMSE estimator in the previous section 
was  considerably  stronger,  namely  that  the  error  was  orthogonal  to  any  function 
h(X )  of  the measured  random  variable,  not  just  to  the  random  variable  itself.) 

The  preceding  development  shows  that  the  properties  of  (i)  unbiasedness  of  the 
estimator,  and  (ii)  orthogonality  of  the  error  to  the  measured  random  variable, 
completely  characterize  the  LMMSE  estimator.  Invoking  these  properties  yields 
the  LMMSE  estimator. 

Going  a  step  further  with  the  geometric  reasoning,  we  ﬁnd  from  Pythagoras’s  the­
orem  applied  to  the  triangle  in  Figure  8.8  that  the  minimum  mean  square  error 
(MMSE)  obtained  through  use  of  the  LMMSE  estimator  is 
2  (1 − ρ2 )  . 
MMSE = E [( Ye − aXe )2 ] = E [Ye 2 ](1 − ρ2 ) = σY 
This  result  could  also  be  obtained  purely  analytically,  of  course,  without  recourse 
2 
to  the  geometric  interpretation.  The  result  shows  that  the  mean  square  error  σY 
that we  had  prior  to  estimation  in  terms  of X  is  reduced  by  the  factor  1 − ρ2  when 
we use X  in an LMMSE estimator.  The closer that ρ is to +1 or −1 (corresponding 
to  strong  positive  or  negative  correlation  respectively),  the  more  our  uncertainty 
about  Y  is  reduced  by  using  an  LMMSE  estimator  to  extract  information  that  X 
carries  about  Y . 

(8.50) 

Our  results  on  the  LMMSE  estimator  can  now  be  summarized  in  the  following 
expressions  for  the  estimator,  with  the  associated  minimum  mean  square  error 
being  given  by  (8.50): 
Ybℓ  = ybℓ (X ) = µY  + 
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

σY X	
σ2  (X − µX ) = µY  + ρ
X 

(8.51) 

σY
σX 

(X − µX )  , 

154  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

or  the  equivalent  but  perhaps more  suggestive  form 
Ybℓ  − µY 
σY 
The  latter expression states  that the normalized deviation of the estimator  from  its 
mean is ρ times the normalized deviation of the observed variable from its mean; the 
more highly correlated Y  and X  are, the more closely we match the two normalized 
deviations. 

X − µX 
σX 

(8.52) 

= ρ

. 

Note  that  our  expressions  for  the  LMMSE  estimator  and  its mean  square  error  are 
the same as those obtained in Example 8.4 for the MMSE estimator in the bivariate 
Gaussian  case.  The  reason  is  that  the MMSE  estimator  in  that  case  turned  out  to 
be  linear  (actually,  aﬃne),  as  already  noted  in  the  example. 

EXAMPLE  8.6 

LMMSE  Estimator  for  Signal  in  Additive  Noise 

We  return  to  Example  8.5,  for  which  we  have  already  computed  the  MMSE  esti­
mator,  and  we  now  design  an  LMMSE  estimator.  Recall  that  the  random  vari­
able  X  denotes  a  noisy  measurement  of  the  angular  position  Y  of  an  antenna,  so 
X  =  Y  + W ,  where  W  denotes  the  additive  noise.  We  assume  the  noise  is  inde­
pendent  of  the  angular  position,  i.e.,  Y  and W  are  independent  random  variables, 
with  Y  uniformly  distributed  in  the  interval  [−1, 1]  and  W  uniformly  distributed 
in  the  interval  [−2, 2]. 
For  the LMMSE estimator of Y  in  terms of X , we need  to determine  the  respective 
means and variances, as well as the covariance, of these random variables.  It is easy 
to  see  that 

µY 

= 0  , µW  = 0  , µX  = 0  ,

σ

2
Y 

= 

1 
3 

,  σ

2
W

= 

4
3 

,

= 

.

1
3 

2 
Y

= 

,  σY X  = σ

X  = σ2 
σ2 
Y  + σ2
W 

1 
,  ρY X  =  √5 

5 
3 
The  LMMSE  estimator  is  accordingly 
b
Yℓ  = 
2 
Y  (1 − ρ2 ) = 
This MMSE  should be compared with  the  (larger) mean  square error of
if we  simply  use µY  = 0  as  our  estimator  for Y ,  and  the  (smaller)  value
using  the MMSE  estimator  in  Example  8.5. 

and  the  associated MMSE  is 

1 
X , 
5 

σ

4 
15 

. 

1 
3
1 
4

obtained 
obtained 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  8.3 

Linear  Minimum  Mean  Square  Error  Estimation  155 

EXAMPLE  8.7 

Single-Point  LMMSE  Estimator  for  Sinusoidal  Random  Process 

Consider  a  sinusoidal  signal  of  the  form 

X (t) = A cos(ω0 t + Θ) 

(8.53) 

where  ω0  is  assumed  known,  while  A  and  Θ  are  statistically  independent  random 
variables,  with  the  PDF  of  Θ  being  uniform  in  the  interval  [0, 2π ].  Thus  X (t)  is  a 
random  signal,  or  equivalently  a  set  or  “ensemble”  of  signals  corresponding  to  the 
various  possible  outcomes  for  A  and  Θ  in  the  underlying  probabilistic  experiment. 
We  will  discuss  such  signals  in more  detail  in  the  next  chapter,  where  we  will  refer 
to  them  as  random  processes.  The  value  that  X (t)  takes  at  some  particular  time 
t  =  t0  is  simply  a  random  variable,  whose  speciﬁc  value  will  depend  on  which 
outcomes  for  A  and  Θ  are  produced  by  the  underlying  probabilistic  experiment. 

(8.54) 

Suppose  we  are  interested  in  determining  the  LMMSE  estimator  for  X (t1 )  based 
on  a measurement  of X (t0 ), where  t0  and  t1  are  speciﬁed  sampling  times.  In  other 
words,  we  want  to  choose  a  and  b  in 
Xb (t1 ) = aX (t0 ) + b 
so  as  to  minimize  the mean  square  error  between  X (t1 )  and Xb (t1 ). 
We  have  established  that  b  must  be  chosen  to  ensure  the  estimator  is  unbiased: 
E [Xb (t1 )] = aE [X (t0 )] + b = E [X (t1 )]  . 
Since  A  and  Θ  are  independent, 
Z  2π  1 
E [X (t0 )] = E {A} 
cos(ω0 t0  + θ) dθ = 0 
2π
0 
and  similarly  E [X (t1 )] = 0,  so  we  choose  b = 0. 
Next  we  use  the  fact  that  the  error  of  the  LMMSE  estimator  is  orthogonal  to  the 
data: 
E [( Xb (t1 ) − X (t1 ))X (t0 )] = 0 
aE [X 2 (t0 )] = E [X (t1 )X (t0 )] 

and  consequently 

or 

. 

(8.55) 

a = 

E [X (t1 )X (t0 )] 
E [X 2 (t0 )] 
The  numerator  and  denominator  in  (8.55)  are  respectively 
Z  2π  1 
E [X (t1 )X (t0 )]  =  E [A2 ] 
2π 
0 
E [A2 ] 
cos{ω0 (t1  − t0 )}
2 

= 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

cos(ω0 t1  + θ) cos(ω0 t0  + θ) dθ 

156  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

and  E [X 2 (t0 )] =  E [A2 ] .  Thus  a = cos{ω0 (t1  − t0 )},  so  the  LMMSE  estimator  is 
2 
b
X (t1 ) = X (t0 ) cos{ω0 (t1  − t0 )} . 
(8.56) 
It  is  interesting  to  observe  that  the  distribution  of  A  doesn’t  play  a  role  in  this 
equation. 
To evaluate  the mean  square error associated with  the LMMSE estimator, we com­
pute  the  correlation  coeﬃcient  between  the  samples  of  the  random  signal  at  t0  and 
t1 .  It  is  easily  seen  thatρ = a = cos{ω0 (t1  − t0 )},  so  the mean  square  error  is 
E [A2 ] ³1 − cos 2 {ω0 (t1  − t0 )}´ 
E [A2 ] 
sin2 {ω0 (t1  − t0 )}  . 
2 
2

(8.57) 

= 

We now extend the LMMSE estimator to the case where our estimation of a random 
variable  Y  is  based  on  observations  of  multiple  random  variables,  say  X1 , . . . , XL , 
gathered  in  the  vector X.  The  aﬃne  estimator  may  then  be  written  in  the  form 
Ybℓ  = ybℓ (X) = a0  + X 
L
aj Xj  . 
j=1 
As we shall see,  the coeﬃcient ai  of this LMMSE estimator can be  found by solving 
a  linear  system  of  equations  that  is  completely  deﬁned  by  the  ﬁrst  and  second 
moments  (i.e.,  means,  variances  and  covariances)  of  the  random  variables  Y  and 
Xj .  The fact that the model (8.58) is linear in the parameters ai  is what results in a 
linear  system of equations;  the  fact  that  the model  is aﬃne  in  the  random variables 
is  what makes  the  solution  only  depend  on  their  ﬁrst  and  second moments.  Linear 
equations  are  easy  to  solve,  and  ﬁrst  and  second  moments  are  generally  easy  to 
determine,  hence  the  popularity  of  LMMSE  estimation. 

(8.58) 

The  development  below  follows  along  the  same  lines  as  that  done  earlier  in  this 
section  for  the  case  where  we  just  had  a  single  observed  random  variable  X ,  but 
we use  the opportunity  to  review  the  logic of  the development and  to provide a  few 
additional  insights. 

We  want  to  minimize  the mean  square  error 
aj Xj )´2 i 
E h³Y  − (a0  + X 
L
j=1 
where  the  expectation  is  computed  using  the  joint  density  of Y  and X.  We use  the 
joint  density  rather  than  the  conditional  because  the  parameters  are  not  going  to 
be picked to be best  for a particular set of measured values x — otherwise we could 
do  as  well  as  the  nonlinear  estimate  in  this  case,  by  setting  a0  = E [Y  X = x]  and 
|
setting all the other ai  to zero.  Instead, we are picking the parameters to be the best 
averaged  over  all  possible  X.  The  linear  estimator  will  in  general  not  be  as  good 

(8.59) 

, 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  8.3 

Linear  Minimum  Mean  Square  Error  Estimation  157 

as  the  unconstrained  estimator,  except  in  special  cases  (some  of  them  important, 
as  in  the  case  of  bivariate  Gaussian  random  variables)  but  this  estimator  has  the 
advantage  that  it  is  easy  to  solve  for,  as  we  now  show. 

To  minimize  the  expression  in  (8.59),  we  diﬀerentiate  it  with  respect  to  ai  for 
· · · 
i = 0, 1, 
, L,  and  set  each  of  the  derivatives  to  0.  (Again,  calculations  involving 
second  derivatives  establish  that  we  do  indeed  obtain  minimizing  values,  but  we 
omit  these  calculation  here.)  First  diﬀerentiating  with  respect  to  a0  and  setting 
the  result  to  0,  we  conclude  that 
E [Y ] = E [ a0  + X 
L
aj Xj  ] = E [Ybℓ ] 
j=1 
a0  = µY  − X 
L
aj  µXj  , 
j=1 
where  µY  = E [Y ]  and  µXj  = E [Xj ].  This  optimum  value  of  a0  serves  to  make  the 
linear  estimator  unbiased,  in  the  sense  that  (8.60)  holds,  i.e.,  the  expected  value  of 
the estimator is the expected value of the random variable we are trying to estimate. 

(8.61) 

(8.60) 

or 

Using  (8.61)  to  substitute  for  a0  in  (8.58),  it  follows  that 
Ybℓ  = µY  + X 
L
aj (Xj  − µXj )  . 
j=1 
In  other  words,  the  estimator  corrects  the  expected  value  µY  of  the  variable  we 
are  estimating,  by  a  linear  combination  of  the  deviations  Xj  − µXj  between  the 
measured  random  variables  and  their  respective  expected  values. 

(8.62) 

, 

(8.63) 

where 

Taking  account  of  (8.62),  we  can  rewrite  our  mean  square  error  criterion  (8.59)  as 
E [{(Y  − µY  ) − (Ybℓ  − µY  )}2 ] = E h³ 
aj Xej )´2 i 
Ye − X 
L
j=1 
Ye = Y  − µY 
and  Xej  = Xj  − µXj  . 
Diﬀerentiating this with respect to each of the remaining coeﬃcients ai , i = 1, 2, ...L, 
and  setting  the  result  to  zero  produces  the  equations 
E [( Ye − X 
L
aj Xej )Xei ] = 0 
j=1 
or  equivalently,  if  we  again  take  account  of  (8.62), 
E [(Y  − Ybℓ )Xei ] = 0 
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

i = 1, 2, ..., L  . 

i = 1, 2, ..., L  . 

(8.66) 

(8.64) 

(8.65) 

158  Chapter  8 
Estimation  with  Minimum  Mean  Square  Error 
Yet  another  version  follows  on  noting  from  (8.60)  that  Y  − Ybℓ  is  orthogonal  to  all 
constants,  in  particular  to  µXi ,  so 
E [(Y  − Ybℓ )Xi ] = 0 
i = 1, 2, ..., L  . 
(8.67) 
All  three  of  the  preceding  sets  of  equations  express,  in  slightly  diﬀerent  forms,  the 
orthogonality of the estimation error to the random variables used in the estimator. 
One  moves  between  these  forms  by  invoking  the  unbiasedness  of  the  estimator. 
The  last  of  these,  (8.67),  is  the  usual  statement  of  the  orthogonality  condition  that 
governs  the  LMMSE  estimator.  (Note  once more  that  the  statement  in  the  case  of 
the MMSE estimator in the previous section was considerably stronger, namely that 
the  error  was  orthogonal  to  any  function  h(X)  of  the  measured  random  variables, 
not  just  to  the  random  variables  themselves.)  Rewriting  this  last  equation  as 
E [Y Xi ] = E [YbℓXi ] 
yields  an  equivalent  statement  of  the  orthogonality  condition,  namely  that  the 
LMMSE  estimator  Ybℓ  has  the  same  correlations  as  Y  with  the  measured  variables 
Xi . 
The  orthogonality  and  unbiasedness  conditions  together  determine  the  LMMSE 
estimator  completely.  Also,  the  preceding  developments  shows  that  the  ﬁrst  and 
second  moments  of  Y  and  the  Xi  are  exactly  matched  by  the  corresponding  ﬁrst 
and  second  moments  of  Ybℓ  and  the  Xi .  It  follows  that  Y  and  Ybℓ  cannot  be  told 
apart  on  the  basis  of  only  ﬁrst  and  second  moments  with  the  measured  variables 
Xi . 
We  focus  now  on  (8.65),  because  it  provides  the  best  route  to  a  solution  for  the 
coeﬃcients  aj ,  j  = 1, . . . , L.  This  set  of  equations  can  be  expressed  as 
LX 
j=1 
where  σXiXj  is  the  covariance  of  Xi  and  Xj  (so  σXiXi  is  just  the  variance  σ2  ), Xi 
and σXi Y  is the covariance of Xi  and Y .  Collecting these equations  in matrix  form, 
we  obtain 
σX1 Y   
a1     
· · ·  σX1XL    
 
 
=  
. . .   
  
 
· · · 
σX2 Y 
a2 
σX2XL 
. . . 
. . . 
. . . 
· · ·  σXLXL 
σXL Y 
aL 
This  set  of  equations  is  referred  to  as  the  normal  equations.  We  can  rewrite  the 
normal  equations  in  more  compact  matrix  notation: 

σX1X1  σX1X2 
σX2X1 
σX2X2 
. . . 
. . . 
σXLX1  σXL X2 

σXi Xj aj  = σXi Y  , 

. 

(8.70) 

i = 1, 2, ..., L 

(8.68) 

(8.69) 

(CXX ) a = CXY 

(8.71) 

where the deﬁnitions of CXX , a, and CXY  should be evident on comparing the  last 
two  equations.  The  solution  of  this  set  of  L  equations  in  L  unknowns  yields  the 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  8.3 

Linear  Minimum  Mean  Square  Error  Estimation  159 

, L,  and  these  values may  be  substituted  in  (8.62)  to  completely 
{aj }  for  j  = 1, 
· · · 
specify  the  estimator.  In matrix  notation,  the  solution  is 

a = (CXX )−1CXY  . 

(8.72) 

It  can  be  shown  quite  straightforwardly  (though  we  omit  the  demonstration)  that 
the  minimum  mean  square  error  obtained  with  the  LMMSE  estimator  is 
2  − CY X (CXX )−1CXY  = σY 
2  − CY Xa  , 
σY 
where  CY X  is  the  transpose  of CXY  . 

(8.73) 

EXAMPLE  8.8 

Estimation  from  Two  Noisy Measurements 

Y  → 

|
| 

→

→

R1 
↓L 
L 
↑R2 

→ 

X1

→ 

X2

FIGURE  8.9  Illustration  of  relationship  between  random  variables  from  Eq.  (8.75) 
for  Example  8.8. 

Assume that Y , R1  and R2  are mutually uncorrelated, and that R1  and R2  have zero 
means and equal variances.  We wish to ﬁnd the linear MMSE estimator for Y , given 
measurements of X1  and X2 .  This estimator takes the form Ybℓ  = a0 + a1X1 + a2X2 . 
Our  requirement  that  Ybℓ  be  unbiased  results  in  the  constraint 
a0  = µY  − a1µX1  − a2µX2  = µY  (1 − a1  − a2 ) 
(8.74) 
Next,  we  need  to  write  down  the  normal  equations,  for  which  some  preliminary 
calculations  are  required.  Since 

X1  = Y  + R1

X2  = Y  + R2 

and  Y ,  R1  and  R2  are  mutually  uncorrelated,  we  ﬁnd 
2 ] = E [Y 2 ] + E [R2 
i ]  , 
E [Xi 
E [X1X2 ] = E [Y 2 ]  , 
E [XiY ] = E [Y 2 ]  . 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

(8.75)


(8.76) 

160  Chapter  8 

Estimation  with  Minimum  Mean  Square  Error 

σ2  + σ2 
R
Y
2
σ
Y

The  normal  equations  for  this  case  thus  become 
a1  ¸ 
¸ · 
· 
a2 
from  which  we  conclude  that 
a1  ¸ 
· 
a2 

σ2  + σ2 
R 
Y
−σ2 
Y 

σ2 
Y 
+

·	

= 

2
R 

2
Y

σ

σ

= 

· 

σ2 
Y 
2
σ
Y 

¸ 

(8.77) 

2σ− Y 
2σ+ R 
2σY

¸ · 

2σY
Yσ2 

¸ 

. 

(8.78) 

1 
(σ2	 + σ2 
)2
R
Y
·
σ2 
Y 
σ+ 
2
Y

2σ

2
R 

− σ4 
Y 
¸1 
1 

= 

Finally,  therefore, 

1 
2σ+ R

σ

2
Y

+

= 

µY 

+1

2(σR

X2 ) 

2σ2 
Y 

2
σ X
Y

Ybℓ 
and applying (8.73) we get that the associated minimum mean square error (MMSE) 
is 

2
2
σ σRY
2
2
σ+ R
2σ
Y
One  can  easily  check  that  both  the  estimator  and  the  associated  MMSE  take  rea­
. 
sonable values at extreme ranges of the signal-to-noise ratio  σ /σR
2
2
Y

(8.79) 

(8.80) 

.	

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

MIT OpenCourseWare
http://ocw.mit.edu 

6.011 Introduction to Communication, Control, and Signal Processing 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

