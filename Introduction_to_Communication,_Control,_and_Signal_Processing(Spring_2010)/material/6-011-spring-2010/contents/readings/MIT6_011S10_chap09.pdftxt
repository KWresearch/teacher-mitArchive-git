C H A P T E R 

9 

Random  Processes 

INTRODUCTION 

Much of your background  in  signals and  systems  is assumed  to have  focused on  the 
eﬀect  of  LTI  systems  on  deterministic  signals,  developing  tools  for  analyzing  this 
class  of  signals  and  systems,  and  using  what  you  learned  in  order  to  understand 
applications  in  communication  (e.g.,  AM  and  FM  modulation),  control  (e.g.,  sta­
bility of  feedback systems), and signal processing (e.g., ﬁltering).  It  is  important to 
develop  a  comparable  understanding  and  associated  tools  for  treating  the  eﬀect  of 
LTI  systems  on  signals  modeled  as  the  outcome  of  probabilistic  experiments,  i.e., 
a  class  of  signals  referred  to  as  random  signals  (alternatively  referred  to  as  random 
processes  or  stochastic  processes).  Such  signals  play  a  central  role  in  signal  and 
system  design  and  analysis,  and  throughout  the  remainder  of  this  text.  In  this 
chapter we  deﬁne  random  processes  via  the  associated  ensemble  of  signals,  and  be­
gin  to  explore  their  properties.  In  successive  chapters  we  use  random  processes  as 
models  for  random  or  uncertain  signals  that  arise  in  communication,  control  and 
signal  processing  applications. 

9.1  DEFINITION  AND  EXAMPLES  OF  A  RANDOM  PROCESS 

In Section 7.3 we deﬁned a random variable X  as a function that maps each outcome 
of  a  probabilistic  experiment  to  a  real  number.  In  a  similar  manner,  a  real-valued 
CT  or  DT  random  process,  X (t)  or  X [n]  respectively,  is  a  function  that  maps 
each  outcome  of  a  probabilistic  experiment  to  a  real  CT  or DT  signal  respectively, 
termed  the  realization  of  the  random  process  in  that  experiment.  For  any  ﬁxed 
time  instant  t  =  t0  or  n  =  n0 ,  the  quantities  X (t0 )  and  X [n0 ]  are  just  random 
variables.  The  collection  of  signals  that  can  be  produced  by  the  random  process  is 
referred  to  as  the  ensemble  of  signals  in  the  random  process. 

EXAMPLE  9.1 

Random  Oscillators 

As  an  example  of  a  random  process,  imagine  a  warehouse  containing  N  harmonic 
oscillators,  each  producing  a  sinusoidal  waveform  of  some  speciﬁc  amplitude,  fre­
quency,  and  phase,  all  of  which  may  be  diﬀerent  for  the  diﬀerent  oscillators.  The 
probabilistic  experiment  that  results  in  the  ensemble  of  signals  consists  of  selecting 
an  oscillator  according  to  some  probability  mass  function  (PMF)  that  assigns  a 
probability  to  each  of  the  numbers  from  1  to N ,  so  that  the  ith  oscillator  is  picked 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

161

162  Chapter  9 

Random  Processes 

Ψ 

�Amplitude 

� 

X (t; ψ) 

t0 

�

t 

ψ 

FIGURE  9.1  A  random  process. 

with  probability  pi .  Associated  with  each  outcome  of  this  experiment  is  a  speciﬁc 
sinusoidal  waveform. 

In  Example  9.1,  before  an  oscillator  is  chosen,  there  is  uncertainty  about  what 
the  amplitude,  frequency  and  phase  of  the  outcome  of  the  experiment  will  be. 
Consequently,  for  this  example,  we  might  express  the  random  process  as 

X (t) = A sin(ωt + φ) 

where  the  amplitude  A,  frequency  ω  and  phase  φ  are  all  random  variables.  The 
value  X (t1 )  at  some  speciﬁc  time  t1  is  also  a  random  variable.  In  the  context  of 
this  experiment,  knowing  the  PMF  associated  with  each  of  the  numbers  1  to  N 
involved  in  choosing  an  oscillator,  as  well  as  the  speciﬁc  amplitude,  frequency  and 
phase  of  each  oscillator,  we  could  determine  the  probability  distributions  of  any  of 
the  underlying  random  variables  A,  ω ,  φ  or  X (t1 ) mentioned  above. 
Throughout  this and  later  chapters, we will be  considering many other  examples of 
random  processes.  What  is  important  at  this  point,  however,  is  to  develop  a  good 
mental picture of what a random process is.  A random process is not just one signal 
but  rather  an  ensemble  of  signals,  as  illustrated  schematically  in  Figure  9.2  below, 
for which the outcome of the probabilistic experiment could be any of the four wave­
forms  indicated.  Each  waveform  is  deterministic,  but  the  process  is  probabilistic 
or  random  because  it  is  not  known  a  priori  which  waveform  will  be  generated  by 
the  probabilistic  experiment.  Consequently,  prior  to  obtaining  the  outcome  of  the 
probabilistic  experiment, many  aspects  of  the  signal  are  unpredictable,  since  there 
is uncertainty associated with which signal will be produced.  After the experiment, 
or  a  posteriori,  the  outcome  is  totally  determined. 

If  we  focus  on  the  values  that  a  random  process  X (t)  can  take  at  a  particular 
instant of time,  say t1  —  i.e.,  if we  look down the entire ensemble at a ﬁxed time — 
what  we  have  is  a  random  variable,  namely  X (t1 ).  If  we  focus  on  the  ensemble  of 
values  taken at an arbitrary  collection  of  ℓ ﬁxed  time  instants  t1  < t2  <  < tℓ  for 
· · · 
some  arbitrary  integer  ℓ,  we  are  dealing  with  a  set  of  ℓ  jointly  distributed  random 
variables  X (t1 ),  X (t2 ), 
,  X (tℓ ),  all  determined  together  by  the  outcome  of  the 
· · · 
underlying  probabilistic  experiment.  From  this  point  of  view,  a  random  process 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  9.1 

Deﬁnition  and  examples  of  a  random  process  163 

X(t) = x (t) 
a 

X(t) = x (t)b 

X(t) = x (t)c 

X(t) = x (t)d 

t 

t 

t 

t 

t 
1 

t
2 

FIGURE  9.2  Realizations  of  the  random  process  X (t) 

can  be  thought  of  as  a  family  of  jointly  distributed  random  variables  indexed  by 
t  (or  n  in  the  DT  case).  A  full  probabilistic  characterization  of  this  collection  of 
random  variables  would  require  the  joint  PDFs  of  multiple  samples  of  the  signal, 
taken  at  arbitrary  times: 

fX (t1 ),X (t2 ),

,X (tℓ ) (x1 , x2 , 
· · · 
··· 

, xℓ )

· · · 
for  all  ℓ  and  all  t1 , t2 , 
, tℓ .
An important set of questions that arises as we work with random processes in later 
chapters of this book is whether, by observing just part of the outcome of a random 
process,  we  can  determine  the  complete  outcome.  The  answer  will  depend  on  the 
details  of  the  random  process,  but  in  general  the  answer  is  no.  For  some  random 
processes,  having  observed  the  outcome  in  a  given  time  interval  might  provide 
suﬃcient  information  to know  exactly which  ensemble member was determined.  In 
other cases  it would not be suﬃcient.  We will be exploring  some of  these aspects  in 
more  detail  later,  but  we  conclude  this  section  with  two  additional  examples  that 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

164  Chapter  9 

Random  Processes 

further  emphasize  these  points. 

EXAMPLE  9.2 

Ensemble  of  batteries 

Consider a collection of N  batteries, each providing one voltage out of a given ﬁnite 
set  of  voltage  values.  The  histogram  of  voltages  (i.e.,  the  number  of  batteries  with 
a  given  voltage)  is  given  in  Figure  9.3.  The  probabilistic  experiment  is  to  choose 

Number of 
Batteries 

Voltage 

FIGURE  9.3  Histogram  of  battery  distribution  for  Example  9.2. 

1  ,  i.e., 
one  of  the  batteries,  with  the  probability  of  picking  any  speciﬁc  one  being  N 
they  are  all  equally  likely  to  be  picked.  A  little  reﬂection  should  convince  you  that 
1  ,  this  normalized  histogram  will 
if  we  multiply  the  histogram  in  Figure  9.3  by  N 
represent  (or  approximate)  the  PMF  for  the  battery  voltage  at  the  outcome  of  the 
experiment.  Since  the  battery  voltage  is  a  constant  signal,  this  corresponds  to  a 
random  process,  and  in  fact  is  similar  to  the  oscillator  example  discussed  earlier, 
but  with  ω = 0  and  φ = 0,  so  that  only  the  amplitude  is  random. 

For  this  example  observation  of  X (t)  at  any  one  time  is  suﬃcient  information  to 
determine  the  outcome  for  all  time. 

EXAMPLE  9.3 

Ensemble  of  coin  tossers 

Consider N  people,  each  independently having written down a  long  random  string 
of  ones  and  zeros, with  each  entry  chosen  independently  of  any  other  entry  in  their 
string  (similar  to  a  sequence  of  independent  coin  tosses).  The  random  process  now 
comprises  this  ensemble  of  strings.  A  realization  of  the  process  is  obtained  by 
randomly  selecting  a person  (and  therefore  one  of  the N  strings  of  ones  and  zeros), 
following  which  the  speciﬁc  ensemble  member  of  the  random  process  is  totally 
determined.  The  random  process  described  in  this  example  is  often  referred  to  as 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  9.1 

Deﬁnition  and  examples  of  a  random  process  165 

the  Bernoulli  process  because  of  the  way  in  which  the  string  of  ones  and  zeros  is 
generated  (by  independent  coin  ﬂips). 

Now  suppose  that  person  shows  you  only  the  tenth  entry  in  the  string.  Can  you 
determine  (or  predict)  the  eleventh  entry  from  just  that  information?  Because  of 
the  manner  in  which  the  string  was  generated,  the  answer  clearly  is  no.  Similarly 
if  the  entire  past  history  up  to  the  tenth  entry  was  revealed  to  you,  could  you 
determine  the  remaining  sequence  beyond  the  tenth?  For  this  example,  the  answer 
is  again  clearly  no. 

While  the  entire  sequence  has  been  determined  by  the  nature  of  the  experiment, 
partial  observation  of  a  given  ensemble  member  is  in  general  not  suﬃcient  to  fully 
specify  that  member. 

Rather  than  looking at  the nth entry of a single ensemble member, we can consider 
the  random  variable  corresponding  to  the  values  from  the  entire  ensemble  at  the 
nth  entry.  Looking down  the  ensemble  at n = 10,  for  example, we would would  see 
ones  and  zeros  with  equal  probability. 

In  the  above  discussion  we  indicated  and  emphasized  that  a  random  process  can 
be  thought  of  as  a  family  of  jointly  distributed  random  variables  indexed  by  t  or 
n.  Obviously  it would  in  general  be  extremely  diﬃcult  or  impossible  to  represent  a 
random process this way.  Fortunately, the most widely used random process models 
have  special  structure  that  permits  computation  of  such  a  statistical  speciﬁcation. 
Also,  particularly when we  are  processing  our  signals with  linear  systems, we  often 
design the processing or analyze the results by considering only the ﬁrst and second 
moments  of  the  process,  namely  the  following  functions: 

Mean: 
µX (ti ) = E [X (ti )], 
Auto-correlation:  RXX (ti , tj ) = E [X (ti )X (tj )],  and 
Auto-covariance:  CXX (ti , tj ) = E [(X (ti ) − µX (ti ))(X (tj ) − µX (tj ))] 
= RXX (ti , tj ) − µX (ti )µX (tj ). 
The  word  “auto”  (which  is  sometime  written  without  the  hyphen,  and  sometimes 
dropped  altogether  to  simplify  the  terminology)  here  refers  to  the  fact  that  both 
samples  in  the  correlation  function  or  the  covariance  function  come  from  the  same 
process; we  shall  shortly encounter an extension of  this  idea, where  the  samples are 
taken  from  two  diﬀerent  processes. 

(9.1) 
(9.2) 

(9.3) 

One  case  in  which  the  ﬁrst  and  second  moments  actually  suﬃce  to  completely 
specify  the  process  is  in  the  case  of  what  is  called  a  Gaussian  process,  deﬁned 
as  a  process  whose  samples  are  always  jointly  Gaussian  (the  generalization  of  the 
bivariate  Gaussian  to  many  variables). 

We can also consider multiple random processes, e.g., two processes, X (t) and Y (t). 
For  a  full  stochastic  characterization  of  this, we need  the PDFs  of  all possible  com­
binations  of  samples  from  X (t), Y (t).  We  say  that  X (t)  and  Y (t)  are  independent 
if  every  set  of  samples  from X (t)  is  independent  of  every  set  of  samples  from  Y (t), 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

166  Chapter  9 

Random  Processes 

so  that  the  joint  PDF  factors  as  follows: 

, xk , y1 , 
· · · 
,Y (t ′ ) (x1 , 
· · · 
, yℓ )
fX (t1 ),
,X (tk ),Y (t ′  ),
1 ··· 
··· 
ℓ 
,X (tk ) (x1 , 
,Y (t ′ ) (y1 , 
= fX (t1 ),
· · · 
· · · 
, yℓ )  . 
, xk ).fY (t ′  ),
··· 
··· 
1 
ℓ
If  only  ﬁrst  and  second moments  are  of  interest,  then  in  addition  to  the  individual 
ﬁrst  and  second  moments  of  X (t)  and  Y (t)  respectively,  we  need  to  consider  the 
cross-moment  functions: 

(9.4) 

Cross-correlation: RX Y  (ti , tj ) = E [X (ti )Y (tj )],  and 
Cross-covariance:  CX Y  (ti , tj ) = E [(X (ti ) − µX (ti ))(Y (tj ) − µY  (tj ))] 
= RX Y  (ti , tj ) − µX (ti )µY  (tj ). 
If CX Y  (t1 , t2 ) = 0  for  all  t1 , t2 ,  we  say  that  the  processes X (t)  and  Y (t)  are  uncor­
related.  Note  again  that  the  term  “uncorrelated”  in  its  common  usage means  that 
the  processes  have  zero  covariance  rather  than  zero  correlation. 

(9.5) 

(9.6) 

Note  that  everything  we  have  said  above  can  be  carried  over  to  the  case  of  DT 
random  processes,  except  that  now  the  sampling  instants  are  restricted  to  be  dis­
crete  time  instants.  In  accordance  with  our  convention  of  using  square  brackets 
[  ]  around  the  time  argument  for  DT  signals,  we  will  write  µX [n]  for  the  mean 
· 
· 
of  a  random  process  X [  ]  at  time  n;  similarly,  we  will  write  RXX [ni , nj ]  for  the 
correlation  function  involving  samples  at  times  ni  and  nj ;  and  so  on. 

9.2  STRICT-SENSE  STATIONARITY 

In  general,  we  would  expect  that  the  joint  PDFs  associated  with  the  random  vari­
ables obtained by sampling a random process at an arbitrary number k  of arbitrary 
times  will  be  time-dependent,  i.e.,  the  joint  PDF 

,X (tk ) (x1 , 
· · · 
··· 
will  depend  on  the  speciﬁc  values  of  t1 , 
, tk .  If  all  the  joint  PDFs  stay  the  same 
· · · 
under  arbitrary  time  shifts,  i.e.,  if 

fX (t1 ),

, xk )

fX (t1 ),

,X (tk +τ ) (x1 , 
· · · 
,X (tk ) (x1 , 
· · · 
··· 
··· 
for  arbitrary  τ ,  then  the  random process  is  said  to be  strict-sense  stationary  (SSS). 
Said  another  way,  for  a  strict-sense  stationary  process,  the  statistics  depend  only 
on  the  relative  times  at  which  the  samples  are  taken,  not  on  the  absolute  times. 

, xk ) = fX (t1+τ ),

(9.7) 

, xk ) 

EXAMPLE  9.4 

Representing  an  i.i.d.  process 

Consider a DT random process whose values X [n] may be regarded as independently 
chosen  at  each  time  n  from  a  ﬁxed  PDF  fX (x),  so  the  values  are  independent  and 
identically  distributed,  thereby  yielding  what  is  called  an  i.i.d.  process.  Such  pro­
cesses  are  widely  used  in  modeling  and  simulation.  For  instance,  if  a  particular 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  9.3  Wide-Sense  Stationarity  167 

DT  communication  channel  corrupts  a  transmitted  signal  with  added  noise  that 
takes  independent  values  at  each  time  instant,  but  with  characteristics  that  seem 
unchanging  over  the  time  window  of  interest,  then  the  noise  may  be  well  modeled 
as an  i.i.d.  process.  It  is also easy to generate an  i.i.d.  process  in a simulation envi­
ronment, provided one can arrange a random-number generator to produce samples 
from  a  speciﬁed  PDF  (and  there  are  several  good  ways  to  do  this).  Processes  with 
more complicated dependence across time samples can then be obtained by ﬁltering 
or  other  operations  on  the  i.i.d.  process,  as  we  shall  see  in  the  next  chapter. 

For  such  an  i.i.d.  process,  we  can  write  the  joint  PDF  quite  simply: 

fX [n1 ],X [n2 ],

,X [nℓ ] (x1 , x2 , 
, xℓ ) = fX (x1 )fX (x2 ) 
fX (xℓ ) 
· · · 
· · · 
··· 
, nℓ .  The  process  is  clearly  SSS. 
for  any  choice  of  ℓ  and  n1 , 
· · · 

(9.8) 

9.3  WIDE-SENSE  STATIONARITY 

Of  particular  use  to  us  is  a  less  restricted  type  of  stationarity.  Speciﬁcally,  if  the 
mean  value  µX (ti )  is  independent  of  time  and  the  autocorrelation  RXX (ti , tj )  or 
equivalently the autocovariance CXX (ti , tj ) is dependent only on the time diﬀerence 
(ti  −  tj ),  then  the  process  is  said  to  be  wide-sense  stationary  (WSS).  Clearly  a 
process  that  is  SSS  is  also  WSS.  For  a  WSS  random  process  X (t),  therefore,  we 
have 

µX (t) = µX 
RXX (t1 , t2 ) = RXX (t1  + α, t2  + α)  for  every  α 
= RXX (t1  − t2 , 0)  . 
(Note  that  for  a Gaussian  process  (i.e.,  a  process  whose  samples  are  always  jointly 
Gaussian) WSS  implies  SSS,  because  jointly  Gaussian  variables  are  entirely  deter­
mined  by  the  their  joint  ﬁrst  and  second moments.) 

(9.10) 

(9.9) 

Two  random  processes  X (t)  and  Y (t)  are  jointly  WSS  if  their  ﬁrst  and  second 
moments  (including  the  cross-covariance)  are  stationary.  In  this  case  we  use  the 
notation  RX Y  (τ )  to  denote  E [X (t + τ )Y (t)]. 

EXAMPLE  9.5 

Random  Oscillators  Revisited 

Consider  again  the  harmonic  oscillators  as  introduced  in  Example  9.1,  i.e. 

X (t; A, Θ) = A cos(ω0 t + Θ) 

where  A  and  Θ  are  independent  random  variables,  and  now  ω0  is  ﬁxed  at  some 
known  value. 

If  Θ  is  actually  ﬁxed  at  the  constant  value  θ0 ,  then  every  outcome  is  of  the  form 
x(t) = A cos(ω0 t + θ0 ),  and  it  is  straightforward  to  see  that  this process  is not WSS 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

168  Chapter  9 

Random  Processes 

(and hence not SSS). For  instance,  if A has a nonzero mean value, µA  = 0,  then the 
expected  value  of  the  process,  namely  µA cos(ω0 t + θ0 ),  is  time  varying.  To  argue 
that the process is not WSS even when µA  = 0, we can examine the autocorrelation 
function.  Note  that  x(t)  is  ﬁxed  at  the  value  0  for  all  values  of  t  such  that ω0 t + θ0 
is  an  odd  multiple  of  π/2,  and  takes  the  values  ±A  half-way  between  such  points; 
the correlation between such samples taken π/ω0  apart in time can correspondingly 
be  0  (in  the  former  case)  or  −E [A2 ]  (in  the  latter).  The  process  is  thus  not WSS. 
On  the  other  hand,  if  Θ  is  distributed  uniformly  in  [−π , π ],  then 
Z  π  1 
cos(ω0 t + θ)dθ = 0  , 
µX (t) = µA 
−π  2π 
CXX (t1 , t2 ) = RXX (t1 , t2 ) 
= E [A2 ]E [cos(ω0 t1  + Θ) cos(ω0 t2  + Θ)] 
E [A2 ] 
2 

cos(ω0 (t2  − t1 ))  , 
so  the  process  is  WSS.  It  can  also  be  shown  to  be  SSS,  though  this  is  not  totally 
straightforward  to  show  formally. 

= 

(9.11) 

(9.12) 

To  simplify  notation  for  a  WSS  process,  we  write  the  correlation  function  as 
RXX (t1  − t2 );  the  argument  t1  − t2  is  referred  to  as  the  lag  at  which  the  corre­
lation  is  computed.  For  the  most  part,  the  random  processes  that  we  treat  will 
be  WSS  processes.  When  considering  just  ﬁrst  and  second  moments  and  not  en­
tire  PDFs  or  CDFs,  it  will  be  less  important  to  distinguish  between  the  random 
process  X (t)  and  a  speciﬁc  realization  x(t)  of  it —  so  we  shall  go  one  step  further 
in  simplifying  notation,  by  using  lower  case  letters  to  denote  the  random  process 
itself.  We  shall  thus  talk  of  the  random  process  x(t),  and —  in  the  case  of  a WSS 
process  —  denote  its  mean  by  µx  and  its  correlation  function  E {x(t + τ )x(t)}  by 
Rxx (τ ).  Correspondingly,  for DT we’ll  refer  to  the  random process x[n] and  (in  the 
WSS  case)  denote  its mean  by  µx  and  its  correlation  function  E {x[n + m]x[n]}  by 
Rxx [m]. 

9.3.1  Some  Properties  of WSS  Correlation  and  Covariance  Functions 

It  is  easily  shown  that  for  real-valued WSS  processes  x(t)  and  y(t)  the  correlation 
and  covariance  functions  have  the  following  symmetry  properties: 

Cxx (τ ) = Cxx (−τ ) 
Rxx (τ ) = Rxx (−τ )  ,
Cxy (τ ) = Cyx (−τ )  . 
Rxy (τ ) = Ryx (−τ )  ,
We  see  from  (9.13)  that  the  autocorrelation  and  autocovariance  have  even  symme­
try.  Similar  properties  hold  for  DT WSS  processes. 

(9.13) 
(9.14) 

Another  important  property  of  correlation  and  covariance  functions  follows  from 
noting  that  the  correlation  coeﬃcient  of  two  random  variables  has  magnitude  not 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

6
Section  9.4 

Summary  of  Deﬁnitions  and  Notation  169 

exceeding  1.  Applying  this  fact  to  the  samples  x(t)  and  x(t + τ )  of  the  random 
process  x(  )  directly  leads  to  the  conclusion  that 
· 

− Cxx (0)  ≤  Cxx (τ )  ≤  Cxx (0)  . 

(9.15) 

In  other  words,  the  autocovariance  function  never  exceeds  in  magnitude  its  value 
2  to  each  term above, we ﬁnd  the  following  inequality holds 
at  the origin.  Adding µx 
for  correlation  functions: 

2  ≤  Rxx (τ )  ≤  Rxx (0)  . 
− Rxx (0) + 2µx 

(9.16) 

In  Chapter  10  we  will  demonstrate  that  correlation  and  covariance  functions  are 
characterized  by  the  property  that  their  Fourier  transforms  are  real  and  non­
negative  at  all  frequencies,  because  these  transforms  describe  the  frequency  dis­
tribution  of  the  expected  power  in  the  random  process.  The  above  symmetry  con­
straints  and  bounds  will  then  follow  as  natural  consequences,  but  they  are  worth 
highlighting  here  already. 

9.4  SUMMARY  OF  DEFINITIONS  AND  NOTATION 

In this section we summarize some of the deﬁnitions and notation we have previously 
introduced.  As  in  Section  9.3,  we  shall  use  lower  case  letters  to  denote  random 
processes,  since we will  only  be  dealing  with  expectations  and  not  densities.  Thus, 
with  x(t)  and  y(t)  denoting  (real)  random  processes,  we  summarize  the  following 
deﬁnitions: 

mean  :

(t) △
µx = E {x(t)} 

autocorrelation  :
cross − correlation  :

Rxx

Rxy

(t1 , t2 ) △
= E {x(t1 )x(t2 )} 
(t1 , t2 ) △
= E {x(t1 )y(t2 )} 

(9.17) 

(9.18) 

(9.19) 

autocovariance  :

(t1 , t2 ) △
(t1 )][x(t2 ) − µx (t2 )]}
= E {[x(t1 ) − µx

Cxx
= Rxx (t1 , t2 ) − µx (t1 )µx (t2 ) 
(9.20)

(t1 , t2 ) △
(t1 )][y(t2 ) − µy (t2 )]}
= E {[x(t1 ) − µx

Cxy
= Rxy (t1 , t2 ) − µx (t1 )µy (t2 ) 
(9.21)

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

cross − covariance  :

170  Chapter  9 

Random  Processes 

strict-sense  stationary  (SSS):  all  joint  statistics  for  x(t1 ), x(t2 ), . . . , x(tℓ )  for  all  ℓ > 0 
and  all  choices  of  sampling  instants  t1 , · · ·  , tℓ 
depend  only  on  the  relative  locations  of  sampling  instants. 
wide-sense  stationary  (WSS):  µx (t)  is  constant  at  some  value  µx ,  and  Rxx (t1 , t2 )  is  a  function 
of  (t1  − t2 )  only,  denoted  in  this  case  simply  by  Rxx (t1  − t2 ); 
hence  Cxx (t1 , t2 )  is  a  function  of  (t1  − t2 )  only,  and 
written  as  Cxx (t1  − t2 ). 
x(t)  and  y(t)  are  individually WSS  and  Rxy (t1 , t2 )  is 
a  function  of  (t1  − t2 )  only,  denoted  simply  by 
Rxy (t1  − t2 );  hence  Cxy (t1 , t2 )  is  a  function  of  (t1  − t2 )  only, 
and  written  as  Cxy (t1  − t2 ). 

jointly  wide-sense  stationary: 

For WSS  processes  we  have,  in  continuous-time  and  with  simpler  notation, 

Rxx (τ ) = E {x(t + τ )x(t)} = E {x(t)x(t − τ )} 
Rxy (τ ) = E {x(t + τ )y(t)} = E {x(t)y(t − τ )}, 
and  in  discrete-time, 

Rxx [m] = E {x[n + m]x[n]} = E {x[n]x[n − m]} 
Rxy [m] = E {x[n + m]y [n]} = E {x[n]y [n − m]}. 
We  use  corresponding  (centered)  deﬁnitions  and  notation  for  covariances: 

Cxx (τ ), Cxy (τ ), Cxx [m],  and  Cxy [m]  . 

(9.22) 
(9.23) 

(9.24) 
(9.25) 

It  is worth noting  that an alternative  convention used  elsewhere  is  to deﬁne Rxy (τ ) 
(τ ) △
as Rxy
= E {x(t)y(t+ τ )}.
In our notation, this expectation would be denoted by 
Rxy (−τ ).  It’s important to be careful to take account of what notational convention 
is  being  followed  when  you  read  this  material  elsewhere,  and  you  should  also  be 
clear  about  what  notational  convention  we  are  using  in  this  text. 

9.5  FURTHER  EXAMPLES 

EXAMPLE  9.6 

Bernoulli  process 

The  Bernoulli  process,  a  speciﬁc  example  of  which  was  discussed  previously  in 
Example  9.3,  is  an  example  of  an  i.i.d.  DT  process  with 

P(x[n] = 1) = p 
P(x[n] = −1) = (1 − p) 
and  with  the  value  at  each  time  instant  n  independent  of  the  values  at  all  other 

(9.26) 
(9.27) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  9.5 

Further  Examples  171 

time  instants.  A  simple  calculation  results  in 
E {x[n]} = 2p − 1 = µx 
(1 
m = 0 
E {x[n + m]x[n]} = 
(2p − 1)2  m = 0 
Cxx [m] = E {(x[n + m] − µx )(x[n] − µx )} 
= {1 − (2p − 1)2 }δ [m] = 4p(1 − p)δ [m]  . 

(9.28) 

(9.29) 

(9.30) 
(9.31) 

EXAMPLE  9.7 

Random  telegraph  wave 

A  useful  example  of  a  CT  random  process  that  we’ll  make  occasional  reference 
to  is  the  random  telegraph  wave.  A  representative  sample  function  of  a  random 
telegraph  wave  process  is  shown  in  Figure  9.4.  The  random  telegraph  wave  can  be 
deﬁned  through  the  following  two  properties: 

x(t) 

+1 

−1 

�  t 

FIGURE  9.4  One  realization  of  a  random  telegraph  wave. 

1. 	 X (0) = ±1  with  probability  0.5. 
2.	 X (t)  changes  polarity  at Poisson  times,  i.e.,  the  probability  of  k  sign  changes

in  a  time  interval  of  length  T  is


P(k  sign  changes  in  an  interval  of  length  T ) = 

(λT )k e−λT 
k ! 

. 

(9.32) 

Property  2  implies  that  the  probability  of  a  non-negative,  even  number  of  sign 
changes  in  an  interval  of  length  T  is 
P(a  non-negative  even  #  of  sign  changes) =  X 
= e−λT  X 
∞ (λT )k
∞ 1 + (−1)k  (λT )k 
e−λT 
2 
k ! 
k !	
k=0 
k=0 
k  even 
Using  the  identity 

(9.33) 

∞ (λT )k 
λT  Xe  = 
k ! 
k=0 
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

6
	
172  Chapter  9 

Random  Processes 

equation  (9.33)  becomes 
P(a  non-negative  even #  of  sign  changes) = e−λT  (eλT  + e−λT ) 
2 
1 
=  (1 + e−2λT )  . 
2 

(9.34) 

= 

= 

1 
2 

. 

Similarly,  the  probability  of  an  odd  number  of  sign  changes  in  an  interval  of  length 
T  is  1 (1 − e−2λT ).  It  follows  that 
2 
P(X (t) = 1) = P(X (t) = 1 X (0) = 1)P(X (0) = 1) 
|
+ P(X (t) = 1|X (0) = −1)P(X (0) = −1) 
1 
=  P(even  #  of  sign  changes  in  [0, t]) 
2 
1 
+  P(odd  #  of  sign  changes  in  [0, t]) 
2 
1 ½ 
1 ½ 
¾ 
¾ 
1 
1 
(1 + e−2λt ) + 
(1 − e−2λt )
2 
2
2
2
Note that because of Property I, the expression  in the  last  line of Eqn.  (9.35)  is not 
needed, since the line before that already allows us to conclude that the answer is  1
2 : 
since  the  number  of  sign  changes  in  any  interval  must  be  either  even  or  odd,  their 
probabilities add up  to 1,  so P (X (t) = 1) =  1
2 .  However,  if Property 1  is  relaxed  to 
1 ,  then  the  above  computation must  be  carried  through 
allow P(X (0) = 1) = p0  =  2
to  the  last  line,  and  yields  the  result 
½ 
½ 
¾ 
¾
1 ©1 + (2p0  − 1)e−2λtª 
1 
1 
(1 − e−2λt )
(1 + e−2λt )  +(1−p0 ) 
2 
2 
2
(9.36) 
Returning  to  the  case  where  Property  1  holds,  so  P(X (t) = 1),  we  get 

P(X (t) = 1) = p0 

(9.35) 

= 

. 

µX (t) = 0,  and 
RXX (t1 , t2 ) = E [X (t1 )X (t2 )] 
= 1 × P (X (t1 ) = X (t2 )) + (−1) × P (X (t1 ) =6 X (t2 )) 
= e−2λ|t2−t1 |  . 

(9.37) 

(9.38) 

In  other  words,  the  process  is  exponentially  correlated  and WSS. 

9.6  ERGODICITY 

The  concept  of  ergodicity  is  sophisticated  and  subtle,  but  the  essential  idea  is  de­
scribed here.  We typically observe the outcome of a random process (e.g., we record 
a  noise waveform)  and want  to  characterize  the  statistics  of  the  random  process  by 
measurements  on  one  ensemble member.  For  instance, we  could  consider  the  time-
average  of  the  waveform  to  represent  the mean  value  of  the  process  (assuming  this 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

6
Section  9.7 

Linear  Estimation  of  Random  Processes  173 

mean  is  constant  for  all  time).  We  could  also  construct  histograms  that  represent 
the  fraction of  time  (rather  than  the probability-weighted  fraction of  the ensemble) 
that the waveform lies in diﬀerent amplitude bins, and this could be taken to reﬂect 
the  probability  density  across  the  ensemble  of  the  value  obtained  at  a  particular 
sampling time.  If the random process is such that the behavior of almost every par­
ticular  realization  over  time  is  representative  of  the  behavior  down  the  ensemble, 
then  the  process  is  called  ergodic. 

A  simple  example  of  a  process  that  is  not  ergodic  is  Example  9.2,  an  ensemble  of 
batteries.  Clearly,  for  this  example,  the behavior  of  any  realization  is not  represen­
tative  of  the  behavior  down  the  ensemble. 

hxi = 

Narrower  notions  of  ergodicity  may  be  deﬁned.  For  example,  if  the  time  average 
1  Z  T 
lim 
T →∞  2T 
−T 
almost  always  (i.e.  for  almost  every  realization  or  outcome)  equals  the  ensemble 
average  µX ,  then  the  process  is  termed  ergodic  in  the  mean.  It  can  be  shown, 
for  instance,  that  a  WSS  process  with  ﬁnite  variance  at  each  instant  and  with  a 
covariance  function  that  approaches  0  for  large  lags  is  ergodic  in  the  mean.  Note 
that  a  (nonstationary)  process  with  time-varying  mean  cannot  be  ergodic  in  the 
mean. 

x(t) dt 

(9.39) 

In  our  discussion  of  random  processes,  we  will  primarily  be  concerned  with  ﬁrst-
and  second-order  moments  of  random  processes.  While  it  is  extremely  diﬃcult 
to  determine  in  general  whether  a  random  process  is  ergodic,  there  are  criteria 
(speciﬁed  in  terms  of  the  moments  of  the  process)  that  will  establish  ergodicity 
in  the  mean  and  in  the  autocorrelation.  Frequently,  however,  such  ergodicity  is 
simply  assumed  for  convenience,  in  the  absence  of  evidence  that  the  assumption 
is  not  reasonable.  Under  this  assumption,  the  mean  and  autocorrelation  can  be 
obtained  from  time-averaging  on  a  single  ensemble  member,  through  the  following 
equalities: 
1  ZT
E {x(t)} =  lim 
T →∞  2T 
−T 
1  ZT
E {x(t)x(t + τ )} =  lim 
T →∞  2T 
−T 
A  random  process  for  which  (9.40)  and  (9.41)  are  true  is  referred  as  second-order 
ergodic. 

x(t)x(t + τ )dt 

x(t)dt 

(9.40) 

and 

(9.41) 

9.7  LINEAR  ESTIMATION  OF  RANDOM  PROCESSES 

A  common  class  of  problems  in  a  variety  of  aspects  of  communication,  control  and 
signal  processing  involves  the  estimation  of  one  random  process  from  observations 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

174  Chapter  9 

Random  Processes 

of  another,  or  estimating  (predicting)  future  values  from  the  observation  of  past 
values.  For example,  it  is common  in communication  systems that  the signal at the 
receiver  is  a  corrupted  (e.g.,  noisy)  version  of  the  transmitted  signal,  and we would 
like  to  estimate  the  transmitted  signal  from  the  received  signal.  Other  examples 
lie  in  predicting  weather  and  ﬁnancial  data  from  past  observations.  We  will  be 
treating  this  general  topic  in much more  detail  in  later  chapters,  but  a  ﬁrst  look  at 
it  here  can  be  beneﬁcial  in  understanding  random  processes. 

We  shall  ﬁrst  consider  a  simple  example  of  linear  prediction  of  a  random  process, 
then a more elaborate example of linear FIR ﬁltering of a noise-corrupted process to 
estimate  the underlying  random  signal.  We  conclude  the  section with  some  further 
discussion  of  the  basic  problem  of  linear  estimation  of  one  random  variable  from 
measurements  of  another. 

9.7.1  Linear  Prediction 

As  a  simple  illustration  of  linear  prediction,  consider  a  discrete-time  process  x[n]. 
Knowing  the  value  at  time  n0  we  may  wish  to  predict  what  the  value  will  be  m 
samples  into  the  future,  i.e.  at  time  n0  + m.  We  limit  the  prediction  strategy  to  a 
linear  one,  i.e.,  with  ˆx[n0  + m]  denoting  the  predicted  value,  we  restrict  ˆx[n0  + m] 
to  be  of  the  form 

xˆ[n0  + m] = ax[n0 ] + b 

(9.42) 

and  choose  the  prediction  parameters  a  and  b  to  minimize  the  expected  value  of 
the  square  of  the  error,  i.e.,  choose  a  and  b  to  minimize 

ǫ = E {(x[n0  + m] − xˆ[n0  + m])2 } 

(9.43) 

or 

(9.44) 

ǫ = E {(x[n0  + m] − ax[n0 ] − b)2 }. 
To  minimize  ǫ  we  set  to  zero  its  partial  derivative  with  respect  to  each  of  the  two 
parameters  and  solve  for  the  parameter  values.  The  resulting  equations  are 
E {(x[n0  + m] − ax[n0 ] − b)x[n0 ]} = E {(x[n0  + m] − xb[n0  + m])x[n0 ]} = 0 
(9.45a) 
E {x[n0  + m] − ax[n0 ] − b} = E {x[n0  + m] − xb[n0  + m]} = 0  . 
(9.45b) 
Equation  (9.45a)  states  that  the  error  x[n0  + m] − xb[n0  + m]  associated  with  the 
optimal  estimate  is  orthogonal  to  the  available  data  x[n0 ].  Equation  (9.45b)  states 
that  the  estimate  is  unbiased. 
Carrying out the multiplications and expectations in the preceding equations results 
in  the  following  equations,  which  can  be  solved  for  the  desired  constants. 

Rxx [n0  + m, n0 ] − aRxx [n0 , n0 ] − bµx [n0 ] = 0 
µx [n0  + m] − aµx [n0 ] − b = 0. 

(9.46a) 
(9.46b) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  9.7 

Linear  Estimation  of  Random  Processes  175 

If we assume that the process is WSS so that Rxx [n0+m, n0 ] = Rxx [m], Rxx [n0 , n0 ] = 
Rxx [0], and also assume that it is zero mean, (µx  = 0), then equations (9.46) reduce 
to 

a = Rxx [m]/Rxx [0] 
b = 0 

(9.47) 
(9.48) 

so  that 

Rxx [m] 
xb[n0  + m] = 
x[n0 ]. 
Rxx [0] 
If  the  process  is  not  zero  mean,  then  it  is  easy  to  see  that 
Cxx [m] 
xb[n0  + m] = µx  + 
(x[n0 ] − µx )  . 
Cxx [0] 
An  extension  of  this  problem  would  consider  how  to  do  prediction  when  measure­
ments of several past values are available.  Rather than pursue this case, we illustrate 
next  what  to  do  with  several  measurements  in  a  slightly  diﬀerent  setting. 

(9.50) 

(9.49) 

9.7.2  Linear  FIR  Filtering 

As another example, which we will treat in more generality in chapter 11 on Wiener 
ﬁltering,  consider  a  discrete-time  signal  s[n]  that  has  been  corrupted  by  additive 
noise d[n].  For  example,  s[n] might be a  signal  transmitted  over  a  channel and d[n] 
the  noise  introduced  by  the  channel.  The  received  signal  r[n]  is  then 

r[n] = s[n] + d[n]. 

(9.51) 

Assume  that  both  s[n]  and  d[n]  are  zero-mean  random  processes  and  are  uncor­
related.  At  the  receiver  we  would  like  to  process  r[n]  with  a  causal  FIR  (ﬁnite 
impulse  response)  ﬁlter  to  estimate  the  transmitted  signal  s[n]. 

d[n] 

s[n] 

�� 
� 
⊕ 
r[n] 

�  sb[n]
FIGURE  9.5  Estimating  the  noise  corrupted  signal. 

h[n] 

If  h[n]  is  a  causal  FIR  ﬁlter  of  length  L,  then 
sb[n] = X 
L−1
h[k ]r[n − k ]. 
k=0 
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

(9.52) 

176  Chapter  9 

Random  Processes 

∂ ǫ 
∂h[m] 

We would  like  to  determine  the  ﬁlter  coeﬃcients  h[k ]  to minimize  the mean  square 
error  between  sb[n]  and  s[n],  i.e.,  minimize  ǫ  given  by 
ǫ = E (s[n] − sb[n])2 
= E (s[n] − X 
L−1
h[k ]r[n − k ])2 . 
k=0 
∂ ǫ 
To  determine  h,  we  set  ∂h[m]  =  0  for  each  of  the  L  values  of  m.  Taking  this 
derivative,  we  get 
= −E {2(s[n] − X 
h[k ]r[n − k ])r[n − m]}
k 
= −E {2(s[n] − sb[n])r[n − m]}
m = 0, 1, 
· · · 
, L − 1 
= 0 
(9.54) 
which  is  the  orthogonality  condition we  should be  expecting:  the  error  (s[n] − sb[n]) 
associated  with  the  optimal  estimate  is  orthogonal  to  the  available  data,  r[n − m]. 
Carrying  out  the  multiplications  in  the  above  equations  and  taking  expectations 
results  in 
L−1X 
h[k ]Rrr [m − k ] = Rsr [m]  , m = 0, 1, 
, L − 1 
· · · 
k=0 
Eqns.  (9.55)  constitute  L  equations  that  can  be  solved  for  the  L  parameters  h[k ]. 
With  r[n] =  s[n] +  d[n],  it  is  straightforward  to  show  that  Rsr [m] =  Rss [m] + 
Rsd [m] and  since we assumed  that s[n] and d[n] are uncorrelated,  then Rsd [m] = 0. 
Similarly,  Rrr [m] = Rss [m] + Rdd [m]. 
These  results  are  also  easily  modiﬁed  for  the  case  where  the  processes  no  longer 
have  zero  mean. 

(9.53) 

(9.55) 

9.8  THE  EFFECT  OF  LTI  SYSTEMS  ON  WSS  PROCESSES 

Your  prior  background  in  signals  and  systems,  and  in  the  earlier  chapters  of  these 
notes,  has  characterized how LTI  systems  aﬀect  the  input  for deterministic  signals. 

We  will  see  in  later  chapters  how  the  correlation  properties  of  a  random  process, 
and the eﬀects of LTI systems on these properties, play an  important role  in under­
standing  and  designing  systems  for  such  tasks  as  ﬁltering,  signal  detection,  signal 
estimation  and  system  identiﬁcation.  We  focus  in  this  section  on  understanding 
in  the  time  domain  how  LTI  systems  shape  the  correlation  properties  of  a  random 
process.  In  Chapter  10  we  develop  a  parallel  picture  in  the  frequency  domain,  af­
ter  establishing  that  the  frequency  distribution  of  the  expected  power  in  a  random 
signal  is  described  by  the  Fourier  transform  of  the  autocorrelation  function. 

Consider an LTI system whose input is a sample function of a WSS random process 
x(t),  i.e., a signal chosen by a probabilistic experiment  from the ensemble that con­
stitutes  the  random process x(t); more  simply, we  say  that  the  input  is  the  random 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  9.8 

The  Eﬀect  of  LTI  Systems  on WSS  Processes  177 

process  x(t).  The  WSS  input  is  characterized  by  its  mean  and  its  autocovariance 
or  (equivalently)  autocorrelation  function. 

Among other considerations, we are  interested  in knowing when the output process 
y(t) —  i.e.,  the ensemble of signals obtained as responses to the signals  in the  input 
ensemble — will itself be WSS, and want to determine its mean and autocovariance 
or  autocorrelation  functions,  as well  as  its  cross-correlation with  the  input  process. 
For  an  LTI  system  whose  impulse  response  is  h(t),  the  output  y(t)  is  given  by  the 
convolution 
Z  +∞ 
Z  +∞ 
−∞ 
−∞ 
for any speciﬁc input x(t) for which the convolution is well-deﬁned.  The convolution 
is well-deﬁned if, for instance, the input x(t) is bounded and the system is bounded-
input  bounded-output  (BIBO)  stable,  i.e.  h(t)  is  absolutely  integrable.  Figure  9.6 
indicates what the two components of the integrand in the convolution integral may 
look  like. 

h(v)x(t − v)dv = 

y(t) = 

x(v)h(t − v)dv 

(9.56) 

x(v) 

h(t - v) 

v 

v 

t

FIGURE  9.6  Illustration  of  the  two  terms  in  the  integrand  of  Eqn.  (9.56) 

Rather  than  requiring  that  every  sample  function of our  input process be bounded, 
it  will  suﬃce  for  our  convolution  computations  below  to  assume  that  E [x2 (t)]  = 
Rxx (0)  is ﬁnite.  With  this assumption,  and also assuming  that  the  system  is BIBO 
stable,  we  ensure  that  y(t)  is  a  well-deﬁned  random  process,  and  that  the  formal 
manipulations  we  carry  out  below  —  for  instance,  interchanging  expectation  and 
convolution  —  can  all  be  justiﬁed  more  rigorously  by  methods  that  are  beyond 
our  scope  here.  In  fact,  the  results  we  obtain  can  also  be  applied,  when  properly 
interpreted,  to  cases  where  the  input  process  does  not  have  a  bounded  second 
moment,  e.g., when x(t)  is  so-called CT white  noise,  for which Rxx (τ ) = δ(τ ).  The 
results  can  also  be  applied  to  a  system  that  is  not  BIBO  stable,  as  long  as  it  has  a 
well-deﬁned  frequency  response H (jω),  as  in  the  case  of  an  ideal  lowpass  ﬁlter,  for 
example. 

We  can  use  the  convolution  relationship  (9.56)  to  deduce  the  ﬁrst- and  second-
order properties of y(t).  What we shall establish  is that y(t)  is  itself WSS, and that 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

178  Chapter  9 

Random  Processes 

x(t)  and  y(t)  are  in  fact  jointly  WSS.  We  will  also  develop  relationships  for  the 
autocorrelation  of  the  output  and  the  cross-correlation  between  input  and  output. 

First,  consider  the  mean  value  of  the  output.  Taking  the  expected  value  of  both 
sides  of  (9.56),  we  ﬁnd 
¾ 
½Z  +∞ 
h(v)x(t − v) dv 
E [y(t)] = E
−∞ 
Z  +∞
h(v)E [x(t − v)] dv 
−∞Z  +∞ 
h(v)µx dv 
−∞Z  +∞ 
= µx 
−∞ 
= H (j 0) µx  = µy  . 

h(v) dv 

(9.57) 

= 

= 

In  other words,  the mean  of  the  output process  is  constant,  and  equals  the mean  of 
the  input  scaled  by  the  the  DC  gain  of  the  system.  This  is  also  what  the  response 
of  the  system  would  be  if  its  input  were  held  constant  at  the  value  µx . 
The  preceding  result  and  the  linearity  of  the  system  also  allow  us  to  conclude  that 
applying the zero-mean WSS process x(t) − µx  to the input of the stable LTI system 
would  result  in  the  zero-mean  process  y(t) − µy  at  the  output.  This  fact  will  be 
useful  below  in  converting  results  that  are  derived  for  correlation  functions  into 
results  that  hold  for  covariance  functions. 

Next  consider  the  cross-correlation  between  output  and  input: 
½·  Z  +∞ 
¾ 
¸
E {y(t + τ )x(t)} = E
h(v)x(t + τ  − v)dv  x(t) 
−∞ 
Z  +∞ 
h(v)E {x(t + τ  − v)x(t)}dv  . 
−∞ 
Since  x(t)  is WSS,  E {x(t + τ  − v)x(t)} = Rxx (τ  − v),  so 
Z  +∞ 
h(v)Rxx (τ  − v)dv 
E {y(t + τ )x(t)} = 
−∞ 
= h(τ ) ∗ Rxx (τ )

= Ryx (τ )  . 

= 

(9.58) 

(9.59)


Note  that  the  cross-correlation  depends  only  on  the  lag  τ  between  the  sampling 
instants  of  the  output  and  input  processes,  not  on  both  τ  and  the  absolute  time 
location  t.  Also,  this  cross-correlation  between  the  output  and  input  is  determinis­
tically  related  to  the  autocorrelation  of  the  input,  and  can  be  viewed  as  the  signal 
that would result if the system input were the autocorrelation function, as indicated 
in  Figure  9.7. 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  9.8 

The  Eﬀect  of  LTI  Systems  on WSS  Processes  179 

Rxx (τ ) 

� 

h(τ ) 

�  Ryx (τ )

FIGURE  9.7  Representation  of  Eqn.  (9.59) 

We  can  also  conclude  that 
Rxy (τ ) = Ryx (−τ ) = Rxx (−τ ) ∗ h(−τ ) = Rxx (τ ) ∗ h(−τ )  , 
where the second equality follows from Eqn.  (9.59) and the fact that time-reversing 
the  two  functions  in  a  convolution  results  in  time-reversal  of  the  result,  while  the 
last equality follows from the symmetry Eqn.  (9.13) of the autocorrelation function. 

(9.60) 

The  above  relations  can  also  be  expressed  in  terms  of  covariance  functions,  rather 
than  in  terms  of  correlation  functions.  For  this,  simply  consider  the  case where  the 
input  to  the  system  is  the  zero-mean  WSS  process  x(t) − µx ,  with  corresponding 
zero-mean output y(t) − µy .  Since the correlation  function  for x(t) − µx  is  the same 
as  the  covariance  function  for  x(t),  i.e.,  since 

Rx−µx ,x−µx (τ ) = Cxx (τ )  , 
the  results  above  hold  unchanged  when  every  correlation  function  is  replaced  by 
the  corresponding  covariance  function.  We  therefore  have,  for  instance,  that 

(9.61) 

¾ 
¸
h(v)x(t + τ  − v)dv  y(t) 

Cyx (τ ) = h(τ ) ∗ Cxx (τ ) 
Next  we  consider  the  autocorrelation  of  the  output  y(t): 
½·  Z  +∞ 
E {y(t + τ )y(t)} = E
−∞ 
Z  +∞ 
h(v) E {x(t + τ  − v)y(t)} dv 
| 
{z 
}
−∞ 
Rxy (τ −v) 
Z  +∞ 
h(v)Rxy (τ  − v)dv 
−∞ 
= h(τ ) ∗ Rxy (τ )

= Ryy (τ )  . 

= 

= 

(9.62) 

(9.63)


Note  that  the  autocorrelation  of  the  output  depends  only  on  τ ,  and  not  on  both 
τ  and  t.  Putting  this  together  with  the  earlier  results,  we  conclude  that  x(t)  and 
y(t)  are  jointly WSS,  as  claimed. 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

180  Chapter  9 

Random  Processes 

The  corresponding  result  for  covariances  is 

Cyy (τ ) = h(τ ) ∗ Cxy (τ )  . 
Combining  (9.63)  with  (9.60),  we  ﬁnd  that 
Ryy (τ ) = Rxx (τ ) ∗  h(τ ) ∗ h(−τ ) = Rxx (τ ) ∗ Rhh (τ )  . 
| 
{z 
} 
△ 
h(τ )∗h(−τ )=Rhh (τ ) 
The function Rhh (τ ) is typically referred to as the deterministic autocorrelation 
function  of  h(t),  and  is  given  by 
Z  +∞ 
−∞ 
For  the  covariance  function  version  of  (9.65),  we  have 
Cyy (τ ) = Cxx (τ ) ∗  h(τ ) ∗ h(−τ ) = Cxx (τ ) ∗ Rhh (τ )  . 
{z 
} 
| 
△ 
h(τ )∗h(−τ )=Rhh (τ ) 
Note  that  the  deterministic  correlation  function  of  h(t)  is  still  what  we  use,  even 
when relating the covariances of the input and output.  Only the means of the input 
and  output  processes  get  adjusted  in  arriving  at  the  present  result;  the  impulse 
response  is  untouched. 

Rhh (τ ) = h(τ ) ∗ h(−τ ) = 

h(t + τ )h(t)dt  . 

(9.66) 

(9.64) 

(9.65) 

(9.67) 

The  correlation  relations  in  Eqns.  (9.59),  (9.60),  (9.63)  and  (9.65),  as  well  as 
their  covariance  counterparts,  are  very  powerful,  and  we  will  make  considerable 
use  of  them.  Of  equal  importance  are  their  statements  in  the  Fourier  and  Laplace 
transform domains.  Denoting the Fourier and Laplace transforms of the correlation 
function  Rxx (τ )  by  Sxx (jω)  and  Sxx (s)  respectively,  and  similarly  for  the  other 
correlation  functions  of  interest,  we  have: 
Syy (jω) = Sxx (jω)|H (jω)| 2 , 
Syy (s) = Sxx (s)H (s)H (−s)  . 
We can denote the Fourier and Laplace transforms of the covariance function Cxx (τ ) 
by Dxx (jω) and Dxx (s) respectively, and similarly for the other covariance functions 
of  interest,  and  then  write  the  same  sorts  of  relationships  as  above. 

Syx (jω) = Sxx (jω)H (jω),
Syx (s) = Sxx (s)H (s),

(9.68) 

Exactly  parallel  results  hold  in  the  DT  case.  Consider  a  stable  discrete-time  LTI 
system whose  impulse response  is h[n] and whose  input  is the WSS random process 
x[n].  Then, as in the continuous-time case, we can conclude that the output process 
y [n]  is  jointly  WSS  with  the  input  process  x[n],  and 
µy  = µx X 
∞
h[n] 
−∞ 
Ryx [m] = h[m] ∗ Rxx [m] 
Ryy [m] = Rxx [m] ∗ Rhh [m]  , 

(9.70) 
(9.71) 

(9.69) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  9.8 

The  Eﬀect  of  LTI  Systems  on WSS  Processes  181 

where  Rhh [m]  is  the  deterministic  autocorrelation  function  of  h[m],  deﬁned  as 
Rhh [m] =  X 
+∞
h[n + m]h[n]  . 
n=−∞ 
The  corresponding  Fourier  and  Z -transform  statements  of  these  relationships  are: 
µy  = H (ej 0 )µx  , Syx (ejΩ ) = Sxx (ejΩ )H (ejΩ ) , Syy (ejΩ ) = Sxx (ejΩ )|H (ejΩ )| 2 , 
µy  = H (1)µx  ,
Syx (z ) = Sxx (z )H (z )  ,
Syy (z ) = Sxx (z )H (z )H (1/z ). 
(9.73) 

(9.72) 

All  of  these  expressions  can  also  be  rewritten  for  covariances  and  their  transforms. 

The basic  relationships  that we have developed  so  far  in  this  chapter  are  extremely 
powerful.  In  Chapter  10  we  will  use  these  relationships  to  show  that  the  Fourier 
transform  of  the  autocorrelation  function  describes  how  the  expected  power  of  a 
WSS  process  is  distributed  in  frequency.  For  this  reason,  the  Fourier  transform  of 
the autocorrelation  function  is termed the power  spectral  density  (PSD) of the 
process. 

The relationships developed in this chapter are also very important in using random 
processes  to measure  or  identify  the  impulse  response  of  an LTI  system.  For  exam­
ple, from (9.70), if the input x[n] to a DT LTI system is a WSS random process with 
autocorrelation  function  Rxx [m] =  δ [m],  then  by  measuring  the  cross-correlation 
between  the  input  and  output  we  obtain  a  measurement  of  the  system  impulse  re­
sponse.  It  is  easy  to  construct an  input process with autocorrelation  function  δ [m], 
for  example  an  i.i.d.  process  that  is  equally  likely  to  take  the  values +1  and −1  at 
each  time  instant. 

As  another  example,  suppose  the  input  x(t)  to  a  CT  LTI  system  is  a  random 
telegraph  wave,  with  changes  in  sign  at  times  that  correspond  to  the  arrivals  in  a 
Poisson  process  with  rate  λ,  i.e., 

. 

P(k  switches  in  an  interval  of  length  T ) = 

(λT )k e−λT 
k ! 
Then, assuming x(0) takes the values ±1 with equal probabilities, we can determine 
that  the  process  x(t)  has  zero  mean  and  correlation  function  Rxx (τ ) =  e−2λ|τ | ,  so 
it  is WSS  (for  t ≥ 0).  If we  determine  the  cross-correlation Ryx (τ ) with  the  output 
y(t)  and  then  use  the  relation 
Ryx (τ ) = Rxx (τ ) ∗ h(τ )  , 
we can obtain the system impulse response h(τ ).  For example,  if Syx (s), Sxx (s) and 
H (s)  denote  the  associated  Laplace  transforms,  then 

(9.74) 

(9.75) 

Syx (s)
Sxx (s) 
Note  that Sxx (s)  is a  rather well-behaved  function of  the  complex variable s  in  this 
case,  whereas  any  particular  sample  function  of  the  process  x(t)  would  not  have 
such  a  well-behaved  transform.  The  same  comment  applies  to  Syx (s). 

H (s) = 

(9.76) 

. 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

182  Chapter  9 

Random  Processes 

As  a  third  example,  suppose  that  we  know  the  autocorrelation  function  Rxx [m] 
of  the  input  x[n]  to  a  DT  LTI  system,  but  do  not  have  access  to  x[n]  and  there­
fore  cannot  determine  the  cross-correlation  Ryx [m]  with  the  output  y [n],  but  can 
determine  the  output  autocorrelation  Ryy [m].  For  example,  if 

(9.77) 

Rxx [m] = δ [m] 
and  we  determine  Ryy [m]  to  be  Ryy [m] = ¡ 
1 ¢|m| ,  then 
2
µ 
1 ¶|m|
Ryy [m] = 
= Rhh [m] = h[m] ∗ h[−m]. 
2 
Equivalently, H (z )H (z−1 ) can be obtained from the Z -transform Syy (z ) of Ryy [m]. 
Additional  assumptions  or  constraints,  for  instance  on  the  stability  and  causality 
of  the  system  and  its  inverse,  may  allow  one  to  recover  H (z )  from  knowledge  of 
H (z )H (z−1 ). 

(9.78) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

MIT OpenCourseWare
http://ocw.mit.edu 

6.011 Introduction to Communication, Control, and Signal Processing 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

