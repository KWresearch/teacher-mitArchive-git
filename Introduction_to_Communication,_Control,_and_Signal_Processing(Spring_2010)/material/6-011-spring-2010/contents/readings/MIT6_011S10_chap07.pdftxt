C H A P T E R 

7 

Probabilistic Models 

INTRODUCTION 

In  the  preceding  chapters  our  emphasis  has  been  on  deterministic  signals.  In  the 
remainder of this text we expand the class of signals considered to include those that 
are  based  on  probabilistic  models,  referred  to  as  random  or  stochastic  processes. 
In  introducing  this  important  class  of  signals,  we  begin  in  this  chapter  with  a 
review of  the basics of probability and  random variables.  We assume  that you have 
encountered  this  foundational  material  in  a  previous  course,  but  include  a  review 
here  for  convenient  reference  and  to  establish  notation.  In  the  following  chapter 
and  beyond,  we  apply  these  concepts  to  deﬁne  and  discuss  the  class  of  random 
signals. 

7.1  THE  BASIC  PROBABILITY  MODEL 

Associated  with  a  basic  probability  model  are  the  following  three  components,  as 
indicated  in  Figure  7.1: 

1. 	 Sample  Space  The  sample  space  Ψ  is  the  set  of  all  possible  outcomes  ψ  of 
the  probabilistic  experiment  that  the  model  represents.  We  require  that  one 
and  only  one  outcome  be  produced  in  each  experiment  with  the model. 
2.	 Event  Algebra  An  event  algebra  is  a  collection  of  subsets  of  the  sample 
space —  referred  to  as  events  in  the  sample  space —  chosen  such  that  unions 
of  events  and  complements  of  events  are  themselves  events  (i.e.,  are  in  the 
collection  of  subsets).  We  say  that  a  particular  event  has  occurred  if  the 
outcome  of  the  experiment  lies  in  this  event  subset;  thus  Ψ  is  the  “certain 
event” because it always occurs, and the empty set ∅ is the “impossible event” 
because  it  never  occurs.  Note  that  intersections  of  events  are  also  events, 
because  intersections  can  be  expressed  in  terms  of  unions  and  complements. 
3.	 Probability  Measure  A  probability  measure  associates  with  each  event  A 
a  number  P (A),  termed  the  probability  of  A,  in  such  a  way  that: 

(a)  P (A) ≥ 0 ; 
(b)  P (Ψ) = 1 ; 
(c)  If  A ∩ B  = ∅,  i.e.,  if  events  A  and  B  are  mutually  exclusive,  then 
P (A ∪ B ) = P (A) + P (B ) . 

c	
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

121

122 

 
Chapter 7

 

Probabilistic  Models 

Sample  Space  Ψ 

Outcome  ψ 

Collection  of  outcomes 
(Event) 

� 

� 

FIGURE  7.1  Sample  space  and  events. 

Note that for any particular case we often have a range of options in specifying what 
constitutes an outcome,  in deﬁning an event algebra, and  in assigning a probability 
measure.  It  is generally convenient  to have as  few elements or outcomes as possible 
in a sample  space, but we need enough of them to enable speciﬁcation of  the events 
of  interest  to  us.  It  is  typically  convenient  to  pick  the  smallest  event  algebra  that 
contains  the  events  of  interest.  We  also  require  that  there  be  an  assignment  of 
probabilities to events that is consistent with the above conditions.  This assignment 
may  be  made  on  the  basis  of  symmetry  arguments  or  in  some  other  way  that  is 
suggested  by  the  particular  application. 

7.2	 CONDITIONAL  PROBABILITY,  BAYES’  RULE,  AND  INDEPEN­
DENCE 

The probability of event A, given that event B  has occurred,  is denoted by P (A B ). 
|
Knowing  that  B  has  occurred  in  eﬀect  reduces  the  sample  space  to  the  outcomes 
in  B ,  so  a  natural  deﬁnition  of  the  conditional  probability  is 

Δ  P (A ∩ B )
P (A|B ) = 
P (B ) 
It  is  straightforward  to verify  that  this deﬁnition of  conditional probability yields a 
valid  probability measure  on  the  sample  space B .  The  preceding  equation  can  also 
be  rearranged  to  the  form 

if  P (B ) > 0  .	

(7.1) 

P (A ∩ B ) = P (A|B )P (B ) .	
We  often write P (AB ) or P (A, B )  for  the  joint probability P (A ∩ B ).  If P (B ) = 0, 
then  the  conditional  probability  in  (7.1)  is  undeﬁned. 

(7.2) 

By  symmetry,  we  can  also  write 

P (A ∩ B ) = P (B |A)P (A) 	
Combining  the  preceding  two  equations,  we  obtain  one  form  of  Bayes’  rule  (or 
theorem),  which  is  at  the  heart  of  much  of  what  we’ll  do  with  signal  detection, 

(7.3) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  7.2 

Conditional  Probability,  Bayes’  Rule,  and  Independence  123 

classiﬁcation,  and  estimation: 

)P 
P (A
(B ) 
|B
(A
P
)

P (B |A) = 
A more detailed form of Bayes’ rule can be written for the conditional probability of 
one  of  a  set  of  events  {Bj }  that  are mutually  exclusive  and  collectively  exhaustive, 
i.e.  Bℓ  ∩ Bm  = ∅  if  ℓ =6 m,  and S Bj  = Ψ.  In  this  case, 
j 
P (A ∩ Bj ) = X 
P (A) = X 
P (A|Bj )P (Bj ) 
j 
j
P (A|Bℓ )P (Bℓ ) 
P (Bℓ A) = 
Pj  P (A|Bj )P (Bj ) 
|
Events  A  and  B  are  said  to  be  independent  if 
P (A B ) = P (A) 
|
or  equivalently  if  the  joint  probability  factors  as 

so  that 

(7.5) 

(7.4) 

(7.6) 

(7.7) 

P (A ∩ B ) = P (A)P (B )  . 
More  generally,  a  collection  of  events  is  said  to  be  mutually  independent  if  the 
probability  of  the  intersection  of  events  from  this  collection,  taken  any  number  at 
a  time,  is  always  the  product  of  the  individual  probabilities.  Note  that  pairwise 
independence  is  not  enough.  Also,  two  sets  of  events  A  and  B  are  said  to  be 
independent  of  each  other  if  the  probability  of  an  intersection  of  events  taken  from 
these two sets always factors into the product of the joint probability of those events 
that  are  in  A  and  the  joint  probability  of  those  events  that  are  in  B . 

(7.8) 

EXAMPLE  7.1 

Transmission  errors  in  a  communication  system 

A  communication  system  transmits  symbols  labeled  A,  B ,  and  C .  Because  of 
errors  (noise)  introduced  by  the  channel,  there  is  a  nonzero  probability  that  for 
each  transmitted  symbol,  the  received  symbol  diﬀers  from  the  transmitted  one. 
Table  7.1  describes  the  joint  probability  for  each  possible  pair  of  transmitted  and 
received  symbols  under  a  certain  set  of  system  conditions. 

Symbol  sent  A 
0.05 
A 
0.13 
B 
0.12 
C 

Symbol  received 
B 
0.10 
0.08 
0.07 

C 
0.09 
0.21 
0.15 

TABLE  7.1  Joint  probability  for  each  possible  pair  of  transmitted  and  received 
symbols 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

124  Chapter  7 

Probabilistic  Models 

For  notational  convenience  let’s  use  As ,  Bs ,  Cs  to  denote  the  events  that  A,  B  or 
C  respectively  is  sent,  and  Ar ,  Br ,  Cr  to  denote  A,  B  or  C  respectively  being  re­
ceived.  So,  for  example, P (Ar , Bs ) = 0.13  and P (Cr , Cs ) = 0.15.  To determine  the 
marginal probability P (Ar ), we  sum  the probabilities  for all  the mutually  exclusive 
ways  that  A  is  received.  So,  for  example, 

P (Ar ) =  P (Ar , As ) + P (Ar , Bs ) + P (Ar , Cs ) 
= 
.05 + .13 + .12 = 0.3  . 

(7.9) 

Similarly  we  can  determine  the marginal  probability  P (As )  as 

P (As ) = P (Ar , As ) + P (Br , As ) + P (Cr , As ) = 0.24 

(7.10) 

In a communication context, it may be important to know the probability, for exam­
ple,  that  C  was  sent,  given  that  B  was  received,  i.e.,  P (Cs Br ).  That  information 
|
is  not  entered  directly  in  the  table  but  can  be  calculated  from  it  using  Bayes’  rule. 
Speciﬁcally,  the  desired  conditional  probability  can  be  expressed  as 

P (Cs , Br )
P (Cs |Br ) = 
P (Br ) 
The  numerator  in  (7.11)  is  given  directly  in  the  table  as  .07.  The  denominator  is 
calculated  as  P (Br ) = P (Br , As ) + P (Br , Bs ) + P (Br , Cs ) = 0.25.  The  result  then 
is  that  P (Cs Br ) = 0.28. 
|
In  communication  systems  it  is  also  often  of  interest  to  measure  or  calculate  the 
probability of a  transmission error.  Denoting  this by Pt  it would correspond  to any 
of  the  following  mutually  exclusive  events  happening: 

(7.11) 

(As  ∩ Br ), (As  ∩ Cr ), (Bs  ∩ Ar ), (Bs  ∩ Cr ), (Cs  ∩ Ar ), (Cs  ∩ Br ) 
Pt  is  therefore  the  sum  of  the  probabilities  of  these  six  mutually  exclusive  events, 
and  all  these  probabilities  can  be  read  directly  from  the  table  in  the  oﬀ-diagonal 
locations,  yielding  Pt  = 0.72. 

(7.12) 

7.3  RANDOM  VARIABLES 

A  real-valued  random  variable  X (  )  is  a  function  that  maps  each  outcome  ψ  of  a 
· 
probabilistic  experiment  to  a  real  number X (ψ),  which  is  termed  the  realization  of 
(or value taken by) the random variable in that experiment.  An additional technical 
requirement  imposed  on  this  function  is  that  the  set  of  outcomes  {ψ}  that maps  to 
the interval X  ≤ x must be an event in Ψ,  for all real numbers x.  We shall typically 
· 
just  write  the  random  variable  as X  instead  of  X (  ) or  X (ψ). 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section 7.4 

Cumulative Distribution, Probability Density, and Probability Mass Function For Random Variables  125 

Ψ 

�Real  line 

� 

X (ψ) 

ψ 

FIGURE  7.2  A  random  variable. 

It  is  often  also  convenient  to  consider  random  variables  taking  values  that  are 
not  speciﬁed  as  real  numbers  but  rather  a  ﬁnite  or  countable  set  of  labels,  say 
L0 , L1 , L2 , . . ..  For  instance,  the  random  status  of  a machine may  be  tracked  using 
the  labels  Idle,  Busy,  and  Failed.  Similarly,  the  random  presence  of  a  target  in  a 
radar  scan  can  be  tracked  using  the  labels  Absent  and  Present.  We  can  think  of 
these  labels  as  comprising  a  set  of  mutually  exclusive  and  collectively  exhaustive 
events,  where  each  such  event  comprises  all  the  outcomes  that  carry  that  label. 
We  refer  to  such  random  variables  as  random  events,  mapping  each  outcome  ψ 
of  a  probabilistic  experiment  to  the  label  L(ψ),  chosen  from  the  possible  values 
L0 , L1 , L2 , . . ..  We  shall  typically  just  write  L  instead  of  L(ψ). 

7.4	 CUMULATIVE  DISTRIBUTION,  PROBABILITY  DENSITY,  AND 
PROBABILITY  MASS  FUNCTION  FOR  RANDOM  VARIABLES 

Cumulative  Distribution  Functions  For  a  (real-valued)  random  variable  X , 
the probability of  the event comprising all ψ  for which X (ψ) ≤ x  is described using 
the  cumulative  distribution  function  (CDF)  FX (x): 

We  can  therefore  write 

FX (x) = P (X  ≤ x)  .	

(7.13) 

P (a < X  ≤ b) = FX (b) − FX (a)  .	
In  particular,  if  there  is  a  nonzero  probability  that X  takes  a  speciﬁc  value  x1 ,  i.e. 
if  P (X  =  x1 )  >  0,  then  FX (x)  will  have  a  jump  at  x1  of  height  P (X  =  x1 ),  and 
FX (x1 ) − FX (x1−) = P (X  = x1 ).  The CDF  is  nondecreasing  as  a  function  of  x;  it 
starts  from  FX (−∞) = 0  and  rises  to  FX (∞) = 1. 
A  related  function  is  the  conditional  CDF  FX |L (x|Li ),  used  to  describe  the  distri­
bution  of X  conditioned  on  some  random  event L  taking  the  speciﬁc  value Li ,  and 
assuming  P (L = Li ) > 0: 

(7.14) 

FX |L (x|Li ) = P (X  ≤ x|L = Li ) = 

P (X  ≤ x, L = Li )
P (L = Li ) 

. 

(7.15) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

126

 

 
Chapter 7

 

Probabilistic  Models 

�FX (x) 

1 

x1 

� 
x 

FIGURE  7.3  Example  of  a  CDF. 

Probability Density Functions  The probability density function (PDF) fX (x) 
of  the  random  variable X  is  the  derivative  of  FX (x): 

fX (x) = 

. 

(7.16) 

dFX (x)
dx 
It  is  of  course  always  non-negative  because  FX (x)  is  nondecreasing.  At  points  of 
discontinuity  in FX (x),  corresponding  to values of x  that have non-zero probability 
of  occurring,  there  will  be  (Dirac)  impulses  in  fX (x),  of  strength  or  area  equal  to 
the  height  of  the  discontinuity.  We  can  write 
Z  b 
P (a < X  ≤ b) = 
a 
(Any  impulse  of  fX (x)  at  b  would  be  included  in  the  integral,  while  any  impulse 
at  a  would  be  left  out  —  i.e.  the  integral  actually  goes  from  a+  to  b+.)  We  can 
heuristically  think  of  fX (x) dx  as  giving  the  probability  that  X  lies  in  the  interval 
(x − dx, x]: 

P (x − dx < X  ≤ x) ≈ fX (x) dx  . 
(7.18) 
Note  that  at  values  of  x  where  fX (x)  does  not  have  an  impulse,  the  probability  of 
X  having  the  value  x  is  zero,  i.e.,  P (X  = x) = 0. 
A  related  function  is  the  conditional  PDF  fX |L (x|Li ),  deﬁned  as  the  derivative  of 
FX |L (x|Li )  with  respect  to  x. 

fX (x) dx  . 

(7.17) 

Probability  Mass  Function  A  real-valued  discrete  random  variable  X  is  one 
that  takes  only  a  ﬁnite  or  countable  set  of  real  values,  {x1 , x2 , · · · }.  (Hence  this  is 
actually  a  random  event  —  as  deﬁned  earlier  —  but  speciﬁed  numerically  rather 
than  via  labels.)  The  CDF  in  this  case  would  be  a  “staircase”  function,  while  the 
PDF  would  be  zero  everywhere,  except  for  impulses  at  the  xj ,  with  strengths  cor­
responding  to  the  respective  probabilities  of  the  xj .  These  strengths/probabilities 
are  conveniently  described  by  the  probability  mass  function  (PMF)  pX (x),  which 
gives  the  probability  of  the  event X  = xj : 

P (X  = xj ) = pX (xj )  . 

(7.19) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  7.5 

Jointly  Distributed  Random  Variables  127 

7.5  JOINTLY  DISTRIBUTED  RANDOM  VARIABLES 

We  almost  always  use models  involving multiple  (or  compound)  random  variables. 
Such  situations  are  described  by  joint  probabilities.  For  example,  the  joint CDF  of 
two  random  variables  X  and  Y  is 

FX,Y  (x, y) = P (X  ≤ x, Y  ≤ y)  . 
The  corresponding  joint  PDF  is 

fX,Y  (x, y) = 

∂ 2FX,Y  (x, y)
∂x ∂ y 

and  has  the  heuristic  interpretation  that 

(7.20) 

(7.21) 

(7.22) 

P (x − dx < X  ≤ x , y − dy < Y  ≤ y) ≈ fX,Y  (x, y) dx dy  . 
The marginal PDF fX (x) is deﬁned as the PDF of the random variable X  considered 
on  its  own,  and  is  related  to  the  joint  density  fX,Y  (x, y)  by 
Z  +∞ 
−∞ 
A  similar  expression  holds  for  the marginal  PDF  fY  (y). 
We  have  already  noted  that  when  the  model  involves  a  random  variable  X  and  a 
random  event  L,  we  may  work  with  the  conditional  CDF 
P (X  ≤ x, L = Li ) 
P (L = Li ) 

FX |L (x Li ) = P (X  ≤ x L = Li ) = 
|
|

fX,Y  (x, y) dy  . 

fX (x) = 

(7.23) 

, 

(7.24) 

provided  P (L  =  Li )  >  0.  The  derivative  of  this  function  with  respect  to  x  gives 
the  conditional PDF fX |L (x|Li ).  When  the model  involves  two  continuous  random 
variables  X  and  Y ,  the  corresponding  function  of  interest  is  the  conditional  PDF 
fX |Y  (x|y)  that  describes  the  distribution  of  X ,  given  that  Y  =  y .  However,  for 
a  continuous  random  variable  Y ,  P (Y  =  y)  =  0,  so  even  though  the  following 
deﬁnition  may  seem  natural,  its  justiﬁcation  is more  subtle: 

fX,Y  (x, y)
fY  (y) 
To  see  the  plausibility  of  this  deﬁnition,  note  that  the  conditional  PDF  fX |Y  (x|y) 
must  have  the  property  that 

fX |Y  (x|y) = 

(7.25) 

. 

fX |Y  (x|y) dx ≈ P (x − dx < X  ≤ x | y − dy < Y  ≤ y) 
but by Bayes’  rule  the quantity on  the  right  in  the above equation can be  rewritten 
as 

(7.26) 

P (x − dx < X  ≤ x | y − dy < Y  ≤ y) ≈ 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

fX,Y  (x, y) dx dy 
fY  (y)dy 

. 

(7.27) 

128  Chapter  7 

Probabilistic  Models 

Combining  the  latter  two  expressions  yields  the  deﬁnition  of  fX |Y  (x|y)  given  in 
(7.25). 

Using  similar  reasoning,  we  can  obtain  relationships  such  as  the  following: 
fX |L (x|Li )P (L = Li ) 
fX (x) 

P (L = Li X  = x) = 
|

. 

(7.28) 

Two random variables X  and Y  are said to be independent or statistically  indepen­
dent  if  their  joint  PDF  (or  equivalently  their  joint  CDF)  factors  into  the  product 
of  the  individual  ones: 

fX,Y  (x, y) =  fX (x)fY  (y)  ,  or 
FX,Y  (x, y) =  FX (x)FY  (y) . 

(7.29) 

This  condition  turns out  to be  equivalent  to having any  collection of  events deﬁned 
in  terms  of X  be  independent  of  any  collection  of  events  deﬁned  in  terms  of  Y . 

For a set of more than two random variables to be  independent, we require that the 
joint  PDF  (or  CDF)  of  random  variables  from  this  set  factors  into  the  product  of 
the  individual  PDFs  (respectively,  CDFs).  One  can  similarly  deﬁne  independence 
of  random  variables  and  random  events. 

EXAMPLE  7.2 

Independence  of  events 

To  illustrate  some  of  the  above  deﬁnitions  and  concepts  in  the  context  of  random 
variables  and  random  events,  consider  two  independent  random variables X  and Y 
for  which  the  marginal  PDFs  are  uniform  between  zero  and  one: 
½ 
fX (x) = 
fY  (y) = ½ 
Because  X  and  Y  are  independent,  the  joint  PDF  fX,Y  (x, y)  is  given  by 

0 ≤ x ≤ 1 
1
0  otherwise 

0 ≤ y ≤ 1 
1
0  otherwise 

fX,Y  (x, y) = fX (x)fY  (y) 

We  deﬁne  the  events  A,  B ,  C  and  D  as  follows: 
1 o 
1 o 
n
, C  = nx < 
n
1 o
A =  y > 
, B  =  y < 
,
2 
2
2
1 o
1 o n
n
1
1
and  y > 
and  y < 
2  ∪  x > 
. 
D =  x < 
2 
2 
2 
These  events  are  illustrated  pictorially  in  Figure  7.4 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  7.6 

Expectations,  Moments  and  Variance  129 

y 
� 
1 

1 
2 

D 

D 

y 
� 
1 

1 
2 

A 

y 
� 
1 

1 
2 

B 

y 
� 
1 

1 
2 C

� 
x 

� 
x

1 

1 
1 
1 
1 
2
2
2
2
FIGURE  7.4  Illustration  of  events  A,  B ,  C ,  and D ,  for  Example  7.2 

1 

1 

� 
x 

� 
x 

1 

Questions that we might ask include whether these events are pairwise independent, 
e.g.  whether  A  and  C  are  independent.  To  answer  such  questions,  we  consider 
whether the joint probability factors into the product of the individual probabilities. 
So,  for  example, 

µ 
P (A ∩ C ) = P  y > 
P (A) = P (C ) = 

1
, x < 
2 

1 ¶ 
2

= 

1 
4 

1
2

= 0 

, y < 

1 
2 
Since  P (A ∩ C ) = P (A)P (C ),  events  A  and  C  are  independent.  However, 
1 ¶ 
µ 
P (A ∩ B ) = P  y > 
2 
P (A) = P (B ) = 

1 
2 
Since  P (A ∩ B ) =6 P (A)P (B ),  events  A  and  B  are  not  independent. 
Note  that  P (A ∩ C ∩ D) =  0  since  there  is  no  region  where  all  three  sets  overlap. 
1
,  so  P (A ∩ C  ∩ D) =6 P (A)P (C )P (D)  and 
However,  P (A) =  P (C ) =  P (D)  = 
2
the  events A, C ,  and D  are  not mutually  independent,  even  though  they  are  easily 
seen  to  be  pairwise  independent.  For  a  collection  of  events  to  be  independent,  we 
require  the  probability  of  the  intersection  of  any  of  the  events  to  equal  the  product 
of  the  probabilities  of  each  individual  event.  So  for  the  3–event  case,  pairwise 
independence  is  a  necessary  but  not  suﬃcient  condition  for  independence. 

7.6  EXPECTATIONS,  MOMENTS  AND  VARIANCE 

For many purposes it suﬃces to have a more aggregated or approximate description 
than  the  PDF  provides.  The  expectation  —  also  termed  the  expected  or  mean 
or  average  value,  or  the  ﬁrst-moment  —  of  the  real-valued  random  variable  X  is 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

130  Chapter  7 

Probabilistic  Models 

denoted  by  E [X ]  or  X  or  µX ,  and  deﬁned  as 
Z 
∞ 
−∞ 
In terms of the probability “mass” on the real line, the expectation gives the location 
of  the  center  of  mass.  Note  that  the  expected  value  of  a  sum  of  random  variables 
is  just  the  sum  of  the  individual  expected  values: 

E [X ] = X  = µX  = 

xfX (x) dx  . 

(7.30) 

E [X + Y ] = E [X ] + E [Y ]  . 

(7.31) 

Other  simple measures  of  where  the  PDF  is  centered  or  concentrated  are  provided 
by  the  median,  which  is  the  value  of  x  for  which  FX (x) = 0.5,  and  by  the  mode, 
which  is  the  value  of  x  for  which  fX (x)  is  maximum  (in  degenerate  cases  one  or 
both  of  these  may  not  be  unique). 

The  variance  or  centered  second-moment  of  the  random  variable  X  is  denoted  by 
σ2  and  deﬁned  as 
X 
σ2  =  E [(X − µX )2 ] = expected  squared  deviation  from  the mean 
Z 
X 
∞ 
(x − µX )2 fX (x)dx 
=
−∞ 
=  E [X 2 ] − µX  , 
2
where  the  last  equation  follows  on  writing  (X  − µX )2  =  X 2  − 2µX X  + µ2  and 
X 
taking  the  expectation  term  by  term.  We  refer  to  E [X 2 ]  as  the  second-moment 
of  X .  The  square  root  of  the  variance,  termed  the  standard  deviation,  is  a  widely 
used  measure  of  the  spread  of  the  PDF. 

(7.32) 

The  focus  of  many  engineering  models  that  involve  random  variables  is  primarily 
on  the means  and  variances  of  the  random  variables.  In  some  cases  this  is  because 
the detailed PDFs  are hard  to determine  or  represent  or work with.  In  other  cases, 
the  reason  for  this  focus  is  that  the means  and  variances  completely  determine  the 
PDFs,  as  with  the  Gaussian  (or  normal)  and  uniform  PDFs. 

EXAMPLE  7.3 

Gaussian  and  uniform  random  variables 

Two  common  PDF’s  that  we  will  work  with  are  the  Gaussian  (or  normal)  density 
and  the  uniform  density: 

Gaussian:  fX (x) = 

1 
e− 1 ( x−m )2 
√2πσ 
2 
σ
½ 
1 
b−a 
0 
The  two parameters m and σ  that deﬁne  the Gaussian PDF  can be  shown  to be  its 
mean  and  standard  deviation  respectively.  Similarly,  though  the  uniform  density 
can  be  simply  parametrized  by  its  lower  and  upper  limits  a  and  b  as  above,  an 

a < x < b 
otherwise 

fX (x) = 

Uniform: 

(7.33) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  7.6 

Expectations,  Moments  and  Variance  131 

equivalent  parametrization  is  via  its  mean  m  = (a + b)/2  and  standard  deviation 
σ = p(b − a)2 /12. 
There are useful statements  that can be made  for general PDFs on  the basis of  just 
the  mean  and  variance.  The  most  familiar  of  these  is  the  Chebyshev  inequality: 
P ³ 
µX | ≥ k´ 
− 
1 
|X
k2  . 
≤ 
σ
X 
This  inequality  implies  that,  for  any  random  variable,  the  probability  it  lies  at 
or  more  than  3  standard  deviations  away  from  the  mean  (on  either  side  of  the 
mean)  is  not  greater  than  (1/32 ) = 0.11.  Of  course,  for  particular  PDFs,  much 
more precise  statements  can be made,  and conclusions derived  from  the Chebyshev 
inequality  can  be  very  conservative.  For  instance,  in  the  case  of  a  Gaussian  PDF, 
the  probability  of  being  more  than  3  standard  deviations  away  from  the  mean  is 
only  0.0026,  while  for  a  uniform  PDF  the  probability  of  being  more  than  even  2 
standard  deviations  away  from  the  mean  is  precisely  0. 

(7.34) 

For  much  of  our  discussion  we  shall  make  do  with  evaluating  the  means  and  vari­
ances of  the  random variables  involved  in our models.  Also, we will be highlighting 
problems  whose  solution  only  requires  knowledge  of  means  and  variances. 

The  conditional  expectation  of  the  random  variable  X ,  given  that  the  random 
variable  Y  takes  the  value  y ,  is  the  real  number 
Z  +∞ 
E [X Y  = y ] = 
xfX |Y  (x y)dx = g(y)  , 
|
|
−∞ 
i.e.,  this  conditional  expectation  takes  some  value  g(y)  when  Y  =  y .  We  may  also 
consider  the  random  variable  g(Y ),  namely  the  function  of  the  random  variable  Y 
that,  for  each  Y  =  y ,  evaluates  to  the  conditional  expectation  E [X Y  =  y ].  We 
|
refer to this random variable g(Y ) as the conditional expectation of X  “given Y ” (as 
opposed to “given Y  = y”), and denote g(Y ) by E [X Y ].  Note that the expectation 
|
E [g(Y )]  of  the  random  variable  g(Y ),  i.e.  the  iterated  expectation  E [E [X Y ]],  is 
|
well deﬁned.  What we  show  in  the next paragraph  is  that  this  iterated  expectation 
works  out  to  something  simple,  namely  E [X ].  This  result  will  be  of  particular  use 
in  the  next  chapter. 

(7.35) 

∞ 

∞ 

Consider  ﬁrst  how  to  compute  E [X ]  when  we  have  the  joint  PDF  fX,Y  (x, y).  One 
way  is  to  evaluate  the marginal  density  fX (x)  of X ,  and  then  use  the  deﬁnition  of 
expectation  in  (7.30): 
x³Z 
E [X ] = Z 
fX,Y  (x, y) dy´ 
dx  . 
−∞ 
−∞ 
However,  it  is  often  simpler  to  compute  the  conditional  expectation  of  X ,  given 
Y  =  y ,  then  average  this  conditional  expectation  over  the  possible  values  of  Y , 
using  the marginal  density  of  Y .  To  derive  this more  precisely,  recall  that 
fX,Y  (x, y) = fX |Y  (x|y)fY  (y) 
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

(7.36) 

(7.37) 

132  Chapter  7 

Probabilistic  Models 

∞ 

and  use  this  in  (7.36)  to  deduce  that 
fY  (y)³Z 
E [X ] = Z 
xfX |Y  (x|y) dx´ 
dy = EY  [EX |Y  [X |Y ]]  . 
−∞ 
−∞ 
We  have  used  subscripts  on  the  preceding  expectations  in  order  to  make  explicit 
which  densities  are  involved  in  computing  each  of  them.  More  simply,  one  writes 

(7.38) 

∞ 

∞ 

(7.39) 

E [X ] = E [E [X Y ]] .	
|
The preceding result has an important implication for the computation of the expec­
tation of a function of a random variable.  Suppose X  = h(Y ), then E [X Y ] = h(Y ), 
|
so 
Z 
E [X ] = E [E [X Y ]] = 
|
−∞ 
This  shows  that  we  only  need  fY  (y)  to  calculate  the  expectation  of  a  function  of 
Y ;  to  compute  the  expectation  of  X  = h(Y ),  we  do  not  need  to  determine  fX (x). 
Similarly,  if X  is  a  function  of  two  random  variables,  X  = h(Y , Z ),  then 
∞	 Z 
Z 
−∞  −∞ 
It  is  easy  to  show  from  this  that  if  Y  and  Z  are  independent,  and  if  h(y , z ) = 
g(y)ℓ(z ),  then 

h(y , z )fY ,Z (y , z )dy dz  . 

h(y)fY  (y)dy  . 

E [X ] = 

(7.41) 

(7.40) 

∞ 

E [g(Y )ℓ(Z )] = E [g(Y )]E [ℓ(Z )]  . 

(7.42) 

7.7	 CORRELATION  AND  COVARIANCE  FOR  BIVARIATE  RANDOM 
VARIABLES 

Consider  a  pair  of  jointly  distributed  random  variables  X  and  Y .  Their  marginal 
PDFs  are  simply  obtained  by  pro jecting  the  probability mass  along  the  y -axis  and 
x-axis  directions  respectively: 
Z 
Z 
−∞ 
−∞	
In  other  words,  the  PDF  of  X  is  obtained  by  integrating  the  joint  PDF  over  all 
possible  values  of  the  other  random  variable  Y  —  and  similarly  for  the  PDF  of  Y . 

fX,Y  (x, y) dx  . 

fX,Y  (x, y) dy , 

fX (x) = 

fY  (y) = 

(7.43) 

∞ 

∞	

It  is of  interest,  just as  in the single-variable case,  to be able to capture the location 
and  spread  of  the  bivariate  PDF  in  some  aggregate  or  approximate  way,  without 
having to describe the full PDF. And again we turn to notions of mean and variance. 
The mean value of  the bivariate PDF  is  speciﬁed by giving  the mean values of  each 
of  its  two  component  random  variables:  the  mean  value  has  an  x  component  that 
is E [X ],  and  a  y  component  that  is E [Y ],  and  these  two  numbers  can  be  evaluated 
from  the  respective marginal  densities.  The  center  of mass  of  the  bivariate  PDF  is 
thus  located  at 

(x, y) = (E [X ], E [Y ])  .	

(7.44) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  7.7 

Correlation  and  Covariance  for  Bivariate  Random  Variables  133 

A  measure  of  the  spread  of  the  bivariate  PDF  in  the  x  direction  may  be  obtained 
from  the  standard  deviation  σX  of  X ,  computed  from  fX (x);  and  a  measure  of 
the  spread  in  the  y  direction  may  be  obtained  from  σY  ,  computed  similarly  from 
fY  (y).  However,  these  two  numbers  clearly  only  oﬀer  a  partial  view.  We  would 
really  like  to  know  what  the  spread  is  in  a  general  direction  rather  than  just  along 
the  two  coordinate  axes.  We  can  consider,  for  instance,  the  standard deviation  (or, 
equivalently,  the  variance)  of  the  random  variable  Z  deﬁned  as 

Z  = αX + βY 

(7.45) 

for  arbitrary  constants  α  and  β .  Note  that  by  choosing  α  and  β  appropriately, 
we  get  Z  =  X  or  Z  =  Y ,  and  therefore  recover  the  special  coordinate  directions 
that  we  have  already  considered;  but  being  able  to  analyze  the  behavior  of  Z  for 
arbitary  α  and  β  allows  us  to  specify  the  behavior  in  all  directions. 

To visualize how Z  behaves, note that Z  = 0 when αx+β y = 0.  This is the equation 
of  a  straight  line  through  the  origin  in  the  (x, y)  plane,  a  line  that  indicates  the 
precise  combinations  of  values  x  and  y  that  contribute  to  determining  fZ (0),  by 
pro jection  of  fX,Y  (x, y)  along  the  line.  Let  us  call  this  the  reference  line.  If Z  now 
takes  a  nonzero  value  z ,  the  corresponding  set  of  (x, y)  values  lies  on  a  line  oﬀset 
from  but  parallel  to  the  reference  line.  We  pro ject  fX,Y  (x, y)  along  this  new  oﬀset 
line  to  determine  fZ (z ). 
Before  seeing  what  computations  are  involved  in  determining  the  variance  of  Z , 
note  that  the  mean  of  Z  is  easily  found  in  terms  of  quantities  we  have  already 
computed,  namely  E [X ]  and  E [Y ]: 

E [Z ] = αE [X ] + βE [Y ]  . 

(7.46) 

As  for  the  variance  of  Z ,  it  is  easy  to  establish  from  (7.45)  and  (7.46)  that 
σ2  = E [Z 2 ] − (E [Z ])2 
Z
where  σ2  and  σ2  are  the  variances  already  computed  along  the  coordinate  direc-
Y 
X
tions  x  and  y ,  and  σX,Y  is  the  covariance  of  X  and  Y ,  also  denoted  by  cov(X, Y ) 
or  CX,Y  ,  and  deﬁned  as 

X  + β 2σ2  + 2αβ σX,Y 
= α2σ2
Y 

(7.47) 

σX,Y  = cov(X, Y ) = CX,Y  = E [(X − E [X ])(Y  − E [Y ])] 
or  equivalently 

(7.48) 

σX,Y  = E [X Y ] − E [X ]E [Y ]  . 
where  (7.49)  follows  from  multiplying  out  the  terms  in  parentheses  in  (7.48)  and 
then  taking  term-by-term  expectations.  Note  that  when  Y  =  X  we  recover  the 
familiar  expressions  for  the  variance  of  X .  The  quantity  E [X Y ]  that  appears  in 
(7.49),  i.e.,  the  expectation  of  the  product  of  the  random  variables,  is  referred  to 
as  the  correlation  or  second  cross-moment  of  X  and  Y  (to  distinguish  it  from  the 
second  self-moments  E [X 2 ]  and  E [Y 2 ]),  and  will  be  denoted  by  RX,Y  : 

(7.49) 

RX,Y  = E [X Y ]  . 

(7.50) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

134  Chapter  7 

Probabilistic  Models 

It is reassuring to note from (7.47) that the covariance σX,Y  is the only new quantity 
needed when  going  from mean  and  spread  computations  along  the  coordinate  axes 
to  such  computations  along  any  axis;  we  do  not  need  a  new  quantity  for  each  new 
direction.  In  summary,  we  can  express  the  location  of  fX,Y  (x, y)  in  an  aggregate 
or  approximate  way  in  terms  of  the  1st-moments,  E [X ]  , E [Y ];  and we  can  express 
the  spread around  this  location  in an aggregate or approximate way  in  terms of  the 
(central)  2nd-moments,  σ2  , σ2  , σX,Y  .
Y 
X
It  is  common  to  work  with  a  normalized  form  of  the  covariance,  namely  the  corre­
lation  coeﬃcient  ρX,Y  : 

σX,Y 
σX σY 
This normalization ensures that the correlation coeﬃcient is unchanged if X  and/or 
Y  is  multiplied  by  any  nonzero  constant  or  has  any  constant  added  to  it.  For 
instance,  the  centered  and  normalized  random  variables 
Y  − µY 
X − µX 
σX 
σY 

ρX,Y  = 

(7.51) 

(7.52) 

. 

V  = 

,

W  = 

, 

each  of  which  has  mean  0  and  variance  1,  have  the  same  correlation  coeﬃcient  as 
X  and  Y .  The  correlation  coeﬃcient  might  have  been  better  called  the  covariance 
coeﬃcient,  since  it  is  deﬁned  in  terms  of  the  covariance  and  not  the  correlation  of 
the  two  random  variables,  but  this  more  helpful  name  is  not  generally  utilized. 
Invoking  the  fact  that  σ2  in  (7.47)  must  be  non-negative,  and  further  noting  from 
Z 
this equation  that σ2 /β 2  is quadratic  in α,  it can be proved by elementary analysis 
Z 
of  the  quadratic  expression  that 

|ρX,Y  | ≤ 1  . 
From  the  various  preceding  deﬁnitions,  a  positive  correlation  RX,Y  >  0  suggests 
that X  and Y  tend to take the same sign, on average, whereas a positive covariance 
σX,Y  > 0 —  or  equivalently  a  positive  correlation  coeﬃcient  ρX,Y  > 0 —  suggests 
that  the  deviations  of X  and  Y  from  their  respective means  tend  to  take  the  same 
sign,  on  average.  Conversely,  a  negative  correlation  suggests  that X  and Y  tend  to 
take opposite signs, on average, while a negative covariance or correlation coeﬃcient 
suggests  that  the  deviations  of  X  and  Y  from  their  means  tend  to  take  opposite 
signs,  on  average. 

(7.53) 

Since  the  correlation  coeﬃcient  of  X  and  Y  captures  some  features  of  the  rela­
tion  between  their  deviations  from  their  respective  means,  we  might  expect  that 
the  correlation  coeﬃcient  can  play  a  role  in  constructing  an  estimate  of  Y  from 
measurements  of  X ,  or  vice  versa.  We  shall  see  in  the  next  chapter,  where  linear 
minimum  mean-square  error  (LMMSE)  estimation  is  studied,  that  this  is  indeed 
the  case. 

The random variables X  and Y  are said to be uncorrelated (or linearly independent, 
a  less  common  and  potentially  misleading  term)  if 

E [X Y ] = E [X ]E [Y ]  , 

(7.54) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  7.7 

Correlation  and  Covariance  for  Bivariate  Random  Variables  135 

or  equivalently  if 

σX,Y  = 0  or  ρX,Y  = 0  . 
Thus  uncorrelated  does  not  mean  zero  correlation  (unless  one  of  the  random  vari­
ables  has  an  expected  value  of  zero).  Rather,  uncorrelated  means  zero  covariance. 
Again, a better term  for uncorrelated might have been non-covariant, but this term 
is  not  widely  used. 

(7.55) 

Note  also  that  independent  random  variables X  and  Y ,  i.e.,  those  for  which 

fX,Y  (x, y) = fX (x)fY  (y)  , 

(7.56) 

are always uncorrelated, but the converse is not generally true:  uncorrelated random 
variables  may  not  be  independent.  If  X  and  Y  are  independent,  then  E [X Y ] = 
E [X ]E [Y ]  so  X  and  Y  are  uncorrelated.  The  converse  does  not  hold  in  general. 
For  instance,  consider  the  case where  the  combination  (X, Y )  takes  only  the  values 
(1, 0),  (−1, 0).  (0, 1)  and  (0, −1),  each  with  equal  probability  1 .  Then  X  and  Y
4 
are  easily  seen  to  be  uncorrelated  but  dependent,  i.e.,  not  independent. 

A ﬁnal bit of terminology that we will shortly motivate and ﬁnd useful occurs in the 
following deﬁnition:  Two random variables X  and Y  are orthogonal if E [X Y ] = 0. 

EXAMPLE  7.4 

Perfect  correlation,  zero  correlation 

Consider  the degenerate  case where Y  is  given by  a deterministic  linear  function  of 
a  random  variable X  (so  Y  is  also  a  random  variable,  of  course): 

Y  = ξX + ζ , 

(7.57) 

where  ξ  and  ζ  are  constants.  Then  it  is  easy  to  show  that  ρX,Y  =  1  if  ξ >  0  and 
ρ = −1 if ξ < 0.  Note that in this case the probability mass is entirely concentrated 
on  the  line  deﬁned  by  the  above  equation,  so  the  bivariate  PDF  —  if  we  insist  on 
talking  about  it!  —  is  a  two-dimensional  impulse  (but  this  fact  is  not  important  in 
evaluating  ρX,Y  ). 
You  should  also  have  no  diﬃculty  establishing  that  ρX,Y  = 0  if 

Y  = ξX 2  + ζ 

(7.58) 

and  X  has  a  PDF  fX (x)  that  is  even  about  0,  i.e.,  fX (−x) = fX (x). 

EXAMPLE  7.5 

Bivariate  Gaussian  density 

The random variables X  and Y  are said to be bivariate Gaussian or bivariate normal 
if  their  joint  PDF  is  given  by 
fX,Y  (x, y) = c  expn−q³ x − 
µY  ´o 
y − 
µX 
σY 
σX 
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

(7.59) 

,

136  Chapter  7 

Probabilistic  Models 

where  c  is  a  normalizing  constant  (so  that  the  PDF  integrates  to  1)  and  q(v , w) 
is  a  quadratic  function  of  its  two  arguments  v  and  w,  expressed  in  terms  of  the 
correlation  coeﬃcient  ρ  of  X  and  Y : 
1 
2πσX σY p1 − ρ2 
1 
(v 2  − 2ρvw + w 2 )	
2(1 − 
ρ2 )
This density is the natural bivariate generalization of the familiar Gaussian density, 
and  has  several  nice  properties: 

q(v , w) = 

(7.61) 

(7.60) 

c =	

•	 The  marginal  densities  of  X  and  Y  are  Gaussian. 
•	 The  conditional  density  of  Y ,  given  X  =  x,  is  Gaussian  with  mean  ρx  and 
variance σ2  (1 − ρ2 )  (which  evidently does not depend on  the value of x);  and 
Y 
similary  for  the  conditional  density  of X ,  given  Y  = y . 
•	 If  X  and  Y  are  uncorrelated,  i.e.,  if  ρ  =  0,  then  X  and  Y  are  actually 
independent,  a  fact  that  is  not  generally  true  for  other  bivariate  random 
variables,  as  noted  above. 
•	 Any  two aﬃne  (i.e.,  linear plus constant) combinations of X  and Y  are  them­
selves  bivariate  Gaussian  (e.g.,  Q  =  X  + 3Y  + 2  and  R  = 7X  + Y  − 3  are 
bivariate  Gaussian). 

The  bivariate  Gaussian  PDF  and  indeed  the  associated  notion  of  correlation  were 
essentially  discovered  by  the  statistician  Francis  Galton  (a  ﬁrst-cousin  of  Charles 
Darwin) in 1886, with help from the mathematician Hamilton Dickson.  Galton was 
actually  studying  the  joint  distribution  of  the  heights  of  parents  and  children,  and 
found  that  the marginals  and  conditionals were well  represented  as Gaussians.  His 
question to Dickson was:  what joint PDF has Gaussian marginals and conditionals? 
The  answer:  the  bivariate  Gaussian!  It  turns  out  that  there  is  a  2-dimensional 
version  of  the  central  limit  theorem,  with  the  bivariate  Gaussian  as  the  limiting 
density,  so  this  is  a  reasonable  model  for  two  jointly  distributed  random  variables 
in many  settings.  There  are  also  natural  generalization  to  many  variables. 

fX1 ,X2 ,

Some  of  the  generalizations  of  the  preceding  discussion  from  two  random  variables 
to many random variables are fairly evident.  In particular, the mean of a joint PDF 
,Xℓ (x1 , x2 , 
· · · 
··· 
in the ℓ-dimensional space of possible values has coordinates that are the respective 
· · · 
individual  means,  E [X1 ], 
, E [Xℓ ].  The  spreads  in  the  coordinate  directions  are 
deduced  from  the  individual  (marginal)  spreads,  σX1 , 
, σXℓ .  To  be  able  to  com­
· · · 
pute the spreads in arbitrary directions, we need all the additional ℓ(ℓ − 1)/2 central 
2nd  moments,  namely  σXi ,Xj  for  all  1 ≤  i < j  ≤  ℓ  (note  that  σXj ,Xi  = σXi ,Xj ) — 
but  nothing  more. 

(7.62) 

, xℓ )	

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  7.8 

A  Vector-Space  Picture  for  Correlation  Properties  of  Random  Variables  137 

7.8	 A VECTOR-SPACE PICTURE FOR CORRELATION PROPERTIES OF 
RANDOM  VARIABLES 

A  vector-space  picture  is  often  useful  as  an  aid  to  recalling  the  second-moment 
relationships  between  two  random  variables  X  and  Y .  This  picture  is  not  just  a 
mnemonic:  there  is  a  very  precise  sense  in  which  random  variables  can  be  thought 
of  (or  are)  vectors  in  a  vector  space  (of  inﬁnite  dimensions),  as  long  as we  are  only 
interested  in  their  second-moment  properties.  Although  we  shall  not  develop  this 
correspondence  in  any  depth,  it  can  be  very  helpful  in  conjecturing  or  checking 
answers  in  the  linear  minimum  mean-square-error  (LMMSE)  estimation  problems 
that  we  shall  treat. 

To  develop  this  picture,  we  represent  the  random  variables  X  and  Y  as  vectors  X 
and  Y  in  some  abstract  vector  space.  For  the  squared  lengths  of  these  vectors, 
we  take  the  second-moments  of  the  associated  random  variables,  E [X 2 ]  and E [Y 2 ] 
respectively.  Recall  that  in Euclidean vector  space  the  squared  length of a vector  is 
the inner product of the vector with itself.  This suggests that perhaps in our vector-
space interpretation the inner product < X, Y > between two general vectors X and 
Y  should  be  deﬁned  as  the  correlation  (or  second  cross-moment)  of  the  associate 
random  variables: 

(7.63) 
< X, Y >= E [X Y ] = RX,Y  .	
This  indeed  turns  out  to  be  the  deﬁnition  that’s  needed.  With  this  deﬁnition,  the 
standard  properties  required  of  an  inner  product  in  a  vector  space  are  satisﬁed, 
namely: 

Symmetry:  < X, Y >=< Y , X > . 

Linearity:  < X, a1Y1  + a2Y2  >= a1  < X, Y1  > +a2  < X, Y2  > 

Positivity:  < X, X >  is  positive  for X = 0,  and  0  otherwise. 

This  deﬁnition  of  inner  product  is  also  consistent  with  the  fact  that  we  often  refer 
to  two  random  variables  as  orthogonal  when  E [X Y ] = 0. 
The  centered  random  variables X − µX  and Y  − µY  can  similary  be  represented  as 
vectors  Xe and  Ye in  this  abstract  vector  space,  with  squared  lengths  that  are  now 
the  variances  of  the  random  variables X  and  Y : 
σ2  = E [(X − µX )2 ] ,
σ2  = E [(Y  − µY  )2 ] 
(7.64) 
X	
Y 
respectively.  The  lengths  are  therefore  the  standard  deviations  of  the  associated 
random variables, σX  and σY  respectively.  The  inner product of  the vectors Xe and 
Ye becomes 
< Xe , Ye >= E [(X − µX )(Y  − µY  )] = σX,Y  , 
(7.65) 
namely  the  covariance  of  the  random  variables. 
In  Euclidean  space  the  inner  product  of  two  vectors  is  given  by  the  product  of  the 
lengths  of  the  individual  vectors  and  the  cosine  of  the  angle  between  them: 
< Xe , Ye >= σX,Y  = σX σY  cos(θ)  ,	
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

(7.66) 

6
138  Chapter  7 

Probabilistic  Models 

�  Y  − µY 

σY 

θ = cos−1 ρ 

σX 

�X − µX 

FIGURE  7.5  Random  Variables  as  Vectors. 

so  the  quantity 

θ = cos−1³  σX,Y  ´ 
σX σY 
can  be  thought  of  as  the  angle  between  the  vectors.  Here  ρ  is  the  correlation 
coeﬃcient  of  the  two  random  variables,  so  evidently 

= cos−1 ρ 

(7.67) 

ρ = cos(θ)  . 

(7.68) 

Thus,  the  correlation  coeﬃcient  is  the  cosine  of  the  angle  between  the  vectors.  It 
is  therefore  not  surprising  at  all  that 

− 1 ≤ ρ ≤ 1  . 
When  ρ  is  near  1,  the  vectors  are  nearly  aligned  in  the  same  direction,  whereas 
when  ρ  is  near  −1  they  are  close  to  being  oppositely  aligned.  The  correlation 
coeﬃcient is zero when these vectors Xe and Ye (which represent the centered random 
variables) are orthogonal, or equivalently,  the corresponding random variables have 
zero  covariance, 
σX,Y  = 0  . 
(7.70) 

(7.69) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

MIT OpenCourseWare
http://ocw.mit.edu 

6.011 Introduction to Communication, Control, and Signal Processing 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

