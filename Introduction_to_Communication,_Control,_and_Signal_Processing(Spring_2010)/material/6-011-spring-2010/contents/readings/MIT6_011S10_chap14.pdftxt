C H A P T E R 

14 

Signal Detection 

14.1  SIGNAL  DETECTION  AS  HYPOTHESIS  TESTING 

In Chapter 13 we  considered hypothesis  testing  in  the  context  of  random variables. 
The detector resulting in the minimum probability of error corresponds to the MAP 
test as developed in section 13.2.1 or equivalently the likelihood ratio test in section 
13.2.3. 

In  this  chapter  we  extend  those  results  to  a  class  of  detection  problems  that  are 
central in radar, sonar and communications,  involving measurements of signals over 
time.  The  generic  signal detection  problem  that we  consider  corresponds  to  receiv­
ing  a  signal  r(t)  over  a  noisy  channel.  r(t)  either  contains  a  known  deterministic 
pulse  s(t)  or  it  does  not  contain  the  pulse.  Thus  our  two  hypotheses  are 

H1  : r(t) = s(t) + w(t)

H0  : r(t) = w(t), 

(14.1)


where  w(t)  is  a  wide-sense  stationary  random  process.  One  example  of  a  scenario 
in  which  this  problem  arises  is  in  binary  communication  using  pulse  amplitude 
modulation.  In  that  context  the  presence  or  absence  of  the  pulse  s(t)  represents 
the  transmission  of  a  “one”  or  a  “zero”.  As  another  example,  radar  and  sonar 
systems are based on  transmitting  a pulse and detecting  the presence or absence of 
an  echo. 

In  our  treatment  in  this  chapter  we  ﬁrst  consider  the  case  in  which  the  noise  is 
white  and  carry  out  the  formulation  and  analysis  in  discrete-time  which  avoids 
some  of  the  subtler  issues  associated  with  continuous-time  white  noise.  We  also 
initially  treat  the  case  in  which  the  noise  is  Gaussian.  In  Section  14.3.4  we  extend 
the discussion  to discrete-time Gaussian  colored noise.  In Section  14.3.2 we discuss 
the  implications  when  the  noise  is  not  Gaussian  and  in  Section  14.3.3  we  discuss 
how  the  results  generalize  to  the  continuous-time  case. 

14.2  OPTIMAL  DETECTION  IN  WHITE  GAUSSIAN  NOISE 

In  the  signal  detection  task  outlined  above,  our  hypothesis  test  is  no  longer  based 
on  the  measurement  of  a  single  (scalar)  random  variable  R,  but  instead  involves  a 
collection  of  L  (scalar)  random  variables  R1 , R2 , . . . , RL . 
Speciﬁcally,  we  receive  the  (ﬁnite-length)  DT  signal  r[n],  n = 1, 2, 
, L,  regarded 
· · · 
as  the  realization  of  a  random  process.  More  simply,  the  signal  r[n]  is  modeled  as 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

247

248  Chapter  14 

Signal  Detection 

the  values  taken  by  a  set  of  random  variables  R[n].  Let  H0  denote  the  hypothesis 
that  the  random  waveform  is  only  white  Gaussian  noise,  i.e. 

H0  :  R[n] = W [n] 
· · · 
where  the  W [n]  for  n = 1, 2, 
, L  are  independent,  zero-mean,  Gaussian  random 
variables,  with  variance  σ2 .  Similarly,  let H1  denote  the  hypothesis  that  the  wave­
form  R[n]  is  the  sum  of  white  Gaussian  noise  W [n]  and  a  known,  deterministic 
signal  s[n],  i.e. 

(14.2) 

H1  :  R[n] = s[n] + W [n] 
(14.3) 
where  the  W [n]  are  again  distributed  as  above.  Our  task  is  to  decide  in  favor  of 
H0  or H1  on  the  basis  of  the  measurements  r[n]. 
The  nature  and  derivation  of  the  solutions  to  such  decision  problems  are  similar 
to  those  in  Chapter  13,  except  that  we  now  use  posterior  probabilities  conditioned 
, r[L])  rather  than 
on  the  entire  collection  of  measurements,  i.e.  P (Hi r[1], r[2], 
· · · 
|
P (Hi r).  Similarly, we use compound (or joint) PDF’s, such as f (r[1], r[2], 
· · · 
|
|
, r[L] Hi )
instead  of  f (r Hi ).  The  associated  decision  regions  Di  are  now  regions  in  an  L­
|
dimensional  space,  rather  than  segments  of  the  real  line. 

For  detection  with  minimum  probability  of  error,  we  again  use  the  MAP  rule  or 
equivalently  compare  the  values  of 

f (r[1], r[2], . . . , r[L]  Hi ) P (Hi ) 
| 
for  i = 0, 1,  and  decide  in  favor  of  whichever  hypothesis  yields  the maximum  value 
of  this  expression,  i.e.  the  form  of  equation  (13.7)  for  the  case  of multiple measure­
ments  is 

(14.4) 

f (r[1], r[2], . . . , r[L]  H1 ) P (H1 ) 
| 

‘H1 ’ 
> 
< 
‘H0 ’ 

| 
f (r[1], r[2], . . . , r[L]  H0 ) P (H0 ) 

(14.5) 

which  also  can  easily be put  into  the  form  of  equation  (13.18)  corresponding  to  the 
likelihood  ratio  test. 

With  W [n]  white  and  Gaussian,  the  conditional  densities  in  (14.5)  are  easy  to 
evaluate,  and  take  the  form 

f (r[1], r[2], . . . , r[L] | H0 ) = 

½ 
(r[n])2 ¾ 
(2πσ2 )(L/2)  Y 
L
1 
exp − 
2σ2 
n=1 
exp − (X  (r[n])2 ) 
L
1 
2σ2 
(2πσ2 )(L/2) 
n=1 
°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

= 

(14.6) 

Section  14.2 

Optimal  Detection  in White  Gaussian  Noise  249 

and 

f (r[1], r[2], . . . , r[L]  H1 ) = 
| 

½ 
¾ 
(2πσ2 )(L/2)  Y 
L
(r[n] − s[n])2 
1 
exp − 
2σ2 
=1 
n
) 
(X 
L
(r[n] − s[n])2 
1 
(2πσ2 )(L/2)  exp − 
2σ2 
n=1 
The  inequality  in  equation  (14.5)  (or  any  inequality  in  general)  will,  of  course  still 
hold  if  a  nonlinear,  strictly  increasing  function  is  applied  to  both  sides.  Because 
of  the  form  of  equations  (14.6)  and  (14.7)  it  is  particularly  convenient  to  replace 
equation  (14.5)  by  applying  the  natural  logarithm  to  both  sides  of  the  inequality. 
The  resulting  inequality,  in  the  case  of  (14.6)  and  (14.7),  is: 

(14.7) 

= 

“H1 ” 
> 
<
“H ” 0 

g = 

¶ 
µ 
X 
LX 
L
P (H0 ) 
σ2  ln 
s 2 [n] 
+ 
r[n]s[n] 
P (H1 )
=1 
n=1 
n
The  sum  on  the  left-hand  side  of  Eq.  (14.8)  is  referred  to  as  the  deterministic 
correlation  between  r[n]  and  s[n],  which  we  denote  as  g .  The  second  sum  on  the 
right-hand  side  is  the  energy  in  the  deterministic  signal  s[n] which we  denote  by  E . 
For  convenience  we  denote  the  threshold  represented  by  the  entire  right  hand  side 
of  (14.8)  as  γ ,  i.e.,  equation  (14.8)  becomes 

(14.8) 

1 
2 

g

“H1 ” 
> 
< 
“H0 ” 
) +  E 
P (H0 )
where  γ  = σ2  ln( 
P (H1 )
2 

γ 

(14.9a) 

(14.9b) 

If  the  Neyman-Pearson  formulation  is  used,  then  the  optimal  decision  rule  is  still 
of  the  form  of  equation  (14.8),  except  that  the  right  hand  side  of  the  inequality  is 
determined  by  the  speciﬁed  bound  on  PF A . 
If  hypothesis  H0  is  true,  i.e.  if  the  signal  s[n]  is  absent,  then  r[n]  on  the  left  hand 
side  of  equation  (14.8)  will  be Gaussian  white  noise  only,  i.e.  g  will  be  the  random 
variable 
X 
L
G =  W [n]s[n] 
n=1 
Since  W [n]  at  each  value  of  n  is  Gaussian,  with  zero  mean  and  variance  σ2 ,  and 
since a weighted,  linear combination of Gaussian random variables is also Gaussian, 
X 
L
2 [n] = σ2
s 
n=1 

the random variable G is Gaussian with mean zero and variance σ2 

(14.10) 

E . 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

250  Chapter  14 

Signal  Detection 

When  the  signal  is  actually  present,  i.e.,  when  H1  holds,  the  random  variable  is 
the  realisation  of  a  Gaussian  random  variable  with  mean  E  and  still  with  variance 
E σ2  or  standard  deviation  σ√E .  The  optimal  test  in  (14.8)  is  therefore  described 
by  Figure  14.1  which  is  of  course  similar  to  that  in  Figure  13.5  : 

f(r|H 
0  ) 

f(r|H
1) 

σ  ε 

γ 

ε 

r  = Σ r[n]s[n] 

FIGURE  14.1  Optimal  test  for  two  hypotheses  with  equal  variances  and  diﬀerent 
means. 

Using  the  facts  summarized  in  this ﬁgure,  and  given  a detection  threshold γ  on  the 
correlation  (e.g.  with  γ  picked  equal  to  the  right  side  of  (14.8),  or  in  some  other 
way),  we  can  compute  PF A ,  PD ,  Pe ,  and  other  probabilities  of  interest. 
Figure  14.1 makes  evident  that  the  performance  of  the  detection  strategy  is  deter­
mined  entirely  by  the  ratio  E /(σ√E ),  or  equivalently  by  the  signal-to-noise  ratio 
E /σ2 ,  i.e.  the  ratio  of  the  signal  energy  E  to  the  noise  variance  σ2 . 

14.2.1  Matched  Filtering 

Since  the  correlation  sum  in  (14.8)  constitutes  a  linear  operation  on  the  measured 
signal, we  can  consider  computing  the  sum  through  the use of an LTI ﬁlter and  the 
output  sampled  at  an  appropriate  time  to  form  the  correlation  sum  g .  Speciﬁcally, 
with  h[n]  as  the  impulse  response  and  r[n]  as  the  input,  the  output  will  be  the 
convolution  sum 
∞X 
r[k ]h[n − k ] 
k=−∞ 
For r[n] = 0 except for 1 ≤ n ≤ L and with h[n] chosen as s[−n], the ﬁlter output at 
n = 0  is PL
k=1 r[k ]s[k ] = g  as  required.  In other words, we choose  the ﬁlter  impulse 
response  to  be  a  time-reversed  version  of  the  target  signal  for  n = 1, 2, . . . , L,  with 
h[n] = 0  elsewhere.  This  ﬁlter  is  said  to  be  the matched  ﬁlter  for  the  target  signal. 
The  structure  of  the  optimum  detector  for  a  ﬁnite-length  signal  in  white  Gaussian 
noise  is  therefore  as  shown  below: 

(14.11) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Matched Filter 

x[k] 

h[k] 
= s[-k] 

Section  14.3 

A  General  Detector  Structure  251 

r = Σ x[k]s[k] 

>  γ 
< 

’H1  ’ 
’H0  ’ 

Sample at 
time zero 

FIGURE  14.2  Optimum  detector 

14.2.2  Signal  Classiﬁcation 

We  can  easily  extend  the  previous  two-hypothesis  problem  to  the multiple  hypoth­
esis case, where Hi ,  i = 0, 1, 
, M − 1 denotes the hypothesis that the signal R[n], 
· · · 
n  = 1, 2, 
· · · 
, L,  is  a  noise-corrupted  version  of  the  ith  deterministic  signal  si [n], 
selected  from  a  possible  set  of M  deterministic  signals: 

Hi  :  R[n] = si [n] + W [n] 

(14.12) 

with  the  W [n]  denoting  independent,  zero-mean,  Gaussian  random  variables  with 
variance σ2 .  This scenario arises, for example, in radar signature analysis.  Diﬀerent 
aircraft  reﬂect  a  radar pulse diﬀerently,  typically with  a distinct  signature  that  can 
be used  to  identify not only  its presence, but  the  type of aircraft.  In  this  case,  each 
of  the  signals  si [n]  and  correspondingly  each  hypothesis  Hi  would  correspond  to 
the  presence  of  a  particular  type  of  aircraft.  Thus,  our  task  is  to  decide  in  favor 
of  one  of  the  hypotheses,  given  a  set  of  measurements  r[n]  of  R[n].  For  minimum 
error  probability,  the  required  test  involves  comparison  of  the  quantities 
Ã 
! 
LX 
r[n]si [n]  − Ei 
2 
n=1 
where  Ei  denotes  the  energy  of  the  ith  signal.  The  largest  of  the  expressions  in 
, M  − 1,  determines  which  hypothesis  is  selected.  If  the 
(14.13),  for  i  = 0, 1, 
· · · 
signals have equal energies and equal prior probabilities, then the above comparison 
reduces  to  deciding  in  favor  of  the  signal with  the  highest  deterministic  correlation 
LX 
r[n]si [n]  . 
n=1 

+ σ2  ln P (Hi ) 

(14.13) 

(14.14) 

14.3  A  GENERAL  DETECTOR  STRUCTURE 

The matched  ﬁlter  developed  in  Section  14.2  extends  to  the  case where we  have  an 
inﬁnite number of measurements rather than just L measurements.  As we will see in 
Section  14.3.4,  it  also  extends  to  the  case  of  colored  noise.  We  shall,  for  simplicity, 
treat  these  extensions  by  assuming  the  general  detector  structure,  shown  in Figure 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

252  Chapter  14 

Signal  Detection 

�  Processor 

r[n] 
↑ 
random 
process 

�  Threshold 

g [n]  � n = 0 
↑ 
↑
random 
random 
process 
variable 

‘H1 ’ 
>�  <
‘H0 ’ 
↑ 
decision 

FIGURE  14.3  A  general  detector  structure. 

11.7,  and  determine  an  optimum  choice  of  processor  and  of  detection  threshold  for 
each  scenario. 

We  are  assuming  that  the  transmitter  and  receiver  are  synchronized,  so  that  we 
test  g [n]  at  a  known  (ﬁxed)  time,  which  we  choose  here  as  n  =  0.  The  choice 
of  0  as  the  sampling  instant  is  for  convenience;  any  other  instant  may  be  picked, 
with  a  corresponding  time-shift  in  the  operation  of  the  processor.  Although  the 
processor  could  in  general  be  nonlinear,  we  shall  assume  the  processing  will  be 
done with  an  LTI  ﬁlter.  Thus  the  system  to  be  considered  is  shown  in Figure  14.4; 
a  corresponding  system  can  be  considered  for  continuous  time. 

r[n] 

�  LTI,  h[n] 

g [n]  � n = 0 

�  Threshold 
G 

‘H1 ’ 
>�  <
‘H0 ’

FIGURE 14.4  Detector structure of Figure 14.3 with the processor as an LTI system. 

It  can  be  shown  formally,  but  is  also  intuitively  reasonable,  that  scaling  h[n]  by  a 
constant gain will not aﬀect the overall performance of the detector  if the threshold 
is  correspondingly adjusted  since a  constant overall gain  scales  the  signal and noise 
identically. 

For  convenience,  we  normalize  the  gain  of  the  LTI  system  so  as  to  have 
+∞X 
h2 [n] = 1 . 
n=−∞ 
If r[n] is a Gaussian random process, then so is g [n], because it is obtained by linear 
processing  of  r[n],  and  therefore  G  is  a  Gaussian  random  variable  in  this  case. 

(14.15) 

14.3.1  Pulse  Detection  in White  Noise 

To  suggest  the  approach we  consider  a very  simple  choice  of LTI processor,  namely 
with  h[n] = δ [n],  so 

H1  : G = g [0] = s[0] + w[0] 
H0  : G = g [0] = w[0]  . 

(14.16) 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  14.3 

A  General  Detector  Structure  253 

Thus,  under  each  hypothesis,  g [0]  is  Gaussian: 

Also  for  convenience  we  assume  that  s[0]  is  positive. 
(g − s[0]) # 
exp " 
2 
1 
H1  : fG|H (g |H1 ) = N (s[0], σ2 ) =  √2
− 
2σ2 
πσ 
· 
g ¸ 
1 
2
exp  − 
. 
H0  : fG|H (g |H0 ) =  √2πσ 
2σ2 
fG|H (g |H0 ) 

fG|H (g |H1 ) 

(14.17) 

� 

� 

0 

s[0] 

g 

FIGURE  14.5  PDF’s  for  the  two  hypotheses  in  Eq.  (14.16). 

This is just the binary hypothesis testing problem on the random variable G treated 
in  Section  13.2  and  correspondingly  the  MAP  rule  for  detection  with  minimum 
probability  of  error  is  given  by 

‘H1 ’ 
>
P (H1  G = g)  < P (H0  G = g) , 
| 
| 
‘H0 ’ 

or,  equivalently,  the  likelihood  ratio  test: 

fG|H (g  | H1 ) 
fG|H (g  | H0 )

‘H1 ’ 
P (H0 )
>
<
‘H0 ’  P (H1 ) 

= η . 

(14.18) 

and  equivalently 

Evaluating  equation  (14.18)  using  equation  (14.17)  leads  to  the  relationship 
g ¸¾  ‘H1 ’ 
· 
½· 
(g − s[0])2 ¸
P (H0 )
2
> 
exp 
− 
< P (H1 ) 
2σ2 
2σ2 
‘H0 ’ 
s2 [0] ¸  ‘H1 ’ 
· 
gs[0] 
P (H0 ) 
> 
exp 
< 
σ2 
2σ2 
0 ’  P (H1 ) 
‘H
or,  taking  the  natural  logarithm  of  both  sides  of  the  likelihood  ratio  test  as  we  did 
in  Section  14.2,  equation  (14.20)  is  replaced  by 
> 1 ’ 
‘H
g < 
‘H0 ’ 

σ2 
P (H0 ) 
ln 
s[0]  P (H1 )

s[0] 
2 

+ 

(14.19) 

(14.20) 

+

− 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

(14.21) 

254  Chapter  14 

Signal  Detection 

We may not know  the a  priori probabilities P(H0 ) and P(H1 ) or,  for other reasons, 
may want  to modify  the  threshold, but  still using a  threshold  test on  the  likelihood 
ratio,  or  a  threshold  test  of  the  form 

‘H1 ’ 
> 
g  <  λ . 
‘H0 ’ 

(14.22) 

Sweeping  the  threshholds  over  all  possible  values  leads  to  the  receiver  operating 
characteristics  as  discussed  in  Section  13.2.5. 

We  next  consider  the  more  general  case  in  which  h[n]  is  not  the  identity  system. 
Then,  under  the  two  hypothesis  we  have: 

H1  :  g [n] =  s[n] ∗ h[n] + w[n] ∗ h[n] 
H0  :  g [n] =  w[n] ∗ h[n] , 

(14.23) 

The  term  w[n] ∗ h[n]  still  represents  noise  but  is  no  longer  white,  i.e.  its  spectral 
shape  is changed by the ﬁlter h[n].  Denoting w[n] ∗ h[n] as v [n],  the autocorrelation 
function  of  v [n]  is 
Rvv [m] = Rww [m] ∗ Rhh [m] 
(14.24) 
and  in  particular  the  mean  v [n]  is  zero  and  its  variance  is 
var{v [n]} = σ2  X 
∞
h2 [n]. 
n=−∞ 
Because  of  the  normalization  in  equation  (14.15)  the  variance  of  v [n]  is  the  same 
as  that of  the white noise,  i.e.  var{v [n]} = σ2 .  Furthermore,  since w[n]  is Gaussian 
so  is  v [n].  Consequently  the  value  g [0]  is  again  a  Gaussian  random  variable  with 
variance  σ2 .  The mean  of  g [0]  under  the  two  hypotheses  is  now: 
H1  :  E {g [n]}  =  X 
∞
n=−∞
H0  :  E {g [n]}  = 0, 
Therefore  equation  (14.17)  is  replaced  by 

h[n]s[−n] , µ 

(14.25) 

(14.26) 

H1  : fG|H (g |H1 ) = N (µ, σ2 )

H0  : fG|H (g |H0 ) = N (0, σ2 ). 

(14.27)


The  probability  density  functions  representing  the  two  hypothesis  are  shown  in 
Figure 14.6 below.  On this ﬁgure we have also indicated the threshold γ  of equation 
(14.27)  above  which  we  would  declare  H1  to  be  true  and  below  which  we  would 
declare  H0  to  be  true.  Also  indicated  by  the  shaded  areas  are  the  areas  under  the 
PDF’s  that  would  correspond  to  PF A  and  PD . 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

Section  14.3 

A  General  Detector  Structure  255 

fG|H (g [0] H0 )  fG|H (g [0] H1 ) 
|
|
�
� 

PF A 
� 

PD 
� 

0 

λ

M 

g [0] 

FIGURE  14.6  Indication  of  the  areas  representing  PF A  and  PD . 

(14.28) 

∞ 

PD  = 

fG (g [0] H1 )dg 
|

The  value  of  PF A  is  ﬁxed  by  the  shape  of  fG|H (g [0]|H0 )  and  the  value  of  the 
threshold  γ .  Since  fG|H (g [0]|H0 )  is  not  dependent  on  h[n],  the  choice  of  h[n]  will 
not  aﬀect  PF A .  The  variance  of  fG (g [0] H1 )  is  also  not  inﬂuenced  by  the  choice  of 
|
h[n]  but  its  mean  µ  is.  In  particular,  as  we  see  from  Figure  14.6,  the  value  of  PD 
is  given  by 
Z 
γ 
which increases as µ increases.  Consequently, to minimize P (error), or alternatively 
to maximize PD  for a given PF A , we want to maximize the value of µ.  To determine 
the  choice  of  h[n]  to  maximize  µ  we  use  the  Schwarz  inequality: 
h[n]s[−n]¯¯¯  ≤ X 
¯¯¯X 
h2 [n] X 
2
s 2 [−n] 
with  equality  if  and  only  if h[n] = cs[−n]  for  some  constant  c.  Since we normalized 
the  energy  in  h[n],  the  optimum  ﬁlter  is  h[n] = ( √1
E )s[−n],  which  is  again  the 
matched ﬁlter.  (This  is as expected,  since the optimum detector  for a known ﬁnite-
length  pulse  in  white  Gaussian  noise  has  already  been  shown  in  Section  14.2.1  to 
have  the  form  we  assumed  here,  with  the  impulse  response  of  the  LTI  ﬁlter  being 
matched to the signal.)  The ﬁlter output g [n] due to the pulse is then  √1
E Rss [n] and 
the  output  due  to  the  noise  is  the  colored  noise  v [n]  with  variance  σ2 .  Since  g [0] 
E  P∞n=−∞ s2 [n]  and  variance  σ2 ,  only  the  energy 
is  a  random  variable with mean  √1
in  the  pulse  and  not  its  speciﬁc  shape,  aﬀects  the  performance  of  the  detector. 

(14.29) 

14.3.2  Maximizing  SNR 

If  w[n]  is  white  but  not  Gaussian,  then  g [0]  is  not  Gaussian.  However,  g [0]  is  still 
distributed  the  same  under  each  hypothesis,  except  that  its  mean  under  H0  is  0 
while  the  mean  under  H1  is  µ  as  given  in  equation  (14.26).  The  matched  ﬁlter 
in  this  case  still  maximizes  the  output  signal-to-noise  ratio  (SNR)  in  the  speciﬁed 
structure  (namely, LTI ﬁltering  followed by  sampling), where  the SNR  is deﬁned as 
E {g [0]|H1 }2 /σ2 .  The square root of the SNR  is the relative separation between the 
means  of  the  two  distributions, measured  in  standard  deviations.  In  some  intuitive 
sense,  therefore, maximizing the SNR tries to separate the two distributions as well 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

256  Chapter  14 

Signal  Detection 

as possible.  However,  this does not  in general necessarily correspond  to minimizing 
the  probability  of  error. 

14.3.3  Continuous-Time Matched  Filters 

All  of  the matched ﬁlter  results developed  in  this  section  carry  over  in  a direct way 
to  continuous-time.  In  Figure  14.7  we  show  the  continuous-time  counterpart  to 
Figure  14.4  As  before,  we  normalize  the  gain  of  h(t)  so  that 

r(t) 

�  LTI  h(t) 

g(t)  � t = 0 
G 

�  Threshold  λ 

‘H1 ’ 
>�  <
‘H0 ’ 

FIGURE  14.7  Continuous-time  matched  ﬁltering. 
Z 
−∞ 
with  r(t)  a  Gaussian  random  process,  g(t)  is  also  Gaussian  and  G  is  a  Gaussian 
random  variable.  Under  the  two  hypotheses  the  PDF  of  G  is  then  given  by 

h2 (t)dt = 1 

(14.30) 

∞ 

where 

and 

(14.31) 

h2 (t)dt = N0 

H1  : fG|H (g H1 ) = N (µ, σ2
G ) 
|
H0  : fG|H (g H0 ) = N (0, σ2 
G )  , 
|
Z 
σ2  = N0 
G 
−∞ 
Z 
∞ 
−∞ 
Consequently,  as  in  the  discrete-time  case,  the  probability  of  error  is  minimized 
by  choosing  h(t)  to  separate  the  two  PDF’s  in  equation  (14.31)  as  much  as  possi­
ble.  With  the  continuous-time version of  the Cauchy-Schwarz  inequality  applied  to 
equation  (14.33) we  then  conclude  that  the optimum  choice  for h(t)  is proportional 
to  s(−t),  i.e.  again  the  matched  ﬁlter 

h(t)s(−t)dt 

(14.32) 

∞ 

µ = 

(14.33) 

EXAMPLE  14.1 

PAM  with Matched  Filter 

Figure  14.8(a)  shows  an  example  of  a  typical  noise-free  binary  PAM  signal  as  rep­
resented by Eq.  (13.1).  The pulse p(t)  is a  rectangular pulse of  length 50  sec.  The 
binary  sequence a[n]  over  the  time  interval  shown  is  indicated  above  the waveform. 
In  the  absence  of  noise,  the  optimal  threshold  detector  of  the  form  of  Figure  14.4 

c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

l
a
n
g
i
s
 
d
e
t
t
i
m
s
n
a
r
T

2

1 

0 

−1 

10 

0 

−10 

l
a
n
g
i
s
 
d
e
v
i
e
c
e
R

2

0 

−2 

t
u
p
t
u
o
 
r
e
t
l
i
f
 
d
e
h
c
t
a
M

Section  14.3 

A  General  Detector  Structure  257 

1

0 

1 

0

0

1

1

0

1

1 

0

0 

1 

0 

200 

400 

0 

200 

400 

600 
Time (s) 
(a) 

600 
Time(s) 
(b) 

800 

1000 

1200 

800 

1000 

1200 

0 

200 

400 

800 

1000 

1200 

600 
Time (s) 
(c) 

FIGURE  14.8  Binary  detection  with  on/oﬀ  signaling 

would  simply  test  at  integer  multiples  of  T  whether  the  received  signal  is  positive 
or  zero.  Clearly  the  probability  of  error  in  this  noise-free  case  would  be  zero. 

In Figure 14.8(b) we  show  the  same PAM  signal but with wideband Gaussian noise 
added.  If  h(t)  is  the  identity  system  and  the  threshold  of  the  detector  is  chosen 
according  to  Eq.  (14.18)  with  P (H0 ) =  P (H1 )  i.e.  using  the  likelihood  ratio 
test  but without  the matched  ﬁlter,  the  decoded binary  sequence  is  0100111111011 
which  has  6  bit  errors.  Figure  14.8(c)  shows  the  output  of  the  matched  ﬁlter,  i.e. 
with  h(t) =  s(−t).  The  detector  threshold  is  again  chosen  based  on  the  likelihood 
ratio  test.  The  resulting  decoded  binary  sequence  is  1010011111000  which  has  2 
bit  errors 

In  Figure  14.9  we  show  the  corresponding  results  when  antipodal  rather  than  on-
oﬀ  signaling  is  used.  Figure  14.9(a)  depicts  the  transmitted  waveform  with  the 
same  binary  sequence  as  was  used  in  Figure  14.8,  and  Figure  14.9(b)  the  received 
signal  including  additive  noise.  If  h(t) = δ(t)  and  P (H0 ) = P (H1 ),  then  the  choice 
of  threshold  for  the  likelihood  ratio  test  is  zero.  The  decoded  binary  sequence  is 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

258  Chapter  14 

Signal  Detection 

l
a
n
g
i
S
 
d
e
t
t
i
m
s
n
a
r
T

2


0


−2 

0 

200 

400 

10 

 
l
a
n
g
i
S
 
d
e
v
i
e
−10 
c
e
R

0 

0 

200 

400 

 
t
u
p
t
u
o
 
r
e
t
l
i
f
 
d
e
h
c
t
a
M

2 

0 

−2 

0 

200 

400 

800 

1000 

1200 

800 

1000 

1200 

800 

1000 

1200 

600 
Time (s) 
(a) 

600 
Time(s) 
(b) 

600 
Time (s) 
(c) 

FIGURE  14.9  Binary  Detection  with  antipodal  signaling 

0001001011001,  resulting  in 4 bit errors.  With h(t) chosen as the matched ﬁlter the 
signal  before  the  threshold  detector  is  that  shown  in  Figure  14.9(c).  The  resulting 
decoded  binary  sequence  is  1010011011001  with  no  bit  errors.  In  Table  14.1  we 
summarize  the results  for  this speciﬁc example based on a simulation with a binary 
sequence  of  length  104 . 

On/Oﬀ  Signaling

Antipodal  Signaling


No matched  ﬁlter  W/ matched  Filter

0.4808 
0.3752

0.4620 
0.2457 

TABLE  14.1  Bit  error  rate  for  a  PAM  signal  illustrating  eﬀect  of  matched  ﬁlter  for 
two  diﬀerent  signaling  schemes. 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  14.3 

A  General  Detector  Structure  259 

14.3.4  Pulse  Detection  in  Colored  Noise 

In Sections 14.2 and 14.3 the optimal detector was developed under the assumption 
that  the  noise  is white.  When  the  noise  is  colored  ,  i.e.  when  its  spectral  density  is 
not  ﬂat,  the  results  are  easily modiﬁed.  We  again  assume  a  detector  of  the  form  of 
Figure  14.4.  The  two  hypotheses  are  now: 

H1  : r[n] = s[n] + v [n],

H0  : r[n] = v [n] , 

(14.34)


where  v [n]  is  again  a  zero-mean  Gaussian  process  but  in  general,  not  white.  The 
autocorrelation function of v [n] is denoted by Rvv [m] and the power spectral density 
by  Svv (ejΩ ).  The  basic  approach  is  to  transform  the  problem  to  that  dealt with  in 
the previous  section by ﬁrst processing r[n] with a whitening ﬁlter as was discussed 
in  Section  10.2.3  ,  which  is  always  possible  as  long  as  Svv (ejΩ )  is  strictly  positive, 
i.e.  it  is not zero at any value of Ω.  This ﬁrst  stage of ﬁltering  is depicted  in Figure 
14.10. 

Whitening  Filter 
r[n] 
rw [n]�

�  hw [n] 

FIGURE  14.10  First  stage  of  ﬁltering 

The  impulse  response  hw [n]  is  chosen  so  that  its  output  due  to  the  input  noise 
v [n]  is  white,  with  variance  σ2  and,  of  course,  will  also  be  Gaussian.  With  this 
pre-processing  the  signal  rw [n]  now  has  the  form  assumed  in  Section  14.3.4  with 
the  white  noise  w[n]  corresponding  to  v [n] ∗ hw [n]  and  the  pulse  s[n]  replaced  by 
p[n] =  s[n] ∗ hw [n].  The  detector  structure  now  takes  the  form  shown  in  Figure 
14.11  where  h[n]  is  again  the  matched  ﬁlter,  but  in  this  case  matched  to  the  pulse 
p[n],  i.e.  hm [n]  is  proportional  to  p[−n]. 

�r[n] 

LTI  hw [n] 

rw [n] 

�

LTI  h[n] 

g [n] 

� n = 0  � 
g [0] 

Threshold  λ 

‘H1 ’ 
>�  <
‘H0 ’ 

FIGURE  14.11  Detector  structure  with  colored  noise. 

Assuming  that  hw [n]  is  invertible  (i.e.  its  Z -transform  has  no  zeros  on  the  unit 
circle)  there  is no  loss of generality  in having ﬁrst applied a whitening ﬁlter.  To  see 
this concretely denote the combined LTI ﬁlter from r[n] to g [n] as hc [n] and assume 
that  if whitening  had  not  ﬁrst  been  applied,  the  optimum  choice  for  the  ﬁlter  from 
r[n]  to  g [n]  is  hopt [n].  Since 

hc [n] = hw [n] ∗ hm [n] 
c°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 

(14.35) 

260  Chapter  14 

Signal  Detection 

where  hm [n]  denotes  the  matched  ﬁlter  after  whitening.  If  the  performance  with 
hopt [n]  is  better  than  with  hc [n],  this  would  imply  that  choosing  hm [n]  as  hopt [n] ∗ 
hinv [n] would  lead  to  better  performance  on  the whitened  signal.  However,  as  seen 
w 
in  Section  14.3,  hm [n] =  p[−n]  is  the  optimum  choice  after  the  whitening  and 
consequently  we  conclude  that 
hm [n] = p[−n] = hopt [n] ∗ hinv 
w  [n] 

(14.36) 

or  equivalently 

hopt [n] = hw [n] ∗ p[−n] 
In  the  following  example  we  illustrate  the  determination  of  the  optimum  detector 
in  the  case  of  colored  noise. 

(14.37) 

EXAMPLE  14.2 

Pulse  Detection  in  Colored  Noise 

Consider  a  pulse  s[n]  in  colored  noise  v [n],  with 

and 

s[n] = δ [n]  . 

(14.38) 

1 
Rvv [m] = (  )|m| ,  so  σ2  = 1 
v 
2 
3/4 
1 
z−1 )(1 −  z )
2 

(1 −  1
2

then  Svv (z ) = 

. 

(14.39) 

The  noise  component  w[n]  of  desired  output  of  the  whitening  ﬁlter  has  autocorre­
lation  function  Rww [m] = σ2 δ [m]  and  consequently  we  require  that 

Svv (z )Hw (z )Hw (1/z ) = σ2 
σ2 
4
= σ2 
Svv (z ) 
3

Thus  Hw (z )Hw (1/z ) = 

(1 − 

1 
1 
z−1 )(1 −  z )  . 
2 
2

(14.40) 

We  can  of  course  choose  σ  arbitrarily  (since  it  will  only  impact  the  overall  gain). 
Choosing  σ2  = 1,  either 

1 
Hw (z ) = (1 −  z−1 ),  or 
2 
1 
Hw (z ) = (1 −  z ) 
2 
Note  that  the  second  of  these  choices  is  non-causal.  There  are  also  other  possi­
bile  choices  since  we  can  cascade  either  choice  with  an  all-pass  Hap (z )  such  that 
Hap (z )Hap (1/z ) = 1. 

(14.41) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

Section  14.3
 

 
 
 
A General Detector Structure

 

261

 

With  the  ﬁrst  choice  for Hw (z )  from  above,  we  have 

1 
z−1 ),
Hw (z ) = (1 − 
2 
1 
hw [n] = δ [n] −  δ [n − 1],
2

σ2  = 3/4,


1

p[n] = s[n] −  s[n − 1],  and 
2 
h[n] = Ap[−n]  for  any  convenient  choice  of  A. 

(14.42) 

In  our  discussion  in  Section  14.3  of  the  detection  of  a  pulse  in  white  noise,  we 
observed  that  the  energy  in  the  pulse  aﬀects  performance  of  the  detector  but  not 
the  speciﬁc pulse  shape.  This was a consequence of  the  fact  that  the ﬁlter  is chosen 
to maximize  the  quantity  √1
E Rss [0]  where  s[n]  is  the  pulse  to  be  detected.  For  the 
case  of  a  pulse  in  colored  noise,  we  correspondingly  want  to  maximize  the  energy 
Ep  in  p[n]  where 

(14.43) 

p[n] = hw [n] ∗ s[n] 
Expressed  in  the  frequency  domain, 

P (ejΩ ) = Hw (ejΩ )S (ejΩ ) 

(14.44) 

and  from  Parseval’s  relation 

Ep  = 

= 

(14.45a) 

π  Z  π 
1 
|Hw (ejΩ )|2 |S (ejΩ )|2dΩ 
2
π 
1  Z−
2 
π
|S (ejΩ )| dΩ 
−π  Svv (ejΩ ) 
2π 
Based only on Eq.  (14.45b), Ep  can be maximized by placing all of the energy of the 
transmitted  signal  s[n]  at  the  frequency  at  which  Svv (ejΩ )  is  minimum.  However, 
in  many  situations  the  transmitted  signal  is  constrained  in  other  ways,  such  as 
peak  amplitude  and/or  time duration.  The  task  then  is  to  choose  s[n]  to maximize 
the  integral  in  Eq.  (14.45b)  under  these  constraints.  There  is  generally  no  closed-
form  solution  to  this  optimization  problem,  but  roughly  speaking  a  good  solution 
will  distribute  the  signal  energy  so  that  it  is  more  concentrated  where  the  power 
Svv (ejΩ )  of  the  colored  noise  is  less. 

(14.45b) 

°Alan  V.  Oppenheim  and  George  C.  Verghese,  2010 
c

MIT OpenCourseWare
http://ocw.mit.edu 

6.011 Introduction to Communication, Control, and Signal Processing 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

