Lecture 24

24.1 Goodness-of-(cid:12)t test.

Suppose that we observe an i.i.d. sample X1 ; : : : ; Xn of random variables that can
take a (cid:12)nite number of values B1 ; : : : ; Br with some unknown to us probabilities
p1 ; : : : ; pr : Suppose that we have a theory (or a guess) that these probabilities are
equal to some particular p(cid:14)1 ; : : : ; p(cid:14)r and we want to test it. This means that we want
to test the hypotheses
(cid:26) H1 : pi = p(cid:14)i for all i = 1; : : : ; r;
otherwise, i.e. for some i; pi 6= p(cid:14)i :
H2 :
If the (cid:12)rst hypothesis is true than the main result from previous lecture tells us that
we have the following convergence in distribution:
r
((cid:23)i (cid:0) np(cid:14)i )2
Xi=1
np(cid:14)i
where (cid:23)i = #fXj : Xj = Big: On the other hand, if H2 holds then for some index i,
pi 6= p(cid:14)i and the statistics T will behave very di(cid:11)erently. If pi is the true probability
(X1 = Bi ) then by CLT (see previous lecture)
(cid:23)i (cid:0) npipnpi ! N (0; 1 (cid:0) pi ):

! (cid:31)2
r(cid:0)1

T =

If we write

=

+ pn

=

(cid:23)i (cid:0) np(cid:14)ipnp(cid:14)i
pi (cid:0) p(cid:14)ipp(cid:14)i
(cid:23)i (cid:0) npi + n(pi (cid:0) p(cid:14)i )
(cid:23)i (cid:0) npipnpi
pnp(cid:14)i
then the (cid:12)rst term converges to N (0; 1 (cid:0) pi ) but the second term converges to plus
or minus 1 since pi 6= p(cid:14)i : Therefore,
((cid:23)i (cid:0) np(cid:14)i )2
np(cid:14)i

! +1

94


LECTURE 24.

95

which, obviously, implies that T ! +1: Therefore, as sample size n increases the
distribution of T under hypothesis H1 will approach (cid:31)2
r(cid:0)1 distribution and under
hypothesis H2 it will shift to +1; as shown in (cid:12)gure 24.1.

H1

y 2
râˆ’1

H2

C

Figure 24.1: Distribution of T under H1 and H2 .

Therefore, the following test looks very natural
(cid:14) = (cid:26) H1 : T (cid:20) c
H2 : T > c;
i.e. we suspect that the (cid:12)rst hypothesis H1 fails if T becomes unusually large. We
can decide what is "unusually large" or how to choose the threshold c by (cid:12)xing the
error of type 1 to be equal to the level of signi(cid:12)cance (cid:11) :
1 (T > c) (cid:25) (cid:31)2
1 ((cid:14) 6= H1 ) =
r(cid:0)1 (c; 1)
since under the (cid:12)rst hypothesis the distribution of T can be approximated by (cid:31)2
r(cid:0)1
distribution. Therefore, we (cid:12)nd c from the table of (cid:31)2
r(cid:0)1 distribution such that (cid:11) =
r(cid:0)1 (c; 1): This test is called the (cid:31)2 goodness-of-(cid:12)t test.
(cid:31)2
Example. Suppose that we have a sample of 189 observations that can take three
values A; B and C with some unknown probabilities p1 ; p2 and p3 and the counts are
given by

(cid:11) =

A B C T otal
58 64 67
189

We want to test the hypothesis H1 that this distribution is uniform, i.e. p1 = p2 =
p3 = 1=3: Suppose that level of signi(cid:12)cance is chosen to be (cid:11) = 0:05: Then the
threshold c in the (cid:31)2 test
(cid:14) = (cid:26) H1 : T (cid:20) c
H2 : T > c
can be found from the condition that

(cid:31)2
3(cid:0)1=2 (c; 1) = 0:05



LECTURE 24.

96

and from the table of (cid:31)2
2 distribution with two degrees of freedom we (cid:12)nd that c = 5:9:
In our case

T =

(58 (cid:0) 189=3)2
189=3

+

(64 (cid:0) 189=3)2
189=3

+

(67 (cid:0) 189=3)2
189=3

= 0:666 < 5:9

which means that we accept H1 at the level of signi(cid:12)cance 0:05:

24.2 Goodness-of-(cid:12)t for continuous distribution.

A similar approach can be used to test a hypothesis that the distribution of the data is
equal to some particular distribution, in the case when observations do not necessarily
take a (cid:12)nite number of (cid:12)xed values as was the case in the last section. Let X1 ; : : : ; Xn
be the sample from unknown distribution
and consider the following hypotheses:
(cid:26) H1 :
=
6=
H2 :
for some particular
0 : To use the result from previous lecture we will discretize the
set of possible values of X s by splitting it into a (cid:12)nite number of intervals I1 ; : : : ; Ir
as shown in (cid:12)gure 24.2. If the (cid:12)rst hypothesis H1 holds then the probability that X
comes from the j th interval is equal to

0

0

(X 2 Ij ) = p(cid:14)j for all j (cid:20) r
otherwise

0 (X 2 Ij ) = p(cid:14)j :
(X 2 Ij ) =
and instead of testing H1 vs. H2 we will consider the following weaker hypotheses
(cid:26) H 01 :
H 02 :
Asking whether H 01 holds is, of course, a weaker question that asking if H1 holds,
because H1 implies H 01 but not the other way around. There are many distributions
di(cid:11)erent from
that have the same probabilities of the intervals I1 ; : : : ; Ir as
:
Later on in the course we will look at other way to test the hypothesis H1 in a more
consistent way (Kolmogorov-Smirnov test) but for now we will use the (cid:31)2 convergence
result from previous lecture and test the derivative hypothesis H 01 : Of course, we are
back to the case of categorical data from previous section and we can simply use the
(cid:31)2 goodness-of-(cid:12)t test above.
The rule of thumb about how to split into subintervals I1 ; : : : ; Ir is to have the
expected count in each subinterval

np(cid:14)i = n

0 (X 2 Ii ) (cid:21) 5













LECTURE 24.

97

0P

I1

I2

I3

op1

op2

o
p3

R

Ir

o

pr

Figure 24.2: Discretizing continuous distribution.

at least 5: For example, we can split into intervals of equal probabilities p(cid:14)i = 1=r and
choose their number r so that
n
r (cid:21) 5:
Example. (textbook, p. 539) We want to test the following hypotheses:
(cid:26) H1 :
H2 :

= N (3:912; 0:25)
otherwise

np(cid:14)i =

1

2

3

4

0.25
I1

0.25
I2

0.25
I3

0.25
I4

R

3.912

Figure 24.3: Total of 4 Sub-intervals.

We are given n = 23 observations and using the rule of thumb we will split into r
equal probability intervals so that

n
r

=

23
r (cid:21) 5 ) r = 4:


LECTURE 24.

98

It is easy to (cid:12)nd the
Therefore, we split into 4 intervals of probability 0:25 each.
endpoints of these intervals for the distribution N (3:912; 0:25) which we will skip and
simply say that the counts of the observations in these intervals are...

