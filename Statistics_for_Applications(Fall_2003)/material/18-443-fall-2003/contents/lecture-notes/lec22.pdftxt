Lecture 22

22.1 One sided hypotheses continued.

It remains to prove the second part of the Theorem from last lecture. Namely, we
have to show that for any (cid:14) 2 K(cid:11)
(cid:5)((cid:14) (cid:3) ; (cid:18)) (cid:21) (cid:5)((cid:14); (cid:18)) for (cid:18) > (cid:18)0 :
Let us take (cid:18) > (cid:18)0 and consider two simple hypotheses

h1 :

=

(cid:18)0 and h2 :

=

(cid:18) :

Let us (cid:12)nd the most powerful test with error of type one equal to (cid:11): We know that
if we can (cid:12)nd a threshold b such that
(cid:18)0 (cid:16) f (X j(cid:18)0 )
< b(cid:17) = (cid:11)
f (X j(cid:18))
then the following test will be the most powerful test with error of type 1 equal to (cid:11):
(cid:14)(cid:18) = ( h1 :
f (X j(cid:18)0 )
f (X j(cid:18)) (cid:21) b
f (X j(cid:18)0 )
f (X j(cid:18)) < b
h2 :
But the monotone likelihood ratio implies that
f (X j(cid:18))
f (X j(cid:18)0 )
f (X j(cid:18)0 )
f (X j(cid:18))
and, since now (cid:18) > (cid:18)0 ; the function V (T ; (cid:18) ; (cid:18)0 ) is strictly increasing in T : Therefore,
we can solve this inequality for T and get that T > cb for some cb :
This means that the error of type 1 for the test (cid:14)(cid:18) can be written as
(cid:18)0 (cid:16) f (X j(cid:18)0 )
< b(cid:17) =
f (X j(cid:18))
86

1
b , V (T ; (cid:18) ; (cid:18)0 ) >

(cid:18)0 (T > cb ):

< b ,

>

1
b








LECTURE 22.

87

But we chose this error to be equal to (cid:11) =
be such that

(cid:18)0 (T > c) which means that cb should

(cid:18)0 (T > cb ) =

(cid:18)0 (T > c) ) c = cb :
Therefore,we proved that the test
(cid:14)(cid:18) = (cid:26) h1 : T (cid:20) c
h2 : T > c
is the most powerful test with error of type 1 equal to (cid:11):
But this test (cid:14)(cid:18) is exactly the same as (cid:14) (cid:3) and it does not depend on (cid:18) : This means
that deciding between two simple hypotheses (cid:18)0 vs. (cid:18) one should always use the same
most powerful decision rule (cid:14) (cid:3) : But this means that (cid:14) (cid:3) is uniformly most powerful test
- what we wanted to prove. Notice that MLR played a key role here because thanks
to MLR the decision rule (cid:14)(cid:18) was independent of (cid:18) : If (cid:14)(cid:18) was di(cid:11)erent for di(cid:11)erent (cid:18)
this would mean that there is no UMP for composite hypotheses because it would be
advantageous to use di(cid:11)erent decision rules for di(cid:11)erent (cid:18) :

Example. Let us consider a family of normal distributions N ((cid:22); 1) with unknown
mean (cid:22) as a parameter. Given some (cid:22)0 consider one sided hypotheses

H1 : (cid:22) (cid:20) (cid:22)0 and H2 : (cid:22) > (cid:22)0 :
As we have shown before the normal family N ((cid:22); 1) has monotone likelihood ratio
with T (X ) = Pn
i=1 Xi : Therefore, the uniformly most powerful test with level of
signi(cid:12)cance (cid:11) will be as follows:
(cid:14) (cid:3) = (cid:26) H1 : Pn
i=1 Xi (cid:20) c
H2 : Pn
i=1 Xi > c:
The threshold c is determined by
(cid:22)0 (X Xi > c):
(cid:22)0 (T > c) =
If the sample comes from N ((cid:22)0 ; 1) then T has distribution N (n(cid:22)0 ; n) and
n
Xi=1

(Xi (cid:0) (cid:22)0 ) (cid:24) N (0; 1)

1
pn

Y =

(cid:11) =

is standard normal. Therefore,
n
Xi=1

(cid:11) =

(cid:22)0 (

Xi > c) =

(cid:22)0 (cid:16)Y =

1
pn

n
Xi=1

(Xi (cid:0) (cid:22)0 ) >

c (cid:0) n(cid:22)0pn (cid:17):








LECTURE 22.

88

Therefore, if using the table of standard normal distribution we (cid:12)nd c(cid:11) such that
(Y > c(cid:11) ) = (cid:11) then
c (cid:0) n(cid:22)0pn
Example. Let us now consider a family of normal distributions N (0; (cid:27) 2 ) with
variance (cid:27) 2 as unknown parameter. Given (cid:27) 2
0 we consider one sided hypotheses

= c(cid:11) or c = (cid:22)0n + pnc(cid:11) :

0 and H2 : (cid:27) 2 > (cid:27) 2
H1 : (cid:27) 2 (cid:20) (cid:27) 2
0 :
Let us (cid:12)rst check if MLR holds in this case. The likelihood ratio is
f (X j(cid:27) 2
i .
1
2 )
1
e(cid:0) 1
e(cid:0) 1
Pn
Pn
i=1 X 2
i=1 X 2
2(cid:27)2
2(cid:27)2
i
(p2(cid:25)(cid:27)2 )n
(p2(cid:25)(cid:27)1 )n
=
2
1
f (X j(cid:27) 2
1 )
e(cid:16) 1
e(cid:16) 1
2 (cid:17)T
2 (cid:17) P X 2
(cid:27)2 (cid:17)n
= (cid:16) (cid:27)1
(cid:27)2 (cid:17)n
i = (cid:16) (cid:27)1
1 (cid:0) 1
1 (cid:0) 1
2(cid:27)2
2(cid:27)2
2(cid:27)2
2(cid:27)2
where T = Pn
i . When (cid:27) 2
2 > (cid:27) 2
i=1 X 2
1 the likelihood ratio is increasing in T and,
therefore, MLR holds. By the above Theorem, the UMP test exists and is given by
(cid:14) (cid:3) = (cid:26) H1 : T = Pn
i=1 X 2
i (cid:20) c
H2 : T = Pn
i=1 X 2
i > c
where the threshold c is determined by
0 (cid:16) n
n
Xi=1 (cid:16) Xi
(cid:27)0 (cid:17)2
0 (cid:17):
c
Xi=1
 (cid:27)2
(cid:27) 2
When Xi (cid:24) N (o; (cid:27) 2
0 ); Xi=(cid:27)0 (cid:24) N (0; 1) are standard normal and, therefore,
n
(cid:27)0 (cid:17)2
Xi=1 (cid:16) Xi
n distribution with n degrees of freedom. If we (cid:12)nd c(cid:11) such that (cid:31)2
has (cid:31)2
n (c(cid:11) ; 1) = (cid:11)
then c = c(cid:11)(cid:27) 2
0 :

X 2
i > c) =

(cid:24) (cid:31)2
n

>

;

(cid:11) =

(

 (cid:27)2
0


