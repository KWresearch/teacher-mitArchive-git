Lecture 18

Testing hypotheses.

(Textbook, Chapter 8)

18.1 Testing simple hypotheses.

1

2

k

=

=
=

H1 :
H2 :
...
Hk :

on some space X ; i.e.
Let us consider an i.i.d. sample X1 ; : : : ; Xn with distribution
X ’s take values in X : Suppose that we don’t know
but we know that it can only
 2 f 
k g.
be one of possible k distributions,
1 ; : : : ;
Based on the data X; : : : ; Xn we have to decide among k simple hypotheses:
8>>><
>>>:
We call these hypotheses simple because each hypothesis asks a simple question about
is equal to some particular speci(cid:12)ed distribution.
whether
To decide among these hypotheses means that given the data vector,
X = (X1 ; : : : ; Xn) 2 X n
we have to decide which hypothesis to pick or, in other words, we need to (cid:12)nd a
decision rule which is a function
(cid:14) : X n ! fH1 ; (cid:1) (cid:1) (cid:1) ; Hk g:
Let us note that sometimes this function (cid:14) can be random because sometimes several
hypotheses may look equally likely and it will make sense to pick among them ran-
domly. This idea of a randomized decision rule will be explained more clearly as we
go on, but for now we can think of (cid:14) as a simple function of the data.

67











LECTURE 18. TESTING HYPOTHESES.

68

Suppose that the ith hypothesis is true, i.e.
decision rule (cid:14) will make an error is

=

i : Then the probability that

((cid:14) 6= Hi jHi ) =
i ((cid:14) 6= Hi );
which we will call error of type i or type i error.
In the case when we have only two hypotheses H1 and H2 the error of type 1

(cid:11)1 =

2 ((cid:14) = H2 )

2 ((cid:14) 6= H2 ) =

1 ((cid:14) 6= H1 )
is also called size or level of signi(cid:12)cance of decision rule (cid:14) and one minus type 2 error
(cid:12) = 1 (cid:0) (cid:11)2 = 1 (cid:0) 
is called the power of (cid:14) .
Ideally, we would like to make errors of all types as small as possible but it is clear
that there is a trade-o(cid:11) among them because if we want to decrease the error of, say,
type 1 we have to predict hypothesis 1 more often, for more possible variations of
the data, in which case we will make a mistake more often if hypothesis 2 is actually
the true one. In many practical problems di(cid:11)erent types of errors have very di(cid:11)erent
meanings.
Example. Suppose that using some medical test we decide is the patient has
certain type of decease. Then our hypotheses are:

H1 : positive; H2 : negative:

Then the error of type one is

((cid:14) = H2 jH1 );
i.e. we predict that the person does not have the decease when he actually does and
error of type 2 is

((cid:14) = H1 jH2 );
i.e. we predict that the person does have the decease when he actually does not.
Clearly, these errors are of a very di(cid:11)erent nature. For example, in the (cid:12)rst case the
patient will not get a treatment that he needs, and in the second case he will get
unnecessary possibly harmful treatment when he doesn not need it, given that no
additional tests are conducted.
Example. Radar missile detection/recognition. Suppose that an image on the
radar is tested to be a missile versus, say, a passenger plane.

Then the error of type one

H1 : missile; H2 : not missile:

((cid:14) = H2 jH1 );










LECTURE 18. TESTING HYPOTHESES.

69

means that we will ignore a missile and error of type 2

((cid:14) = H2 jH1 );
means that we will possibly shoot down a passenger plane (which happened before).
Another example could be when guilty or not guilty verdict in court is decided
based on some tests and one can think of many examples like this. Therefore, in many
situations it is natural to control certain type of error, give garantees that this error
does not exceed some acceptable level, and try to minimize all other types of errors.
For example, in the case of two simple hypotheses, given the largest acceptable error
of type one (cid:11) 2 [0; 1]; we will look for a decision rule in the class
K(cid:11) = f(cid:14) : (cid:11)1 =
1 ((cid:14) 6= H1 ) (cid:20) (cid:11)g
and try to (cid:12)nd (cid:14) 2 K(cid:11) that makes the error of type 2; (cid:11)2 =
possible, i.e. maximize the power.

2 ((cid:14) 6= H2 ); as small as

18.2 Bayes decision rules.

We will start with another way to control the trade-o(cid:11) among di(cid:11)erent types of errors
that consists in minimizing the weighted error.
Given hypotheses H1 ; : : : ; Hk let us consider k nonnegative weights (cid:24) (1); : : : ; (cid:24) (k)
that add up to one Pk
i=1 (cid:24) (i) = 1: We can think of weights (cid:24) as an apriori probability
on the set of our hypotheses that represent their relative importance. Then the Bayes
error of a decision rule (cid:14) is de(cid:12)ned as
k
k
Xi=1
Xi=1
which is simply a weighted error. Of course, we want to make this weigted error as
small as possible.
De(cid:12)nition: Decision rule (cid:14) that minimizes (cid:11)((cid:24) ) is called Bayes decision rule.
Next theorem tells us how to (cid:12)nd this Bayes decision rule in terms of p.d.f. or p.f.
or the distributions
i :
Theorem. Assume that each distribution

i has p.d.f or p.f. fi (x): Then

i((cid:14) 6= Hi );

(cid:11)((cid:24) ) =

(cid:24) (i)(cid:11)i =

(cid:24) (i)

(cid:14) = Hj if (cid:24) (j )fj (X1 ) : : : fj (Xn ) = max
1(cid:20)i(cid:20)k

(cid:24) (i)fi (X1 ) : : : fi (Xn )

is the Bayes decision rule.
In other words, we choose hypotheses Hj if it maximizes the weighted likelihood
function

(cid:24) (i)fi(X1 ) : : : fi (Xn)







LECTURE 18. TESTING HYPOTHESES.

70

(cid:11)((cid:24) ) =

If this maximum is achieved simultaneously on several hy-
among all hypotheses.
potheses we can pick any one of them, or at random.
Proof. Let us rewrite the Bayes error as follows:
k
Xi=1
i((cid:14) 6= Hi )
(cid:24) (i)
k
(cid:24) (i) Z I ((cid:14) 6= Hi )fi (x1 ) : : : fi (xn )dx1 : : : dxn
Xi=1
=
k
= Z
(cid:24) (i)fi (x1 ) : : : fi (xn )(cid:16)1 (cid:0) I ((cid:14) = Hi )(cid:17)dx1 : : : dxn
Xi=1
k
(cid:24) (i) Z fi (x1 ) : : : fi (xn )dx1 : : : dxn
Xi=1
=
}
{z
|
this joint density integrates to 1 and P (cid:24) (i) = 1
k
(cid:0) Z
Xi=1
(cid:24) (i)fi (x1 ) : : : fi (xn )I ((cid:14) = Hi )dx1 : : : dxn
k
= 1 (cid:0) Z
Xi=1
To minimize this Bayes error we need to maximize this last integral, but we can
actually maximize the sum inside the integral

(cid:24) (i)fi (x1 ) : : : fi (xn )I ((cid:14) = Hi )dx1 : : : dxn :

(cid:24) (1)f1 (x1 ) : : : f1 (xn )I ((cid:14) = H1 ) + : : : + (cid:24) (k)fk (x1 ) : : : fk (xn )I ((cid:14) = Hk )

by choosing (cid:14) appropriately. For each (x1 ; : : : ; xn ) decision rule (cid:14) picks only one
hypothesis which means that only one term in this sum will be non zero, because if
(cid:14) picks Hj then only one indicator I ((cid:14) = Hj ) will be non zero and the sum will be
equal to

(cid:24) (j )fj (x1 ) : : : fj (xn ):

Therefore, to maximize the integral (cid:14) should simply pick the hypothesis that maxi-
mizes this expression, exactly as in the statement of the Theorem. This (cid:12)nishes the
proof.


