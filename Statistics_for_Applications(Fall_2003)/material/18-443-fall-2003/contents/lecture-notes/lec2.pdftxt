Lecture  2


2.1  Some  probability  distributions. 

Let  us  recall  some  common  distributions  on  the  real  line  that  will  be  used  often  in 
this  class.  We  will  deal  with  two  types  of  distributions: 

1.  Discrete 

2.  Continuous 

 

Discrete  distributions. 
Suppose  that  a  set  X  consists  of  a  countable  or  ﬁnite  number  of  points, 
X  = {a1 , a2 , a3 , · · ·}
.
on X  can be deﬁned  via  a  function p(x)  on X  with 

Then  a probability distribution 
the  following  properties: 
1.  0 ∀ p(ai ) ∀ 1, 
2.  �∗ 
i=1 p(ai ) = 1. 
p(x)  is  called  the  probability  function.  If X  is  a  random  variable with  distribution 
then p(ai ) = 
  (ai ) is a probability that X  takes value ai .  Given a function � : X  ≈ 
the  expectation  of  �(X )  is  deﬁned  by 
 
∗

�(ai )p(ai ) 
i=1 
(Absolutely)  continuous  distributions. 
is  deﬁned  via  a  probability  density  function 
Continuous  distribution 
on 
� 
 
 
�
∗
such  that  p(X )  →  0  and 
(p.d.f.)  p(x)  on 
p(X )dx  = 1.  If  a  random  vari-
� 
−∗
able  X  has  distribution 
then  the  chance/probability  that  X  takes  a  value  in  the 
 

, 

 

� 

�(X ) = 

3


�
LECTURE  2. 

4 

interval  [a, b]  is  given  by 

  b 

p(x) = 

� 

�(X ) = 

p(x)dx. 

�(x)p(x)dx. 

�
  (X  ⊆  [a, b]) = 
a
Clearly,  in  this  case  for  any  a  ⊆ 
  (X  =  a)  =  0.  Given  a  function
we  have 
� 
�  : X  ≈ 
,  the  expectation  of  �(X )  is  deﬁned  by 
� 
 
∗ 
�
−∗ 
Notation.  The  fact that a random variable X  has distribution 
  will be denoted 
. 
by X  ∩ 
 
Example  1.  Normal (Gaussian) Distribution N (ϕ, δ 2 ) with mean ϕ and variance 
δ 2  is  a  continuous  distribution  on  with  probability  density  function: 
� 
1 
(x−�)2 
e− 
for  x ⊆ (−≤, ≤).
∞2νδ
2�2 
Normal  distribution  often  describes  continuous  random  variables  that  can  be  af­
fected  by  a  sum  of many  independent  factors,  for example,  person’s  height  or weight, 
ﬂuctuations  of  stock market,  etc.  In  this  case,  the  reason  for having  normal distribu­
tion  lies  in  the  Central  Limit  Theorem. 
Example  2.  Bernoulli  Distribution  B (p)  describes  a  random  variable  that  can 
=  {0, 1}.  The  distribution  is  described  by  a 
take  only  two  possible  values,  i.e. 
X
probability  function 
  (X  = 1) = p,  p(0) = 
  (X  = 0) = 1 − p  for  some  p ⊆  [0, 1]. 
Example  3.  Exponential  Distribution  E (ϕ)  is  a  continuous  distribution  with 
p.d.f. 
 
�
ϕe−ϕx  x 
0, 
→
x < 0. 
0 
Here,  ϕ > 0  is  the  parameter  of  the  distribution. 
This  distribution  has  the  following  nice  property.  If  a  random  variable X  ∩ E (ϕ) 
then  probability  that X  exceeds  level  t  for  some  t > 0  is 
 
∗ 
�
  (X  ⊆  [t, ≤)) = 
  (X  → t) = 
t 
For  s >  0,  the  probability  that  X  will  exceed  level  t + s  given  that  it  exceeded  level 
t  can  be  computed  as  follows: 

ϕe−ϕxdx = e−ϕt . 

p(x) = 

p(1) = 

  (X t + s X 
→ 
| → 

t) = 

→ 
  (X t + s, X 
→
  (X t) 
→
=  e−ϕ(t+s) /e−ϕt  = e−ϕs  = 

t) 

  (X t + s)
→
= 
→ 
  (X t)
  (X s),→ 

LECTURE  2. 

i.e. 

5 

t) = 

  (X t + s X 
→ 
| → 
→
  (X s).
In  other  words,  if  we  think  of  X  as  a  lifetime  of  some  ob ject  in  some  random  con­
ditions,  then  this  property  means  that  the  chance  that  X  will  live  longer  then  t + s 
given  that  it  lived  longer  than  t  is  the  same  as  the  chance  that  X  lives  longer  than 
t  in  the  ﬁrst  place.  Or,  if  X  is  “alive”  at  time  t  then  it  is  like  new,  so  to  speak. 
Therefore,  some  natural  examples  that  can  be  decribed  by  exponential  distribution 
are  the  life  span  of  high  quality  products,  or  soldiers  at  war. 
Example  4.  Poisson  Distribution  �(∂)  is  a  discrete  distribution  with 

p(k) = 

= {0, 1, 2, 3, . . .},
X 
∂k 
e−�  for  k = 0, 1, 2, , . . . 
  (X  = k) = 
k ! 
Poisson  distribution  could  be  used  to  describe  the  following  random  ob jects:  the 
number  of  stars  in  a  random  area  of  the  space;  number  of misprints  in  a  typed  page; 
number of wrong connections  to your phone number; distribution of bacteria on some 
surface  or  weed  in  the  ﬁeld.  All  these  examples  share  some  common  properties  that 
give rise to a Poisson distribution.  Suppose that we count a number of random ob jects 
in  a  certain  region  T  and  this  counting  process  has  the  following  properties: 
1.  Average number  of ob jects  in any  region S  ∼ T  is proportional  to the  size  of S ,
i.e.  Count(S ) = ∂ S . Here  S denotes  the  size  of S,  i.e.  length, area, volume, 
|
|
|
|
� 
etc.  Parameter  ∂ > 0  represents  the  intensity  of  the  process. 

2.  Counts  on  disjoint  regions  are  independent. 

3.  Chance  to  observe  more  than  one  ob ject  in  a  small  region  is  very  small,  i.e. 
  (Count(S ) → 2)  becomes  small  when  the  size  S gets  small. 
| 
|
We will  show  that  these  assumptions  will  imply  that  the number  of ob jects  in  the 
region  T ,  Count(T ),  has  Poisson  distribution  �(∂ T )  with  parameter  ∂ T .
|
|
|
|

0 

T/n 

X1 

X2 

. . . . . . . 

T 
− Counts on small subintervals 

Xn 

Figure  2.1:  Poisson  Distribution 

For simplicity,  let us assume  that the region T  is an interval  [0, T ] of length T . Let 
us  split  this  interval  into  a  large  number  n  of  small  equal  subintervals  of  length  T /n 

LECTURE  2. 

6 

and  denote  by  Xi  the  number  of  random  ob jects  in  the  ith  subinterval,  i = 1, . . . , n. 
By  the  ﬁrst  property  above, 

� 

Xi  = 

. 

p(x) =	

� 

Xi  = 

we  can  write, 

∂T 
n 
On  the  other  hand,  by  deﬁnition  of  expectation 
 

  (Xi  = k) = 0 + 
  (Xi  = 1) + πn , 
k
k�0 
 
where  πn  =  �
  (Xi  =  k),  and  by  the  last  property  above  we  assume  that  πn 
k�2 k
becomes  small  with  n,  since  the  probability  to  observe  more  that  two  ob jects  on  the 
interval  of  size  T /n  becomes  small  as  n  becomes  large.  Combining  two  equations 
  (Xi  =  1)  ∅  ∂ T  .  Also,  since  by  the  last  property  the  probability  that 
above  gives, 
n 
any  count  Xi  is  → 2  is  small,  i.e. 
 T �
 
�
  (at  least  one Xi  → 2) ∀ no 
n 
 
 ∂T �k�
n ��
⎜
n 
k
(∂T )k 
e−�T
k ! 

≈ 
Example  5:  Uniform  Distribution  U [0, χ ]  has  probability  density  function 
 
�
Finally,  let  us  recall  some  properties  of  normal  distribution.  If  a  random  variable 
X  has  normal  distribution  N (ϕ, δ 2 )  then  the  r.v. 
X − ϕ 
δ 

  (Count(T ) = X1  +

1 
x ⊆  [0, χ ], 
ν ,
0,  otherwise. 

≈ 0  as  n ≈ ≤, 

Y  = 

∩ N (0, 1)
has  standard  normal  distribution.  To  see  this,  we  can  write, 
  bπ+ϕ 
 X − ϕ 
  (X  ⊆  [aδ + ϕ, bδ + ϕ])  =  �
1 
  �
�
[a, b] = 
aπ+ϕ  ∞2νδ
δ  ⊆
  b 
�
1 
2 y
e−  dy , 
= 
a  ∞2ν
2

∂T �n−k 
n 

(x−�)2 
e− 
2�2  dx 

+ Xn  = k)	

· · ·

∅ 

 
1 −

	
LECTURE  2. 

7 

� 

1 = 

−∗ 

where  in  the  last  integral  we  made  a  change  of  variables  y  = (x − ϕ)/δ.  This,  of 
course,  means  that  Y  ∩ N (0, 1).  The  expectation  of  Y  is 
 
∗ 
�
1 
2
y 
e−  dy = 0 
Y  = 
y ∞2ν
2
−∗ 
since  we  integrate  odd  function.  To  compute  the  second  moment  Y 2 ,  let  us  ﬁrst 
� 
2 
2α e− y 
is  a  probability  density  function,  it  integrates  to  1,  i.e. 
note  that  since  ≥1
2
 
�
If  we  integrate  this  by  parts,  we  get, 
 
 
�
�
∗  1 
���
∗ 
1
−∗  − 
dy =  ∞
1 = 
−∗  ∞2ν
2ν
 
=  0 + �
∗ 
1 
2
e− y 
y 2 ∞2ν
. 
dy =  � 
2
−∗ 
Thus,  the  second  moment  Y 2  = 1.  The  variance  of  Y  is 
� 
Var(Y ) =  Y 2  − (  Y )2  = 1 − 0 = 1. 
� 

y
2 
y
(−y )e−  dy 
∞
2
2ν 

∗  1 
∞2ν

2 
ye− y 
2

2
y 
e− 
2

dy . 

2

y
e− 
2

 

∗ 

−∗ 

Y 2

�
