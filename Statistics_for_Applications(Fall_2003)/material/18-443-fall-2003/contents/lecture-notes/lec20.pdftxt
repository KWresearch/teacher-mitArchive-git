Lecture 20

20.1 Randomized most powerful test.

In theorem in the last lecture we showed how to (cid:12)nd the most powerful test with level
of signi(cid:12)cance (cid:11) (which means that (cid:14) 2 K(cid:11) ), if we can (cid:12)nd c such that
1(cid:16) f1 (X )
< c(cid:17) = (cid:11):
f2 (X )
This condition is not always ful(cid:12)led, especially when we deal with discrete distribu-
tions as will become clear from the examples below. But if we look carefully at the
proof of that Theorem, this condition was only necessary to make sure that the like-
lihood ratio test has error of type 1 exactly equal to (cid:11): In our next theorem we will
show that the most powerful test in class K(cid:11) can always be found if one randomly
breaks the tie between two hypotheses in a way that ensures that the error of type
one is equal to (cid:11):
Theorem. Given any (cid:11) 2 [0; 1] we can always (cid:12)nd c 2 [0; 1) and p 2 [0; 1] such
that
= c(cid:17) = (cid:11):
1(cid:16) f1 (X )
< c(cid:17) + (1 (cid:0) p)
1(cid:16) f1 (X )
f2 (X )
f2 (X )
In this case, the most powerful test (cid:14) 2 K(cid:11) is given by
(cid:14) = 8><
f1 (X )
H1 :
f2 (X ) > c
f1 (X )
f2 (X ) < c
H2 :
>:
f1 (X )
H1 or H2 :
f2 (X ) = c
where in the last case of equality we break the tie at random by choosing H1 with
probability p and choosing H2 with probability 1 (cid:0) p:
This test (cid:14) is called a randomized test since we break a tie at random if necessary.

(20.1)

76




LECTURE 20.

77

(cid:11)1 =

1 ((cid:14) 6= H1 ) =

Proof. Let us (cid:12)rst assume that we can (cid:12)nd c and p such that (20.1) holds. Then
the error of type 1 for the randomized test (cid:14) above can be computed:
= c(cid:17) = (cid:11)
< c(cid:17) + (1 (cid:0) p)
1(cid:16) f1 (X )
1(cid:16) f1 (X )
f2 (X )
f2 (X )
since (cid:14) does not pick H1 exactly when the likelihood ratio is less than c or when it is
equal to c in which case H1 is not picked with probability 1 (cid:0) p: This means that the
randomized test (cid:14) 2 K(cid:11) . The rest of the proof repeats the proof of the last Theorem.
We only need to point out that our randomized test will still be Bayes test since in
the case of equality

(20.2)

= c

f1 (X )
f2 (X )
the Bayes test allows one to break the tie arbitrarily and we choose to break it
randomly in a way that ensures that the error of type one will be equal to (cid:11); as in
(20.2).
The only question left is why we can always choose c and p such that (20.1) is
satis(cid:12)ed. If we look at the function

F (t) =

F (c) =

< t(cid:17)
 (cid:16) f1 (X )
f2 (X )
as a function of t; it will increase from 0 to 1 as t increases from 0 to 1: Let us keep in
mind that, in general, F (t) might have jumps. We can have two possibilities: either
(a) at some point t = c the function F (c) will be equal to (cid:11); i.e.
< c(cid:17) = (cid:11)
 (cid:16) f1 (X )
f2 (X )
or (b) at some point t = c it will jump over (cid:11); i.e.
 (cid:16) f1 (X )
< c(cid:17) < (cid:11)
f2 (X )
= c(cid:17) (cid:21) (cid:11):
f2 (X ) (cid:20) c(cid:17) = F (c) +
 (cid:16) f1 (X )
 (cid:16) f1 (X )
f2 (X )
Then (20.1) will hold if in case (a) we take p = 1 and in case (b) we take
1 (cid:0) p = ((cid:11) (cid:0) F (c)).  (cid:16) f1 (X )
= c(cid:17):
f2 (X )

F (c) =

but




LECTURE 20.

78

Example. Suppose that we have one observation X with Bernoulli distribution
and two simple hypotheses about the probability function f (X ) are

H1 : f1 (X ) = 0:2X 0:81(cid:0)X
H2 : f2 (X ) = 0:4X 0:61(cid:0)X :

Let us take the level of signi(cid:12)cance (cid:11) = 0:05 and (cid:12)nd the most powerful (cid:14) 2 K0:05 :
In (cid:12)gure 20.1 we show the graph of the function
< c(cid:17):
1(cid:16) f1 (X )
f2 (X )
Let us explain how this graph is obtained. First of all, the likelihood ratio can take

F (c) =

1

0.2

1/2

C

4/3

Figure 20.1: Graph of F (c).

only two values:

f1 (X )
f2 (X )

= (cid:26) 1=2 if X = 1
4=3 if X = 0:

< co = ; is empty and F (c) =

If c (cid:20) 1
2 then the set
n f1 (X )
f2 (X )
if 1
2 < c (cid:20) 4
3 then the set
< co = fX = 1g and F (c) =
n f1 (X )
f2 (X )
and, (cid:12)nally, if 4
3 < c then the set
< co = fX = 0 or 1g and F (c) =
n f1 (X )
f2 (X )

1 (;) = 0;

1 (X = 1) = 0:2

1 (X = 0 or 1) = 1;





LECTURE 20.

79

as shown in (cid:12)gure 20.1. The function F (c) jumps over the level (cid:11) = 0:05 at the point
c = 1=2: To determine p we have to make sure that the error of type one is equal to
0:05; i.e.

1(cid:16) f1 (X )
1(cid:16) f1 (X )
< c(cid:17) + (1 (cid:0) p)
= c(cid:17) = 0 + (1 (cid:0) p)0:2 = 0:05
f2 (X )
f2 (X )
which gives that p = 3
4 : Therefore, the most powerful test of size (cid:11) = 0:05 is
(cid:14) = 8><
>:
where in the last case X = 1 we pick H1 with probability 3
4 or H2 with probability 1
4 :

f1 (X )
f2 (X ) > 1
2 or X = 0
f1 (X )
f2 (X ) < 1
2 or never
f1 (X )
f2 (X ) = 1
2 or X = 1;

H1 :
H2 :
H1 or H2 :

20.2 Composite hypotheses. Uniformly most pow-
erful test.

We now turn to a more di(cid:14)cult situation then the one when we had only two simple
(cid:18)0 that comes
hypotheses. We assume that the sample X1 ; : : : ; Xn has distribution
from a set of probability distributions f 
(cid:18) ; (cid:18) 2 (cid:2)g. Given the sample, we would
like to decide whether unknown (cid:18)0 comes from the set (cid:2)1 or (cid:2)2 ; in which case our
hypotheses will be

H1 : (cid:18) 2 (cid:2)1 (cid:18) (cid:2)
H2 : (cid:18) 2 (cid:2)2 (cid:18) (cid:2):
Given some decision rule (cid:14) , let us consider a function

(cid:5)((cid:14); (cid:18)) =

(cid:18) ((cid:14) 6= H1 ) as a function of (cid:18) ;
which is called the power function of (cid:14): The power function has di(cid:11)erent meaning
depending on whether (cid:18) comes from (cid:2)1 or (cid:2)2 ; as can be seen in (cid:12)gure 20.2.
For (cid:18) 2 (cid:2)1 the power function represents the error of type 1; since (cid:18) actually
comes from the set in the (cid:12)rst hypothesis H1 and (cid:14) rejects H1 : If (cid:18) 2 (cid:2)2 then the
power function represents the power, or one minus error of type two, since in this
case (cid:18) belongs to a set from the second hypothesis H2 and (cid:14) accepts H2 : Therefore,
ideally, we would like to minimize the power function for all (cid:18) 2 (cid:2)1 and maximize it
for all (cid:18) 2 (cid:2)2 :
Consider

(cid:11)1 ((cid:14) ) = sup
(cid:18)2(cid:2)1

(cid:5)((cid:14); (cid:18)) = sup
(cid:18)2(cid:2)1 

(cid:18) ((cid:14) 6= H1 )





LECTURE 20.

80

P(d, q)

a

0

Q2
P
Maximize       in        Region

Q1

Q2

q

Q1
P
Minimize       in        Region

Figure 20.2: Power function.

which is called the size of (cid:14) and which represents the largest possible error of type 1.
As in the case of simple hypotheses it often makes sense to control this largest possible
error of type one by some level of signi(cid:12)cance (cid:11) 2 [0; 1] and to consider decision rules
from the class
K(cid:11) = f(cid:14) ; (cid:11)1 ((cid:14) ) (cid:20) (cid:11)g:
Then, of course, we would like to (cid:12)nd the decision rule in this class that also maximizes
the power function on the set (cid:2)2 ; i.e. miminizes the errors of type 2: In general, the
decision rules (cid:14) , (cid:14) 0 2 K(cid:11) may be incomparable, because in some regions of (cid:2)2 we might
have (cid:5)((cid:14); (cid:18)) > (cid:5)((cid:14) 0 ; (cid:18)) and in other regions (cid:5)((cid:14) 0 ; (cid:18)) > (cid:5)((cid:14); (cid:18)). Therefore, in general,
it may be impossible to maximize the power function for all (cid:18) 2 (cid:2)2 simultaneously.
But, as we will show in the next lecture, under certain conditions it may be possible
to (cid:12)nd the best test in class K(cid:11) that is called the uniformly most powerful test.
De(cid:12)nition. If we can (cid:12)nd (cid:14) 2 K(cid:11) such that
(cid:5)((cid:14); (cid:18)) (cid:21) (cid:5)((cid:14) 0 ; (cid:18)) for all (cid:18) 2 (cid:2)2 and all (cid:14) 0 2 K(cid:11)
then (cid:14) is called the Uniformly Most Powerful (UMP) test.

