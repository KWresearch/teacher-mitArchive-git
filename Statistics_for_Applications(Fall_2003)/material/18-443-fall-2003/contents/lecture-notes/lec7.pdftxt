Lecture  7


We  showed  that 

ν (S − m(χ))l ∈n  ∀ (  ν (S − m(χ))2 )1/2 (nI (χ))1/2 . 
� 
Next,  let us compute the left hand side.  We showed that 
ν l ∈ (X1 χ) = 0 which implies 
|
� 
that 
 

� 
ν S  l ∈n  − m(χ)  ν l ∈ = 
n

ν l ∈ (Xi χ) = 0 
|

and,  therefore, 

ν l ∈ = 
n 

νS  l ∈ .
n

ν (S − m(χ))l ∈ = 
n 
Let X  = (X1 , . . . , Xn )  and  denote  by 

� 

� 

� 

�(X χ) = f (X1 χ) . . . f (Xn χ)
|
|
|
the  joint  p.d.f.  (or  likelihood)  of  the  sample  X1 , . . . , Xn  We  can  rewrite  l ∈
n 
of  this  joint  p.d.f.  as 

in  terms 

l ∈ = 
n 

.

�
� χ 

ν S l ∈ = 
n 

ν S (X )
 

�∈ (X χ)
|
�(X |
χ)

log �(X χ) = 
|

 
log f (Xi χ) = 
|

n
�  
� χ 
i=1 
Therefore,  we  can  write 
 
�
�∈(X χ) 
�∈ (X |χ) 
| �(X )dX
=  S (X ) 
� 
�(X χ)

�(X χ) 
|

|
 
�  �
�
S (X )�∈(X |χ)dX  = 
S (X )�(X χ)dX  = 
|
� χ 
Of  course,  we  integrate  with  respect  to  all  coordinates,  i.e.  dX  =  dX1  . . . dXn .  We 
ﬁnally  proved  that 
m∈ (χ) ∀  (  ν (S − m(χ))2 )1/2 (nI (χ))1/2  = (Varν (S ))1/2 (nI (χ))1/2 
� 
28 

ν S (X ) = m∈ (χ).

� 
� 
� χ 

= 

�
�
�
�
LECTURE  7. 

29 

which  implies  Rao-Cr´amer  inequality. 

(m∈ (χ))2 
. 
nI (χ) 

Varν (S ) → 
The  inequality  will  become  equality  only  if  there  is  equality  in  the  Cauchy  in­
equality  applied  to  random  variables 
S − m(χ)  and  l ∈ .n
But  this  can  happen  only  if  there  exists  t = t(χ)  such  that 
n 

S − m(χ) = t(χ)l ∈ = t(χ) 
n 
i=1 

 
l ∈ (Xi χ).
|

7.1  Eﬃcient  estimators. 

Deﬁnition:  Consider  statistic  S  = S (X1 , . . . , Xn)  and  let 

m(χ) = 

� 

ν S (X1 , . . . , Xn). 

We  say  that  S  is  an  eﬃcient  estimate  of m(χ)  if 

(m∈ (χ))2 
,
nI (χ) 

� 

ν (S − m(χ))2  =
i.e.  equality  holds  in  Rao-Cr´amer’s  inequality. 
In  other words,  eﬃcient  estimate  S  is  the best possible  unbiased  estimate  of m(χ) 
in a sense that it achieves the smallest possible value for the average squared deviation 
ν (S − m(χ))2  for  all  χ .

� 
We  also  showed  that  equality  can  be  achieved  in Rao-Cr´
amer’s  inequality  only  if 
n 

S  = t(χ) 
i=1 
for some function t(χ). The statistic S 
n ) must a function of the sample 
= S (X1 , · · · , X
only  and  it  can  not  depend  on  χ .  This  means  that  eﬃcient  estimates  do  not  always 
exist  and  they  exist  only  if  we  can  represent  the  derivative  of  log-likelihood  l ∈ as
n 
n 

i=1 
where  S  does  not  depend  on  χ .  In  this  case,  S  is  an  eﬃcient  estimate  of m(χ).


 
l ∈ (Xi χ) + m(χ)
|

 
l ∈ (Xi |χ) = 

l ∈ = 
n 

S − m(χ) 
,
t(χ)

LECTURE  7. 

30 

Exponential-type  families  of  distributions.  Let  us  consider  the  special  case 
of  so  called  exponential-type  families  of  distributions  that  have  p.d.f.  or  p.f.  f (x χ)|
that  can  be  represented  as: 

f (x|χ) = a(χ)b(x)e c(ν)d(x) . 

In  this  case  we  have, 

l ∈ (x χ)  = 
|

=

� 
(log a(χ) + log b(x) + c(χ)d(x))
� χ 

�	
log f (x|χ) = 
� χ 

a∈ (χ)

+ c∈ (χ)d(x). 
a(χ) 

This  implies  that 

and 

. 

 
d(Xi ) 

n 

+ c∈ (χ) 
i=1 
 
l ∈ (Xi χ) − 
|

n	

i=1 
n	
 
1 
d(Xi) = 
n 
i=1	
n1 
S  = 
n	
i=1 
then  S  will  be  an  eﬃcient  estimate  of m(χ). 
Example.  Consider  a  family  of  Poisson  distributions  �(∂)  with  p.f. 

 
a∈ (χ) 
l ∈ (Xi |χ) = n
a(χ)
n
1  
nc∈ (χ) 
i=1 
d(Xi )  and m(χ) = 

a∈ (χ)
− 
a(χ)c∈ (χ)

a∈ (χ)
a(χ)c∈ (χ)

If  we  take 

� 

ν S  = 

∂x 
e−�  for  x = 0, 1, . . . 
f (x|∂) = 
x! 
This  can  be  expressed  as  exponential-type  distribution  if  we  write 
 
 
∂x 
1 
�
�
−�  =  e−� 
log ∂ x 
. 
exp	
e
 
  x! 
x! 
� ⎛� � �⎛��
�⎛��
 
c(�)  d(x) 
a(�)  �⎛��
b(x) 
 
 
n
n
1 
1 
¯
Xi  = X 
d(Xi ) = 
n 
n
i=1 
i=1 
is eﬃcient estimate of its expectation m(∂) =  �S  =  �X1  = ∂. We can also compute 
� 
its  expectation  directly  using  the  formula  above: 
−(−e−� ) 
e−� ( 1  = ∂. 
� ) 

a∈ (∂) 
�S  = − 
a(∂)c∈ (∂)

As  a  result, 

S  = 

= 

� 

	
�
LECTURE  7. 

31 

Maximum  likelihood  estimators.  Another  interesting  consequence  of  Rao-
Cr´amer’s  theorem  is  the  following.  Suppose  that  the MLE  χˆ is  unbiased: 

If  we  take  S  = χˆ and m(χ) = χ  then  Rao-Cr´amer’s  inequality  implies  that 

�  χˆ = χ . 

1 
. 
nI (χ) 

Var(χˆ) → 
On  the  other hand when we  showed  asymptotic  normality  of  the MLE we  proved  the 
following  convergence  in  distribution: 
 
1  �
∞n(χˆ − χ) ≈ N  0,
�
I (χ) 
In  particular,  the  variance  of  ∞n(χˆ −  χ)  converges  to  the  variance  of  the  normal 
distribution  1/I (χ),  i.e. 

. 

Var(∞n(χˆ − χ)) = nVar(χˆ) ≈ 
which means  that Rao-Cr´amer’s  inequality  becomes  equality  in  the  limit.  This  prop­
erty  is  called  the  asymptotic  eﬃciency  and  we  showed  that  unbiased  MLE  is  asymp­
totically  eﬃcient.  In  other  words,  for  large  sample  size  n  it  is  almost  best  possible. 

1 
I (χ) 

