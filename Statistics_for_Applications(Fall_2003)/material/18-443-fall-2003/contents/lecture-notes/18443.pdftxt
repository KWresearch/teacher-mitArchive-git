18.443.  Statistics  for  Applications.


by 

Dmitry  Panchenko 

Department  of  Mathematics 
Massachusetts  Institute  of  Technology 

Contents 


1  Estimation  theory. 
Introduction .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
1.1 

2

3

4 

5 

6 

7 

8 

9 

10 

2.1  Some  probability  distributions.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

3.1  Method  of moments.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

4.1  Maximum  likelihood  estimators. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
5.1  Consistency  of MLE.
5.2  Asymptotic  normality  of MLE.  Fisher  information.
.  .  .  .  .  .  .  .  .  . 

6.1  Rao-Cr´
amer  inequality.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

7.1  Eﬃcient  estimators. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

8.1  Gamma  distribution.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
8.2  Beta  distribution. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

9.1  Prior  and  posterior  distributions.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

10.1  Bayes  estimators.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
10.2  Conjugate  prior  distributions.

1

1


3

3


8

8


13

14


17

17

20


24

25 

28

29


32

32

33


35

35


38

38

39


ii 

11.1  Suﬃcient  statistic.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

42

42


45

12.1  Jointly  suﬃcient  statistics.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
46

12.2  Improving  estimators  using  suﬃcient  statistics.  Rao-Blackwell  theorem.  47


13.1  Minimal  jointly  suﬃcient  statistics.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
13.2  α2  distribution.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

14.1  Estimates  of  parameters  of  normal  distribution.

.  .  .  .  .  .  .  .  .  .  .  . 

15.1  Orthogonal  transformation  of  standard  normal  sample.

.  .  .  .  .  .  .  . 

16.1  Fisher  and  Student  distributions.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

11 

12 

13 

14 

15 

16 

17 

49

49

51


53

53


56

56


60

60


63

63


67

67

69


71

73


76

76

79


81

81

82


86

86


17.1  Conﬁdence  intervals  for  parameters  of  normal  distribution.

.  .  .  .  .  . 

18  Testing  hypotheses. 
18.1  Testing  simple  hypotheses.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
18.2  Bayes  decision  rules.

19 

20 

21 

22 

19.1  Most  powerful  test  for  two  simple  hypotheses.

.  .  .  .  .  .  .  .  .  .  .  .  . 

20.1  Randomized most  powerful  test. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
20.2  Composite  hypotheses.  Uniformly most  powerful  test. .  .  .  .  .  .  .  .  . 

21.1  Monotone  likelihood  ratio.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
21.2  One  sided  hypotheses.

22.1  One  sided  hypotheses  continued. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

iii


23 

24 

25 

23.1  Pearson’s  theorem.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

24.1  Goodness-of-ﬁt  test.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
24.2  Goodness-of-ﬁt  for  continuous  distribution. .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

25.1  Goodness-of-ﬁt  for  composite  hypotheses. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

89

89


94

94

96


99

99


26 

103

26.1  Test  of  independence. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  103


27 

28 

27.1  Test  of  homogeneity. 

107

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  107


28.1  Kolmogorov-Smirnov  test.

110

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  110


29  Simple  linear  regression. 
29.1  Method  of  least  squares.
29.2  Simple  linear  regression.

116

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  116

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  118


30 

30.1  Joint  distribution  of  the  estimates.

120

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  120


31 

124

31.1  Statistical  inference  in  simple  linear  regression. .  .  .  .  .  .  .  .  .  .  .  .  .  124


32 

32.1  Classiﬁcation  problem.

128

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  128


iv


List  of  Figures


2.1  Poisson Distribution 

. . . . . . . . . . . . . . . . . . . . . . . . . . . 

4.1  Maximum Likelihood Estimator (MLE) . . . . . . . . . . . . . . . . . 

5.1  Maximize over χ  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
5.2  Diagram (t − 1) vs. log t  . . . . . . . . . . . . . . . . . . . . . . . . . 
5.3  Lemma: L(χ) ∀ L(χ0 ) . . . . . . . . . . . . . . . . . . . . . . . . . . . 
9.1  Prior distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
9.2  Posterior distribution. 
. . . . . . . . . . . . . . . . . . . . . . . . . . 

14.1 Unit Vectors Transformation. 
. . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 
14.2 Unit Vectors Fact. 

15.1 Unit Vectors. 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

16.1 Cumulative Distribution Function. 
. . . . . . . . . . . . . . . . . . . 
17.1 P.d.f. of α2

n−1  distribution and ϕ  conﬁdence interval. . . . . . . . . . . 
17.2 tn−1  distribution.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
19.1 Bayes Decision Rule.  . . . . . . . . . . . . . . . . . . . . . . . . . . . 

20.1 Graph of F (c). 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
20.2 Power function.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

21.1 One sided hypotheses. 
. . . . . . . . . . . . . . . . . . . . . . . . . . 
21.2 Solving for T .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

23.1 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
23.2 Pro jections of θg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
23.3 Rotation of the coordinate system. 
. . . . . . . . . . . . . . . . . . . 

24.1 Distribution of T  under H1  and H2 . . . . . . . . . . . . . . . . . . . . 

5


15


18

19

20


36

37


54

55


58


61


64

65


72


78

80


82

84


89

93

93


95


v 

24.2  Discretizing continuous distribution. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
24.3  Total of 4 Sub-intervals.

97

97


25.1  Free parameters of a three point distribution.
.  .  .  .  .  .  .  .  .  .  .  .  .  100

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  102

25.2  Goodness-of-ﬁt for Composite Hypotheses.

28.1  C.d.f.  and empirical d.f.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  111

28.2  The case when F  = F0 . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  113

28.3  Fn  and F0  in the example.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  114


29.1  The least-squares line.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  116


31.1  Conﬁdence Interval. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  125


32.1  Example.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  131


vi


⇒
List  of  Tables


. . . . . . . . . . . . . . . . . . . . . . . . . . . .  103

26.1 Contingency table. 
26.2 Montana outlook poll. 
. . . . . . . . . . . . . . . . . . . . . . . . . .  106


27.1 Test of homogeneity 

. . . . . . . . . . . . . . . . . . . . . . . . . . .  107


vii


Lecture  1


Estimation  theory. 

1.1 

Introduction 

Let  us  consider  a  set  X  (probability  space)  which  is  the  set  of  possible  values  that 
some  random  variables  (random  ob ject)  may  take.  Usually  X  will  be  a  subset  of 
,
� 
for  example  {0, 1},  [0, 1],  [0, ≤), 
,  etc. 
� 
I.  Parametric  Statistics. 
We  will  start  by  considering  a  family  of  distributions  on  X : 
ν , χ  ⊆  �},  indexed  by  parameter  χ .  Here,  �  is  a  set  of  possible  parameters 
•  {and  probability 
 
ν  describes  chances  of  observing  values  from  subset  of X,  i.e. 
 
for A ∼ X , 
ν (A)  is  a  probability  to  observe  a  value  from A. 
 
•  Typical  ways  to  describe  a  distribution: 
–  probability  density  function  (p.d.f.), 
–  probability  function  (p.f.), 
–  cumulative  distribution  function  (c.d.f.). 

For example, if we denote by N (ϕ, δ 2 ) a normal distribution with mean ϕ and variance 
δ 2 ,  then  χ = (ϕ, δ 2)  is  a  parameter  for  this  family  and � =  × [0, ≤). 
� 
Next we will assume that we are given X  = (X1 , · · · , Xn) - independent  identically 
distributed  (i.i.d.)  random  variables  on X , drawn  according  to  some  distribution 
ν0 
 
from  the  above  family,  for  some  χ0  ⊆  �,  and  suppose  that  χ0  is  unknown.  In  this 
setting  we  will  study  the  following  questions. 

1.  Estimation  Theory. 
Based  on  the  observations  X1 , · · · , Xn  we  would  like  to  estimate  unknown  pa­
rameter  χ0 ,  i.e.  ﬁnd  χˆ = 
χˆ(X1 , · · · , X
n )  such  that  χˆ approximates  χ0 .  In  this 
case  we  also  want  to  understand  how  well  χˆ approximates  χ0 . 

1 

LECTURE  1.  ESTIMATION  THEORY. 

2 

2.  Hypothesis  Testing. 
Decide which of the hypotheses about χ0  are likely or unlikely.  Typical hypothe­
ses: 
•  χ0  = χ1?  for  some  particular  χn ? 
•  χ0  � χ1 
•  χ0  = 
⇒  χ1 
Example:  In  a  simple  yes/no  vote  (or  two  candidate  vote)  our  variable  (vote) 
can  take  two  values,  i.e.  we  can  take  the  space  X  =  {0, 1}.  Then  the  distribution  is 
described  by 
  (0) = 1 − p 
  (1) = p, 
for  some  parameter  p ⊆ � =  [0, 1]. The  true  parameter  p0  is  unknown.  If we  conduct 
a  poll  by  picking  n  people  randomly  and  if X1 , · · · , Xn  are  their  votes  then: 
1.Estimation  theory.  What  is  a  natural  estimate  of  p0? 
#(1∈s  among X1 , · · · , Xn ) 
n 

∩ p0 

pˆ = 

 

How  close  is  ˆp  to  p0? 
2.  Hypothesis  testing.  How  likely  or  unlikely  are  the  following: 
1 
•  Hypothesis  1:  p0  >  2 
1 
•  Hypothesis  2:  p0  <  2 
II.  Non-parametric  Statistics 
In  the  second  part  of  the  class  the  questions  that we  will  study  will  be  somewhat 
diﬀerent.  We will  still assume  that the observations X  = (X1 , · · · , Xn) have unknown 
,  but  we  won’t  assume  that 
comes  from  a  certain  parametric  family 
distribution 
 
 
ν , χ ⊆ �}.  Examples  of  questions  that may  be  asked  in  this  case  are  the  following: 
{
•  Does 
come  from  some  parametric  family  {  
ν , χ ⊆ �}? 
 
•  Is 
  = 
0  for  some  speciﬁc 
0? 
 
 
If  we  have  another  sample  X ∈  = (X ∈
m )  then, 
∈
1 , · · · , X
Do X  and X ∈  have  the  same  distribution?

•

If  we  have  paired  observations  (X1 , Y1 ), · · · , (Xn , Yn):

•  Are X  and  Y  independent  of  each  other? 
•  Classiﬁcation/regression  problem:  predict  Y  as  a  function  of X ;  i.e., 
Y  = f (X ) +  small  error  term  . 

Lecture  2


2.1  Some  probability  distributions. 

Let  us  recall  some  common  distributions  on  the  real  line  that  will  be  used  often  in 
this  class.  We  will  deal  with  two  types  of  distributions: 

1.  Discrete 

2.  Continuous 

 

Discrete  distributions. 
Suppose  that  a  set  X  consists  of  a  countable  or  ﬁnite  number  of  points, 
X  = {a1 , a2 , a3 , · · ·}
.
on X  can be deﬁned  via  a  function p(x)  on X  with 

Then  a probability distribution 
the  following  properties: 
1.  0 ∀ p(ai ) ∀ 1, 
2.  �∗ 
i=1 p(ai ) = 1. 
p(x)  is  called  the  probability  function.  If X  is  a  random  variable with  distribution 
then p(ai ) = 
  (ai ) is a probability that X  takes value ai .  Given a function � : X  ≈ 
the  expectation  of  �(X )  is  deﬁned  by 
 
∗

�(ai )p(ai ) 
i=1 
(Absolutely)  continuous  distributions. 
is  deﬁned  via  a  probability  density  function 
Continuous  distribution 
on 
� 
 
 
�
∗
such  that  p(X )  →  0  and 
(p.d.f.)  p(x)  on 
p(X )dx  = 1.  If  a  random  vari-
� 
−∗
able  X  has  distribution 
then  the  chance/probability  that  X  takes  a  value  in  the 
 

, 

 

� 

�(X ) = 

3


�
LECTURE  2. 

4 

interval  [a, b]  is  given  by 

  b 

p(x) = 

� 

�(X ) = 

p(x)dx. 

�(x)p(x)dx. 

�
  (X  ⊆  [a, b]) = 
a
Clearly,  in  this  case  for  any  a  ⊆ 
  (X  =  a)  =  0.  Given  a  function
we  have 
� 
�  : X  ≈ 
,  the  expectation  of  �(X )  is  deﬁned  by 
� 
 
∗ 
�
−∗ 
Notation.  The  fact that a random variable X  has distribution 
  will be denoted 
. 
by X  ∩ 
 
Example  1.  Normal (Gaussian) Distribution N (ϕ, δ 2 ) with mean ϕ and variance 
δ 2  is  a  continuous  distribution  on  with  probability  density  function: 
� 
1 
(x−�)2 
e− 
for  x ⊆ (−≤, ≤).
∞2νδ
2�2 
Normal  distribution  often  describes  continuous  random  variables  that  can  be  af­
fected  by  a  sum  of many  independent  factors,  for example,  person’s  height  or weight, 
ﬂuctuations  of  stock market,  etc.  In  this  case,  the  reason  for having  normal distribu­
tion  lies  in  the  Central  Limit  Theorem. 
Example  2.  Bernoulli  Distribution  B (p)  describes  a  random  variable  that  can 
=  {0, 1}.  The  distribution  is  described  by  a 
take  only  two  possible  values,  i.e. 
X
probability  function 
  (X  = 1) = p,  p(0) = 
  (X  = 0) = 1 − p  for  some  p ⊆  [0, 1]. 
Example  3.  Exponential  Distribution  E (ϕ)  is  a  continuous  distribution  with 
p.d.f. 
 
�
ϕe−ϕx  x 
0, 
→
x < 0. 
0 
Here,  ϕ > 0  is  the  parameter  of  the  distribution. 
This  distribution  has  the  following  nice  property.  If  a  random  variable X  ∩ E (ϕ) 
then  probability  that X  exceeds  level  t  for  some  t > 0  is 
 
∗ 
�
  (X  ⊆  [t, ≤)) = 
  (X  → t) = 
t 
For  s >  0,  the  probability  that  X  will  exceed  level  t + s  given  that  it  exceeded  level 
t  can  be  computed  as  follows: 

ϕe−ϕxdx = e−ϕt . 

p(x) = 

p(1) = 

  (X t + s X 
→ 
| → 

t) = 

→ 
  (X t + s, X 
→
  (X t) 
→
=  e−ϕ(t+s) /e−ϕt  = e−ϕs  = 

t) 

  (X t + s)
→
= 
→ 
  (X t)
  (X s),→ 

LECTURE  2. 

i.e. 

5 

t) = 

  (X t + s X 
→ 
| → 
→
  (X s).
In  other  words,  if  we  think  of  X  as  a  lifetime  of  some  ob ject  in  some  random  con­
ditions,  then  this  property  means  that  the  chance  that  X  will  live  longer  then  t + s 
given  that  it  lived  longer  than  t  is  the  same  as  the  chance  that  X  lives  longer  than 
t  in  the  ﬁrst  place.  Or,  if  X  is  “alive”  at  time  t  then  it  is  like  new,  so  to  speak. 
Therefore,  some  natural  examples  that  can  be  decribed  by  exponential  distribution 
are  the  life  span  of  high  quality  products,  or  soldiers  at  war. 
Example  4.  Poisson  Distribution  �(∂)  is  a  discrete  distribution  with 

p(k) = 

= {0, 1, 2, 3, . . .},
X 
∂k 
e−�  for  k = 0, 1, 2, , . . . 
  (X  = k) = 
k ! 
Poisson  distribution  could  be  used  to  describe  the  following  random  ob jects:  the 
number  of  stars  in  a  random  area  of  the  space;  number  of misprints  in  a  typed  page; 
number of wrong connections  to your phone number; distribution of bacteria on some 
surface  or  weed  in  the  ﬁeld.  All  these  examples  share  some  common  properties  that 
give rise to a Poisson distribution.  Suppose that we count a number of random ob jects 
in  a  certain  region  T  and  this  counting  process  has  the  following  properties: 
1.  Average number  of ob jects  in any  region S  ∼ T  is proportional  to the  size  of S ,
i.e.  Count(S ) = ∂ S . Here  S denotes  the  size  of S,  i.e.  length, area, volume, 
|
|
|
|
� 
etc.  Parameter  ∂ > 0  represents  the  intensity  of  the  process. 

2.  Counts  on  disjoint  regions  are  independent. 

3.  Chance  to  observe  more  than  one  ob ject  in  a  small  region  is  very  small,  i.e. 
  (Count(S ) → 2)  becomes  small  when  the  size  S gets  small. 
| 
|
We will  show  that  these  assumptions  will  imply  that  the number  of ob jects  in  the 
region  T ,  Count(T ),  has  Poisson  distribution  �(∂ T )  with  parameter  ∂ T .
|
|
|
|

0 

T/n 

X1 

X2 

. . . . . . . 

T 
− Counts on small subintervals 

Xn 

Figure  2.1:  Poisson  Distribution 

For simplicity,  let us assume  that the region T  is an interval  [0, T ] of length T . Let 
us  split  this  interval  into  a  large  number  n  of  small  equal  subintervals  of  length  T /n 

LECTURE  2. 

6 

and  denote  by  Xi  the  number  of  random  ob jects  in  the  ith  subinterval,  i = 1, . . . , n. 
By  the  ﬁrst  property  above, 

� 

Xi  = 

. 

p(x) =	

� 

Xi  = 

we  can  write, 

∂T 
n 
On  the  other  hand,  by  deﬁnition  of  expectation 
 

  (Xi  = k) = 0 + 
  (Xi  = 1) + πn , 
k
k�0 
 
where  πn  =  �
  (Xi  =  k),  and  by  the  last  property  above  we  assume  that  πn 
k�2 k
becomes  small  with  n,  since  the  probability  to  observe  more  that  two  ob jects  on  the 
interval  of  size  T /n  becomes  small  as  n  becomes  large.  Combining  two  equations 
  (Xi  =  1)  ∅  ∂ T  .  Also,  since  by  the  last  property  the  probability  that 
above  gives, 
n 
any  count  Xi  is  → 2  is  small,  i.e. 
 T �
 
�
  (at  least  one Xi  → 2) ∀ no 
n 
 
 ∂T �k�
n ��
⎜
n 
k
(∂T )k 
e−�T
k ! 

≈ 
Example  5:  Uniform  Distribution  U [0, χ ]  has  probability  density  function 
 
�
Finally,  let  us  recall  some  properties  of  normal  distribution.  If  a  random  variable 
X  has  normal  distribution  N (ϕ, δ 2 )  then  the  r.v. 
X − ϕ 
δ 

  (Count(T ) = X1  +

1 
x ⊆  [0, χ ], 
ν ,
0,  otherwise. 

≈ 0  as  n ≈ ≤, 

Y  = 

∩ N (0, 1)
has  standard  normal  distribution.  To  see  this,  we  can  write, 
  bπ+ϕ 
 X − ϕ 
  (X  ⊆  [aδ + ϕ, bδ + ϕ])  =  �
1 
  �
�
[a, b] = 
aπ+ϕ  ∞2νδ
δ  ⊆
  b 
�
1 
2 y
e−  dy , 
= 
a  ∞2ν
2

∂T �n−k 
n 

(x−�)2 
e− 
2�2  dx 

+ Xn  = k)	

· · ·

∅ 

 
1 −

	
LECTURE  2. 

7 

� 

1 = 

−∗ 

where  in  the  last  integral  we  made  a  change  of  variables  y  = (x − ϕ)/δ.  This,  of 
course,  means  that  Y  ∩ N (0, 1).  The  expectation  of  Y  is 
 
∗ 
�
1 
2
y 
e−  dy = 0 
Y  = 
y ∞2ν
2
−∗ 
since  we  integrate  odd  function.  To  compute  the  second  moment  Y 2 ,  let  us  ﬁrst 
� 
2 
2α e− y 
is  a  probability  density  function,  it  integrates  to  1,  i.e. 
note  that  since  ≥1
2
 
�
If  we  integrate  this  by  parts,  we  get, 
 
 
�
�
∗  1 
���
∗ 
1
−∗  − 
dy =  ∞
1 = 
−∗  ∞2ν
2ν
 
=  0 + �
∗ 
1 
2
e− y 
y 2 ∞2ν
. 
dy =  � 
2
−∗ 
Thus,  the  second  moment  Y 2  = 1.  The  variance  of  Y  is 
� 
Var(Y ) =  Y 2  − (  Y )2  = 1 − 0 = 1. 
� 

y
2 
y
(−y )e−  dy 
∞
2
2ν 

∗  1 
∞2ν

2 
ye− y 
2

2
y 
e− 
2

dy . 

2

y
e− 
2

 

∗ 

−∗ 

Y 2

�
Lecture  3


3.1  Method  of  moments. 
Consider  a  family  of  distributions  {  
ν  :  χ  ⊆  �}  and  and  consider  a  sample  X  = 
ν0 ,  where  χ0  ⊆  �.  We 
(X1 , . . . , Xn )  of  i.i.d.  random  variables  with  distribution 
 
assume  that χ0  is unknown  and we want  to construct  an estimate  χˆ = χˆ
n (X1 , · · · , X
n )
of  χ0  based  on  the  sample X. 
Let us  recall  some  standard  facts  from probability  that we be  often used  through­
out  this  course. 
•  Law  of  Large  Numbers  (LLN): 
If  the  distribution  of  the  i.i.d.  sample  X1 , . . . , Xn  is  such  that  X1  has  ﬁnite 
expectation,  i.e.  X1 < ≤,  then  the  sample  average 
| 
|
� 
X1  + . . . + Xn 
¯
≈ 
Xn  =	
X1 
� 
n 
converges  to  the  expectation  in  some  sense,  for  example,  for  any  arbitrarily 
small  π > 0, 

¯
  ( Xn  −  X1 > ε) ≈ 0  as  n ≈ ≤.
| 
|
Convergence  in  the  above  sense  is  called  convergence  in  probability. 
Note.  Whenever  we  will  use  the  LLN  below  we  will  simply  say  that  the  av­
erage  converges  to  the  expectation  and  will  not  mention  in  what  sense.  More 
mathematically  inclined  students  are  welcome  to  carry  out  these  steps  more 
rigorously,  especially  when  we  use  LLN  in  combination  with  the  Central  Limit 
Theorem. 
•  Central  Limit  Theorem  (CLT): 
If  the  distribution  of  the  i.i.d.  sample  X1 , . . . , Xn  is  such  that  X1  has  ﬁnite 
expectation  and  variance,  i.e.	 |  X1
| < ≤  and  Var(X ) < ≤,  then

� 
∞n(Xn  −  X1 ) ≈ d  N (0, δ 2 ) 
¯

� 
8 

�
LECTURE  3. 

9 

 

b 

a

2 
x 
e− 
2�2  dx. 

1 
∞2νδ

converges  in  distribution  to  normal  distribution  with  zero  mean  and  variance 
2 ,  which  means  that  for  any  interval  [a, b], 
δ
 
≈ �
  �∞n(Xn  −  X1 ) ⊆  [a, b] �
¯
� 
Motivating  example.  Consider  a  family  of  normal  distributions 
{N (ϕ, δ 2 )  : ϕ ⊆ 
.→ 0}
� 
Consider  a  sample  X1 , . . . , Xn  ∩  N (ϕ0 , δ0
2 )  with  distribution  from  this  family  and 
suppose  that  the  parameters  ϕ0 , δ0  are  unknown.  If  we  want  to  estimate  these  pa­
rameters based on the sample  then the  law of  large numbers  above provides a natural 
way  to  do  this.  Namely,  LLN  tells  us  that 
≈ 

as  n ≈ ≤ 

¯
ˆϕ = Xn 

X1  = ϕ0

, δ 2 

and,  similarly, 

δ 2ˆ = 

+ X 2 
n

2  + . . . + X 2 
X1
n  ≈  X 2  = Var(X ) +  X 2  = δ0
2  + ϕ2 
0 .
1 
n 
These  two  facts  imply  that 
 X1  + 
2  + 
+ Xn �2 
· · ·
· · ·
− �
X1
≈  �  X 2  − ( � 
n 
n 
δ 2  as  the  estimates  of  unknown  ϕ0 , δ 2  since 
It,  therefore,  makes  sense  to  take  ˆ
ϕ  and  ˆ
0
by  the  LLN  for  large  sample  size  n  these  estimates  will  approach  the  unknown  pa­
rameters. 
We  can  generalize  this  example  as  follows. 
Suppose  that  the  parameter  set  � ∼ 
and  suppose  that  we  can  ﬁnd  a  function 
� 
g  : X  ≈ 
such  that  a  function 
� 

X )2  = δ0
2 . 

m(χ) = 

� 

ν g (X ) : � ≈ Im(�) 
ν  denotes  the  expectation  with  respect  to  the 

� 

has  a  continuous  inverse  m−1 .  Here 
ν .  Take 
distribution 

 

 g (X1  + · · ·
+ g (Xn) �
g) = m−1�
ˆ
χ = m−1 (¯
n 
as the estimate of χ0 .  (Here we implicitely assumed that ¯g  is always in the set Im(m).) 
Since  the  sample  comes  from  distribution  with  parameter  χ0 ,  by  LLN  we  have 
g¯ ≈ 

ν0 g (X1 ) = m(χ0 ). 

� 

�
�
�
LECTURE  3. 

10 

Since  the  inverse  m−1  is  continuous,  this  implies  that  our  estimate 
χˆ = m−1 (¯g) ≈ m−1 (m(χ0 )) = χ0 
converges  to  the  unkown  parameter  χ0 . 
Typical  choices  of  the  function  g  are  g (x) = x  or  x2 .  The  quantity  X k  is  called 
� 
the  k th  moment  of X  and,  hence,  the  name  - method  of  moments. 
Example:  Family  of  exponential  distributions  E (ϕ)  with  p.d.f. 
 
�

ϕe−ϕx , x 
0, 
→
x < 0

0, 

p(x) = 

Take  g (x) = x.  Then


m(ϕ) =  ϕ g (X ) =  ϕX  = 
� 

1

. 
ϕ 

( 1  is  the  expectation  of  exponential  distribution,  see  Pset  1.)  Let  us  recall  that  we 
ϕ 
can  ﬁnd  inverse  by  solving  for  ϕ  the  equation 

We  have, 

Therefore,  we  take 

1 
m(ϕ) = λ ,  i.e.  in  our  case  = λ . 
ϕ 

ϕ = m−1 (λ ) = 

1 
. 
λ 

1
¯
ϕ = m−1 (¯
g) = m−1 (X ) =  ¯X 
ˆ

as  the  estimate  of  unkown  ϕ0 . 
Take  g (x) = x2 .  Then 

m(ϕ) =  ϕg (X 2 ) =  ϕX 2  = 
� 

2 
. 
ϕ2 

The  inverse  is 

and  we  take 

 
2 
λ 

ϕ = m−1 (λ ) = 

�
¯
g) = m−1 (X 2 ) = 
ϕ = m−1 (¯
ˆ

as  another  estimate  of  ϕ0 . 
The  question  is,  which  estimate  is  better? 

 

2
X¯2 

�

�
�
LECTURE  3. 

11 

1.	 Consistency.  We  say  that  an  estimate  χˆ is  consistent  if  χˆ ≈ χ0  in  probability 
..  We have  shown above that by construction  the estimate by method 
as n ≈ ≤
of moments  is  always  consistent. 

2.  Asymptotic  Normality.  We  say  that  χˆ is  asymptotically  normal  if 
∞n(χˆ − χ0 ) ≈d  N (0, δ 2  )ν0 
where  δ 2  �  is  called  the  asymptotic  variance  of  the  estimate  χˆ.
ν0 
Theorem.  The  estimate  χˆ = m−1 (¯g)  by  the method  of moments  is  asymptotical ly 
normal  with  asymptotic  variance 

V arν0 (g ) 
(m∈ (χ0 ))2 . 
Proof.  Writing  Taylor  expansion  of  the  function m−1  at  point m(χ0 )  we  have 

δ 2 
ν0 

= 

(m−1 )∈∈ (c)
2! 

g − m(χ0 ))2 
(¯

g) = m−1 (m(χ0 )) + (m−1 )∈ (m(χ0 ))(¯
m−1 (¯	
g − m(χ0 )) + 
where  c ⊆  [m(χ0 ), g¯].  Since  m−1 (m(χ0 )) = χ0 ,  we  get 
(m−1 )∈∈ (c)
g − m(χ0 )2 
g) − χ0  = (m−1 )∈ (m(χ0 ))(¯
m−1 (¯	
g − m(χ0 ) + 
)(¯
2! 
Let  us  prove  that  the  left  hand  side  multiplied  by  ∞n  converges  in  distribution  to 
normal  distribution. 
(m−1 )∈∈ (c)  1
∞n(m−1 (¯
g) − χ0 ) = (m−1 )∈ (m(χ0 )) ∞n(¯	
(∞n(¯
g − m(χ0 )))2
g − m(χ0 )) +
2!  ∞n  �
 
 
 
 
 
 
�
⎛�
�
�
⎛�
(3.1) 
Let  us  recall  that 
g (X1 ) + · · ·
+ g (Xn) 
, g (X1) = m(χ0 ). 
n 
Central  limit  theorem  tells  us  that 
∞n(¯g − m(χ0 ) ≈ N (0, Varν0 (g (X1 ))) 
where  convergence  is  in  distribution.  First  of  all,  this  means  that  the  last  term  in 
(3.1)  converges  to  0  (in  probability),  since  it  has  another  factor  of  1/∞n.  Also,  since 
from  calculus  the  derivative  of  the  inverse 

g¯ =	

(m−1 )∈ (m(χ0 )) = 

1
m∈ (m−1 (m(χ0 ))) 

= 

1 
, 
m∈ (χ0 ) 

�
LECTURE  3. 

12 

the  ﬁrst  term  in  (3.1)  converges  in  distribution  to 

1 
m∈ (χ0 )

 

(m−1 )∈ (m(χ0 ))∞n(m−1 (¯
g) − χ0 ) ≈ 

  V arν0 (g (X1 )) �
�
N (0, Varν0 (g (X1 ))) = N  0,
(m∈ (χ0 ))2 
What  this  result  tells  us  is  that  the  smaller  V ar�0 (g) 
is  the  better  is  the  estimate 
m� (ν0 ) 
χˆ in  the  sense  that  it  has  smaller  deviations  from  the  unknown  parameter  χ0  asymp­
totically. 

Lecture  4


Let  us  go  back  to  the  example  of  exponential  distribution  E (ϕ)  from  the  last  lecture 
and  recall  that  we  obtained  two  estimates  of  unknown  parameter  ϕ0  using  the  ﬁrst 
and  second  moment  in  the method  of moments.  We  had: 

1.  Estimate  of  ϕ0  using  ﬁrst moment: 

g (X ) = X,  m(ϕ) =  ϕ g (X ) = 
� 

1
ϕ

1 
, ϕ1  = m−1 (¯
g ) =  ¯ . 
ˆ
X 

2.  Estimate  of  ϕ  using  second  moment: 

 

g (X ) = X 2 , m(ϕ) =  ϕg (X 2 ) = 
� 

2
ϕ2 , ϕ2  = m−1 (¯
g) = 
ˆ

�
How  to  decide  which  method  is  better?  The  asymptotic  normality  result  states: 
  Varν0 (g (X )) �
 
∞n(m−1 (¯
�
g) − χ0 ) ≈ N  0,
.
(m∈ (χ0 ))2 
It  makes  sense  to  compare  two  estimates  by  comparing  their  asymptotic  variance. 
Let  us  compute  it  in  both  cases: 
1.  In  the  ﬁrst  case: 

2 
¯ . 
X 2 

= 

1 
Varϕ0 (g (X ))  Varϕ0 (X ) 
ϕ2 
0  = ϕ2 
0 . 
=
1 
1 
(m∈ (ϕ0 ))2 
(− ϕ
(− ϕ
2 )2 
2 )2
0 
0
In  the  second  case we will need  to  compute  the  fourth moment  of  the  exponential 
distribution.  This  can  be  easily  done  by  integration  by  parts  but  we  will  show  a 
diﬀerent  way  to  do  this. 
The  moment  generating  function  of  the  distribution  E (ϕ)  is: 
 
∗ 
�(t) =  ϕe tX  = �
= 
� 
k=0 

ϕ 
ϕ − t 

e txϕe−ϕxdx = 

tk 
ϕk , 

 

∗ 

0 

13 

LECTURE  4. 

14 

where  in  the  last  step  we  wrote  the  usual  Taylor  series.  On  the  other  hand,  writing 
the  Taylor  series  for  etX  we  can  write, 
∗ 
∗
�(t) =  ϕe tX  =  ϕ 

� 
k=0 
k=0 
Comparing  the  two  series  above  we  get  that  the  k th  moment  of  exponential  distribu­
tion  is 

 (tX )k 
k ! 

 tk 
� 
k ! 

ϕX k . 

= 

2.  In  the  second  case: 

� 

ϕX k  = 

k ! 
. 
ϕk 

4! 
2 
2 )2 
ϕ4  − ( ϕ
ϕ0 X 4  − (  X 2 )2 
Varϕ0 (g (X ))  Varϕ0 (X 2 ) 
� 
ϕ0 
0 
0
=
4 
4 
4 
(m∈ (ϕ0 ))2 
3 )2 
3 )2 
3 )2 
(− ϕ
(− ϕ
(− ϕ
0 
0
0
Since  the asymptotic variance  in the ﬁrst case  is  less  than the asymptotic variance 
in  the  second  case,  the  ﬁrst  estimator  seems  to  be  better. 

5
ϕ2 
4  0 

= 

= 

=

� 

4.1  Maximum  likelihood  estimators. 

(Textbook,  Section  6.5) 
As  always  we  consider  a  parametric  family  of  distributions  {  
ν , χ  ⊆  �}.  Let 
f (X χ)  be  either  a  probability  function  or  a  probability  density  function  of  the  dis­
|
tribution 
ν .  Suppose  we  are  given  a  sample  X1 , . . . , Xn  with  unknown  distribution 
 
ν ,  i.e.  χ  is  unknown.  Let  us  consider  a  likelihood  function 

�(χ) = f (X1 χ) × . . . × f (Xn χ)
|
|
seen as a function of the parameter χ  only.  It has a clear  interpretation.  For example, 
if  our  distributions  are  discrete  then  the  probability  function 

 

f (x χ) = 
|
 
is  a  probability  to  observe  a  point  x  and  the  likelihood  function 

ν (X  = x)

�(χ) = f (X1 χ) × . . . × f (Xn χ) = 
ν (X1 ) × . . . × 
|
|
 
is  the  probability  to  observe  the  sample  X1 , . . . , Xn . 
In the continuous case  the likelihood  function �(χ) is the probability density  func­
tion  of  the  vector  (X1 , · · · , Xn ). 

ν (X1 , · · · , X
n )

ν (Xn ) = 

 

 

�
LECTURE  4. 

15 

Deﬁnition:  (Maximum  Likelihood  Estimator.)  Let  χˆ be  the  parameter  that 
maximizes  �(χ),  i.e. 

�(χˆ) = max �(χ). 
ν 
Then  χˆ is  called  the maximum  likelihood  estimator  (MLE). 
To  make  our  discussion  as  simple  as  possible,  let  us  assume  that  the  likelihood 
function  behaves  like  shown  on  the  ﬁgure  4.1,  i.e.  the  maximum  is  achieved  at  the 
unique  point  ˆχ . 

ϕ(θ) 

X1, ..., Xn 

Max. Pt. 

Θ <− θ 

Distribution Range

^θ 

Best Estimator Here (at max. of fn.) 

Figure  4.1:  Maximum  Likelihood  Estimator  (MLE) 

When ﬁnding the MLE it sometimes easier to maximize the log-likelihood function 
since 

 
log f (Xi χ).
|

�(χ) ≈  maximize  ∈ log �(χ) ≈  maximize 
maximizing  �  is  equivalent  to maximizing  log �. Log-likelihood  function  can be writ­
ten  as 
n 

i=1 
Let  us  give  several  examples  of MLE. 
Example  1.  Bernoulli  distribution  B (p). 
X 
= {0, 1}, 
  (X  = 0) = 1 − p,  p ⊆  [0, 1].
  (X  = 1) = p, 
Probability  function  in  this  case  is  given  by 
 
�
x = 1, 
p, 
1 − p,  x = 0. 
Likelihood  function 
�(p) =  f (X1 p)f (X2 p) . . . f (Xn p)

| X1 +

|
|
1� s (1 − p)#  of  = p 
0� s 
#  of 
+Xn )
···+Xn (1 − p)n−(X1+···
=  p 

log �(χ) = 

f (x p) = 
|

LECTURE  4. 

16 

and  the  log-likelihood  function 
+ Xn ) log p + (n − (X1  +
log �(p) = (X1  +
+ Xn )) log(1 − p).
· · ·
· · ·
To maximize  this  over  p  let  us  ﬁnd  the  critical  point  d log 
�(p)  = 0, 
dp
1 
1
p − (n − (X1  +
+ Xn ) 
1 − p 

· · ·
Solving  this  for  p  gives,


+ Xn ))

(X1  +

= 0.

· · ·

X

1  +

p = 

· · ·
n 

+ Xn


¯
= X 

 

.


. 

(Xi−�)2 
e− 
2�2 

1 
∞2νδ

likelihood  function 

¯
and  therefore  ˆp = X  is  the MLE.

Example  2.  Normal  distribution  N (ϕ, δ 2 )  has  p.d.f.

1 
(X−�)2

e− 
f (X |(ϕ, δ 2)) =
 ∞2νδ
2�2 
 
n 
�(ϕ, δ 2 ) = ⎭
i=1 
and  log-likelihood  function 
 
n 
 
(X − ϕ)2 �
1 
�
log �(ϕ, δ 2 ) =  
log ∞2ν  − log δ − 
2δ 2 
i=1 
 
n
1  
1
(Xi  − ϕ)2 . 
=  n log ∞2ν  − n log δ − 
2δ 2 
i=1 
We  want  to  maximize  the  log-likelihood  with  respect  to  ϕ  and  δ 2 .  First,  obviously, 
for  any  δ  we  need  to minimize  �(Xi  − ϕ)2  over  ϕ.  The  critical  point  condition  is 
 
 
d  
(Xi  − ϕ)2  = −2 
(Xi  − ϕ) = 0. 
dϕ 
¯
Solving  this  for  ϕ  gives  that  ˆϕ = X . Next,  we  need  to maximize 
 
n
1  
1
(Xi  − X )2 
¯
n log ∞2ν  − n log δ − 
2δ 2 
i=1 
over  δ.  The  critical  point  condition  reads, 
 
1  
n 
(Xi  − X )2  = 0 
¯
− δ
δ 3 
and  solving  this  for  δ  we  get 
 
n 
1 
(Xi  − X )2 
δ 2 
¯
ˆ = 
n 
i=1 

+

is  the MLE  of  δ 2 . 

Lecture  5


Let  us  give  one more  example  of MLE. 
Example  3.  The  uniform  distribution  U [0, χ ]  on  the  interval  [0, χ ]  has  p.d.f. 
 
�

1 
ν ,  0 ∀ x ∀ χ , 
0,  otherwise 

f (x χ) = 
|

The  likelihood  function 

�(χ) = 

 
f (Xi χ) = 
|

n 
⎭
i=1 

I (X1 , . . . , Xn  ⊆  [0, χ ])

1 
n
χ
1 
nχ

= 

I (max(X1 , . . . , Xn) ∀ χ). 
Here  the  indicator  function  I (A)  equals  to  1  if A  happens  and  0  otherwise.  What we 
wrote  is  that  the  product  of  p.d.f.  f (Xi χ)  will  be  equal  to  0  if  at  least  one  of  the 
|
factors  is  0  and  this  will  happen  if  at  least  one  of Xi s  will  fall  outside  of  the  interval 
[0, χ ]  which  is  the  same  as  the maximum  among  them  exceeds  χ .  In  other  words, 

�(χ) = 0  if  χ < max(X1 , . . . , Xn), 

and 

1 
if  χ → max(X1 , . . . , Xn). 
nχ
Therefore,  looking  at  the  ﬁgure  5.1  we  see  that  χˆ = max(X1 , . . . , Xn )  is  the MLE. 

�(χ) = 

5.1  Consistency  of  MLE. 

Why  the  MLE  χˆ converges  to  the  unkown  parameter  χ0 ?  This  is  not  immediately 
obvious  and  in  this  section  we  will  give  a  sketch  of  why  this  happens. 

17 

LECTURE  5. 

18 

ϕ(θ) 

θ 

max(X1, ..., Xn) 

Figure  5.1:  Maximize  over  χ 

 
log f (Xi χ) 
|

First  of  all, MLE  χˆ is  a maximizer  of 
n1 
n 
i=1 
which  is  just  a  log-likelihood  function normalized by  1  (of course,  this does not aﬀect 
n 
the  maximization).  Ln (χ)  depends  on  data.  Let  us  consider  a  function  l(X χ) =|
log f (X χ)  and  deﬁne 
|
L(χ) = 
ν0 l(X |χ), 
where  we  recall  that  χ0  is  the  true  uknown  parameter  of  the  sample  X1 , . . . , Xn .  By 
the  law  of  large  numbers,  for  any  χ , 
ν0 l(X χ) = L(χ).
Ln (χ) ≈ 
|
Note  that  L(χ)  does  not  depend  on  the  sample,  it  only  depends  on  χ .  We  will  need 
the  following 
Lemma.  We  have,  for  any  χ , 

Lnχ = 

� 

L(χ) ∀ L(χ0 ). 
Moreover,  the  inequality  is  strict  L(χ) < L(χ0 )  unless 

which  means  that 

 
ν0 . 

ν  = 

 

 

ν0 (f (X χ) = f (X χ0 )) = 1.
|
|

�
LECTURE  5. 

19 

Proof.  Let  us  consider  the  diﬀerence 


L(χ) − L(χ0 ) = 

ν0 (log f (X |χ) − log f (X |χ0 )) = 

� 

ν0  log 

f (X χ)

|
. 
χ0 ) 
f (X |

t−1 

log t 

t 

0 

1

� 

ν0  log 

Figure  5.2:  Diagram  (t − 1)  vs.  log t 
Since  (t − 1)  is  an  upper  bound  on  log t  (see  ﬁgure  5.2)  we  can  write 
  � � f (x χ) 
 
� f (X χ) 
|
|
�
�
χ0 ) − 1 
χ0 ) − 1 = 
f (x χ0 )dx 
|
� 
ν0 
f (X |
f (x|
 
 
�
�
f (x χ)dx − 
f (x χ0 )dx = 1 − 1 = 0.
|
|
Both  integrals  are  equal  to 1 because we  are  integrating  the probability  density  func­
tions.  This  proves  that  L(χ) − L(χ0 )  ∀  0.  The  second  statement  of  Lemma  is  also 
clear. 

f (X χ)
|
χ0 )  ∀ 
f (X |
= 

We  will  use  this  Lemma  to  sketch  the  consistency  of  the MLE. 
Theorem:  Under  some  regularity  conditions  on  the  family  of  distributions, MLE 
ˆχ  is  consistent,  i.e.  χˆ ≈ χ0  as  n ≈ ≤. 
The  statement  of  this  Theorem  is  not  very  precise  but  but  rather  than  proving  a 
rigorous  mathematical  statement  our  goal  here  to  illustrate  the  main  idea.  Mathe­
matically  inclined  students  are  welcome  to  come  up  with  some  precise  statement. 
Proof. 

�
LECTURE  5. 

20 

We  have  the  following  facts: 
1.  χˆ is  the maximizer  of  Ln (χ)  (by  deﬁnition). 
2.  χ0  is  the maximizer  of  L(χ)  (by  Lemma). 
3.  ≡χ  we  have  Ln (χ) ≈ L(χ)  by  LLN. 
This  situation  is  illustrated  in ﬁgure  5.3.  Therefore,  since  two  functions Ln  and L 
are getting  closer,  the points  of maximum  should  also get  closer which  exactly means 
that  χˆ ≈ χ0 . 

Ln(θ) 

θL(  ) 

θ 

^θ 
MLE 

θ0 

Figure  5.3:  Lemma:  L(χ) ∀ L(χ0 ) 

5.2	 Asymptotic  normality  of  MLE.  Fisher  infor­
mation. 

We  want  to  show  the  asymptotic  normality  of MLE,  i.e.  that 
∞n(χˆ − χ0 ) ≈d  N (0, δ 2 
M LE )  for  some  δ 2 
M LE . 
Let  us  recall  that  above  we  deﬁned  the  function  l(X χ)  =  log f (X χ).  To  simplify 
|
|
the  notations  we  will  denote  by  l ∈ (X χ), l ∈∈ (X χ),  etc.  the  derivatives  of  l(X χ)  with 
|	
|
|
respect  to  χ . 

LECTURE  5. 

21 

 

Deﬁnition.  (Fisher  information.)  Fisher  Information  of  a  random  variable  X 
with  distribution 
ν  : χ ⊆ �}  is  deﬁned  by 
ν0  from  the  family  {  
� � 
�2
ν0 (l ∈ (X |χ0 ))2 
I (χ0 ) = 
log f (X χ0 ) 
� 
|
ν0 
� χ 
Next  lemma  gives  another  often  convenient  way  to  compute  Fisher  information. 
Lemma.  We  have, 

.

� 

ν0 l ∈∈ (X χ0 ) � 
|

� 

� 2 
ν0  � χ2  log f (X χ0 ) = −I (χ0 ).
|

Proof.  First  of  all,  we  have 

l ∈ (X |χ) = (log f (X |χ))∈  = 

f ∈ (X χ)
|
f (X |
χ) 

and 

(log f (X χ))∈∈  = 
|
Also,  since  p.d.f.  integrates  to  1, 

f ∈∈ (X χ)
|
χ)  − 
f (X |

(f ∈ (X |χ))2 
.
f 2 (X χ)|

 

�
f (x χ)dx = 1,
|
if  we  take  derivatives  of  this  equation  with  respect  to  χ  (and  interchange  derivative 
and  integral,  which  can  usually  be  done)  we  will  get, 
 
 
�
�
�
� 2 
� 
� χ2 f (x|χ)dx = 
f (x χ)dx = 0  and 
|
� χ 
To  ﬁnish  the  proof  we  write  the  following  computation 
 
�
� 2 
(log f (x χ0 ))∈∈f (x χ0 )dx
ν0  � χ2  log f (X χ0 ) = 
|
|
|
� 
 
� � f ∈∈ (x|χ0 )  � f ∈ (x χ0 ) �2�
|
f (x|χ0 )  − 
f (x χ0 )dx
|
χ0 ) 
f (x
|
 
�
ν0 (l ∈ (X χ0 ))2  = 0 − I (χ0  = −I (χ0 ).
f ∈∈ (x χ0 )dx − 
|
|
� 

ν0 l ∈∈ (X |χ0 ) = 
= 

f ∈∈ (x χ)dx = 0. 
|

= 

 

We  are  now  ready  to  prove  the main  result  of  this  section. 

�
�
�
LECTURE  5. 

22 

Theorem.  (Asymptotic  normality  of MLE.) We  have,

 
1  �
∞n(χˆ − χ0 ) ≈ N  0,
�
I (χ0 ) 
�n
Proof.  Since MLE  χˆ is maximizer  of  Ln (χ) =  1 
i=1  log f (Xi |χ)  we  have, 
n 

. 

L

n∈ (χˆ) = 0. 

Let  us  use  the Mean  Value  Theorem 
f (a) − f (b)
= f ∈ (c)  or  f (a) = f (b) + f ∈ (c)(a − b)  for  c ⊆  [a, b] 
a − b 
with  f (χ) = Ln∈ (χ), a = χˆ and  b = χ0 .  Then  we  can  write, 
n ( ˆ

0 = Ln∈ (χˆ) = Ln∈ (χ0 ) + L∈∈ χ1 )(χˆ − χ0 )
ˆ

for  some  χˆ1  ⊆  [χ , χ0 ].  From  here  we  get  that 
∞nLn∈ (χ0 ) 
and ∞n(χˆ − χ0 ) = −
n∈ (χ0 )
L
χˆ − χ0  = 
. 
− 
n (χˆ1 )
n (χˆ1 ) 
L∈∈
L∈∈
Since  by  Lemma  in  the  previous  section  χ0  is  the maximizer  of  L(χ),  we  have 

L∈ (χ0 ) = 

� 

ν0 l ∈ (X χ0 ) = 0. 
|

(5.1)

(5.2)

=  ∞n

∞nLn∈ (χ0 ) =  ∞n

Therefore,  the  numerator  in  (5.1) 
 1 
n
�
�
l ∈ (Xi χ0 ) − 0 
(5.3) 
|
n 
i=1

 1 
 
n

 
�
�
�
�
ν0 l ∈ (X1 χ0 )  ≈ N  0, Varν0 (l ∈ (X1 χ0 )) 
l ∈ (Xi χ0 ) − 
|
|
|
n 
i=1 
converges  in  distribution  by  Central  Limit  Theorem. 
Next,  let  us  consider  the  denominator  in  (5.1).  First  of  all, we  have  that  for  all  χ , 
 
1 
ν0 l ∈∈ (X1 χ)  by  LLN. 
l ∈∈ (Xi χ) ≈ 
|
|
n 
Also,  since  χˆ1  ⊆  [χ , χ0 ]  and  by  consistency  result  of  previous  section  χˆ ≈  χ0 ,  we  have 
ˆ
χˆ1  ≈ χ0 . Using  this  together  with  (5.4)  we  get 
n ( ˆ
ν0 l ∈∈ (X1 χ0 ) = −I (χ0 )  by  Lemma  above. 
L∈∈ χ1 ) ≈ 
|
� 

L∈∈
n (χ) = 

(5.4)

 

� 

� 



LECTURE  5. 

23 

Combining  this  with  (5.3)  we  get 
∞nLn∈ (χ0 ) 
�
1 )  ≈ N 
− 
n (χˆ
L∈∈

Finally,  the  variance, 

  Varν0 (l ∈ (X1 χ0 )) �
(I (χ0 ))2|
0,

 
. 

ν0 (l ∈ (X χ0 ))2  − (  ν0 l ∈ (x χ0 ))2  = I (χ0 ) − 0
Varν0 (l ∈ (X1 χ0 )) = 
|
|
|
� 
where  in  the  last  equality  we  used  the  deﬁnition  of  Fisher  information  and  (5.2). 

�
Lecture  6


Let  us  compute  Fisher  information  for  some  particular  distributions. 
Example  1.  The  family  of  Bernoulli  distributions  B (p)  has  p.f. 

and  taking  the  logarithm 

f (x p) = p x (1 − p)1−x
|

� 
� p 

I (p) = − 
� 

1 − x 
(1 − p)2

log f (x p) = x log p + (1 − x) log(1 − p).
|
The  second  derivative  with  respect  to  parameter  p  is 
� 2 
1 − x
x 
x
p  − 
, 
log f (x p) = 
� p2  log f (x p) = − 
p2  − 
|
|
1 − p 
then  we  showed  that  Fisher  information  can  be  computed  as: 
� 2 
1 − p 
1 
1 − 
X
� 
� p2  log f (X p) = 
(1 − p)2  = 
|
.
p(1 − p) 
(1 − p)2 
¯
The MLE of p is  ˆp = X  and the asymptotic normality result from last lecture becomes 
∞n( ˆp − p0 ) ≈ N (0, p0 (1 − p0 )) 
which,  of  course,  also  follows  directly  from  the  CLT. 
Example.  The  family  of  exponential  distributions  E (ϕ)  has  p.d.f. 
 
�

ϕe−ϕx , x 
0 
→
x < 0 
0, 

f (x ϕ) = 
|

X 
p2 

+

p 
=  +
p2 

and,  therefore,


log f (x ϕ) =  log ϕ − ϕx ≥ 
|
24 

2 
1

�
�ϕ2  log f (x ϕ) = − 
|
ϕ2

. 

�
