Lecture 26

26.1 Test of independence.

In this lecture we will consider the situation when data comes from the sample space
X that consists of pairs of two features and each feature has a (cid:12)nite number of
categories or, simply,

X = f(i; j ) : i = 1; : : : ; a; j = 1; : : : ; bg:
on X then each Xi
If we have an i.i.d. sample X1 ; : : : ; Xn with some distribution
is a pair (X 1
i ; X 2
i ) where X 1
i can take a di(cid:11)erent values and X 2
i can take b di(cid:11)erent
values. Let Nij be a count of all observations equal to (i; j ); i.e. with (cid:12)rst feature
equal to i and second feature equal to j; as shown in table below.

Table 26.1: Contingency table.
Feature 2
(cid:1) (cid:1) (cid:1)
1
b
2
(cid:1) (cid:1) (cid:1) N1b
N11 N12
(cid:1) (cid:1) (cid:1) N2b
N21 N22
...
...
...
...
(cid:1) (cid:1) (cid:1) Nab
Na1 Na2

Feature 1
1
2
...
a

We would like to test the independence of two features which means that

(X = (i; j )) =

(X 1 = i)

(X 2 = j ):

In we introduce the notations

(X = (i; j )) = (cid:18)ij ;

(X 1 = i) = pi and

(X 2 = j ) = qj ;

103








LECTURE 26.

104

then we want to test that for all i and j we have (cid:18)ij = piqj : Therefore, our hypotheses
can be formulated as follows:
(cid:26) H1 : (cid:18)ij = pi qj for some (p1 ; : : : ; pa ) and (q1 ; : : : ; qb )
H2 :
otherwise
Of course, these hypotheses fall into the case of composite (cid:31)2 goodness-of-(cid:12)t test from
previous lecture because our random variables take

r = a (cid:2) b
possible values (all pairs of features) and we want to test that their distribution comes
from the family of distributions with independent features described by the hypothesis
H1 : Since pi s and qj s should add up to one

p1 + : : : + pa = 1 and q1 + : : : + qb = 1

one parameter in each sequence, for example pa and qb ; can be computed in terms of
other probabilities and we can take (p1 ; : : : ; pa(cid:0)1 ) and (q1 ; : : : ; qb(cid:0)1 ) as free parameters
of the model. This means that the dimension of the parameter set is

s = (a (cid:0) 1) + (b (cid:0) 1):
Therefore, if we (cid:12)nd the maximum likelihood estimates for the parameters of this
model then the chi-squared statistic:
(Nij (cid:0) np(cid:3)i q (cid:3)j )2
T = Xi;j
np(cid:3)i q (cid:3)j
converges in distribution to (cid:31)2
(a(cid:0)1)(b(cid:0)1) distribution with (a (cid:0) 1)(b (cid:0) 1) degrees of
freedom. To formulate the test it remains to (cid:12)nd the maximum likelihood estimates
of the parameters. We need to maximize the likelihood function
i Yj
= Yi
i Yj
(piqj )Nij = Yi
Yi;j
Pj Nij
pNi+
where we introduced the notations

ab(cid:0)(a(cid:0)1)(cid:0)(b(cid:0)1)(cid:0)1 = (cid:31)2
r(cid:0)s(cid:0)1 = (cid:31)2
! (cid:31)2
(a(cid:0)1)(b(cid:0)1)

qPi Nij
j

p

qN+j
j

Nij

Ni+ = Xj
for the total number of observations in the ith row or, in other words, the number of
observations with the (cid:12)rst feature equal to i and
N+j = Xi

Nij

LECTURE 26.

105

pNi+
i

pi = 1

given that

for the total number of observations in the j th column or, in other words, the number
of observations with the second feature equal to j: Since pi s and qj s are not related to
each other it is obvious that maximizing the likelihood function above is equivalent
and Qj qN+j
to maximizing Qi pNi+
separately. Let us not forget that we maximize
j
i
given the constraints that pi s and qj s add up to 1 (otherwise, we could let them be
equal to +1). Let us solve, for example, the following optimization problem:
a
maximize Yi
Xi=1
or taking the logarithm
a
Xi=1
maximize X Ni+ log pi given that
We can use the method of Lagrange multipliers. If we consider the function
a
Xi=1
L = X Ni+ log pi (cid:0) (cid:21)(
then we need to (cid:12)nd the saddle point of L by maximizing it with respect to pi s and
minimizing it with respect to (cid:21): Taking the derivative with respect to pi we get

pi (cid:0) 1)

pi = 1:

@L
@ pi

= 0 )

Ni+
pi

= (cid:21) ) pi =

Ni+
(cid:21)

and taking the derivative with respect to (cid:21) we get
a
Xi=1

= 0 )

@L
@(cid:21)

pi = 1:

Combining these two conditions we get
X pi = X Ni+
(cid:21)
and, therefore, we get that the MLE for pi :

=

n
(cid:21)

= 1 ) (cid:21) = n

Similarly, the MLE for qj is:

p(cid:3)i =

Ni+
n

:

q (cid:3)j =

N+j
n

:

LECTURE 26.

106

Therefore, chi-square statistic T in this case can be written as
(Nij (cid:0) Ni+N+j =n)2
T = Xi;j
Ni+N+j =n
and the decision rule is given by
(cid:14) = (cid:26) H1 : T (cid:20) c
H2 : T > c
where the threshold is determined from the condition

(cid:31)2
(a(cid:0)1)(b(cid:0)1) (c; +1) = (cid:11):
Example. In 1992 poll 189 Montana residents were asked were asked whether
their personal (cid:12)nancial status was worse, the same, or better than one year ago. The
opinions were divided into three groups by the income rage: under 20K, between 20K
and 35K, and over 35K. We would like to test if the opinion was independent of the
income range at the level of signi(cid:12)cance (cid:11) = 0:05:

Table 26.2: Montana outlook poll.
b = 3
a = 3 Worse Same Better
(cid:20) 20K
12
15
20
(20K, 35K)
24
27
32
(cid:21) 35K
23
22
14
58
64
67

47
83
59
189

The chi-square statistic is
(20 (cid:0) 47(cid:2)58
189 )2
47(cid:2)58
189

T =

+ : : : +

(23 (cid:0) 67(cid:2)59
189 )2
67(cid:2)59
189

= 5:21

and the threshold c:

(a(cid:0)1)(b(cid:0)1) (c; +1) = (cid:31)2
(cid:31)2
4 (c; 1) = (cid:11) = 0:05 ) c = 9:488:
Since T = 5:21 < c = 9:488 we accept the hypotheses H1 that the opinion is inde-
pendent of the income range.

