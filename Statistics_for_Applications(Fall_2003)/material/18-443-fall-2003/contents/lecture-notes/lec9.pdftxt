Lecture 9

9.1 Prior and posterior distributions.

(Textbook, Sections 6.1 and 6.2)
Assume that the sample X1 ; : : : ; Xn is i.i.d. with distribution
(cid:18)0 that comes from
the family f 
(cid:18) : (cid:18) 2 (cid:2)g and we would like to estimate unknown (cid:18)0 : So far we have
discussed two methods - method of moments and maximum likelihood estimates. In
both methods we tried to (cid:12)nd an estimate ^(cid:18) in the set (cid:2) such that the distribution
^(cid:18)
in some sense best describes the data. We didn’t make any additional assumptions
about the nature of the sample and used only the sample to construct the estimate of
(cid:18)0 : In the next few lectures we will discuss a di(cid:11)erent approach to this problem called
Bayes estimators. In this approach one would like to incorporate into the estimation
process some apriori intuition or theory about the parameter (cid:18)0 : The way one describes
this apriori intuition is by considering a distribution on the set of parameters (cid:2) or,
in other words, one thinks of parameter (cid:18) as a random variable. Let (cid:24) ((cid:18)) be a p.d.f.
of p.f. of this distribution which is called prior distribution. Let us emphasize that
(cid:24) ((cid:18)) does not depend on the sample X1 ; : : : ; Xn ; it is chosen apriori, i.e. before we
even see the data.
Example. Suppose that the sample has Bernoulli distribution B (p) with p.f.

f (xjp) = px(1 (cid:0) p)1(cid:0)x for x = 0; 1;
where parameter p 2 [0; 1]: Suppose that we have some intuition that unknown pa-
rameter should be somewhere near 0:4: Then (cid:24) (p) shown in (cid:12)gure 9.1 can be a possible
choice of a prior distribution that re(cid:13)ects our intution.

After we choose prior distribution we observe the sample X1 ; : : : ; Xn and we would
like to estimate the unknown parameter (cid:18)0 using both the sample and the prior
distribution. As a (cid:12)rst step we will (cid:12)nd what is called the posterior distribution

35



LECTURE 9.

36

x (p)

0

0.4

p

1

Figure 9.1: Prior distribution.

of (cid:18) which is the distribution of (cid:18) given X1 ; : : : ; Xn : This can be done using Bayes
theorem.
Total probability and Bayes theorem. If we consider a disjoint sequence of
(S1i=1 Ai ) = 1 then for any event B we
events A1 ; A2 ; : : : so that Ai \ Aj = ; and
have
1Xi=1 
(B \ Ai ):
Then the Bayes Theorem states the equality obtained by the following simple com-
puation:

(B ) =

(A1 \ B )
(B )

(B jAi )
(B jAi )
(A1 )
(A1 )
(A1 jB ) = 
= 
P1i=1 
P1i=1 
(B jAi )
(B \ Ai )
(Ai )
We can use Bayes formula to compute
(cid:24) ((cid:18) jX1 ; : : : ; Xn) (cid:0) p.d.f. or p.f. of (cid:18) given the sample

=

:

if we know

f (X1 ; : : : ; Xn j(cid:18)) = f (X1 j(cid:18)) : : : f (Xn j(cid:18))
- p.d.f. or p.f. of the sample given (cid:18) ; and if we know the p.d.f. or p.f. (cid:24) ((cid:18)) of (cid:18) :
Posterior distribution of (cid:18) can be computed using Bayes formula:
f (X1 ; : : : ; Xn j(cid:18))(cid:24) ((cid:18))
R(cid:2) f (X1 ; : : : ; Xn j(cid:18))(cid:24) ((cid:18))d(cid:18)
f (X1 j(cid:18)) : : : f (Xn j(cid:18))(cid:24) ((cid:18))
g (X1 ; : : : ; Xn )

(cid:24) ((cid:18) jX1 ; : : : ; Xn ) =

=

where

g (X1 ; : : : ; Xn) = Z(cid:2)

f (X1 j(cid:18)) : : : f (Xn j(cid:18))(cid:24) ((cid:18))d(cid:18) :









LECTURE 9.

37

Example. Very soon we will consider speci(cid:12)c choices of prior distributions and
we will explicitly compute the posterior distribution but right now let us brie(cid:13)y
give an example of how we expect the data and the prior distribution a(cid:11)ect the
posterior distribution. Assume again that we are in the situation described in the
above example when the sample comes from Bernoulli distribution and the prior
distribution is shown in (cid:12)gure 9.1 when we expect p0 to be near 0:4 On the other
hand, suppose that the average of the sample is (cid:22)X = 0:7: This seems to suggest that
our intuition was not quite right, especially, if the sample size if large. In this case we
expect that posterior distribution will look somewhat like the one shown in (cid:12)gure 9.2
- there will be a balance between the prior intuition and the information contained
in the sample. As the sample size increases the maximum of prior distribution will
eventually shift closer and closer to (cid:22)X = 0:7 meaning that we have to discard our
intuition if it contradicts the evidence supported by the data.

x

(p) − Prior Distribution

x

(p)

x

(p | X1, ..., Xn) − Posterior Distribution

p

0

0.4
0.7
Lies Somewhere Between
0.4 and 0.7

Figure 9.2: Posterior distribution.

