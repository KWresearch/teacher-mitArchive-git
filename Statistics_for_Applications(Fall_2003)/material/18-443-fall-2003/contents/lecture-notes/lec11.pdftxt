Lecture 11

11.1 Su(cid:14)cient statistic.

f 

(Textbook, Section 6.7)
We consider an i.i.d.
sample X1 ; : : : ; Xn with distribution
(cid:18) from the family
(cid:18) : (cid:18) 2 (cid:2)g: Imagine that there are two people A and B, and that
1. A observes the entire sample X1 ; : : : ; Xn ;
2. B observes only one number T = T (X1 ; : : : ; Xn ) which is a function of the
sample.
Clearly, A has more information about the distribution of the data and, in par-
ticular, about the unknown parameter (cid:18) : However, in some cases, for some choices of
function T (when T is so called su(cid:14)cient statistics) B will have as much information
about (cid:18) as A has.
De(cid:12)nition. T = T (X1 ; (cid:1) (cid:1) (cid:1) ; Xn ) is called su(cid:14)cient statistics if
0 (X1 ; : : : ; Xn jT );
(cid:18) (X1 ; : : : ; Xn jT ) =
i.e. the conditional distribution of the vector (X1 ; : : : ; Xn) given T does not depend
0 :
on the parameter (cid:18) and is equal to
If this happens then we can say that T contains all information about the param-
eter (cid:18) of the disribution of the sample, since given T the distribution of the sample
is always the same no matter what (cid:18) is. Another way to think about this is: why the
second observer B has as much information about (cid:18) as observer A? Simply, given T ,
the second observer B can generate another sample X 01 ; : : : ; X 0n by drawing it accord-
0 (X1 ; (cid:1) (cid:1) (cid:1) ; Xn jT ). He can do this because it does not require
ing to the distribution
the knowledge of (cid:18) : But by (11.1) this new sample X 01 ; : : : ; X 0n will have the same
distribution as X1 ; : : : ; Xn ; so B will have at his/her disposal as much data as the
(cid:12)rst observer A.
The next result tells us how to (cid:12)nd su(cid:14)cient statistics, if possible.
Theorem. (Neyman-Fisher factorization criterion.) T = T (X1 ; : : : ; Xn ) is su(cid:14)-
cient statistics if and only if the joint p.d.f. or p.f. of (X1 ; : : : ; Xn) can be represented

(11.1)

42






LECTURE 11.

as

43

f (x1 ; : : : ; xn j(cid:18)) (cid:17) f (x1 j(cid:18)) : : : f (xn j(cid:18)) = u(x1 ; : : : ; xn )v (T (x1 ; : : : ; xn ); (cid:18))
for some function u and v : (u does not depend on the parameter (cid:18) and v depends on
the data only through T .)
Proof. We will only consider a simpler case when the distribution of the sample
is discrete.
1. First let us assume that T = T (X1 ; : : : ; Xn) is su(cid:14)cient statistics. Since the
distribution is discrete, we have,

(11.2)

f (x1 ; : : : ; xn j(cid:18)) =
i.e. the joint p.f.
is just the probability that the sample takes values x1 ; : : : ; xn : If
X1 = x1 ; : : : ; Xn = xn then T = T (x1 ; : : : ; xn ) and, therefore,

(cid:18) (X1 = x1 ; : : : ; Xn = xn );

(cid:18) (X1 = x1 ; : : : ; Xn = xn ) =

(cid:18) (X1 = x1 ; : : : ; Xn = xn ; T = T (x1 ; : : : ; xn )):

We can write this last probability via a conditional probability

(cid:18) (X1 = x1 ; : : : ; Xn = xn ; T = T (x1 ; : : : ; xn ))
(cid:18) (X1 = x1 ; : : : ; Xn = xn jT = T (x1 ; : : : ; xn ))
All together we get,

=

(cid:18) (T = T (x1 ; : : : ; xn )):

(cid:18) (X1 = x1 ; : : : ; Xn = xn jT = T (x1 ; : : : ; xn ))
f (x1 ; : : : ; xn j(cid:18)) =
Since T is su(cid:14)cient, by de(cid:12)nition, this means that the (cid:12)rst conditional probability

(cid:18) (T = T (x1 ; : : : ; xn )):

(cid:18) (X1 = x1 ; : : : ; Xn = xn jT = T (x1 ; : : : ; xn )) = u(x1 ; : : : ; xn )
for some function u independent of (cid:18) ; since this conditional probability does not
depend on (cid:18) : Also,

(cid:18) (T = T (x1 ; : : : ; xn )) = v (T (x1 ; : : : ; xn ); (cid:18))

depends on x1 ; : : : ; xn only through T (x1 ; : : : ; xn ): So, we proved that if T is su(cid:14)cient
then (11.2) holds.
2. Let us now show the opposite, that if (11.2) holds then T is su(cid:14)cient. By
de(cid:12)nition of conditional probability, we can write,

(cid:18) (X1 = x1 ; : : : ; Xn = xn jT (X1 ; : : : ; Xn ) = t)
(cid:18) (X1 = x1 ; : : : ; Xn = xn ; T (X1 ; : : : ; Xn) = t)
= 
(cid:18) (T (X1 ; : : : ; Xn) = t)

:

(11.3)













LECTURE 11.

44

First of all, both side are equal to zero unless

t = T (x1 ; : : : ; xn );

(11.4)

because when X1 = x1 ; : : : ; Xn = xn ; T (X1 ; : : : ; Xn ) must be equal to T (x1 ; : : : ; xn ):
For this t; the numerator in (11.3)

(cid:18) (X1 = x1 ; : : : ; Xn = xn ; T (X1 ; : : : ; Xn) = t) =

(cid:18) (X1 = x1 ; : : : ; Xn = xn );

since we just drop the condition that holds anyway. By (11.2), this can be written as

u(x1 ; : : : ; xn )v (T (x1 ; : : : ; xn ); (cid:18)) = u(x1 ; : : : ; xn )v (t; (cid:18)):

As for the denominator in (11.3), let us consider the set

A(t) = f(x1 ; : : : ; xn ) : T (x1 ; : : : ; xn ) = tg
of all possible combinations of the x’s such that T (x1 ; : : : ; xn ) = t: Then, obviously,
the denominator in (11.3) can be written as,

(cid:18) ((X1 ; : : : ; Xn ) 2 A(t))
(cid:18) (T (X1 ; : : : ; Xn ) = t) =
= X(x1 ;(cid:1)(cid:1)(cid:1);xn )2A(t) 
(cid:18) (X1 = x1 ; : : : ; Xn = xn ) = X(x1 ;(cid:1)(cid:1)(cid:1);xn )2A(t)
where in the last step we used (11.2) and (11.4). Therefore, (11.3) can be written as

u(x1 ; : : : ; xn )v (t; (cid:18))

=

u(x1 ; : : : ; xn )v (t; (cid:18))
u(x1 ; : : : ; xn )
PA(t) u(x1 ; : : : ; xn )v (t; (cid:18))
PA(t) u(x1 ; : : : ; xn )
and since this does not depend on (cid:18) anymore, it proves that T is su(cid:14)cient.
Example. Bernoulli Distribution B (p) has p.f. f (xjp) = px (1 (cid:0) p)1(cid:0)x for x 2
f0; 1g: The joint p.f. is
f (x1 ; (cid:1) (cid:1) (cid:1) ; xn jp) = pP xi (1 (cid:0) p)n(cid:0)P xi = v (X Xi ; p);
it depends on x’s only through the sum P xi : Therefore, by Neyman-Fisher
i.e.
factorization criterion T = P Xi is a su(cid:14)cient statistic. Here we set
v (T ; p) = pT (1 (cid:0) p)n(cid:0)T and u(x1 ; : : : ; xn ) = 1:





