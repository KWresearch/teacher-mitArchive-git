Lecture 19

In the last lecture we found the Bayes decision rule that minimizes the Bayes eror
k
k
Xi=1
Xi=1
Let us write down this decision rule in the case of two simple hypothesis H1 ; H2 : For
simplicity of notations, given the sample X = (X1 ; : : : ; Xn ) we will denote the joint
p.d.f. by

i ((cid:14) 6= Hi ):

(cid:24) (i)(cid:11)i =

(cid:11) =

(cid:24) (i)

fi (X ) = fi (X1 ) : : : fi (Xn ):

Then in the case of two simple hypotheses the Bayes decision rule that minimizes the
Bayes error

is given by

or, equivalently,

2((cid:14) 6= H2 )

1 ((cid:14) 6= H1 ) + (cid:24) (2)

(cid:11) = (cid:24) (1)
(cid:14) = 8<
(cid:24) (1)f1(X ) > (cid:24) (2)f2 (X )
H1 :
H2 :
(cid:24) (2)f2(X ) > (cid:24) (1)f1 (X )
:
H1 or H2 : (cid:24) (1)f1(X ) = (cid:24) (2)f2 (X )
(cid:14) = 8><
f1 (X )
f2 (X ) > (cid:24) (2)
(cid:24) (1)
f1 (X )
f2 (X ) < (cid:24) (2)
(cid:24) (1)
>:
f2 (X ) = (cid:24) (2)
f1 (X )
(cid:24) (1)
(Here 1
0 = +1, 0
1 = 0.) This kind of test if called likelihood ratio test since it is
expressed in terms of the ratio f1 (X )=f2 (X ) of likelihood functions.
Example. Suppose we have only one observation X1 and two simple hypotheses
= N (0; 1) and H2 :
= N (1; 1): Let us take an apriori distribution given by
H1 :

H1 :
H2 :
H1 or H2 :

(19.1)

(cid:24) (1) =

1
2

and (cid:24) (2) =

1
2

;

71






LECTURE 19.

72

i.e. both hypothesis have equal weight, and (cid:12)nd a Bayes decision rule (cid:14) that minimizes

1
2 

2 ((cid:14) 6= H2 )

1 ((cid:14) 6= H1 ) +

1
2 
Bayes decision rule is given by:
(cid:14) (X1 ) = 8><
>:
This decision rule has a very intuitive interpretation. If we look at the graphs of these
p.d.f.s ((cid:12)gure 19.1) the decision rule picks the (cid:12)rst hypothesis when the (cid:12)rst p.d.f. is
larger, to the left of point C; and otherwise picks the second hypothesis to the right
of point C:

H1 :
H2 :
H1 or H2 :

f1 (X )
f2 (X ) > 1
f1 (X )
f2 (X ) < 1
f1 (X )
f2 (X ) = 1

PDF

f1

f2

0

H1

C H2

1

Figure 19.1: Bayes Decision Rule.

X1

Example. Let us now consider a similar but more general example when we
have a sample X = (X1 ; (cid:1) (cid:1) (cid:1) ; Xn ); two simple hypotheses H1 :
= N (0; 1) and
= N (1; 1); and arbitrary apriori weights (cid:24) (1); (cid:24) (2): Then Bayes decision rule
H2 :
is given by (19.1). The likelihood ratio can be simpli(cid:12)ed:
i . 1
1
e(cid:0) 1
2 P X 2
(p2(cid:25) )n
(p2(cid:25) )n
1
2 Pk
n
i=1 ((Xi(cid:0)1)2(cid:0)X 2
2 (cid:0)P Xi
i ) = e
= e

e(cid:0) 1
2 P(Xi(cid:0)1)2

f1 (X )
f2 (X )

=

Therefore, the decision rule picks the (cid:12)rst hypothesis H1 when

n
2 (cid:0)P Xi >

e

(cid:24) (2)
(cid:24) (1)



LECTURE 19.

or, equivalently,

73

n
X Xi <
2 (cid:0) log
Similarly, we pick the second hypothesis H2 when
X Xi >
In case of equality, we pick any one of H1 ; H2 :

n
2 (cid:0) log

(cid:24) (2)
(cid:24) (1)

(cid:24) (2)
(cid:24) (1)

:

:

19.1 Most powerful test for two simple hypothe-
ses.

Now that we learned how to construct the decision rule that minimizes the Bayes
error we will turn to our next goal which we discussed in the last lecture, namely,
how to construct the decision rule with controlled error of type 1 that minimizes error
of type 2: Given (cid:11) 2 [0; 1] we consider the class of decision rules
K(cid:11) = f(cid:14) :
1 ((cid:14) 6= H1 ) (cid:20) (cid:11)g
and will try to (cid:12)nd (cid:14) 2 K(cid:11) that makes the type 2 error (cid:11)2 =
possible.
Theorem: Assume that there exist a constant c, such that
< c(cid:17) = (cid:11):
1(cid:16) f1 (X )
f2 (X )
(cid:14) = ( H1 :
f1 (X )
f2 (X ) (cid:21) c
f1 (X )
H2 :
f2 (X ) < c
is the most powerful in class K(cid:11) .
Proof. Take (cid:24) (1) and (cid:24) (2) such that

2 ((cid:14) 6= H2 ) as small as

Then the decision rule

(19.2)

i.e.

(cid:24) (1) + (cid:24) (2) = 1;

(cid:24) (2)
(cid:24) (1)

= c;

(cid:24) (1) =

1
1 + c

and (cid:24) (2) =

c
1 + c

:




LECTURE 19.

74

Then the decision rule (cid:14) in (19.2) is the Bayes decision rule corresponding to weights
(cid:24) (1) and (cid:24) (2) which can be seen by comparing it with (19.1), only here we break the
tie in favor of H1 : Therefore, this decision rule (cid:14) minimizes the Bayes error which
means that for any other decision rule (cid:14) 0 ;

2((cid:14) 0 6= H2 ):

(19.3)

(cid:24) (1)

1((cid:14) 0 6= H1 ) + (cid:24) (2)
2 ((cid:14) 6= H2 ) (cid:20) (cid:24) (1)
1((cid:14) 6= H1 ) + (cid:24) (2)
By assumption in the statement of the Theorem, we have
< c(cid:17) = (cid:11);
1(cid:16) f1 (X )
1 ((cid:14) 6= H1 ) =
f2 (X )
which means that (cid:14) comes from the class K(cid:11) : If (cid:14) 0 2 K(cid:11) then
1 ((cid:14) 0 6= H1 ) (cid:20) (cid:11)

and equation (19.3) gives us that

(cid:24) (1)(cid:11) + (cid:24) (2)

2 ((cid:14) 6= H2 ) (cid:20) (cid:24) (1)(cid:11) + (cid:24) (2)

2((cid:14) 0 6= H2 )

and, therefore,

2 ((cid:14) 0 6= H2 ):
2 ((cid:14) 6= H2 ) (cid:20) 
This exactly means that (cid:14) is more powerful than any other decision rule in class K(cid:11) :

Example. Suppose we have a sample X = (X1 ; : : : ; Xn ) and two simple hypothe-
ses H1 :
= N (0; 1) and H2 :
= N (1; 1): Let us (cid:12)nd most powerful (cid:14) with the
error of type 1

(cid:11)1 (cid:20) (cid:11) = 0:05:
According to the above Theorem if we can (cid:12)nd c such that
< c(cid:17) = (cid:11) = 0:05
1(cid:16) f1 (X )
f2 (X )
then we know how to (cid:12)nd (cid:14): Simplifying this equation gives
2 (cid:0) log c(cid:17) = (cid:11) = 0:05
1(cid:16)X Xi >
n
2 (cid:0) log c)(cid:17) = (cid:11) = 0:05:
1(cid:16) 1
1
n
pn X Xi > c0 =
pn
(
But under the hypothesis H1 the sample comes from standard normal distribution
1 = N (0; 1) which implies that the random variable
1
pn X Xi

Y =

or

















LECTURE 19.

75

is standard normal. We can look up in the table that

(Y > c0 ) = (cid:11) = 0:05 ) c0 = 1:64
and the most powerful test (cid:14) with level of signi(cid:12)cance (cid:11) = 0:05 will look like this:
(cid:14) = ( H1 :
1pn P Xi (cid:20) 1:64
1pn P Xi > 1:64:
H2 :
Now, what will the error of type 2 be for this test?

=

Y =

1
pn

(cid:11)2 =

pn X Xi (cid:20) 1:64(cid:17)
2(cid:16) 1
2 ((cid:14) 6= H2 ) =
n
(Xi (cid:0) 1) (cid:20) 1:64 (cid:0) pn(cid:17):
2(cid:16) 1
Xi=1
pn
The reason we subtracted 1 from each Xi is because under the second hypothesis X â€™s
have distribution N (1; 1) and random variable
n
Xi=1
will be standard normal. Therefore, the error of type 2 for this test will be equak to
(Y < 1:64 (cid:0) pn): For example, when the sample size n = 10 this
the probability
will be
p10) = 0:087
(Y < 1:64 (cid:0)
(cid:11)2 =
from the table of normal distribution.

(Xi (cid:0) 1)







