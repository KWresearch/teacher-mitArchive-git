6.006 Intro to Algorithms
Python Cost Model

Recitation 2

September 14, 2011

Operators in Erik’s notes, minus what was covered in lecture:

1. list operations:

(a) L1.extend(L2)
(b) L2 = L1[i:j]
(c) b = (x in L) or L.index(x) or L.find(x)

2. tuple and str

3. set

4. heapq

Other points of interest:

1. equality checking (e.g. list1 == list2)

2. lists versus generators

Reference (cid:29) Python Cost Model . The Web site has runtime interpolation for various Python
operations. The running times for various-sized inputs were measured, and then a least-square ﬁt
was used to ﬁnd the coefﬁcient for the highest order term in the running time.
Difference between generators and lists. A good explanation is here: http://wiki.python.
org/moin/Generators

Document Distance
docdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py
show algorithmic optimizations that improve the running time.
We start out by understanding the structure of the straightforward implementation, and then
we’ll look at the changes in each of the successive versions.

docdist1
We ﬁrst look at main to get a high-level view of what’s going on in the program.
1 def main():
if len(sys.argv) != 3:
2
print "Usage: docdist1.py filename_1 filename_2"
3
else:
4
5
filename_1 = sys.argv[1]

16.006 Intro to Algorithms

Recitation 2

September 14, 2011

6
7
8
9
10

filename_2 = sys.argv[2]
sorted_word_list_1 = word_frequencies_for_file(filename_1)
sorted_word_list_2 = word_frequencies_for_file(filename_2)
distance = vector_angle(sorted_word_list_1,sorted_word_list_2)
print "The distance between the documents is: %0.6f (radians)"%distance
The method processes the command-line arguments, and calls word frequencies for file
for each document, then calls vector angle on the resulting lists. How do these methods
match up with the three operations outlined in lecture? It seems like word frequencies for file
is responsible for operation 1 (split each document into words) and operation 2 (count word fre-
quencies), and then vector angle is responsible for operation 3 (compute dot product).
Next up, we’ll take a look at word frequencies for file.
1 def word_frequencies_for_file(filename):
2
line_list = read_file(filename)
3
word_list = get_words_from_line_list(line_list)
4
freq_mapping = count_frequency(word_list)
return freq_mapping
5
The method ﬁrst calls read file, which returns a list of lines in the input ﬁle. We’ll omit
the method code, because it is not particularly interesting, and we’ll assume that read file’s
running time is proportional to the size of the input ﬁle. The input from read line is given
to get words from line list, which computes operation 1 (split each document into
words). After that, count frequency turns the list of words into a document vector (operation
2).
1 def get_words_from_line_list(L):
2
word_list = []
for line in L:
3
4
words_in_line = get_words_from_string(line)
5
word_list = word_list + words_in_line
return word_list
6
7
8 def get_words_from_string(line):
9
word_list = []
10
character_list = []
for c in line:
11
if c.isalnum():
12
13
character_list.append(c)
elif len(character_list)>0:
14
15
word = "".join(character_list)
16
word = word.lower()
17
word_list.append(word)
18
character_list = []
if len(character_list)>0:
19
20
word = "".join(character_list)
21
word = word.lower()
22
word_list.append(word)
return word_list
23
get words from string takes one line in the input ﬁle and breaks it up into a list of
words. TODO: line-by-line analysis. The running time is O(k), where k is the length of the line.

26.006 Intro to Algorithms

Recitation 2

September 14, 2011

get words from line list calls get words from string for each line and
combines the lists into one big list. Line 5 looks innocent but is a big performance killer, because
W 2
using
to combine W lists of length
is
.
+
k O(
)
k
k
The output of get words from line list is a list of words, like [’a’, ’cat’,
’in’, ’a’, ’bag’]. word frequencies from file passes this output to count frequency,
which turns it into a document vector that counts the number of occurrences of each word, and
looks like [[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].
1 def count_frequency(word_list):
2
L = []
for new_word in word_list:
3
for entry in L:
4
if new_word == entry[0]:
5
6
entry[1] = entry[1] + 1
break
7
else:
8
9
L.append([new_word,1])
return L
10
The implementation above builds the document vector by takes each word in the input list and
looking it up in the list representing the under-construction document vector. In the worst case of
a document with all different words, this takes O(W 2 × l) time, where W is the number of words
in the document, and l is the average word length.
count frequency is the last function call in word frequencies for file. Next
up, main calls vector angle, which performs operation 3, computing the distance metric.
1 def vector_angle(L1,L2):
2
numerator = inner_product(L1,L2)
3
denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))
return math.acos(numerator/denominator)
4
(cid:33)
(cid:32)
(cid:18) L1 · L2
(cid:19)
The method is a somewhat straightforward implementation of the distance metric
(cid:112)
1 · L2
L
|L1||L2|
(L1 · L1)(L2 · L2)
of computing cross products.
and delegates to inner product for the hard work
1 def inner_product(L1,L2):
2
sum = 0.0
for word1, count1 in L1:
3
for word2, count2 in L2:
4
if word1 == word2:
5
6
sum += count1 * count2
return sum
7
inner product is a straightforward inner-product implementation that checks each each
word in the ﬁrst list against the entire second list. The nested loops at lines 3 and 4 give the
algorithm its running time of Θ(L1L2 ), where L1 and L2 are the lengths of the documents’ vectors
(the number of unique words in each document).

= arccos

arccos

,

36.006 Intro to Algorithms

Recitation 2

September 14, 2011

O( W 2
k

docdist1 Performance Scorecard
Method
Time
get words from line list
) = O(W 2 )
count frequency
O(W L)
word frequencies for file
O(W 2 )
inner product
O(L1L2 )
vector angle
2 ) = O(L2
O(L1L2 + L2
1 + L2
1 + L2
2 )
main
O(W 2 + W 2 )
2
1
We assume that k (number of words per line) is a constant, because the documents will need
to ﬁt on screens or paper sheets with a ﬁnite length. W (the number of words in a document)
is ≥ L (the number of unique words in a document). L2
2 ) because
1 + L2
1L2 = O(L1 + L2
2
2 + L
2 ≥ L1L2 . Proof (assuming L1 , L2 ≥ 0):
1 + L2
L2

1 − L2 ) ≥ 0
2
(L
2 − 2L1L2 ≥ 0
1 + L2
L2
2 ≥ 2L1L2
L2
1 + L2
2 ≥ L1L2
1 + L2
L2

docdist2
The document distance code invokes the Python proﬁler to identify the code that takes up the most
CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 if __name__ == "__main__":
import cProfile
2
3
cProfile.run("main()")
You can proﬁle existing programs without changing them by adding -m cProfile -s
Time to Python’s command line. For example, the command below will run and proﬁle program.py.
python -m cProfile -s time program.py
The proﬁler output for docdist1 shows that the biggest time drain is get words from line list.
The problem is that when the + operator is used to concatenate lists, it needs to create a new list
and copy the elements of both its operands. Replacing + with extend yields a 30% runtime
improvement.
1 def get_words_from_line_list(L):
2
word_list = []
for line in L:
3
4
words_in_line = get_words_from_string(line)
5
word_list.extend(words_in_line)
return word_list
6

46.006 Intro to Algorithms
Recitation 2
September 14, 2011
to +, which needs to create (cid:80)a new list, and therefore takes Θ(1 + n + m) time. So concatenating
extend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed
W
W lists of k elements takes
time.
k k = Θ(W )
k
docdist2 Performance Scorecard
Method
get words from line list
count frequency
word frequencies for file
inner product
vector angle
main

Time
O(W )
O(W L)
O(W L)
O(L1L2 )
O(L2
1 + L2
2 )
O(W1L1 + W2L2 )

docdist3
Proﬁling docdist2 points to count frequency and inner product as good targets for op-
timizations. We’ll speed up inner product by switching to a fast algorithm. However, the
algorithm assumes that the words in the document vectors are sorted. For example, [[’a’, 2],
[’cat’, 1], [’in’, 1], [’bag’, 1]] needs to become [[’a’, 2], [’bag’, 1],
[’cat’, 1], [’in’, 1]].
First off, we add a step to word frequencies for file that sorts the document vector
produced by count frequency.
1 def word_frequencies_for_file(filename):
2
line_list = read_file(filename)
3
word_list = get_words_from_line_list(line_list)
4
freq_mapping = count_frequency(word_list)
5
insertion_sort(freq_mapping)
return freq_mapping
6
Then we implement insertion sort using the algorithm in the textbook.
1 def insertion_sort(A):
for j in range(len(A)):
2
3
key = A[j]
4
i = j-1
while i>-1 and A[i]>key:
5
6
A[i+1] = A[i]
7
i = i-1
8
A[i+1] = key
return A
9
Insertion sort runs in O(L2 ) time, where L is the length of the document vector to be sorted.
Finally, inner product is re-implemented using a similar algorithm to the merging step in
Merge Sort.
1 def inner_product(L1,L2):
2
sum = 0.0

56.006 Intro to Algorithms

Recitation 2

September 14, 2011

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

i = 0
j = 0
while i<len(L1) and j<len(L2):
# L1[i:] and L2[j:] yet to be processed
if L1[i][0] == L2[j][0]:
# both vectors have this word
sum += L1[i][1] * L2[j][1]
i += 1
j += 1
elif L1[i][0] < L2[j][0]:
# word L1[i][0] is in L1 but not L2
i += 1
else:
# word L2[j][0] is in L2 but not L1
j += 1
return sum
The new implementation runs in Θ(L1 + L2 ), where L1 and L2 are the lengths of the two
document vectors. We observe that the running time for inner product (and therefore for
vector angle) is asymptotically optimal, because any algorithm that computes the inner prod-
uct will have to read the two vectors, and reading will take Ω(L1 + L2 ) time.

docdist3 Performance Scorecard
Time
Method
get words from line list
O(W )
count frequency
O(W L)
insertion sort
O(L2 )
word frequencies for file O(W L + L2 ) = O(W L)
inner product
O(L1 + L2 )
vector angle
O(L1 + L2 )
main
O(W1L1 + W2L2 )

docdist4
The next iteration addresses count frequency, which is the biggest time consumer at the
moment.
1 def count_frequency(word_list):
2
D = {}
for new_word in word_list:
3
if new_word in D:
4
5
D[new_word] = D[new_word]+1
else:
6
7
D[new_word] = 1
return D.items()
8
The new implementation uses Python dictionaries. The dictionaries are implemented using
hash tables, which will be presented in a future lecture. The salient feature of hash tables is that

66.006 Intro to Algorithms

Recitation 2

September 14, 2011

inserting an element using dictionary[key] = value and looking up an element using
dictionary[key] both run in O(1) expected time.
Instead of storing the document vector under construction in a list, the new implementation
uses a dictionary. The keys are the words in the document, and the value are the number of times
each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)
time, building a document vector out of W words takes O(W ) time.

docdist4 Performance Scorecard
Time
Method
get words from line list
O(W )
count frequency
O(W )
insertion sort
O(L2 )
word frequencies for file O(W + L2 ) = O(L2 )
inner product
O(L1 + L2 )
vector angle
O(L1 + L2 )
main
O(L2
1 + L2 )2

docdist5
This iteration simpliﬁes get words from string that breaks up lines into words. First off,
the standard library function string.translate is used for converting uppercase characters
to lowercase, and for converting punctuation to spaces. Second, the split method on strings is
used to break up a line into words.
1 translation_table = string.maketrans(string.punctuation+string.uppercase,
2
" "*len(string.punctuation)+string.lowercase)
3
4 def get_words_from_string(line):
5
line = line.translate(translation_table)
6
word_list = line.split()
return word_list
7
The main beneﬁt of this change is that 23 lines of code are replaced with 5 lines of code.
This makes the implementation easier to analyze. A side beneﬁt is that many functions in the
Python standard library are implemented in directly in C (a low-level programming language that
is very close to machine code), which gives them better performance. Although the running time is
asymptotically the same, the hidden constants are much better for the C code than for our Python
code presented in docdist1.

docdist5 Performance Scorecard

Identical to the docdist4 scorecard.

76.006 Intro to Algorithms

Recitation 2

September 14, 2011

docdist6
Now that all the distractions are out of the way, it’s time to tackle insertion sort, which is
takes up the most CPU time, by far, in the proﬁler output for docdist5.
The advantages of insertion sort are that it sorts in place, and it is simple to implement.
However, its worst-case running time is O(N 2 ) for an N -element array. We’ll replace insertion
sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modify
word frequencies for file.
1 def word_frequencies_for_file(filename):
2
line_list = read_file(filename)
3
word_list = get_words_from_line_list(line_list)
4
freq_mapping = count_frequency(word_list)
5
freq_mapping = merge_sort(freq_mapping)
return freq_mapping
6
The merge sort implementation closely follows the pseudocode in the textbook.
1 def merge_sort(A):
2
n = len(A)
if n==1:
3
return A
4
5
mid = n//2
6
L = merge_sort(A[:mid])
7
R = merge_sort(A[mid:])
return merge(L,R)
8
9
10 def merge(L,R):
11
i = 0
12
j = 0
13
answer = []
while i<len(L) and j<len(R):
14
if L[i]<R[j]:
15
16
answer.append(L[i])
17
i += 1
else:
18
19
answer.append(R[j])
20
j += 1
if i<len(L):
21
22
answer.extend(L[i:])
if j<len(R):
23
24
answer.extend(R[j:])
return answer
25
The textbook proves that merge sort runs in Θ(n log n) time. You should apply your
knowledge of the Python cost model to convince yourself that the implementation above also runs
in Θ(n log n) time.

86.006 Intro to Algorithms

Recitation 2

September 14, 2011

docdist6 Performance Scorecard
Time
Method
get words from line list
O(W )
count frequency
O(W )
merge sort
O(L log L)
word frequencies for file O(W + L log L) = O(L log L)
inner product
O(L1 + L2 )
vector angle
O(L1 + L2 )
main
O(L1 log L1 + L2 log L2 )

docdist7
Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s
proﬁler output, we notice that merge is the function with the biggest runtime footprint. Merge
sort’s performance in practice is great, so it seems that the only way to make our code faster is to
get rid of sorting altogether.
This iteration switches away from the sorted list representation of document vectors, and in-
stead uses the Python dictionary representation that was introduced in docdist4. count frequency
already used that representation internally, so we only need to remove the code that converted the
Python dictionary to a list.
1 def count_frequency(word_list):
2
D = {}
for new_word in word_list:
3
if new_word in D:
4
5
D[new_word] = D[new_word]+1
else:
6
7
D[new_word] = 1
return D
8
This method still takes O(W ) time to process a W -word document.
word frequencies for file does not call merge sort anymore, and instead re-
turns the dictionary built by count frequency.
1 def word_frequencies_for_file(filename):
2
line_list = read_file(filename)
3
word_list = get_words_from_line_list(line_list)
4
freq_mapping = count_frequency(word_list)
return freq_mapping
5
Next up, inner product makes uses dictionary lookups instead of merging sorted lists.
1 def inner_product(D1,D2):
2
sum = 0.0
for key in D1:
3
if key in D2:
4
5
sum += D1[key] * D2[key]
return sum
6

96.006 Intro to Algorithms

Recitation 2

September 14, 2011

The logic is quite similar to the straightforward inner product in docdist1. Each word
in the ﬁrst document vector is looked up in the second document vector. However, because the
document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1 )
time, where L1 is the length of the ﬁrst document’s vector.

docdist7 Performance Scorecard
Method
get words from line list
count frequency
word frequencies for file
inner product
vector angle
main

Time
O(W )
O(W )
O(W )
O(L1 + L2 )
O(L1 + L2 )
O(W1 + W2 )

docdist8
At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove
this, by noting that each of the 3 main operations runs in time proportional to its input size, and
each operation needs to read all its input to produce its output. However, there is still room for
optimizing and simplifying the code.
There is no reason to read each document line by line, and then break up each line into words.
The last iteration processes reads each document into one large string, and breaks up the entire
document into words at once.
First off, read file is modiﬁed to return a single string, instead of an array of strings. Then,
get words from line list is renamed to get words from file, and is simpliﬁed,
because it doesn’t need to iterate over a list of lines anymore. Last, word frequencies for file
is updated to reﬂect the renaming.
1 def get_words_from_text(text):
2
text = text.translate(translation_table)
3
word_list = text.split()
return word_list
4
5
6 def word_frequencies_for_file(filename):
7
text = read_file(filename)
8
word_list = get_words_from_text(text)
9
freq_mapping = count_frequency(word_list)
return freq_mapping
10

106.006 Intro to Algorithms

Recitation 2

September 14, 2011

docdist8 Performance Scorecard
Method
get words from text
count frequency
word frequencies for
inner product
vector
angle
main

file

Time
O(W )
O(W )
O(W )
O(L1 + L2 )
O(L1 + L2 )
O(W1 + W2 )

11MIT OpenCourseWare
http://ocw.mit.edu

6.006 Introduction to Algorithms
Fall 2011
 
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .

