6.006 Intro to Algorithms
Asymptotic Complexity

Recitation 1

September 9, 2011

These notes aim to help you build an intuitive understanding of asymptotic notation. They are a
supplement to the material in the textbook, not a replacement for it.
Informally, asymptotic notation takes a 10,000 feet view of the function’s growth. For example,
let’s look at f1 (x) = x2 and f2 (x) = 1.1x2 + (x1.9 + 10) sin(10x + 1.5) + 30. f2 looks a lot more
complex than f1 . For small values of x, the functions’ graphs also look very different. However,
if we increase the scale by 1000 times, we get a very different picture. It looks like, in the bigger
picture of things, f1 and f2 aren’t so different after all.

Asymptotic notation, also known as “big-Oh” notation, uses the symbols O , Θ and Ω. The
notation, f2 (x) = Θ(x2 ), is really misleading, because it makes it seem like Θ(x2 ) is a function.
f (x) = Θ(x2 ) implies both a lower bound and an upper bound, as the graph below shows. The
graph shows a visual proof that f
2 (x) = Θ(x ), by showing that it’s bounded from above by 1.2x ,
2
2
and that it is bounded from below by 0.9x2 . These two functions only differ from the function
inside the Θ by a constant factor.

Θ “constrains” a function both from above and from below. O only makes a statement about
the upper bound of a function, and Ω only makes a statement about the function’s lower bound.
The left graph below plots g(x) = (1 + sin( x + 1.5))x2 + x1.5 + 10
, and proves visually that
200
g(x) = O(x2 ). Note that we cannot say g(x) = Θ(x2 ), because the coefﬁcient of x2 becomes 0
for some values of x, so g(x) is only bounded from below by x1.5 (g(x) = Ω(x1.5 )).

 0 20 40 60 80 100 120 140 160 180 0 1 2 3 4 5 6 7 8 9 10f(x)xx ** 21.1 * x ** 2 + (x ** 1.5 + 10) * sin(x * 10 + 1.5) + 30 0 2e+07 4e+07 6e+07 8e+07 1e+08 1.2e+08 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000f(x)xx ** 21.1 * x ** 2 + (x ** 1.5 + 10) * sin(x * 10 + 1.5) + 30 0 2e+07 4e+07 6e+07 8e+07 1e+08 1.2e+08 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000f(x)x0.9 * x ** 21.2 * x ** 21.1 * x ** 2 + (x ** 1.5 + 10) * sin(x * 10 + 1.5) + 3016.006 Intro to Algorithms

Recitation 1

September 9, 2011

, and proves visually
The right graph above shows h(x) = (1 + sin( x + 1.5))x1.5 + x1.4 + 10
200
that h(x) = Ω(x1.4 ). h(x) = Θ(x1.4 ) because, for most values of x, the coefﬁcient of x1.5 is
positive, so the x1.5 term dominates the function’s value.

Asymptotic Drowning
In asymptotic notation, we can reduce complex functions involving logarithms, using the following
rules.
• log(n100 ) = 100 log(n) = Θ(log(n)) - constant exponents don’t matter
• log

- constant bases don’t matter

= Θ(log(n))

log(5)

(n) = log(n)
log(5)

Don’t confuse an exponential inside a logarithm with a logarithm inside an exponential. For
example:
• nlog(5) cannot be simpliﬁed; we can get some loose bounds for it by observing that log(5) ≈
2.3219 . . ., so n2 ≤ nlog(5) ≤ n3
The rules were extracted from the textbook, which also contains useful rules for other functions
that we’ll meet in 6.006, such as polynomials.

Exercises
Compute simple, tight asymptotic bounds for f (n), where f (n) is the following:
• 1080 (the number of atoms in the universe)
• logln 5 (loglog 100 n)
• (20n)7
• log (cid:0) n
(cid:1) (hint: use Stirling’s approximation, n!
• 5log(3)n3 + 1080n2 + log(3)n3.1 + 6006
n
2

( N )N
2πn
e

√

≈

)

 0 5e+07 1e+08 1.5e+08 2e+08 2.5e+08 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000f(x)x2.1 * x ** 2(1 + sin(x / 200 + 1.5)) * x ** 2 + x ** 1.5 + 10 0 500000 1e+06 1.5e+06 2e+06 2.5e+06 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000f(x)xx ** 1.4(1 + sin(x / 200)) * x ** 1.5 + x ** 1.4 + 102(cid:54)
6.006 Intro to Algorithms

Recitation 1

September 9, 2011

Solutions
• 1080 (the number of atoms in the universe) is Θ(1) because there is no n in it, so it’s a
constant (although a mighty big one)
• logln 5 (loglog 100 n) is Θ(log(log(n))) after applying the properties of logarithms
• (20n)7 is Θ(n7 ), because (20n)7 = 207 × n7 = Θ(n7 ) (207 is a big constant factor)
• 5log(3)n3 + 1080n2 + log(3)n3.1 + 6006 is Θ(n3.1 ). Eliminate the constant factors to obtain
(cid:1) is O(n). Use the binomial identity to obtain n
• log (cid:0) n
(cid:0)
(cid:1)
Θ(n3 ) + Θ(n2 ) + Θ(n3.1 ) + Θ(1), then observe that Θ(n3.1 ) dominates the sum.
( n !)2 , then apply Stirling’s
n = n!
n
2
2
2
and the logarithm properties.
approximation

Recurrences

Recurrences show up when trying to analyze the running time of divide-and-conquer algorithms.
In a nutshell, divide-and-conquer is a general approach that suggests breaking down big problems
into many smaller sub-problems that are manageable to solve, and then combining the solutions
for the small sub-problems to obtain the solution to the bigger problem.

1. Divide Break a problem into smaller sub-problems

2. Conquer Really small subproblems are easy

3. Proﬁt Combine answers to sub-problems

Sample Recurrence
Binary search is the canonical example of divide and conquer. If I would be looking for the word
“algorithms” in a physical dictionary (we’ll need to stop using this example when e-books rule
the world), I would open the book right in the middle, see that the ﬁrst word on the left page is
“minotaur”, and conclude that it’s safe to ignore the right half of the book. Then I would split the
left half of the dictionary into two, and perhaps I would see the word “gargantuan”. Again, I know
I can ignore all the pages to the right of “gargantuan”, so I’d focus on the left half of the pages.
Eventually, I will ﬁnd my word, and my search will take much less time than it would if I would
go through each page individually.
Binary search maps to the divide-and-conquer paradigm as follows. Suppose I am looking for
a number x in a sorted array of numbers A[1 . . . n]

1. Divide Compare A[ n ]
2 , recurse
2 with . If the numbers are equal, report success. If
x < A[ n ]
x
2 − 1]. Otherwise, recurse on A[ n + 1 . . . n]
(repeat the process focusing) on A[1 . . . n
.
2
2. Conquer If the array A is empty, x cannot be in it, so I can report failure.

36.006 Intro to Algorithms

Recitation 1

September 9, 2011

3. Proﬁt If I have found x, the problem is obviously solved. However, if I have not found x in
the half of the array that I recursed on, I need to convince myself that I’m not missing out
by ignoring the other half of the array, and declare that x is not in A, despite the fact that I
completely ignored half of its elements.

Let T (n) be the time it takes to ﬁnd x in A[1 . . . n], or give up. For simplicity, let’s focus on
the case where x does not exist in A. Convince yourself that this uncovers the worst-case runnning
time for binary search.
Assuming that making a guess takes a constant amount of time, we can write the following
recurrence for T (n).

T (n) = T (

n
2
T (1) = Θ(1)

) + Θ(1)

Remember, from the section on asymptotics, that any constant is Θ(1), so we could have used
more base cases, if that would have been convenient for us. We could even say that T (1) up to
T (1000) are Θ(1), and our recurrence only works for n > 1000.
Let’s aim to guess the closed form formula for T (n). Since we’re making a guess, we’ll be a bit
sloppy. Θ(1) is a constant, so we can rewrite the recurrence as T (n) = T ( n ) + c
. The advantage
2
of using c is that we can resist the urge of adding up the Θ(1) terms. Then we’ll expand it a few
times:

) + c

) + c + c

n
2
n
4
n
) + c + c + c
8
n
16

) + c + c + c + c

T (n) = T (

T (n) = T (

T (n) = T (

T (n) = T (
...
T (n) = T (

n
2i
2i = 1, so that
We can stop expanding when we hit a base case, so we want to set i such that n
. we get
. Using
we obtain
.
T ( n ) = Θ(1)
T (n) = Θ(1) + iΘ(1)
i = log(n)
T (n) = Θ(log(n))
2i

) + i × c

Recurrence Traps
One potential pitfall in a proof involving big-O notation is the fact that the notation hides infor-
mation about the constants involved. To illustrate this problem, we shall prove that the function
f (n) = n is O(1). The proof is by induction, with n = 1 as the base case. Clearly, f (1) = 1
is a constant, so we can say that f (1) = O(1). The inductive step quickly follows: assume
f (x) = O(1) for all x < n.

46.006 Intro to Algorithms

Recitation 1

September 9, 2011

f (n) = n
= n − 1 + 1
= f (n − 1) + 1
= O(1) + 1
= O(1)

What went wrong here? The problem here occurred in the inductive step. By deﬁnition, f (n)
is O(1) if and only if there exists some constant c such that for sufﬁciently large values of n,
f (n) < c. This constant c must be the same for all values of n. When we assume that f (n − 1)
is O(1), the constant used is not the same as the constant used when we prove that that f (n) is
O(1). The big-O notation hides the fact that the constant increases with every step of the inductive
process, and is therefore not actually constant.
To avoid this trap, it is generally a good idea to avoid the use of big-O notation in proofs by
induction. It is usually a good policy to replace all uses of big-O and big-Ω with their deﬁnitions,
picking ﬁxed variables to represent the constants involved. If we attempt to do this in the proof that
f (n) is O(1), our inductive assumption becomes f (x) < c for all x < n. Therefore, the attempted
proof that f (n) < c becomes:

f (n) = n
= n − 1 + 1
= f (n − 1) + 1
< c + 1.

This lets us avoid the error.

2-D Peak Finding: Algorithm 5

After class, several students asked about a variant of the peak-ﬁnding algorithm presented in Lec-
ture 1. In Lecture 1, the basic algorithm is:

1. Find the location (r, c) that has the maximum value of in the middle column.
2. Look at the values at locations (r, c − 1) and (r, c + 1).
If neither value is greater than
the value at (r, c), then it is a peak, so return it. Otherwise, recurse into one of the halves
containing a greater value.

The suggested modiﬁcation to the algorithm is:

1. Treat the middle column as a one-dimensional peak problem, and ﬁnd a one-dimensional
peak (r, c) using the algorithm given in lecture.

56.006 Intro to Algorithms
Recitation 1
2. Look at the values at locations (r, c − 1) and (r, c + 1).
If neither value is greater than
the value at (r, c), then it is a peak, so return it. Otherwise, recurse into one of the halves
containing a greater value.

September 9, 2011

This algorithm has been implemented in the same framework as the existing algorithms. The
implementation can be found in the support ﬁles for this recitation, and can be run in much the
same way. You are encouraged to try it out on your own.
We begin by analyzing the efﬁciency of the new algorithm. Let T (m, n) denote the runtime of
the algorithm when run on a matrix with m rows and n columns. The number of elements in the
middle column is m, so the time required to ﬁnd a one-dimensional peak is O(log m). Checking
the two-dimensional neighbors of the one-dimensional peak requires O(1) time. The recursive
call reduces the number of columns to at most (cid:98)n/2(cid:99), but does not change the number of rows.
Therefore, we may write the following recurrence relation for the runtime of the algorithm:
T (m, n) = O(1) + O(log m) + T (m, (cid:98)n/2(cid:99)).

Intuitively, the number of rows in the problem does not change over time, so the cost per
recursive call is always O(1) + O(log m). The number of columns n is halved at every step, so the
number of recursive calls is at most O(1 + log n). So we may guess a bound of O((1 + log m)(1 +
log n)). To show this bound more formally, we must ﬁrst rewrite the recurrence relation using
constants c1 , c2 > 0, instead of big-O notation:
T (m, n) ≤ c1 + c2 log m + T (m, (cid:98)n/2(cid:99)).
We now want to show that T (m, n) ≤ c3 (1+ log m)(1+ log n). We will show this by induction.
Assume that this is true for all n < k . We wish to show that this is also true for n = k . We may
perform the following substitution:
T (m, k) ≤ c1 + c2 log m + T (m, (cid:98)k/2(cid:99))
≤ c1 + c2 log m + c3 (1 + log m)(1 + log(cid:98)k/2(cid:99))
≤ c1 + c2 log m + c3 (1 + log m)(1 + log k − log 2)
≤ c1 + c2 log m + c3 (1 + log m)(1 + log k) − c3 (1 + log m) log 2
≤ c3 (1 + log m)(1 + log k) + (c1 + c2 log m − c3 (1 + log m) log 2)
So as long as it is possible to set c3 sufﬁciently high to make (c1 + c2 log m − c3 (1 + log m) log 2) ≤
0, we know that we will have proved the inductive step. To ensure that this is true, it is sufﬁcient
to set c3 = (c1 + c2 )/ log 2, resulting in the following inequality:
c1 + c2 log m − c3 (1 + log m) log 2 = c1 + c2 log m − ((c1 + c2 )/ log 2) · (1 + log m) log 2
= c1 + c2 log m − (c1 + c2 )(1 + log m)
= c1 + c2 log m − c1 − c1 log m − c2 − c2 log m
= −c1 log m − c2
≤ 0

66.006 Intro to Algorithms

Recitation 1

September 9, 2011

Hence, the runtime of the algorithm is O((1 + log m)(1 + log n)). This is faster than the algorithm
seen in Lecture 1, but is it also correct?
If the algorithm were incorrect, how would we ﬁnd a counterexample for it? We might begin
by looking at the differences between the algorithm presented in Lecture 1, which is known to
be correct, and the algorithm that we are studying. In the ﬁrst step, both algorithms look at the
central column; one ﬁnds the maximum, and one ﬁnds a peak. A good technique for constructing
a counterexample would be to begin with a matrix in which the 1D peak found by the algorithm
would not be the maximum of the central column. To that end, we will start with a 5 × 5 matrix
with a central column satisfying that property:

? ? 0 ? ?
? ? 2 ? ?
? ? 1 ? ?
? ? 0 ? ?
? ? 7 ? ?

The one-dimensional peak-ﬁnding algorithm will examine the very center of the matrix, and will
recurse on the upper half (because its neighbor to the south is strictly smaller). Therefore, it will
ﬁnd the one-dimensional peak 2. If 2 is a two-dimensional peak, the algorithm will (correctly)
return it. So if we want to construct a counterexample, 2 must have a neighbor that is strictly
greater. The algorithm must pick a half to recurse on; for simplicity, we force it to recurse on the
left:

? ? 0 0 0
? 3 2 0 0
? ? 1 0 0
? ? 0 0 0
? ? 7 0 0

This will cause the algorithm to examine only the two left-most columns of the matrix. Is it possible
to ensure that there are no peaks in those columns? Because there is a large value adjacent to this
region, it is:

1 2 0 0 0
2 3 2 0 0
3 4 1 0 0
4 5 0 0 0
5 6 7 0 0

Hence, we have an example where the algorithm will recurse into a subproblem that does not
contain any two-dimensional peaks. As a result, the algorithm cannot return a peak, and must
therefore be incorrect. This counterexample may be found in the ﬁle counter.py in the code
distributed with these recitation notes.

7MIT OpenCourseWare
http://ocw.mit.edu

6.006 Introduction to Algorithms
Fall 2011
 
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .

