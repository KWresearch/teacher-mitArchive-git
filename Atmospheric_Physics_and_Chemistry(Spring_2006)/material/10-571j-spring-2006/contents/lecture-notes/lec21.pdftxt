R. G. Prinn, 12.806/10.571: Atmospheric Physics & Chemistry, April 27, 2006
Estimating Surface/Internal Sources and Sinks

Figure by MIT OCW.

Measurement Equation 

 
 
In Lagrangian framework: 
 

Observing Station

s ', t ', j

(0,0)

Time

v(s ',t ')

s, t, l, k

Back Trajectory

Position

Velocity

Figure by MIT OCW.

xdt '

=

=

)

 

 from its initial condition 

(
)
y * 0, 0  is given by  

 
(
y s, t
Change in mole fraction 
(
(
(
)
)
)
y * 0, 0
y s, t
y * s, t
−
=
t
∫
0
s
s '
d
x
⎞
⎛
∫
ds '                 v
=⎜
⎟
t '
d
v
⎠
⎝
0
where x is the net chemical production (ignoring molecular diffusion and assuming 
perfect definition of back trajectory) 
 
Real measurements of y have errors  ε  
s
t
x
(
)
(
)
∫
0
y s, t
v
0
True 
value 
if 
perfect 
model 

Measure-
ment 
error 

Real 
measure-
ment 

=

ds '
+ ε

s, t

 
In discrete form: 
+∑
t
0
 
h x
y
=
iε
ij
i
j
j

 
In vector-matrix form for multiple observing sites i 
0
t
y Hx=
ε
+  
 
measurement equation
⎤
H h⎡
= ⎣
partial derivative (or observation) matrix, and  
 is the 
where 
⎦

ij

)
0
t
y , x . 
j
i

⎫
0
y
y
∂ ⎪
∂
compute in model
h
   
=
=
i
i
⎬
ij
t
x
x
∂ ⎪⎭
∂
j
j
y , x  denotes an estimate of  (
where  (
)
j
i
 
Eulerian models: Here x refers to grid points in the Eulerian model rather than to points 
along a back trajectory.  To utilize the above equations we must define: 
)
(
(
)
x
x grid point
x ref , grid point
=
−
)  
(
)
(
y
y grid point
y ref , grid point
−
=
where y(ref) is the mole fraction computed in a reference run using best available 
(x ref )
tx
estimates 
. 
 of the state vector prior to estimating 
 
The measurement equation expresses an apparent linear relation between the observation 
0y
tx
vector 
 and the unknowns contained in the 
.  It is an expression of the 
state vector 
forward problem.  The theory of the Linear Kalman Filter allows us to perform the 
tx
inverse problem in which we optimally estimate 
 given a model and a time series of 
0y
observations 
 at one or more sites. 
 
Optimal estimation – the “cost” function (J)
 
J
∂
Seek the estimate x of 
x
∂
( )p
=
probability distribution function of 
(Notation: 
)
(
)
(
0
t
No knowledge of p x ,  p y  
(a) 
 and observations  (
)
J = sum of squares of differences between predictions  (
(
) (
)
T
0
0
y Hx
y Hx
=
−
−
(called “least squares” minimization) 
 
 
)0
(
Know p y  and hence R  expectation ⎡
⎤
T
εε⎣
=
⎦   
(b)  
 that maximizes the conditional probability 
Then seek estimate x of 
(
)
(
)
T
1
0
0
−
y Hx R y Hx
−
−

(
)
t
0
p y x : 
1R −  weights each squared difference by inverse of  
(
variance of each relevant observation) 
(called “weighted least squares” minimization or “maximum likelihood”) 
(
)
(
)
0
t
Know p y  and p x  
Then seek x that maximizes 

(
)
0
t
p x y  which is defined from Bayes Theorem: 

tx

 that minimizes J (i.e. 

 
(c)  

 

=

0
) 
( ) ) 

tx

   

y Hx=

)0y  

J

=

 

 

 

t
o
p(x | y )

o
t
t
p(y | x )p(x )
o
p(y )
(called the “minimum variance Bayes estimate” – see below for the definition of J in 
this case for the Kalman filter) 
 

=

 

KALMAN FILTER 
 
Suppose we have an optimal estimate of the state vector available prior to 
o
ky
consideration of the kth measurement 
in a data series and we wish to obtain a 
a
a
kν  using this measurement. We can thus 
kx
new optimal estimate 
and its error 
propose in general 
 
 
 
 
′
f
a
ν = Κ ν + Κ ε
 
 
 
k k
k
k
k
 
 
 
 
and seek to specify the matrices  Κ k and 
 

′Κ k . Using the measurement equation (5) 

= Κ

′
f
x
k k

+ Κ

o
ky

a
x
k

 

 

 

 

 

 

 

 

 

 

 

 

k

(13) 

(14)                              

o

y

=

t

Hx

+

ε

 

(15)                              

(16)                              

 
t
f
f
+ ν , the demand that the random 
x=
= Η
x
y
kx
to define
, the definition
k
k
k
k
k
measurement errors have zero mean Ε ε
=k
( [ ] 0) , and finally, the demand that 
′Κ = Ι − Κ Η
a
Ε ν
=
( [
] 0)
estimations are unbiased
, we can show that
. Hence the new 
k
k
k
estimate is 
 
with an error 
 
 
and estimation error covariance matrix 
 

a
f
ν = Ι − Κ Η ν + Κ ε
)
(
k
k
k
k
k k

o
[y
k

f
x
k k

+ Κ

− Η

a
x
k

f
x
k

=

]

 

 

 

 

k

k

 

 

 

 

 

 

Τ
) ]  

(17) 

a
a
a
Ρ = Ε ν ν
[ (
k
k
k
 
Substituting (16) into (17), using the definition of the measurement error covariance 
= Ε ε ε T
k ]
R
[
, and demanding that measurement errors and state errors are 
matrix 
k
k
Τ
f
f T
= Ε ε ν
Ε ν ε
=
]
0
[
[ (
) ]
 we obtain 
uncorrelated (so that 
k k
k
k
 
 
 
a
Ρ =
 
 
k
 
We now use the criterion of optimality to determine Κ k . Since we will assume we 
op(y )
tp(x )
Κ k which minimizes the cost 
 and
, we will choose a value for 
know 
function  J  (equation 12) for the minimum variance Bayes estimate. Specifically 
 
 
 
 

νa T a
= Ε ν
J
[(
)
k ]
 
k
k
Ρa
k[
]
 
     = trace

f
Ι − Κ Η Ρ Ι − Κ Η
(
)
(
k
k
k
k

 (19) 

(18) 

+ Κ

 
 

T
)

Κ

R

T
k

 

 

 

 

 

 

 

 

 

 

k

k

k

Evaluating  ∂
J /
k
have 

Κ =
∂
k

0

 and solving for the so-called “Kalman Gain” matrix 

Κ k

 we 

 
f
T
f
T
Κ = Ρ Η Η Ρ Η +
[
k
k
k
k k
k
 
Substituting (20) into (18) then yields 
 

−
1
R ]
k

 

f
a
Ρ = Ι − Κ Η Ρ
k]
[
k
k
k

 

 

 

 

 

 

 

 

 

 

 

 

(20) 

(21) 

 
Finally, using the state space equation (7)   
 

=
x(t) M(t, t )x(t )
o
o

+ η
(t, t )   
o

 
f
kx
we then obtain the estimates of  needed in (15) and 
 

Ρf
k needed in (21) 

−
1

   

=f
x M xa
−
k
k 1 k
 
T
f
a
Ρ = Μ Ρ Μ +
−
−
−
k 1 k 1
k 1
k

 

kQ  
−
1

 

 

 

 

 

 

 

 

 

 

(22) 

(23)                              

 
−Ρa
a
= Ε η ηT
k 1x
− ]
[
Q
where 
k 1 are the optimal outputs from the previous 
 and 
, and 
−
−
−
k 1
k 1 k 1
iteration of the filter. From our earlier discussion (Section 3), Q  could represent 
random forcing in the system model due to transport model errors. 
 
To use the filter we must provide initial (a priori) estimates for x and P. Then 
a
a
−Ρ
(x ,
)
from any prior output estimates 
, we use measurement k information 
−
k 1
k 1
o
y , R
) together with equations (22), (23), (20), 
) and model information (
(
H , Q
k
k
k
k
a
Ρa
kx
and 
k  for inputs to the next step. The filter 
(15), and (21) to provide outputs 
equations are summarized in Table 1. 
Some intuitive concepts regarding the DKF are useful in understanding its 
 
−
Κ → Η 1
operation. First, from equation (20), the gain matrix 
(its “maximum” value) 
k
k
−
Κ → Ρ Ηf
T
1
k R
0→
as the measurement error covariance (noise) matrix 
 (its 
and 
,
kR
k
k
k
f
−a
x
kx
→ ∞
varies 
. Since the update in the state vector 
“minimum” value) as 
kR
k
Κ k
linearly with 
, it is clear that measurements noisy enough so that  much 
kR
Η Ρ Ηf
T
exceeds 
, will contribute much less to improvement of the state vector 
k
k k
estimation. 
T
In this respect we can usefully consider  Η Ρ Ηf
 
k as the error covariance matrix 
k k
for the measurement estimates 
. This emphasizes the importance of the weighting 
ky
and the distortions created if erroneous 
are used. Note 
of the data inherent in 
kR
kR
can include model error, mismatch error, and instrumental error as noted 
that 
kR
earlier. 
Κ
=
Ι
, we 
Second, using (21), and recognizing that the maximum value of 
 
kH
k
f
a
Ρ ≤ Ρ
see 
 with equality occurring for infinitely noisy measurements. Hence, the 
k
k
Ρk
error covariance matrix 
(whose diagonal elements are the variances of the state 
vector element estimates) decreases by amounts sensitively dependent on the 
measurement errors. 

Third, we note from (23), that random forcings η in the system (state-space) 
 
model [equation (7)], which are represented here by Q, will increase the 
extrapolated error covariance matrix  Ρf
k by amounts depending on the relative values 
T
Μ Ρ Μa
of 
and the extrapolation matrix 
k 1 in the absence of system (state-
−k 1Q
−
−
−
k 1 k 1
space) model noise. The inclusion of Q lessens the influence (or memory) of previous 
iterations in the filter operation. In the extreme, sufficiently large values of Q will 
Ρk
and hence 
prevent the capability of even non-noisy measurements to decrease 
increase the confidence in the state vector estimate. In other words excellent (non-
noisy) measurements are of little use if the system (state-space) model is very noisy 
η  introduced by random transport errors). 
(e.g., through random variations 

o
y
k

(1

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

a
Ρ =
k

Κ =
k

+ η
k

−
1

 

a
x
k

−

f
x
k

−

y )
k

 

=

t
H x
k k

 −
1
R )
k

=

H x  f
k
k

= Κ

o
(y
k

k

+ ε
k

; y
k

) f
− Κ Η Ρ
k
k
k

 

Equation 

=
x M x
−
−
k
k 1 k 1

= Μf
x
k

a
x 1  
−
−
k 1 k

f
T
f
T
Ρ Η Η Ρ Η +
(
k
k
k k
k

T
f
a
Ρ = Μ Ρ Μ +
−
−
−
k 1 k 1
k 1
k

 
 
Table 1: Kalman Filter Equations*
 
Definition 
 
Measurement equation (model) 
 
System (state) equation (model) 
 
State update   
 
Error Update   
 
 
Kalman gain update   
 
State time extrapolation 
 
Error time extrapolation 
 
System random forcing covariance   
 
Measurement error covariance 
 
Estimation error covariance   
 
Input measurement matrix   
 
Input system random forcing covariance 
 
Input state extrapolation 
 
Input measurement   
 
Input measurement error covariance 
 
→ − →f
− − −
(k 1) ,
estimate
 
 
 
 
Filter iteration  
→a
→ −
extrapolate
(k 1) ,
 
 
 
 
 
 
 
 
f
→
→ − − −
(k) ,
 
 
 
 
 
 
 
 
___________________________________________________________________ 
*A superscript a or superscript f denotes respectively the value before (f) or after 
(a) an update of an estimate using measurements, and k denotes the 
measurement number. In general, errors are assumed random with zero mean 
and measurement and estimation errors are uncorrelated. 
 

= Η = ∂
∂
ky / xk  
k

Ρ = Ε ν ν T
(
k
k k

= Ε ε ε T
(
k k

)  

= Ε η ηT
(
k )  
k

= Μ k  

= Q  
k

= R  k

kQ 1  
−

 

 

 

 

 o
ky

Q
k

R

k

 

 

 

 

 

 

 

 

 

 

 

 

)  

 

 

 

  

