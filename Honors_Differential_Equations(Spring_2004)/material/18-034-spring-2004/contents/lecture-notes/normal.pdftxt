18.700  JORDAN  NORMAL  FORM  NOTES 

These  are  some  supplementary  notes  on  how  to  ﬁnd  the  Jordan  normal  form  of  a  small 
matrix.  First we  recall  some  of  the  facts  from  lecture,  next we  give  the  general  algorithm  for 
ﬁnding  the  Jordan  normal  form  of  a  linear  operator,  and  then we will  see  how  this works  for 
small matrices. 

1.  Facts 
Throughout  we  will  work  over  the  ﬁeld  C  of  complex  numbers,  but  if  you  like  you  may 
replace  this  with  any  other  algebraically  closed  ﬁeld.  Suppose  that  V  is  a  C­vector  space  of 
→
V  is  a  C­linear  operator.  Then  the  characteristic 
dimension  n  and  suppose  that  T  :  V
polynomial  of  T  factors  into  a  product  of  linear  terms,  and  the  irreducible  factorization  has 
the  form 
cT (X ) = (X − λ1 )m1 (X − λ2 )m2  . . . (X − λr )mr , 
(1) 
for  some  distinct  numbers  λ1 , . . . , λr  ∈  C  and  with  each  mi  an  integer  m1  ≥  1  such  that 
m1  + · · ·
+ mr  = n.
Recall  that  for  each  eigenvalue  λi ,  the  eigenspace  Eλi  is  the  kernel  of  T  −  λiIV  .  We 
generalized  this  by  deﬁning  for  each  integer  k = 1, 2, . . .  the  vector  subspace 
E(X−λi )k  = ker(T  − λiIV )k . 
It  is  clear  that  we  have  inclusions 
Eλi  = EX−λi  ⊂ E(X−λi )2 
⊂  · · · ⊂ E(X−λi )e 
⊂  . . . . 
(3) 
Since  dim(V ) =  n,  it  cannot  happen  that  each  dim(E(X−λi )k )  <  dim(E(X−λi )k+1 ),  for  each 
k  =  1, . . . , n.  Therefore  there  is  some  least  integer  ei  ≤  n  such  that  E(X−λi )ei  = E(X−λi )ei+1 . 
As  was  proved  in  class,  for  each  k  ≥  ei  we  have  E(X −λi )k  =  E(X−λi )ei ,  and  we  deﬁned  the 
generalized  eigenspace  E gen  to  be  E(X−λi )ei .
λi 

(2) 

It was proved  in  lecture  that  the  subspaces E gen , . . . , E gen  give a direct  sum decomposition 
λr 
λ1 
of  V .  From  this  our  criterion  for  diagonalizability  of  follows:  T  is  diagonalizable  iﬀ  for  each 
i  = 1, . . . , r ,  we  have  E gen  =  Eλi .  Notice  that  in  this  case  T  acts  on  each  E gen  as  λi  times
λi 
λi 
the  identity.  This motivates  the deﬁnition of  the  semisimple  part  of T  as  the unique C­linear 
operator S  : V  → V  such that for each i = 1, . . . , r  and for each v ∈ E gen  we have S (v) = λiv .
We  deﬁned  N  = T  − S  and  observed  that  N  preserves  each  E gen  and  is  nilpotent,  i.e.  there 
λi 
exists  an  integer  e ≥ 1  (really  just  the maximum  of  e1 , . . . , er )  such  that N e  is  the  zero  linear 
λi 
operator.  To  summarize: 

(A)  The  generalized  eigenspaces  E gen , . . . , E gen  deﬁned  by 
λr 
λ1 
E gen  = {v ∈ V |∃e, (T  − λiIV )e (v) = 0}, 
λi 

Date :  Fall  2001. 

1 

(4) 

2 

18.700  JORDAN  NORMAL  FORM  NOTES 

give  a  direct  sum  decomposition  of  V .  Moreover,  we  have  dim(E gen )  equals  the  algebraic 
λi 
multiplicity  of  λi , mi . 

(B)  The  semisimple  part  S  of  T  and  the  nilpotent  part  N  of  T  deﬁned  to  be  the  unique 
C­linear  operators  V  → V  such  that  for  each  i = 1, . . . , r  and  each  v ∈ E gen  we  have 
λi 
S (v) = S (i) (v) = λiv , N (v) = N (i) (v) = T (v) − λiv , 
satisfy  the  properties: 
� 
� 
�
�

(1)  S  is  diagonalizable  with  cS (X ) = cT (X ),  and  the  λi ­eigenspace  of  S  is  E gen  (for  T ).
λi 
→
E gen 
(2)  N  is  nilpotent,  N  preserves  each  E gen  and  if  N (i)  :  E gen 
is  the  unique  linear 
λi 
λi 
λi 
ei−1 
ei
operator  with N (i) (v) = N (v),  then 
N (i)
N (i)
= 0. 
is  nonzero  but 
(3)  T  = S + N . 
(4)  SN  = N S . 
→
(5)  For  any  other  C­linear  operator  T �  : V
V ,  T �  commutes  with  T  (T �T  = T T � )  iﬀ  T �
commutes with both S  and N .  Moreover T �  commutes with S  iﬀ for each i = 1, . . . , r , 
(E gen ) ⊂ E gen 
we  have  T �
. 
λi 
λi 
(6)  If (S � , N � )  is any pair of a diagonalizable operator S �  and a nilpotent operator N �  such 
that  T  =  S �  + N �  and  S �N �  =  N �S � ,  then  S �  =  S  and  N �  =  N .  We  call  the  unique 
pair  (S, N )  the  semisimple­nilpotent  decomposition  of  T . 
� 
� 
(C)  For  each  i  = 1, . . . , r ,  choose  an  ordered  basis  B (i)  = (v (i) 
, . . . , v
� 
� 
1 
B = 
B (1) , . . . , B (r)  be  the  concatenation,  i.e. 
B =  v1  , . . . , vm1 , v1  , . . . , vm2 , . . . , v1  , . . . , v (r) 
(1) 
(2) 
(r)
(2) 
(1) 
. 
mr 
� 
� 
� 
� 
For  each  i  let  S (i) , N (i)  be  as  above  and  deﬁne  the mi  × mi  matrices 
D(i)  =  S (i) 
B(i) ,B(i)  , C (i)  = 
N (i)
B(i) ,B(i)  . 
Then  we  have  D(i)  = λiImi  and  C (i)  is  a  nilpotent  matrix  of  exponent  ei .  Moreover  we  have 
⎛ 
⎞
the  block  forms  of  S  and N : 
⎟⎟⎠

⎜⎜⎝

⎛ ⎜⎜⎜⎝ 
⎞
⎟⎟⎟⎠

C (1) 
. . .  0m1×mr 
0m1×m2 
C (2) 
. . .  0m2×mr 
0m2×m1 
. 
.
.
.
. 
.
.
.
. 
. 
.
.
C (r)
. . . 
0mr ×m1  0mr ×m2 
Notice  that D(i)  has a nice  form with  respect  to ANY basis B (i)  for E gen .  But we might hope 
λi 
to  improve  C (i)  by  choosing  a  better  basis. 

0m1×m2 
λ1Im1 
0m2×m1  λ2Im2 
. 
.
. 
.
. 
.
0mr ×m1  0mr ×m1 

. . .  0m1×mr 
. . .  0m2×mr 
.
.
.
.
. 
.
. . .  λr Imr 

mi )  of  E gen  and  let 
(i)
λi 

[S ]

B,B 

= 

[N ]
B,B 

= 

,

.

(6) 

(7) 

(5) 

(8) 

(9) 

3 

. 

J

a 

= 

(10) 


In  other  words, 

0 0 0 
1 0 0 
0 1 0 
.
.
.
. 
.
.
. 
.
.
0 0 0 
0 0 0 

. . .  0 0

. . .  0 0

. . .  0 0

. 
.
.
. 
.
.
. 
.
.
. . .  0 0

. . .  1 0


18.700  JORDAN  NORMAL  FORM  NOTES 
⎛ 
⎞
A  very  simple  kind  of  nilpotent  linear  transformation  is  the  nilpotent  Jordan  block,  i.e. 
→

TJa  : Ca  Ca  where  Ja  is  the matrix 
⎜⎜⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎟⎟⎠

(11) 
Jae1  = e2 , Jae2  = e3 , . . . , Jaea−1  = ea , Jaea  = 0. 
Notice  that  the  powers  of  Ja  are  very  easy  to  compute.  In  fact  J a  = 0a,a ,  and  for  d  = 
1, . . . , a − 1,  we  have 
a 
d e2  = ed+2 , . . . , Ja
d ea−d  = ea , J d ea+1−d  = 0, . . . , J d ea  = 0. 
d e1  = ed+1 , Ja
Ja
a 
a
d ) = span(ea+1−d , ea+2−d , . . . , ea ). 
Notice  that  we  have  ker(Ja
⎛ 
⎞ 
A  nilpotent matrix  C  ∈ Mm×m (C)  is  said  to  be  in  Jordan  normal  form  if  it  is  of  the  form 
⎜⎜⎜⎜⎝

⎟⎟⎟⎟⎠

. . .  0a1×at  0a1×b 
0a1×a2 
Ja1 
. . .  0a2×at  0a2×b 
0a2×a1 
Ja2 
. 
.
.
.
.
. 
. 
.
.
.
,

.

. 
. 
.
.
. . . 
0at×b 
0at×a1  0at×a1 
Jat 
. . .  0b×at 
0b×a1 
0b×a1 
0b×b 
where  a1  ≥ a2  ≥  · · · ≥ a
t  ≥ 2  and  a1  + · · · + at  + b = m.
� 
� 
We  say  that  a  basis  B (i)  puts  T (i)  in  Jordan  normal  form  if C (i)  is  in  Jordan  normal  form. 
B (1)
, . . . , B (r)  puts  T  in  Jordan  normal  form  if  each  B (i)  puts  T (i) 
We  say  that  a  basis  B = 
in  Jordan  normal  form. 

(12) 

C  =


(13) 


WARNING: Usually such a basis  is not unique.  For example,  if T  is diagonalizable,  then 
ANY  basis  B (i)  puts  T (i)  in  Jordan  normal  form. 

2.  Algorithm 
In this section we present the general algorithm for ﬁnding bases B (i)  which put T  in Jordan 
normal  form. 

Suppose  that  we  already  had  such  bases.  How  could  we  describe  the  basis  vectors?  One 
d (e1 ) and also that spane1 
observation is that for each Jordan block Ja , we have that ed+1  = Ja
and  ker(J a−1 )  give  a  direct  sum  decomposition  of  Ca .
a 
� 
� 
What  if  we  have  two  Jordan  blocks,  say 
, a1  ≥ a2 . 

(14) 

J  =

J
0a

2

a
1 
×

a1 

0a1×a2 
Ja2 

4 

18.700  JORDAN  NORMAL  FORM  NOTES 

This  is  the matrix  such  that 

J e1  = e2 , . . . , J ea1−1  = ea1 , J ea1  = 0, J ea1+1  = ea1+2 , . . . , J ea1+a2−1  = ea1+a2 , J ea1+a2  = 0. 
(15) 
Again  we  have  that  ed+1  = J de1  and  ed+a1 +1  = J dea1 +1 .  So  if  we  wanted  to  reconstruct  this 
basis,  what  we  really  need  is  just  e1  and  ea1+1 .  We  have  already  seen  that  a  distinguishing 
feature  of  e1  is  that  it  is  an  element  of  ker(J a1 )  which  is  not  in  ker(J a1−1 ).  If  a2  =  a1 , 
then  this  is  also  a  distinguishing  feature  of  ea1+1 .  But  if  a2  <  a1 ,  this  doesn’t  work.  In 
this  case  it  turns  out  that  the  distinguishing  feature  is  that  ea1+1  is  in  ker(J a2 )  but  is  not  in 
ker(J a2−1 ) + J (ker(J a2+1 )).  This motivates  the  following  deﬁnition: 
Deﬁnition  1.  Suppose  that  B  ∈  Mn×n (C)  is  a  matrix  such  that  ker(B e ) =  ker(B e+1 ).  For 
� 
� 
each  k = 1, . . . , e,  we  say  that  a  subspace  Gk  ⊂ ker(B k )  is  primitive  (for  k)  if 
�� 
� 
(1)  Gk  + ker(B k−1 ) + B  �ker(B k+1 ) = ker(B k ),  and 
(2)  Gk  ∩  ker(B k−1 ) + B  ker(B k+1 ) = 
.{0}
Here  we  make  the  convention  that  B 0  = In . 
� 
�It  is  clear  that  for  each  k  we  can  ﬁnd  a  primitive  Gk :  simply  ﬁnd  a  basis  for  ker(B k−1 ) + 
B  ker(B k+1 )  and  then  extend  it  to  a  basis  for  all  of  ker(B k ).  The  new  basis  vectors  will 
span  a  primitive Gk . 

Now we are ready to state the algorithm.  Suppose that T  is as  in the previous section.  For 
� 
� 
each  eigenvalue  λi ,  choose  any  basis  C  for  V  and  let  A  = [T ]C ,C .  Deﬁne  B  =  A − λiIn .  Let 
<  ku  ≤  n  be  the  distinct  integers  such  that  there  exists  a  nontrivial  primitive 
1  ≤  k1 
<  · · · 
subspace  Gkj .  For  each  j  = 1, . . . , u,  choose  a  basis  v [j ]1 , . . . , v [j ]pj 
for  Gkj .  Then  the 
� 
desired  basis  is  simply 
B (i)  =  v [u]1 , B v [u]1 , . . . , B u−1 v [u]1 , 
v [u]2 , B v [u]2 , . . . , B ku−1 v [u]2 , . . . , v [u]
, . . . , B ku−1 v [u]p1 , . . . , 
� 
pu 
v [j ]i , B v [j ]i , . . . , B kj −1 v [j ]i , . . . , v [1]1 , . . . , B k1−1 v [1]1 , . . . , 
v [1]p1 , . . . , B k1−1 v [1]p1 
. 
When  we  perform  this  for  each  i = 1, . . . , r ,  we  get  the  desired  basis  for  V . 

3.  Small  cases 

The algorithm above sounds more complicated than  it  is.  To  illustrate this, we will present 
a  step­by­step  algorithm  in  the  2 × 2  and  3 × 3  cases  and  illustrate  with  some  examples. 
3.1.  Two­by­two  matrices.  First  we  consider  the  two­by­two  case.  If  A  ∈  M2×2 (C)  is  a 
matrix,  its  characteristic  polynomial  cA (X )  is  a  quadratic  polynomial.  The  ﬁrst  dichotomy 
is  whether  cA (X )  has  two  distinct  roots  or  one  repeated  root. 
Two  distinct  roots  Suppose  that  cA (X ) = (X  − λ1 )(X  − λ2 )  with  λ1  =  λ2 .  Then  for 
each  i = 1, 2 we  form  the matrix Bi  = A − λiI2 .  By performing Gauss­Jordan  elimination we 
may  ﬁnd  a  basis  for  ker(Bi ).  In  fact  each  kernel  will  be  one­dimensional,  so  let  v1  be  a  basis 

�
18.700  JORDAN  NORMAL  FORM  NOTES 
5 
� 
�
for  ker(B1 )  and  let  v2  be  a  basis  for  ker(B2 ).  Then with  respect  to  the  basis  B = (v1 , v2 ),  we 
will  have 
λ1  0 
[A] = 
(16) 
. 
B 
0  λ2 
Said  a  diﬀerent  way,  if  we  form  the  matrix  P  = (v1 |v2 )  whose  ﬁrst  column  is  v1  and  whose 
� 
� 
second  column  is  v2 ,  then  we  have 
P −1 . 

A = P

(17) 

λ1  0 
0  λ2 

(20) 

λ1 
0 

0 
λ2 

[A]B,B  = 

(18) 
(19) 

Also  S  = A  and N  = 02×2 . 

To  summarize: 
span(v1 ) = Eλ1  = ker(A − λ1I2 ) = ker(A − λ1I2 )2  = · · · = E gen 
, 
λ1 
span(v2 ) = Eλ2  = ker(A − λ2I1 ) = ker(A − λ2I2 )2  = · · · = E gen 
. 
� 
� 
� 
� 
λ2 
Setting  B = (v1 , v2 )  and  P  = (v1 |v2 ), We  also  have 
0 
P −1 . 
λ1 
, A = P 
0 
λ2 
� 
� 
Now  we  consider  an  example.  Consider  the matrix 
38  −70 
21  −39 
. 
A = 
(21) 
The characteristic polynomial is X 2 − trace(A)X + det(A), which is X 2 + X − 12.  This factors 
as  (X + 4)(X − 3),  so we  are  in  the  case  discussed  above.  The  two  eigenvalues  are −4  and  3. 
� 
� 
First  we  consider  the  eigenvalue  λ1  = −4.  Then  we  have 
−70 
42 
21 − 35 
. 
B1  = A + 4I2  = 
(22) 
Performing Gauss­Jordan  elimination  on  this matrix  gives  a  basis  of  the  kernel:  v1  = (5, 3)† . 
� 
� 
Next  we  consider  the  eigenvalue  λ2  = 3.  Then  we  have 
35  −70 
B2  = A − 3I2  = 
21  −42 
. 
(23) 
Performing Gauss­Jordan  elimination  on  this matrix  gives  a  basis  of  the  kernel:  v2  = (2, 1)† . 
�� �� 
�� �� 
2 
5
E−4  = span 
, E3  = span 
. 
� 
� 
�
�
1 
3
P −1 , P  = 
. 

We  conclude  that: 

and  that 

(24) 

(25) 

A = P 

−4 
0

0 
3 

5 2 
3 1 

6 

18.700  JORDAN  NORMAL  FORM  NOTES 
One  repeated  root:  Next suppose that cA (X ) has one repeated root:  cA (X ) = (X − λ1 )2 . 
Again  we  form  the  matrix  B1  = A − λ1I2 .  There  are  two  cases  depending  on  the  dimension 
� 
� 
of  Eλ1  =  ker(B1 ).  The  ﬁrst  case  is  that  dim(Eλ1 )  =  2.  In  this  case  A  is  diagonalizable.  In 
fact,  with  respect  to  some  basis  B  we  have 
. 

λ1  0 
0  λ1 
But,  if  you  think  about  it,  this means  that A  has  the  above  form with  respect  to ANY  basis. 
In  other words,  our  original matrix,  expressed with  respect  to  any basis,  is  simply λ1I2 .  This 
case  is  readily  identiﬁed,  so  if  A  is  not  already  in  diagonal  form  at  the  beginning  of  the 
problem,  we  are  in  the  second  case. 

[A]B,B  = 

(26) 

0 0 
1 0 

S  = λ1I2  = 

� 
P −1 . 

In  the  second  case  Eλ1  has  dimension  1.  According  to  our  algorithm,  we  must  ﬁnd  a 
primitive  subspace  G2  ⊂  ker(B1
2 ) =  C2 .  Such  a  subspace  necessarily  has  dimension  1,  i.e. 
it  is  of  the  form  span(v1 )  for  some  v1 .  And  the  condition  that  G2  be  primitive  is  precisely 
that  v1  �∈ ker(B1 ).  In  other  words,  we  begin  by  choosing ANY  vector  v1  �∈ ker(B1 ).  Then  we 
deﬁne  v2  =  B (v1 ).  We  form  the  basis  B  = (v1 , v2 ),  and  the  transition  matrix  P  = (v1 v2 ).
|
� 
� 
� 
� 
Then  we  have  Eλ1  = span(v2 )  and  also 
λ  0 
λ  0 
P −1 . 
[A]B,B  = 
, A = P
1  λ 
1  λ 
� 
� 
� 
This  is  the  one  case  where  we  have  nontrivial  nilpotent  part: 
, N  = A − λ1I2  = B1  = P
λ  0 
0  λ 
� 
� 
Let’s  see  how  this  works  in  an  example.  Consider  the matrix  from  the  practice  problems: 
−5  −4 
1  −1 
A = 
(29) 
. 
The  trace  of  A  is  −6  and  the  determinant  is  (−5)(−1) − (−4)(1)  =  9.  So  cA (X ) =  X 2  + 
6X  + 9 = (X  + 3)2 .  So  the  characteristic  polynomial  has  a  repeated  root  of  λ1  =  −3.  We 
� 
� 
form  the matrix  B1  = A + 3I2 , 
−2  −4 
. 
2 
1
Performing Gauss­Jordan elimination (or just by  inspection) a basis  for the kernel  is given by 
(2, −1)† .  So  for  v1  we  choose  ANY  vector  which  is  not  a multiple  of  this  vector,  for  example 
� 
�� 
�� � �
� 
v1  = e1  = (1, 0)† .  Then  we  ﬁnd  that  v2  = B1v1  = (−2, 1)† .  So  we  deﬁne 
−2 
1  −2 
B =
1
, 
, P  = 
. 
� 
�
� 
� 
0 
1 
1 
0
−
−
0 
3  −3 
3  −3  P −1 . 
0
, A = P 
[A]B,B  = 
� 
� 
1
1
The  semisimple  part  is  just  S  = −3I2 ,  and  the  nilpotent  part  is: 
0 0 
P −1 . 
1 0 

B1  = A + 3I2  = 

Then  we  have 

N  = B1  = P 

(31) 

(32) 

(33) 

(27) 

(28) 

(30) 

18.700  JORDAN  NORMAL  FORM  NOTES 

7 

3.2.  Three­by­three matrices.  This is basically as in the last subsection, except now there 
are  more  possible  types  of  A.  The  ﬁrst  question  to  answer  is:  what  is  the  characteristic 
polynomial  of  A.  Of  course  we  know  this  is  cA (X )  =  det(X I3  − A).  But  a  faster  way  of 
calculating  this  is  as  follows.  We  know  that  the  characteristic  polynomial  has  the  form 
cA (X ) = X 3  − trace(A)X 2  + tX − det(A), 
(34) 
for  some  complex  number  t  ∈  C.  Usually  trace(A)  and  det(A)  are  not  hard  to  ﬁnd.  So  it 
only  remains  to  determine  t.  This  can  be  done  by  choosing  any  convenient  number  c  ∈  C 
other  than  c = 0,  computing  det(cI2  − A)  (here  it  is  often  useful  to  choose  c  equal  to  one  of 
the  diagonal  entries  to  reduce  the  number  of  computations),  and  then  solving  the  one  linear 
equation 
ct + (c 3  − trace(A)c  − det(A)) = det(cI2  − A), 
2
to  ﬁnd  t.  Let’s  see  an  example  of  this: ⎛
⎞ 
1  ⎠ . 
D = ⎝  −1
−
1 
2
1 
2
0  −1
3 
⎞ 
⎛
Here  we  easily  compute  trace(D) =  6  and  det(D) =  8.  Finally  to  compute  the  coeﬃcient  t, 
we  set  c = 2  and  we  get 
det(2I2  − A) = det ⎝  −1
1  −2  ⎠ = 0. 
0  −1
1 
1  −1 
0

(35) 

(36) 

(37) 

Plugging  this  in,  we  get 

(2)3  − 6(2)2  + t(2) − 8 = 0 
(38) 
or  t  =  12,  i.e.  cA (X ) =  X 3  − 6X 2  + 12X  − 8.  Notice  from  above  that  2  is  a  root  of  this 
polynomial  (since  det(2I3  − A) = 0).  In  fact  it  is  easy  to  see  that  cA (X ) = (X − 2)3 . 

Now that we know how to compute cA (X ) in a more eﬃcient way, we can begin our analysis. 
There are three cases depending on whether cA (X ) has three distinct roots, two distinct roots, 
or  only  one  root. 
Three  roots:  Suppose  that  cA (X ) = (X  − λ1 )(X  − λ2 )(X  − λ3 )  where  λ1 , λ2 , λ3  are 
distinct.  For  each  i  = 1, 2, 3  deﬁne  Bi  =  λ1I3  − A.  By  Gauss­Jordan  elimination,  for  each 
Bi  we  can  compute  a  basis  for  ker(Bi ).  In  fact  each  ker(Bi )  has  dimension  1,  so  we  can  ﬁnd 
⎞ 
⎛ 
|⎛ 
⎞ 
a  vector  vi  such  that  Eλ1  =  ker(Bi )  =  span(vi ).  We  form  a  basis  B  = (v1 , v2 , v3 )  and  the 
transition matrix  P  = (v1 |v2 v3 ).  Then  we  have 
[A]B,B  = ⎝  0 
⎠ , A = P ⎝  0 
⎠ P −1 . 
λ1  0 
0 
λ1  0 
0 
λ2 
λ2 
0 
0 
0 
0 
λ2 
We  also  have  S  = A  and N  = 0. 

0 
0 
λ2 

(39) 

8 

18.700  JORDAN  NORMAL  FORM  NOTES 
⎛
⎞ 
Let’s  see  how  this  works  in  an  example.  Consider  the matrix 
A = ⎝  8  −8 2  ⎠ . 
7  −7 2 
4  −4 1 
⎛
⎞ 
It  is  easy  to  see  that  trace(A) = 0  and  also  det(A) = 0.  Finally  we  consider  the  determinant 
of  I3  − A.  Using  cofactor  expansion  along  the  third  column,  this  is: 
det  −8 9  −2  ⎠ = −2((−8)4 − 9(−4)) − (−2)((−6)4 − 7(−4)) = −2(4) + 2(4) = 0.  (41) 
⎝ 
−6 
7  −2 
−4 
4
0
So  we  have  the  linear  equation 
13  − 0 ∗ 12  + t ∗ 1 − 0 = 0, t = −1. 
(42) 
Thus  cA (X ) = X 3  − X  =  (X + 1)X (X − 1).  So  A  has  the  three  eigenvalues  λ1  = −1, λ2  = 
0, λ3  =  1.  We  deﬁne  B1  =  A − (−1)I3 , B2  =  A, B3  =  A − I3 .  By  Gauss­Jordan  elimination 
⎛⎛ ⎞⎞ 
⎛⎛ ⎞⎞ 
we  ﬁnd 
E−1  = ker(B1 ) = span ⎝⎝  4  ⎠⎠ , E0  = ker(B2 ) = span ⎝⎝  1 
1  ⎠⎠
3
0 ⎛⎛  ⎞⎞ 
, 
2
E1  = ker(B3 ) = span ⎝⎝  2  ⎠⎠ . 
2 
⎞ 
⎛⎛  ⎞  ⎛  ⎞  ⎛  ⎞⎞ 
⎛ 
1 
⎠⎠ , P  = ⎝  4 
⎠ , ⎝  2 
⎠ , ⎝  1 
B = ⎝⎝  4 
⎠ . 
3  1  2 
3 
1 
2 
2 
1 
⎞ 
⎛ 
⎞ 
⎛ 
2 
1 
0 
2 
0 
1 
[A]B,B  = ⎝ 
⎠ , A = P ⎝ 
⎠ P −1 . 
−1 
−1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
Two  roots:  Suppose  that  cA (X )  has  two  distinct  roots,  say  cA (X ) = (X − λ1 )2 (X − λ2 ). 
Then  we  form  B1  = A − λ1I3  and  B2  = A − λ2I3 .  By  performing  Gauss­Jordan  elimination, 
we  ﬁnd  bases  for  Eλ1  =  ker(B1 )  and  for  Eλ2  =  ker(B2 ).  There  are  two  cases  depending  on 
the  dimension  of  Eλ1 . 

Then  we  have 

We  deﬁne 

(40) 

(43) 

(44) 

The  ﬁrst  case  is  when  Eλ1  has  dimension  2.  Then  we  have  a  basis  (v1 , v2 )  for  Eλ1  and  a 
⎛ 
⎞
⎛ 
⎞ 
basis  v3  for  Eλ2 .  With  respect  to  the  basis  B  = (v1 , v2 , v3 )  and  deﬁning  P  = (v1 v2 v3 ),  we 
|
|
[A]B,B  = ⎝  0  λ1  0  ⎠ , A = P ⎝  0  λ1  0  ⎠ P −1 . 
have 
0 
0 
λ1  0
λ1  0
0  λ2 
0  λ2 
0
0
In  this  case  S  = A  and N  = 0. 

(45) 

18.700  JORDAN  NORMAL  FORM  NOTES 

9 

and 

(46) 

(47) 

(48) 

Also  we  have 

The  second  case  is  when  Eλ1  has  dimension  2.  Using  Gauss­Jordan  elimination  we  ﬁnd 
∈  E gen
a  basis  for  E gen  =  ker(B1
2 ).  Choose  any  vector  v1 
which  is  not  in  Eλ1  and  deﬁne 
λ1 
λ1 
v2  = B1v1 .  Also using Gauss­Jordan  elimination we may ﬁnd  a vector  v3  which  forms  a basis 
for  Eλ2 .  Then  with  respect  to  the  basis  B  = (v1 , v2 , v3 )  and  forming  the  transition  matrix 
⎛ 
⎞
⎛ 
⎞ 
P  = (v1 |v2 v3 ),  we  have 
|
[A]B,B  = ⎝  1  λ1  0  ⎠ , A = P ⎝  1  λ1  0  ⎠ P −1 . 
0 
0 
λ1  0
λ1  0
⎛ 
⎞
⎛ 
⎞ 
0  λ2 
0  λ2 
0
0
[S ]B,B  = ⎝  0  λ1  0  ⎠ , S  = P ⎝  0  λ1  0  ⎠ P −1 , 
0 
0 
λ1  0
λ1  0
⎞
⎛ 
⎛ 
⎞ 
0  λ2 
0  λ2 
0
0
[N ]B,B  = ⎝  1 0 0  ⎠ , A = P ⎝  1 0 0  ⎠ P −1 . 
0 0 0 
0 0 0 
0 0 0 
0 0 0 
⎛
⎞ 
Let’s  see  how  this  works  in  an  example.  Consider  the matrix 
3  −1  ⎠ . 
A = ⎝  0
0 
0
3
−1 
0
2
It  isn’t  hard  to  show  that  cA (X ) = (X − 3)2 (X − 2).  So  the  two  eigenvalues  are  λ1  = 3  and 
⎞ 
⎛
⎛
⎞ 
λ2  = 2.  We  deﬁne  the  two matrices 
1  −1  ⎠ . 
B1  = A − 3I3  = ⎝  0
0  −1  ⎠ , B2  = A − 2I3  = ⎝  0
0 
0 
1
0
0
0
−1 
0  −1 
−1 
0
0
By  Gauss­Jordan  elimination  we  calculate  that  E2  =  ker(B2 )  has  a  basis  consisting  of  v3  = 
(0, 1, 1)† .  By  Gauss­Jordan  elimination,  we  ﬁnd  that  E3  =  ker(B1 )  has  a  basis  consisting  of 
⎛
⎞ 
(0, 1, 0)† .  In  particular  it  has  dimension  1,  so  we  have  to  keep  going.  We  have 
2  = ⎝  1 0 1  ⎠ . 
0 0 0 
B1
1 0 1 
By Gauss­Jordan elimination (or inspection), we conclude that a basis consists of (1, 0, −1)† , (0, 1, 0)† . 
2 )  which  isn’t  in  E3  is  v1  =  (1, 0, −1)† .  We  deﬁne  v2  =  B1v1  = 
⎛⎛ 
⎞ ⎛ ⎞ ⎛ ⎞⎞ 
⎛ 
⎞ 
A  vector  in  E gen  =  ker(B1
3 
(0, 1, 0)† .  Then  with  respect  to  the  basis 
B = ⎝⎝  0  ⎠ ⎝  1  ⎠ , ⎝  1  ⎠⎠ , P  = ⎝  0  1 1  ⎠ . 
1 
0 
0 0 
0
1
, 
−1 
−1 0 1
⎞ 
⎛ 
⎞
⎛ 
1 
0
[A]B,B  = ⎝  1 3 0  ⎠ , A = P ⎝  1 3 0  ⎠ P −1 . 
3 0 0 
3 0 0 
0 0 2 
0 0 2 

we  have 

(50) 

(49) 

(51) 

(52) 

(53) 

10 
We  also  have  that  ⎛ 
[S ]B,B  = ⎝  0 
3  0  0 
3 
0 
⎛ 
0 
2 
0 
[N ]B,B  = ⎝  1 
0  0  0 
0 
0 
0 
0 
0 

18.700  JORDAN  NORMAL  FORM  NOTES 
⎞ 
⎞ 
⎛ 
⎞ 
⎛ 
⎠ P −1  = ⎝  −1 −1 
⎠ , 
⎠ , S  = P ⎝  0 
3  0  0 
3  0  0 
−1 
3 
0 
3 
⎞ 
⎞ 
⎛ 
⎞ 
⎛ 
2 
0 
2 
0 
0 
⎠ . 
⎠ P −1  = ⎝  1 
⎠ , N  = P ⎝  1 
0  0  0 
0  0  0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 

(54) 

(55) 

One  root:  The  ﬁnal  case  is  when  there  is  only  a  single  root  of  cA (X ),  say  cA (X ) = 
(X  − λ1 )3 .  Again  we  form  B1  =  A1  − λ1I3 .  This  case  breaks  up  further  depending  on  the 
dimension  of  Eλ1  = ker(B1 ).  The  simplest  case  is  when  Eλ1  is  three­dimensional,  because  in 
this  case  A  is  diagonal  with  respect  to  ANY  basis  and  there  is  nothing more  to  do. 

Dimension  2  Suppose  that  Eλ1  is  two­dimensional.  This  is  a  case  in  which  both  G1  and 
G2  are  nontrivial.  We  begin  by  ﬁnding  a  basis  (w1 , w2 )  for  Eλ1 .  Choose  any  vector  v1  which 
is not  in Eλ1  and deﬁne  v2  = B1v1 .  Then ﬁnd a vector  v3  in Eλ1  which  is NOT  in  the  span of 
⎛ 
⎞
⎛ 
⎞ 
v2  .  Deﬁne  the  basis  B = (v1 , v2 , v3 )  and  the  transition matrix  P  = (v1 v2 v3 ).  Then  we  have 
|
|
[A]B,B  = ⎝  1  λ1  0  ⎠ , A = P ⎝  1  λ1  0  ⎠ P −1 . 
0 
0 
λ1  0
λ1  0
0  λ1 
0  λ1 
0
0

(56) 

Notice  that  there  is a Jordan block of  size 2 and a Jordan block of  size 1.  Also, S  = λ1I3  and 
we  have N  = B1 . 
⎛
⎞ 
Let’s  see  how  this  works  in  an  example.  Consider  the matrix 
0  ⎠ . 
⎝ 
−1  −1 
0 
1  −3
0  −2 
0
It  is  easy  to  compute  cA (X ) =  (X + 2)3 .  So  the  only  eigenvalue  of  A  is  λ1  = −2.  We  deﬁne 
⎛
⎞ 
B1  = A − (−2)I3 ,  and  we  have 
B1  = ⎝  1  −1 0  ⎠ . 
1  −1 0 
0 
0
0

(57) 

A =

(58) 

By  Gauss­Jordan  elimination,  or  by  inspection,  we  see  that  E−2  =  ker(B1 )  has  a  basis 
((1, 1, 0)† , (0, 0, 1)† ).  Since  this  is  2­dimensional,  we  are  in  the  case  above.  So  we  choose  any 
vector  not  in  E−2 ,  say  v1  =  (1, 0, 0)† .  We  deﬁne  v2  = B1v1  =  (1, 1, 0)† .  Finally,  we  choose  a 
⎛⎛ ⎞ ⎛ ⎞ ⎛ ⎞⎞ 
⎞ 
⎛ 
vector  in  Eλ1  which  is  not  in  the  span  of  v2 ,  say  v3  = (0, 0, 1)† .  Then  we  deﬁne 
B = ⎝⎝  0  ⎠ , ⎝  1  ⎠ , ⎝  0  ⎠⎠ , P  = ⎝  0 1 0  ⎠ . 
1 1 0 
0 
1
1
1 
0 0 1 
0
0

(59) 

We  have 

⎛ 
⎞
⎛ 
18.700  JORDAN  NORMAL  FORM  NOTES 
[A]B,B  = ⎝ 
0  ⎠ , A = P ⎝ 
−
2  −2
0 
0
1
0  −2 
0
We  also  have  S  = −2I3  and N  = B1 . 

⎞ 
0  ⎠ P −1 . 
−
2  −2
0 
0
1
0  −2 
0

11 

(60) 

(61) 

Dimension One In the ﬁnal case for three by three matrices, we could have that cA (X ) = 
(X − λ1 )3  and  Eλ1  =  ker(B1 )  is  one­dimensional.  In  this  case  we  must  also  have  ker(B1
2 )  is 
2 )  and  then  choose  ANY 
two­dimensional.  By  Gauss­Jordan  we  compute  a  basis  for  ker(B1
2 ).  We  deﬁne  v2  =  B1v1  and  v3  =  B1v2  =  B1
2v1 . 
vector  v1  which  is  not  contained  in  ker(B1
⎛ 
⎞
⎛ 
⎞ 
Then  with  respect  to  the  basis  B  = (v1 , v2 , v3 )  and  the  transition  matrix  P  = (v1 v2 v3 ),  we 
|
|
[A]B,B  = ⎝  1  λ1  0  ⎠ , A = P ⎝  1  λ1  0  ⎠ P −1 . 
have 
0 
0 
λ1  0
λ1  0
1  λ1 
1  λ1 
0
0
We  also  have  S  = λ1I3  and N  = B1 . 
⎛
⎞ 
Let’s  see  how  this  works  in  an  example.  Consider  the matrix 
A = ⎝  1
0  ⎠ . 
5  −4 0 
1
2  −3 3 
The  trace  is  visibly  9.  Using  cofactor  expansion  along  the  third  column,  the  determinant  is 
+3(5 ∗ 1 − 1(−4))  =  27.  Finally,  we  compute  det(3I3  − A)  =  0  since  3I3  − A  has  the  zero 
vector  for  its  third  column.  Plugging  in  this  gives  the  linear  relation 
(3)3  − 9(3)2  + t(3) − 27 = 0, t = 27. 
(63) 
So we have  cA (X ) = X 3 − 9X 2 + 27X − 27.  Also we  see  from  the above  that X  = 3  is a  root. 
In  fact  it  is  easy  to  see  that  cA (X ) = (X − 3)3 .  So  A  has  the  single  eigenvalue  λ1  = 3. 
We  deﬁne  B1  = A1  − 3I3 ,  which  is  ⎛
⎞ 
B1  = ⎝  1  −2 0  ⎠ . 
2  −4 0 
2  −3 0 
By  Gauss­Jordan  elimination  we  see  that  E3  =  ker(B1 )  has  basis  (0, 0, 1)† .  Thus  we  are  in 
⎞ 
⎛
the  case  above.  Now  we  compute 
2  = ⎝  0
0  ⎠ . 
0 
0
0
0
B1
1  −2 0 

(62) 

(64) 

(65) 

2 ) has basis ((2, 1, 0)† , (0, 0, 1)† )). 
Either by Gauss­Jordan elimination or by inspection, we see that ker(B1
So  for  v1  we  choose  any  vector  not  in  the  span  of  these  vectors,  say  v1  =  (1, 0, 0)† .  Then  we 
deﬁne  v2  = B1v1  =  (2, 1, 2)†  and  we  deﬁne  v3  = B1v2  = B1
2v1  =  (0, 0, 1)† .  So  with  respect  to 

12 

18.700  JORDAN  NORMAL  FORM  NOTES 
⎛⎛ ⎞ ⎛ ⎞ ⎛ ⎞⎞ 
⎞ 
⎛ 
the  basis  and  transition matrix 
B = ⎝⎝  0  ⎠ , ⎝  1  ⎠ , ⎝  0  ⎠⎠ , P  = ⎝  0 1 0  ⎠ , 
1 2 0 
0 
1
2
⎛ 
⎞
⎛ 
⎞ 
0 2 1 
1 
2
0
[A]B,B  = ⎝  1 3 0  ⎠ , A = P ⎝  1 3 0  ⎠ P −1 . 
3 0 0 
3 0 0 
0 1 3 
0 1 3 
We  also  have  S  = 3I3  and N  = B1 . 

we  have 

(66) 

(67) 

