18.034  SOLUTIONS  TO  PROBLEM   SET  8  

Due  date:  Friday,  April  23  in  lecture.   Late  work  will  be   accepted  only  with   a   medical  note   or  for 
another  Institute­approved reason. You  are  strongly  encouraged to  work  with  others,  but   the  ﬁnal 
write­up  should  be  entirely  your  own  and   based  on  your   own  understanding. 
This  problem  set  is  essentially  a  reading  assignment.  I had  originally  intended to  present  the   material 
in  the  problem  set  in  lecture.  However,  this  material  is  less  relevant  to  other   topics  in  this  course, 
and  there  is  no   time   to  present  it  in  lecture.  Each  student  will  receive  10  points  simply  for   reading 
through  this  problem  set.  At  several  places,  you   are  asked   to   work­through  and  write­up  details  in 
the  derivation.  You  will  turn  in  this  write­up  to  be  graded. 
· · · 
Problem  1(30  points)  A   collection   of   N 
identical   particles   of  mass  m0  =
=  mN −1  =  m
are  allowed  to  oscillate   about  their  equilibrium  positions.  Denote  by  x0 (t), x1 (t), . . . , xN −1 (t)  the 
displacement  of  the   masses  from  equilibrium.  The  mass   m0  is   connected  to  a   motionless  base  by   a 
spring.  It   is  also  connected  to  mass  m1  by  a   spring.  For  i  = 1, . . . , N  − 2,   mass   mi  is   connected 
� 
to   mass  mi−1  and  mass  mi+1   by  springs.   Finally,  mass  mN −1  is  connected  to  mass   mN −2  and  to  a 
motionless  base   by  a  spring.  Each  spring  is  identical   and  has   spring  constant  κ.  Time  is  measured 
⎡ 
⎤
κ/m  equals   1.  The  equations   of  motion  for  this  system  are,

in  units  so   that  the   frequency  
⎥⎥⎥⎦

⎢⎢⎢⎣


, 

x0 
x1 
. . . 
xN −1 

(AN )i,j  = 

In  other  words, 

⎧ ⎪⎪⎨ ⎪⎪⎩

1,
j =  i +  1, 
−2,
j =  i, 
j =  i − 1,

1, 
0, 

otherwise

In  this  problem,   you  will   determine   the   general  solution  of  this  system  of   linear   diﬀerential  equa­
tions.  Because   of  the  special  nature  of  this  problem  (namely,  every  eigenvalue   of  A has   multiplicity 
1),  it  is  not  necessary  to  reduce   to  a  system  of   ﬁrst­order  linear   diﬀerential  equations. 
(a),  Step  1(10  points)  In  this   step,  you   will  ﬁnd,  for  each  integer  n,  all   the  roots   of   a  polynomial 
pn  that   is  deﬁned  inductively  below.   Please  read  the  whole  derivation.   In  your   writeup,  only  ﬁl l   in 
the  missing  steps  in  the   third  paragraph  below.  You   do  not  need  to  ﬁll  in   the   missing  steps  in   the 
other  paragraphs  (but  you  are  encouraged  to  work  them  out  for   yourself ). 
1 

where  AN  is  the  N × N  matrix,  

AN  = 

x��  =  AN x,   x  = 
⎡  −2
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣

0 
0
1
1  −2
0 
1
1  −2
1 
0
1  −2 
0
0
.
.

.
.  
. 
.
.
.
.
.
.
. 
0
0
0
0 
0 
0
0
0

⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦


0
0 
. . . 
0
0 
. . . 
0
0 
. . . 
0
0 
. . . 
.
. 

.
.
.
.
.
. 
. 
. . .  −2
1

1  −2

. . . 

⎧⎨ ⎩

Let  u  be  an  indeterminate.  Let  p0 , p1 , .  .  .   , pn , .   .  .  be   a  sequence  of  real  polynomials   in  u  that  satisfy 
the  linear  diﬀerence   equation, 

pn+2  − 2upn+1  +  pn  = 0,  
p0  = 1,  
p1  = 2u 
− 1,   and  p3 
− 4u.  (In  
where  n  varies  among   all  nonnegative   integers.  In  particular,  p2 
= 8u
= 4u2 
3
is  a  polynomial   with  integer   coeﬃcients.)  In  this  step,  you  will  prove  that  a  closed 
fact,  each  pn 
� 
formula  for  pn  is, 
n
(u  − cos(kπ/(n   +  1))). 
k=1 

pn  = 2n 

(1) 

Conclude  that, 

As  a   reality  check,  observe  this   gives,  
2(u  − cos(π/2))  = 2u 
p1  = 
4(u  − cos(π/3))(u  − cos(2π/3))  = 4(u  − 1/2)(u  +  1/2)  = 4u − 1,  
√
√
p2  = 
2
2)  = 8u − 4u 
p3  =  8(u  − cos(π/4))(u  − cos(2π/4))(u  − cos(3π/4))  = 8(u  − 1/
2)(u)(u  +  1/
3
Write   up  a  careful  proof  of  the  missing  steps  in  the  next  paragraph.  To  prove  the  formula, 
observe  that  it  suﬃces   to   check  when  |u|  >  1;  two   polynomials   in   u  that  agree  for   inﬁnitely   many 
values  of
are  equal  (you  don’t  have  to  write  a  proof   of  this).  Assume  that
=
+
r ,
−
n
n 
−
u
p
C
r
C
n 
+
+ 
where
,
.
=
=
on
For 
that  do  not  depend
are  continuous  functions   in
−
−
−
r , r
, r , r
C , C
r
r
n
u
+
+
+
prove  that  r  satisﬁes  the  characteristic  polynomial,  
2 r  − 2ur  +  1  = 0.  
�

r =  a  ± b,   a  =  u,  b  = 
u − 1.
± 
2
�

Plugging this  in  to  the  equations  for  p0  and  p1 ,  conclude  that, 
C+  +  C
= 1, 
−
(C+  +  C− )a  +  (C+  − C− )
b  = 2a, 
�

Solve  this  system  of  linear  equations  to  get, 
C+  = (a  +  b)/2b,  
=  −(a  − b)/2b
− 
C
�

Therefore  one  solution  of  Equation  1   for  |u| >  1  is,  
u − 1.  
(a  +  b)n+1  − (a  − b)n+1
]/2b,  a  =  u,  b  = 
pn  = [
2
But,  of  course,   there   is  a  unique   solution:  for  each  n  ≥ 2,  pn  can  be  determined  recursively  in   terms 
of  pn−1  and  pn−2 .  Therefore,   this   is  the  solution  of  Equation  1.  
Solution:  Substituting  in  pn  =  C+ r+  +  C− r−  to  the  ﬁnite  diﬀerence  equation  yields, 
n
n
C+ (r+  − 2ur+  +  1)r+  +  C− (r−  − 2ur−  +  1)r−  = 0, 
n
2 
n 
2 
for  every  n  ≥ 0.  Clearly,  if  r±  − 2ur +  1  =  0,  this  equation  is  satisﬁed.  This  is  all   that  is  required 
±
2
for  the  rest   of  the   argument:  the   strategy  is  to  ﬁnd  one  solution  of  the  ﬁnite  diﬀerence  equation, 
not   every   solution  (in   the  last  step,  we  observe   there  is  a  unique  solution).  However,  it   is  reasonable 
to   check  that  every  solution  of  the  ﬁnite   diﬀerence   equation  of   the  form  pn  =  C+ r+  +  C− rn  does 
n
−
satisfy  r±  − 2ur +  1   =   0.  Suppose   that  r+  − 2ur+  +  1  =  0.  Plugging  in  n  = 0, 1  above  gives, 
± 
2
2 
C− (r−  − 2ur−  +  1)  =  −C+ (r+  − 2ur+1), 
2
2
C+ (r+  − 2ur+  +  1)(r+  − r− ) = 0 
2
2 

�
Therefore  either  C+  = 0  or  r =  r+  (or  both).  If   C+  = 0,  then  the   conditions  p0  = 1, p1  = 2u  imply 
−
that   C = 1  and  r = 2u.  But  then  p2  = 4u2  which contradicts   that  p2  = 4u2  − 1.   If   r =  r+ ,  then 
− 
− 
−
=  p0 rn  =  r+ .  Therefore  r+  = 2u.  But  then  p2  = 4u2  which  again  contradicts 
pn  = (C+  +  C− )r+ 
n
n
+ 
that   p2  = 4u2  − 1.  This  contradiction  proves  that  r+  − 2ur+  +  1  =  0.  A  similar   argument  proves  
2
that  r−  − 2ur +  1  =  0.  This   argument  also   proves   that  r+  =  r− .
− 
2
� 
Because  r+  =  r and  both  are  solutions  of  r2  − 2ur  +  1,  
− 
r =  a  ± b,   a  =  u,   b  = 
u − 1,
± 
2
up  to   relabeling  r+  and  r− .  For  any  choice  of   C+  and  C− ,  the  sequence  pn  =  C+ r+  + C− rn  satisﬁes  
n
−
� 
the  ﬁnite  diﬀerence   equation   pn+2  − 2upn+1  +  pn  =  0.   The  conditions  p0  = 1  and  p1  =
2u  imply 
that, 
C+  +  C
= 1, 
−
(C+  +  C− )a  +  (C+  − C− )
b  = 2a, 
� 
In  particular,  (C+  − C− )b  =  a.  Therefore, 
(C+  +  C− )b  = 
b, 
(C+  − C− )b  =  a, 
� 
Solving,  2bC+  =  a  +  b  and  2bC =  −(a  − b),  i.e., 
− 
C+  = (a  +  b)/2b,  
=  −(a  − b)/2b
− 
C
� 
For  this  choice   of  C+  and  C− ,  pn  satisﬁes   all  three  conditions.  Therefore   one  solution   is, 
u − 1.  
(a  +  b)n+1  − (a  − b)n+1
]/2b,  a  =  u,  b  = 
pn  =  C+ r+  +  C− r n  = [
n 
− 
2

End  of  the   solution 
√
For  the  rest  of  this   part,   do   not  write  up  the  missing  details.  The  equation  above  also 
u2  − 1  is  interpreted  as  a  complex   number.  Let 
|
|
makes  sense  and  is   correct  if  u <  1,  where   b  = 
|u| <  1.  Then  pn (u) = 0   iﬀ  (a
b)n+1  = (a  − b)n+1 .  Both  a  + b  and  a  − b  are   nonzero  (because  the 
+
imaginary  part  of  each  complex  number  is  nonzero).  Therefore  (a + b)n+1  = (a − b)n+1  iﬀ   one  of  the 
following   equations   holds,  
(a  +  b) =  ζk (a  − b),  ζ  =  e
i2πk/(n+1)
, 
as  k  varies  among  the  integers  0, .   .   .   , n.   Of   course  for  k  = 0,   the  equation  is   a + b  =  a − b,  i.e.  b  = 0. 
Since  b  = 0,  this  case  is  ruled  out. 
Let  k  = 1, .  .  .  , n.  If  (a  +  b) =  ζk (a  − b),  then 
(ζk  − 1)a  = (ζk  +  1)b. 

Squaring  both   sides,  

i.e., 

Solving   for  u2  gives, 

i.e., 

Simplifying, 

(ζk  − 1)2 a  = (ζk  +  1)2 b2  , 
2
(ζk  − 1)2 u  = (ζk  +  1)2 (u  − 1). 
2
2

2(2ζk )u  =  ζ 2  +  2ζk  +  1,
2
k 
2 u  = (ζk  +  ζ −1  +  2)/4.
k 
−1  =  e 
i2πk/(n+1)   +  e−i2πk/(n+1)  
ζk  +  ζk
3 

= 2 cos(2πk/(n   +  1)). 

�
�
�
Also,  2 +  2 cos(θ) = 4 cos2 (θ/2).   Therefore,  u2  =  cos2 (πk/(n +  1)).  So  u =  ± cos(πk/(n +  1)).   But 
of  course   − cos(πk/(n +  1))  =   cos(π(n +  1   − k)/(n +  1)).   Therefore,  every  root  of  pn (u)  = 0  with 
|u| < 1  is  of  the  form  cos(πk/(n +  1))  for   some  integer   k = 1, . . . , n.  

Reversing  the  steps  above,  cos(πk/(n + 1))  is  a  root  of   pn  for  every  k = 1, . . . , n.   Also,  for   0  < θ < π , 
the  function  cos(θ) is  strictly  decreasing.  Therefore  the   real  numbers   cos(πk/(n + 1))  are  all  distinct. 
This  gives  n  distinct real  roots  of  the   degree   n  polynomial  pn .  Since  a   polynomial  of   degree  n  has 
at   most  n  real  roots,  counted  with   multiplicity,   every  root  of   pn  is  of  the  form  cos(πk/(n + 1)),  and 
each  of  these  roots  has  multiplicity  1.  It  is  straightforward  to   compute  that   the   leading  coeﬃcient 
�	
of  pn  is  2n .  Therefore, 
n
k=1 

(u − cos(πk/(n +  1))). 

pn  = 2n 

PN +2  = det 

−1
0 
λ +  2  
0
−1  λ +  2 
−1
0 
−1  λ +  2 
−1 
0 
−1  λ +  2 
0 
0
.
.  
.
. 
.
.
.
. 
.
.
.
. 
0  
0
0
0
0
0
0
0 

(b),  Step  2(10  points)  For   each  integer   N  ≥  1,  deﬁne   PN (λ)  to   be  the  characteristic   polynomial  
det(λIN ×N  − AN ).  Deﬁne   P0 (λ)  = 1.  Using  cofactor  expansion  along  the  ﬁrst  row,   prove 
that  the  sequence  of   polynomials  P0 , P1 , P2 , . . .  satisﬁes  Equation   1  where  u =  +  1. 
λ
2 
Solution:  By  direct  computation,   P0  = 1  and  P1  =  λ +  2.  Let  N  ≥  0  and  consider  PN +2 .  By 
⎤
⎡ 
cofactor  expansion   along the  ﬁrst  row, 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣

0 
0
. . . 
0 
0
. . . 
0 
0
. . . 
0  
0
. . . 
.  
.
.
.
.
.
.
. 
. 
−1

. . .  λ +  2 
−1  λ +  2

. . . 
⎤
⎡  ∗
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎤

⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣

∗ 
∗
∗
∗	 λ +  2 
−1
0 
−1 
−1  λ +  2 
∗ 
−1  λ +  2 
∗ 
0	
. 
.
. 
.
. 
.
.
.
.
.
. 
.
∗ ∗ 
⎡

0 
0
0
0
0
0 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣

∗ 
∗ ∗ 
∗ 
−1	 ∗
−1 
0 
0  ∗  λ +  2 
−1 
−1  λ +  2 
0  ∗
.
. 
.
. 
.  
. 
.
. 
.
. 
. 
.  
0  ∗ 
0  
0
0  ∗ 
0
0 
where  the  asterisks  are   just  placeholders  for  then  entries  to  be  deleted   in  computing  the  determi­
nant.  The  ﬁrst  determinant  above,   is  simply  PN +1 . For  the   second  determinant,  perform  cofactor 
4 

∗

∗
. . . 
0 
0
. . . 
0
0
. . . 
0
0
. . . 
.  
.
.
.
.
.
.
. 
. 
−1
. . .  λ +  2 
−1  λ +  2 
. . .

∗

∗
. . . 
0
0
. . . 
0
0  
. . . 
0 
0
. . . 
.  
.
.
.
.
.
.
. 
. 
−1

. . .  λ +  2 
−1  λ +  2

. . . 

equals,


(λ +  2)det 

+1det 

det 

expansion  along  the  ﬁrst  column   (which   has  only  1  nonzero   entry),
 ⎤
⎡  ∗ ∗ 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣

∗ 
∗ 
∗
∗ 
. . . 
−1  ∗
−1 
0 
0
0
. . . 
0  ∗  λ +  2 
−1 
0  
0
. . . 
−1  λ +  2 
0  ∗
0  
0
. . . 
= 
.
. 
.
. 
.
.  
.
.  
. 
.
. 
.	
.
.
.
. 
.
. 
. 
.  
. 
0  ∗ 
−1

⎡

⎤

. . .  λ +  2 
0
0  
−1  λ +  2

0  ∗ 
0  
0
. . .

⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣

∗
∗ 
∗ 
∗ ∗ 
∗ 
. . . 
∗ 
∗
∗ 
∗ ∗ 
∗ 
. . . 
∗	 ∗  λ +  2 
−1 
0 
0
. . . 
−1  λ +  2 
∗ ∗ 
0
0
. . . 
.
.  
.
. 
.
.  
.
. 
.  
.
.
.
. 
.	
.
. 
. 
.  
. 
.
. 
∗ ∗ 
−1
. . .  λ +  2 
0  
0
−1  λ +  2 
∗ ∗ 
0
0  
. . . 
⎧⎨ 
This  is  −PN .  Therefore,  PN +2  = (λ +  2)PN +1  − PN .  Deﬁning  u = 
1 +
 λ 
2 ,  this   is, 
PN +2  − 2uPN +1  +  PN  = 0, 

⎩

P0  = 1,

P1  = 2u


−1det 

PN (λ) =

End   of  the  solution 
N� 
It  then  follows  that  the   characteristic   polynomial  of   AN  is, 
(λ +  2(1  − ck )),  ck  =  cos(πk/(N +  1)). 
k=1 
Therefore  the   eigenvalues  of  AN  are  λk  =  −2 +  2ck  =  −(2  sin(πk/(N +  1)))2 ,  for  k = 1, . . . , N . 
(c),  Step  3(10  points)  Let  the  integer  N  ≥ 1  be  ﬁxed.  For  each  integer   k = 1, . . . , N ,  consider  the 
⎡ ⎢⎢⎢⎣ 
⎤
eigenvalue  λk .  Denote, 
⎥⎥⎥⎦

x(k),0 
x(k),1 
. . . 
x(k),N −1 
the  eigenvector  of   AN  with  eigenvalue  λk  that  is   normalized  by  the   condition  x(k),0  =  1.  
Prove  that  the   sequence  x(k),0 , x(k),1 , . . .  satisﬁes  Equation  1   where  u =  ck .  It  then  follows 
that  for  each  n = 0, . . . , N − 1,  the   entry  x(k),n  is   given  by, 
� 
n
(cos(πk/(N +  1))  − cos(π l/(n +  1)))  . 
l=1 

x(k),n  = 2n 

x(k)  = 

⎧⎨ ⎩

Solution:  Because  x(k)  is  an  eigenvector   with  eigenvalue   λk ,  Ax(k)  =  λk x(k) .  Expanding  this  out, 
(−2)x(k),0 
x(k),1  = 
+ 
λk x(k),0 , 
+ (−2)x(k),n+1  +  x(k),n+2  =  λk x(k),n+1 , n = 0, . . . , N − 3,  
x(k),n 
(−2)x(k),N −1 
x(k),N −2  + 
= 
λk x(k),N −1 

5 

In  particular,  x(k),0  = 1   by  deﬁnition,  and  the   ﬁrst  equation   gives  x(k),1  = 2 +  λk  = 2ck .  Therefore, 
for  n = 0, . . . , N − 3,  we   have,
⎧⎨ ⎩

x(k),n+2  − 2ck x(k),n+1  +  x(k),n  = 0, 

x(k),0  = 1,

x(k),1  = 2ck

This  is  Equation  1  with  u =  ck .  Therefore,  for   n = 0, . . . , N − 1, 
� 
n
(cos(πk/(N +  1))  − cos(π l/(n +  1)))  . 
x(k),n  = 2n 
l=1 
As  a   double­check,   observe   that  we  also  have  −2ck x(k),N −1  + x(k),N −2  =  0.  This  is  equivalent  to  the 
equation  x(k),N  = 0,  where   x(k),N  is  deﬁned  by   the  product  formula  above.  In  the  product  formula, 
the  factor  for  l =  k  is  cos(πk/(N +  1))  − cos(πk/(N +  1))  = 0.   Thus  x(k),N  = 0,  as  it  must. 
End  of  the  solution 
(d),  Step   4( 0  points)  Having  computed  the  eigenvalues  λk  and  an  eigenvector  x(k) ,  it  is  now 
straightforward  to  solve  the  linear  system  of   diﬀerential  equations.  Let  k be   an  integer,   k = 1, . . . , N . 
Let  zk (t) be  a  real­valued   function.  Then  zk (t)x(k)  is  a  solution  of  the  linear  system  iﬀ, 
�
zk (t)x(k)  =  A(zk (t)x(k) ) =  zk (t)Ax(k)  =  zk (t)λk x(k) . 
Therefore  zk (t)x(k)  is  a  solution  iﬀ, 
zk (t) =  λk zk (t) =  −(2 sin(πk/(N +  1)))2 zk (t).  
�
Denote  ωk  = 2 sin(πk/(N +  1)).  The   general  solution   of   this  diﬀerential  equation  is, 
zk (t) =  Ak  cos(ωk t) +  Bk  sin(ωk t). 
� 
Therefore  the   general  solution  of  the  linear   system  of   diﬀerential  equations  is, 
N
(Ak  cos(ωk t) +  Bk  sin(ωk t))x(k) ,  ωk  = 2 sin(πk/(N +  1)), 
� 
�T 
k=1 
n
(cos(πk/(N +  1))  − cos(π l/(n +  1)))
 .
l=1 

�

x(k),0 , . . . , x(k),N −1 

x(t) =

x(k)  = 

,  x(k),n  = 2n 

Problem  2(10  points)  The  general  linear   system  of  ﬁrst  order   diﬀerential  equations  not  necessarily 
with  constant  coeﬃcients   is, 

x� (t) =  A(t)x(t). 
Apart   from  the  existence  and  uniqueness  theorem  proved   in  the  beginning   of  the   semester,  there  is 
little  we  can  say  about  the   solution  of  this  diﬀerential   equation.  However,  if  A(t) has  a  special  form, 
sometimes  we  can  get  an  equation  for  the  solution  that  is  a  bit  more  explicit.  In  this  problem  you 
will  deduce  such  a  result. 
(a),  Step   1(5  points).   Let  g(λ, t) be  a  C∞  function  deﬁned  on  the  (λ, t)­plane.   Let  S  be   an  n × n 
real  matrix.  Assume   that  the   eigenvalues  of   S ,  λ1 , . . . , λk  are   all   real  and  that  none  of  the  eigenspace 
is  deﬁcient,  i.e.  for  i = 1, . . . , k ,  the   generalized  eigenspace  V gen  equals  the  usual  eigenspace  Vλi .
λi 
Deﬁne  g(S, t) to  be  the  unique   matrix  such  that  for  each   i = 1, . . . , k ,  g(S, t) preserves  the   eigenspace 
Vλi  and  the  restriction  of  g(S, t) to  this   subspace  equals  g(λi , t) times  the  identity matrix.  
6 

g(µj , t) = 

� 
� 
Let  v  be  an  n­vector  and  consider  g(S, t)v.  Write  up   the   proof   of   the  following  fact.  The 
function  of  t,  g(S, t)v   is  C∞  and  for  every  nonnegative  integer  r , 
r 
∂ r
d (g(S, t)v) = 
(S, t)v. 
∂ tr
r 
dt
Solution:  It  suﬃces  to  prove  that  for   some   choice  of   ordered  basis  B ,  every  entry  of  the  matrix 
A = [g(S, t)]B,B  is  a  C∞  function,  and  dr 
dtr  (Ai,j ) is   the   (i, j )­entry  of  [(  ∂ r 
∂ tr  g)(S, t)]B,B .
Because  S  is  diagonalizable,  there   exists  an  ordered  basis  B  = (v1 , . . . , vn ) such  that  each   vj  is  an 
eigenvector  for  S .  For  j = 1, . . . , n,  deﬁne  µj  by  S vj  =  µj vj .  By  deﬁnition, [g(S, t)]B,B  is  a  diagonal  
matrix  whose  (j, j )­entry  is  g(µj , t).  Therefore  every  entry  is  a   C∞  function,  and 
∂ r g 
rd
r  (µj , t),
r 
dt
∂ t
which  is  the  (j, j )­entry  of  the   diagonal  matrix  [(  ∂ r 
∂ tr  g)(S, t)]B,B . 
End  of  the   solution 
(b),  Step  2(5  points)  Next,   let  A be  an  n × n  real  matrix  whose  eigenvalues   are   all  real,  but  such  
that  the  eigenspaces   of  A  might  be   deﬁcient.   Denote  by  λ1 , . . . , λk  the  eigenvalues  of   A. For  each 
i = 1, . . . , k ,  denote   by  V gen  the   generalized  eigenspace   of  A.  Deﬁne  S  to  be  the   unique  matrix  such 
λi 
that  for  each  i = 1, . . . , k ,  S  preserves   V gen  and  the  restriction   of  S  to  V gen  is   λi  times   the   identity 
matrix.  Deﬁne   N  =  A − S .  Then  S  commutes  with  N ,  the   matrix   S  is   diagonalizable,  and   the 
λi 
λi 
matrix  N  is  nilpotent,  i.e.  N e+1  = 0  for  some  integer  e. 
�
�
�  1 
Deﬁne  g(A, t)  to  be  the   matrix, 
e
m!
m=0 

∂m 
m  g 
∂λ
� 
� 
Let  v  be  an  n­vector  and  consider  g(A, t)v.  Write  up  the   proof   of   the  following  fact.  The 
function  of  t,  g(A, t)v  is  C∞  and  for  every  nonnegative  integer  r , 
∂ r
r 
d (g(A, t)v) = 
(A, t)v. 
∂ tr
r 
dt
(Hint:  Use  the   equality   of  mixed   partial   derivatives,  and  reduce  to  part  (a)  ). 
Solution:  First  of  all,  by  part  (a),   each  of  the   functions  wm (t) = ( ∂λm  g)(S, t)v   is   a  C∞  function. 
∂m
Therefore  each  (1/m!)N mwm (t)  is  a  C∞  function  (after   all,  N m  is  a  constant  matrix).  Thus   the 
ﬁnite  sum  g(A, t)v  is  a  C∞  function. 
� 
�
The  equation, 
r 
∂ r
d (g(A, t)v) = 
(A, t)v,
g 
r 
∂ tr
dt
is  proved  by  induction  on  the   positive  integer  r .  Let  r  =  1.  By  part  (a),   each  of  the  functions 
� �
�� 
� 
�
∂m( ∂λm  g)(S, t)v  is  C∞  and, 
∂m 
∂m 
∂ 
d
(S, t)v  = 
(S, t)v . 
m  g 
g 
∂λm 
dt 
∂λ
∂ t 
� �� 
� 
� 
�
Because  g is  C∞ ,  mixed  partial  derivatives   are   equal,  i.e. 
∂m 
∂m 
d 
∂ g 
(S, t)v . 
(S, t)v  = 
∂λm 
∂λm 
∂ t 
dt 

g 

g(A, t) = 

N m

(S, t). 

g 

g 

7 

N m

dr 
dtr 

∂ g 
.∂ t 

Therefore, 

Therefore, 

h(A, t)v  =

� � 
∂ g 
(A, t)v. 
∂ t 

� �� 
∂ g 
(S, t)v  = 
∂ t 

� 
(g(A, t)v) =  �  1 
e
∂m 
d 
m!
∂λm 
dt
m=1 
This  is  the   case  r  = 1  in  the  induction   above. 
By  way   of  induction,   suppose  that  r  >  1  and  that  the  result  is  proved  for  r − 1.  Deﬁne  h(λ, t) = 
� 
� 
Then,  by  the  induction  hypothesis, 
dr−1 
∂ r−1h 
(A, t)v. 
∂ tr−1 
dtr−1 
�  � 
dr−1  d
� 
� 
� 
� 
∂ g 
(A, t)v  = 
g(A, t)v  =
g(A, t)v  = 
dtr−1  dt
∂ t 
dr−1 
∂ r−1h 
∂ r g 
(A, t)v. 
(A, t)v  = 
∂ tr−1 
dtr−1 
∂ tr 
The  second  equality  follows  from  the  case   r  = 1  and  the  fourth   equality  follows   from  the   induction 
hypothesis.   Therefore  the   result  is  proved  for  r .  So  the  result  is   proved  by  induction  on  r . 
End   of  the  solution 
(c),  Step   3( 0   points)  Let  g(λ, t) and   h(λ, t) be   C∞  functions  on  the   (λ, t)­plane.  Let  A  be   an  n × n 
matrix  all   of  whose  eigenvalues  are   real.  Then   g(S, t),  g(A, t),   h(S, t) and  h(A, t) all   commute  with 
each  other.  Moreover,  
·
·
g(S, t)h(S, t) = (g h)(S, t),   and  g(A, t)h(A, t) = (g h)(A, t).

dr−1 
dtr−1 

h(A, t)v  = 

To   prove  that  g(S, t)  and  h(S, t)  commute   with  each   other,  that  they  commute  with  g(A, t)  and 
· 
h(A, t),  and  that  g(S, t)h(S, t)  = (g h)(S, t),  it  suﬃces  to  prove  this  after   restricting   to  V gen  for
λi
every  i  = 1, .   .  .  , k .  But  for  any C∞  function  f (λ, t),  the  restriction  of  f (S, t) to  V gen   is  simply  f (λi , t)
λi 
times  the  identity  matrix.  Denote  this   by  f (S, t)i .  Therefore   f (S, t)i  commutes  with  every  linear 
operator  on  V gen  .  In  particular,  g(S, t)i  and  h(S, t)i  commute   with  each  other   and  with  g(A, t)i  and
λi 
·
·
h(A, t)i .  Also,  g(S, t)i  h(S, t)i  is  simply  g(λi , t) h(λi , t) times  the  identity matrix.   This  is   the  same 
·
· · ·
·
as  (g h)(S, t)i .  Therefore   g(S, t)
h(S, t) = (g h)(S, t).
· 
To   prove  that  g(A, t)  and  h(A, t)  commute  with  each  other  and  that  g(A, t)h(A, t)  = (g h)(A, t),
it  suﬃces  to  prove   this  after  restricting  to  V gen  for  every  i  = 1, .  .   .  , k .  Denote  by  Si  and  Ni  the 
� 
�
�
�
λi 
�  1 
�  1 
restrictions  of  S  and  N  respectively  to   V gen .  By  deﬁnition, 
λi 
e
e
∂ l 
· 
(S, t)i 
g(A, t)ih(A, t)i  = 
N l 
∂λl g 
i 
m!
l!
m=0 
l=0 
All  of  the  matrices  in  this  last  sum  commute.  Therefore  we   may  rearrange   the   (ﬁnite)  sum  in  
g(A, t)ih(A, t)i  =  � �  1 1 
whatever  order   we  wish, 
e
e
N l+m ( ∂ l g 
∂λl  · 
l! m!
i 
�  p � �
l=0  m=0 
�  1  �  p
Deﬁne  p  =  l +  m.  For  p  > e,   N p  =  0.  Therefore  the  sum  equals, 
∂ p−mg ∂mh 
e
m  ∂λp−m  · 
N p 
p!
i 
∂λm 
m=0 
p=0 
8 

)(S, t)i . 
� 
(S, t)i . 

g(A, t)ih(A, t)i  = 

∂mh 
∂λm 

∂m 
∂λm 

(S, t)i . 

N m 
i 

h 

But  of  course,


� �  � 
∂ p−mg ∂mh 
p
· 
p
m  ∂λp−m 
∂λm 
�

�

m=0 

.

N p 

Hence, 

g(A, t)ih(A, t)i  = 

·
(S, t)i  = (g h)(A, t)i .

∂ p (gh) 
=

∂λp 
� 
e 1 
∂ p (gh) 
p!
∂λp 
p=0 
Therefore  g(A, t) h(A, t) = (g h)(A, t).  Because  g h  =  h · g ,  it  follows   that  g(A, t) commutes   with
·
·
·
h(A, t). 
(d),  Step  4(0  points)  Let  Lλ  be  a  linear  diﬀerential  operator   of  order  n  +  1  in  t, 
∂ n+1 
∂ n 
Lλ  =
∂ tn  +  · · · +  a1 (λ, t) +  a0 (λ, t),
∂ 
+  an (λ, t)
∂ tn+1 
∂ t 
where  each  of  the  functions  aj (λ, t) is  a  C∞  function.  Let  y(λ, t) be  a  C∞  function  that  is  a   solution 
of  Lλy(λ, t) = 0. 
Let  A  be  a   real  n  × n  matrix   all  of   whose   eigenvalues   are  real.  Let  v  be   any  n­vector.  Then   the 
function  y(t) =  y(A, t)v  is  a  C∞  function  in  t  that  is   a  solution  of   the  linear   system  of   diﬀerential  
equations, 
LAy(t) = 0, 

i.e., 

Assume  that 

dn+1 
dn 
· · ·
dtn+1  y(t) + 
+  a0 (A, t)y(t) = 0. 
y(t) + 
an (A, t)
dtn 
(e),  Step   5(0  points)  Let  t0  be  a   real  number. For  s  ≥  t0 ,  let  �
K (λ; s,  t)  be  the  unique   solution  of 
This  follows  immediately  from  Steps  1–3.

⎧⎪⎪⎪⎪⎪⎪⎨

��
the  initial   value  problem,
LλK (λ; s,  t)
= 0,

K (λ; s,  s) = 0, 
⎪⎪⎪⎪⎪⎪⎩

��
. . . 
∂ n−1 
= 0, 
K (λ; s,  s)
∂ tn−1 
�
∂ n 
= 1
K (λ; s,  s)
∂ tn 
C∞  function  in  all  3   variables  λ,  s  and  t. 
K (λ; s,  t) is  a 
Let  f (t)   be  an  n­vector  whose  entries  are   continuous  functions  of   t.  By  the  same   arguments  as  in 
⎧⎪⎪⎪⎨ ⎪⎪⎪⎩

the  handout  on  Green’s  functions,  the  unique   solution  of   the  IVP, 
LAy(t) =  f(t),

y(t0 ) = 0,

.
. . 
y(n) (t0 ) = 0 
�

�
y(t) = 
K (A; s,  t)f(s)ds,  
t0 
where  the  integration  is   done   entry­by­entry.  

is  given  by,


t 

9 

(f ),  Step   6(0  points)  This   is   somewhat  beside  the  point,   but  if   g(λ,   t)  is   an  analytic  function  in   λ 
� 
whose  expansion  about  the  origin  is, 
∞
g(λ,  t) = 
cr (t)λr  , 
r=0 
and  if  all  the   eigenvalues  of   A  are  within  the  radius   of  convergence   of  this  power   series,  then  the 
� 
following   series   converges  to  g(A,  t), 
∞
cr (t)Ar  . 
r=0 
In  particular,  this  holds  if  g(λ,  t) is  a  polynomial  in  λ  or  if  g(λ,   t) =  e . 
λt

10 

