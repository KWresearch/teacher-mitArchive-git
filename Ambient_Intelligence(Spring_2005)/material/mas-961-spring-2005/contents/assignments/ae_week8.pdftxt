Amy Eastment 

Week 8 

In reading this, I’m left wondering – what kinds of leaps and bounds in technology do we 
need to make AR feasible? The processing power to render these images and the 
wearable hardware just isn’t there yet, at least not in a convenient form.  

As the article points out – the key is getting these to work at the right time and right place. 
We don’t even have the technology to “monitor” the situation – no perfected machine 
vision, no perfected voice-recognition, and presumably similar failings in parsing other 
types of sensors. And even if we did get that part straightened out, how does one create a 
system robust enough to know how to react to every possible scenario? Until we improve 
the accuracy on both counts, AR will be next to useless.  

The article mentions augmenting other sensory inputs besides sight – I’d be curious to see 
what kinds of things could be conceived and developed (off of the top of my head: 
sounds or scents that serve as reminders to the user would be interesting, and scents in 
particular would be a good way to create a minimum-distraction conveyance of 
information). What sorts of tangible augmentations could be done, and would they be 
more or less effective than a visible display? Is the visible display the most effective for 
all tasks? The examples given (using AR when repairing something, etc) are good ideas, 
but surely there are scenarios where extra visual data would make things more confusing, 
rather than less confusing. 

Unlike most of the other technologies we’ve seen, this one seems to be offering the user 
privacy – they are the only person who is able to see/hear/etc. the information from their 
AR devices. It’s not being broadcast from multiple screens or following them along on 
the network. And in the right format, the information presented by AR doesn’t seem like 
it would be terribly distracting (if kept to the bare minimum and overlaying only a small 
portion of the real space, I’d venture that it would probably remain in the background). 
On the other hand, it sounds like AR would also isolate every user from one and other – 
every user gets their own private space, and presumably it would only be visible by them. 
It’s much different from a computer screen – we can always look over someone’s 
shoulder to see what they’re doing, or get the reassurance when they turn away from 
whatever they are doing to make eye contact with us, that they are paying attention to us. 
With AR, there’s no way for the boss to know for sure that his employees aren’t playing 
solitaire, no way to truly look someone in the eye and know for sure that they are looking 
back at you. Or if they are, what sorts of things crop up on their display about you? It’s a 
privacy that comes with good and bad consequences.  

