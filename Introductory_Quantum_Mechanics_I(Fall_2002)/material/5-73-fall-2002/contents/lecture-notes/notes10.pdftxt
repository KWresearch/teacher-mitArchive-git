5.73 Lecture #10

10 - 1

Matrix Mechanics

should have read CDTL pages 94-121
read CTDL pages 121-144 ASAP

Last time: Numerov-Cooley Integration of 1-D Schr. Eqn. Defined on a Grid.
2-sided boundary conditions
nonlinear system - iterate to eigenenergies (Newton-Raphson)
So far focussed on ψ(x) and Schr. Eq. as differential equation.
{Ei, ψi(x)} ↔ V(x)
Variety of methods

Often we want to evaluate integrals of the form
a    is “mixing coefficient”
∫
overlap of special
ψ
φ
=
{φ} is complete set of “basis
* ( )
x
ψ on standard
functions {φ}
functions”

( )
x dx

i

a

i

expectation values
transition moments

∫

OR
* ˆx nφ jdx ≡ x n(
φ i

)

ij

There are going to be elegant tricks for evaluating these integrals and relating one
integral to others that are already known.  Also “selection” rules for knowing
automatically which integrals are zero:  symmetr y, commutation rules

Today: begin matrix mechanics - deal with matrices composed of these integrals -
focus on manipulating these matrices rather than solving a differential
equation - find eigenvalues and eigenvectors of matrices instead
(COMPUTER “DIAGONALIZATION”)

* Perturbation Theory:  tricks to find approximate eigenvalues of infinite matrices

* Wigner-Eckart Theorem and 3-j coefficients: use symmetry to identify and inter-
relate values of nonzero integrals

* Density Matrices:  information about state of system as separate from
measurement operators

H
I
G
H
L
I
G
H
T
S

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

10 - 2

First Goal: Dirac notation as convenient NOTATIONAL simplification
It is actually a new abstract picture
(vector spaces) — but we will stress the utility (ψ ↔ |  〉  relationships)
rather than the philosophy!

Find equivalent matrix form of standard ψ(x) concepts and methods.
∫
j dx = δ ij
ψ i
*ψ

1.  Orthonormality 

2.  completeness

ψ(x) is an arbitrary function 

(expand ψ)

A.  Always possible to expand ψ(x) uniquely in a COMPLETE BASIS SET {φ}
∑
a iφ i (x)
ψ (x) =
i

∫
a i = φ i
*ψdx

*

mixing coefficient — how to get it?
*∫
φ j
left multiply by

(expand Bψ)

B.

ψ
ψ
φ
φ
ˆ
 in terms of { }.
 in { } since we can write 
Always possible to expand 
B
{ }
= ∑
φ
φ
ˆ
What are the  b j
So simplify the question we are asking to 
?
B i
b j j
j

  Multiply by 

∫

*
φ
j

∫
* ˆBφ idx ≡ B ji
b j = φ j
∑
ˆBφ i = B jiφ j
j

note counter-intuitive pattern of
indices.  We will return to this.

*  The effect of any operator on ψi is to give a linear combination of ψj’s.

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

10 - 3

3.  Products of Operators
ˆA ˆB(
)φ i = ˆA ˆBφ i
(
) = ˆA B jiφ j
∑
j
can move numbers (but not operators)  around freely
∑
∑ ˆAφ j =
∑
φ k
= B ji
note repeated index
k
j
j
)
(
∑
∑
φ k =
k
j, k

B jiA kj

AB(
)

φ k

ki

=

A kjB ji

*  Thus product of 2 operators follows the rules of matrix multiplication:
ˆA ˆB acts like A B

Recall rules for matrix multiplication:









indices are
A row,column

order
matters!

must match # of columns on left to # of rows on right
×(
) →
) ⊗
×(
×(
N N
N N
N N
×(
) ⊗
×(
) →
×(
)
1
1
1 1
N
N
" column vector"
" row vector"
×(
×(
) →
) ⊗
1
1
N
N
row
column
vector
vector

×(
N N

)

)

a matrix
a number

a matrix!

Need a notation that accomplishes all of this memorably and compactly.

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

Dirac’s bra and ket notation

Heisenberg’s matrix mechanics

ket

 is a column matrix,  i. e.  a vector






a1
a 2
M
a N






10 - 4

ψ (x) =

ai
*ψdx
φ i

contains all of the “mixing coefficients” for ψ expressed in some basis set.
[These are projections onto unit vectors in N-dimensional vector space.]
Must be clear what state is being expanded in what basis
]
∫[
∑
φ i (x)
i
*ψdx
φ1
φ2
* ψdx
M
* ψdx
φN

φ ← bookkeeping device (RARE)

* nothing here is a function of x

* ψ expressed in φ basis

* a column of complex #s

ψ =















∫
∫

∫

OR,  a pure state in its own basis
0


1


0




M
0



one 1,  all others 0

φ2 =

φ

bra

ψ

(

…

)

N

b

 is a row matrix

b b
2,
1
ψ
*
 contains all mixing coefficients for 
]
[
∫∑
φ
*
i
i

φ ψ
i

x
( )

dx

=

x
* ( )

*

{ }
φ
*
 in 
 basis set
ψ
(This is  *  of 

(x) above)

The  *  stuff is needed to make sure  ψ ψ = 1 even though  φ i ψ  is complex.

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

10 - 5

The symbol 〈a|b〉,  a bra–ket, is defined in the sense of product of
(1 × N) ⊗ (N × 1) matrices → a 1 × 1 matrix:  a number!

Box Normalization in both ψ and  〈 | 〉  pictures
∫
1 = ψ * ψdx
∫(
∑
φ i
ψ =
*ψdx
i
(
∑
j

)
φ i
)φ j
φ jψ * dx
*

ψ* =

expand both in
φ basis
(


φ ψ
*
dx
i

∫

∫

)
c.c.

∫




φ ψ
j

*

dx

=

1

=

1

∑
,
i j

=

ψ ψ
*
dx

∫
∫∑
j
real, positive #’s

φ ψ
*
dx
j

2

φ φ
*
j

dx
i

δij

p

forces 2 sums to
collapse into 1

We have proved that sum of |mixing coefficients|2 = 1.  These are called
“mixing fractions” or “fractional character”.

now in 

 picture

ψ ψ

=






∫
∫
φ ψ
φ ψ
L
dx
dx
*
*
2
1
3
444444
444444
1
2
row vector:  “bra”
















∫
φ ψ
*
dx
1
∫
φ ψ
*
dx
2
M1 24 34
column 
vector “ket”











=

∫∑
j

2

φ ψ
*
dx
j

same result

[CTDL talks about dual vector spaces — best to walk before you run.  Always
translate 〈 〉  into ψ picture to be sure you understand the notation.]

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

10 - 6

Any symbol   〈 〉     is a complex number.
Any symbol |  〉 〈 |   is a square matrix.

(
again  ψψ ψ φ ψ φ
=
1
2


φ ψ
)
1

φ ψ
…

2

M
=
ψ φ φ ψ ψψ
i
i





=

1

=

∑
i

 

(
what is   φ1 φ1 = 1   0  …   0

1 unit matrix
1




0
)




M


0



=

1






1 0 L


0 0


O
M

a shorthand for
specifying only
the important
part of an
infinite matrix

what is 













O
  
“completeness” or “closure” involves insertion of 1 between any two symbols.

unit or identity
matrix = 1

1 0
0 1

φ i φ i

  =

∑
i

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

10 - 7

Use 1 to evaluate matrix elements of product of 2 operators, AB (we know how
to do this in ψ picture).
square matrix

φ φ
A
i
j

= … …(
0 1 0

i-th

i-th

A

)(












)





0
:
1
:
0


A


)
A


2


M
φ φ φ φ
A
B
i
k
k
j
1

=

1

j

j

j-th position – picks
out j-th column of A

A

ij

= … …(
0 1 0

φ φ
AB
i
j

=

=

  

∑
k

∑
k

A B
ik

kj

= (

)
AB

ij

a number

In Heisenberg picture, how do we get exact equivalent of ψ(x)?
basis set δ(x,x0) for all x0 – this is a complete basis (eigenbasis
for x^, eigenvalue x0) - perfect localization at any x0

i.e.,   ∫

〈x ψ 〉 is the same thing as ψ(x)
 ↑
x is continuously variable ↔ δ(x)
overlap of state vector ψ with δ(x) – a complex number. ψ(x) is a complex
function of a real variable.

′(
)
′ =
x dx


( )
x


′
,
x x

ψ

ψ




δ

(

)

*

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

other  ψ ↔ 〈  〉  relationships

10 - 8

1.  All observable quantities are represented by a Hemitian operator (Why –
because expectation values are always real). Definition of Hermitian operator.
A ij = A ji
*

A = A†

or

Easy to prove that if all
expectation values of A
are real, then A = A† and
vice-versa

2.  Change of basis set
↔ u
A

A

φ

A
φ

φ
A
ij

≡
φ
i
= ∑
, l
k

1st index is u

=

φ
{ } to {u}

φ

j

=

φ

i

1 1
A

φ

j

k

u
u
k
i
*   ≡   Sik
†
Ski
↑
φ
u
∑ S A S
≤
u
l l
j
ik
k
l
,
k

A

u

l

u

l

φ

j

← φ

 Slj
S is Frequently used
↑
to denote “overlap”
integral
u
j-th column of
S
) ≡
u
SA
ij

φ
A
ij

= (
S

≤

φ = ≤ u
A
S A S

a special kind of
transformation (unitary)

(different from usual
T–1AT “similarity”
transformation)

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10
What kind of matrix is S?

10 - 9

j

φ

≡

u

l

u

l

S
l

*
S
l

|
u

l

φ
|

j
φ
|

=
] =
= [
≤
*
S
j
j
j
l
j
≤
means take complex conjugate and interchange indices.
 and  ≤
:
Using the definitions of 
S
S
  
φ
φ
=
j
j
φ
=
u

†
S S
j
jk
l
†
S S
j
jk
l

u
k
φ

|    |
1

=

u

l

u

k

u

k

|

l

|

|

j
δ

|

j
=

=

|

u

k

l

k

1 lk

∑
j

  

u
∑
l
j
u

l

=

∴ SS† = 1

OR

S† = S–1 “Unitary”

a very special and
convenient property.

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

10 - 10

Unitary transf ormations preserve both normalizati on and orthogonality .

φ
=
†
u
A S A S
φ
=
=
†
†
u
†
SS A SS A
SA S
φ
=
u
†
A SA S
Take matrix element of both sides of equation:
)
= (
φ
=
†
u
SA S
i
= ∑
φ
k
,
l

u
j
φ

†
S
j
l

A

A

|

A

|

S

ik

|

|

u
ij

u

ij

k

l

 

u j

= ∑
∴
  
l
φ → u via

φ S
l

≤
j
l

j - th column of  ≤
S

†S S
,

 

 :  

u j

 is j - th column of  †
S

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

10 - 11

Thus,

j-th

=

u

j

















0
M
1
M
0

=










≤
S
1
j
≤
S
2
j
M
M
≤
S
nj










φ

pq

∴

q

- th column of 
S

φ
A pq

q

| |
A
u S
nq
n

≤
S
u
pm m

Alternatively,
= (
)
=
φ
φ
≤
u
| |
S A S
A
= ∑p
mn
∑φ
=
q
n


0


M




1


M




0







S
nq u

u S
n
nq

q

q

S
1
S
2
M








φ

q

=

=

q-th

φ

updated 9/1 8/0 2 8 :55 AM

5.73 Lecture #10

10 - 12

Commutation Rules

*

ˆA, ˆB[
] = ˆA ˆB − ˆB ˆA
e. g.   ˆx, ˆp[
] = ih
= ihφ

(
means  xp - px

)φ = x

dφ
dx

h
i




− h
i

φ + x

dφ
dx




*
AB(
)

If  ˆA and  ˆB are Hermitian,  is  ˆA ˆB Hermitian?

∑
= A ikBkj
k

ij

Hermitian A and B
* = BA(
∑
∑
= A ki
= B jk
* A ki
* B jk
*

)
*
ji

but this is not what we need to say AB is Hermitian:  That would be:
)
AB(
)
= AB(
*
ji

ij

AB is Hermitian only if  A, B[
] = 0
1
] is Hermitian if A and B are Hermitian.
[
2 AB + BA

However,  

This is the foolproof way to construct a new Hermitian operator out of
simpler Hermitian operators.
Standard prescription for the Correspondence Principle.

updated 9/1 8/0 2 8 :55 AM

