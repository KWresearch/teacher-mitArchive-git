Artificial Neural Networks


Stephan Dreiseitl 
University of Applied Sciences 
Upper Austria at Hagenberg 

Harvard-MIT Division of Health Sciences and Technology
HST.951J: Medical Decision Support

Knowledge


textbook


verbal


rules


experience 

non-verbal 

patterns 

rule-based systems


pattern recognition 

A real-life situation…


…and its abstraction 

(f, 30,1,0,67.8,12.2,…)

(m, 52,1,1,57.4,8.9,…)

(m, 28, 1,1,51.1,19.2,…) 

(f, 46, 1,1,16.3,9.5.2,…) 
(m, 65,1,0,56.1,17.4,…) 

(m, 38, 1,0,22.8,19.2,…) 


Model(p)

Another real-life situation


benign lesion 

malignant lesion


Example: Logistic regression 

1

y = 
1 + e− ( b1x1 +b2 x2 +b0 ) 

So why use ANNs?


•	 Human brain good at pattern 
recognition 
•  Mimic structure and processing of brain: 
– Parallel processing 
– Distributed representation 
•  Expect: 
– Fault tolerance 
– Good generalization capability 
– More flexible than logistic regression 

Overview


•  Motivation 
•  Perceptrons 
•  Multilayer perceptrons 
•  Improving generalization 
•  Bayesian perspective 

Terminology


input 

output 

weights 

learning 

covariate 

dependent var. 

parameters 

estimation 

ANN topology


Artificial neurons


Activation functions


Hyperplanes


•	 A vector w = (w1,…,wn) defines a 
hyperplane 
•  Hyperplane divides n-space of points 
x = (x1,…,xn): 
�  w1 x1 + … + wn xn  > 0 
�  w1 x1 + … + wn xn  = 0 (the plane itself) 
�  w1 x1 + … + wn xn  < 0 
•  Abbreviation: w • x := w1 x1 + … + wn xn 

Linear separability


•  Hyperplane through origin: w • x  = 0 
•	 Bias w0  to move hyperplane from origin: 
w • x + w0  = 0 

Linear separability 

•  Convention: w := (w0,w), x := (1,x) 
•  Class labels ti  ∈{+1,-1} 
•  Error measure E =  -Σ  ti  (w • xi) 
i miscl. 
•  How to minimize E? 

Linear separability 

Error measure E =  -Σ  ti  (w • xi)  ≥ 0 
i miscl. 

+1 
-1 

× 

{ x | w • x > 0  } 

{ x | w • x < 0 } 

Gradient descent 

•  Simple function minimization algorithm 
•  Gradient is vector of partial derivatives 
•	 Negative gradient is direction of 
steepest descent 

Perceptron learning 

•	 Find minimum of E by iterating 
wk+1  = wk  – η gradw E 
•  E =  -Σ  ti  (w • xi)  ⇒ 
i miscl.

gradw E = -Σ  ti  xi

i miscl.

“online” version: pick misclassified xi

wk+1  = wk  + η  ti  xi


•	

Perceptron learning 

•  Update rule wk+1  = wk  + η  ti  xi 
•	 Theorem: perceptron learning 
converges for linearly separable sets 

From perceptrons to  

multilayer perceptrons

Why? 

Multilayer perceptrons


•  Sigmoidal hidden layer 
•  Can represent arbitrary decision regions 
•  Can be trained similar to perceptrons 

Decision theory 

•  Pattern recognition not deterministic 
•  Needs language of probability theory 
•  Given abstraction x: 

C1


x 

model


C2


Decide C1  if P(C1|x) > P(C2|x)


Some background math 

•	 Have data set D = {(xi,ti)} drawn from 
probability distribution P(x,t) 
•	 Model P(x,t) given samples D by ANN 
with adjustable parameter w 
•	 Statistics 
analogy: 

Some background math 

•  Maximize likelihood of data D 
•  Likelihood L = Π p(xi,ti) = Π p(ti|xi)p(xi) 
•  Minimize -log L = -Σ log p(ti|xi) -Σ log p(xi) 
•  Drop second term: does not depend on w 
•  Two cases: regression and classification 

Likelihood for regression 

•  For regression, targets t are real values 
•  Minimize -Σ log p(ti|xi) 
•	 Assume network outputs y(xi,w) are noisy 
targets ti 
•	 Minimizing –log L equivalent to minimizing 
Σ (y(xi,w) – ti)2 
(sum-of-squares error) 

Likelihood for classification 


•  For classification, targets t are class labels 
•  Minimize -Σ log p(ti|xi) 
•  Assume network outputs y(xi,w) are P(C1|x) 
•	 Minimizing –log L equivalent to minimizing 
-Σ ti log y(xi,w) +(1 – ti) * log(1-y(xi,w)) 
(cross-entropy error) 

Backpropagation algorithm 

•	 Minimizing error 
function by gradient 
descent: 
wk+1  = wk  – η gradw E 
•	 Iterative gradient 
calculation by 
propagating error 
signals 

Backpropagation algorithm 

Problem: how to set learning rate η ? 

Better: use more advanced minimization 
algorithms (second-order information) 

Backpropagation algorithm 

Classification 

cross-entropy 

Regression 

sum-of -squares 

ANN output for regression 

Mean of p(t|x) 

ANN output for classification 

P(t = 1|x) 

Improving generalization 

Problem: memorizing (x,t) combinations 
(“overtraining”) 

0.7  0.5  0

-0.5  0.9  1

-0.2  -1.2  1

0.3  0.6  1

-0.2  0.5  ?


Improving generalization


•  Need test set to judge performance 
•	 Goal: represent information in data set, 
not noise 
•  How to improve generalization? 
– Limit network topology 
– Early stopping 
– Weight decay 

Limit network topology 

•  Idea: fewer weights ⇒ less flexibility 
•  Analogy to polynomial interpolation: 

Limit network topology


Early stopping


•	 Idea: stop training when information (but 
not noise) is modeled 
•	 Need validation set to determine when 
to stop training 

Early stopping


Weight decay


•	 Idea: control smoothness of network 
output by controlling size of weights 
•  Add term - α||w||2  to error function 

Weight decay


Bayesian perspective 


•	 Error function minimization corresponds 
to maximum likelihood (ML) estimate: 
single best solution wML 
•  Can lead to overtraining 
•	 Bayesian approach: consider weight 
posterior distribution p(w|D). 
•	 Advantage: error bars for regression, 
averaged estimates for classification 

Bayesian perspective 

•  Posterior = likelihood * prior 
•  p(w|D) = p(D|w) p(w)/p(D) 
•	 Two approaches to approximating 
p(w|D): 
– Sampling 
– Gaussian approximation 

Sampling from p(w|D) 

prior  *  likelihood = 

posterior


Gaussian approx. to p(w|D) 

•  Find maximum wMAP  of p(w|D) 
•	 Approximate p(w|D) by Gaussian 
around wMAP 
•  Fit curvature: 

Gaussian approx. to p(w|D)  


•	 Max p(w|D) = min –log p(w|D) = 
min –log p(D|w) –log p(w) 
•  Minimizing first term: finds ML solution 
•	 Minimizing second term: for zero-mean 
Gaussian prior p(w) adds term - α||w||2 
•	 Therefore, adding weight decay 
amounts to finding MAP solution! 

Bayesian example for 

regression 


Bayesian example for 

classification


Summary


•  ANNs inspired by functionality of brain 
•  Nonlinear data model 
•  Trained by minimizing error function 
•  Goal is to generalize well 
•  Avoid overtraining 
•  Distinguish ML and MAP solutions 

Pointers to the literature

•	 Lisboa PJ. A review of evidence of health benefit from artificial 
neural networks in medical intervention. Neural Netw. 2002 
Jan;15(1):11-39. 
•	 Almeida JS. Predictive non-linear modeling of complex data by 
artificial neural networks. Curr Opin Biotechnol. 2002 
Feb;13(1):72-6. 
•	 Kononenko I. Machine learning for medical diagnosis: history, 
state of the art and perspective. Artif Intell Med. 2001 
Aug;23(1):89-109. 
•	 Dayhoff JE, DeLeo JM. Artificial neural networks: opening the 
black box. Cancer. 2001 Apr 15;91(8 Suppl):1615-35. 
•	 Basheer IA, Hajmeer M. Artificial neural networks: 
fundamentals, computing, design, and application. J Microbiol 
Methods. 2000 Dec 1;43(1):3-31. 
•	 Bishop, CM. Neural Networks for Pattern Recognition . Oxford 
University Press 1995. 

