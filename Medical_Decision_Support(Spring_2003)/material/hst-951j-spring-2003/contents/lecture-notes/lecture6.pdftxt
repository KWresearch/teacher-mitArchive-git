Bayesian Networks 

Learning From Data


Marco F. Ramoni

Children’s Hospital Informatics Program

Harvard Medical School


HST951 (2003)


Harvard-MIT Division of Health Sciences and Technology
HST.951J: Medical Decision Support

Introduction 

� Bayesian  networks  were  originally  developed  as  a 
knowledge  representation  formalism,  with  human 
experts their only source. 
� Their two main features are: 
�  The ability to represent deep knowledge 
(knowledge as it is available in textbooks), 
improving portability, reusability, and modularity. 
�  They are grounded in  statistics and graph theory. 
� Late 
the  statistical 
that 
realize 
’80s,  people 
foundations  of  Bayesian  networks  makes  it  possible 
to learn them from data rather than from experts. 

HST 951


Outline 

�  Learning from data. 
�  Learning Bayesian networks. 
�  Learning probability distributions. 
�	 Learning network structures. 
�  The classical way. 
�  The Bayesian way. 
�  Searching the space of possible models. 
�  A couple of examples. 
�  Lurking variables, hidden variables, and causality. 

HST 951


Components

Qualitative: A dependency graph made by:
Node: a variable X, with a set of states {x1,…,xn}.
Arc: a dependency of a variable X on its parents P.
Quantitative:  The  distributions  of  a  variable  X given 
each combination of states pi of its parents P.

A p(A)
A p(A)
Y
0.3
0.3
Y
O
0.7
O
0.7

E p(E)
E p(E)
L
0.8
0.8
L
H
0.2
H
0.2

A

I 
A  E 
I 
A  E 
L 
Y 
L 
L 
Y 
L 
H 
Y 
L 
H 
L 
Y 
L 
Y 
H 
H 
Y 
L 
Y 
H  H 
H  H 
Y 
O 
L 
L 
O 
L 
L 
H 
L 
O 
H 
O 
L 
O  H 
L 
O  H 
L 
O  H  H 
O  H  H 
  
  
Education ; I=; I=Income
A=A=AgeAge; E=; E=Education
Income
HST 951

E

I

p(I|A,E)  
p(I|A,E)  
0.9 
0.9 
0.1 
0.1 
0.5 
0.5 
0.5 
0.5 
0.7 
0.7 
0.3 
0.3 
0.2 
0.2 
0.8 
0.8 

The Age of the Experts 

� The traditional source of 
knowledge  is  a  human 
expert. 
� The  traditional  trick  is to 
ask  for  a  “causal  graph” 
and  then  squeeze  the 
numbers out of her/him. 
� The acquisition is easier 
than  the  traditional  one 
but  still… 
it  can  be 
painful. 

HST 951


Learning Bayesian Networks

� Learning a Bayesian network means to learn.
� The conditional probability distributions,
� The graphical model of dependencies.

A p(A)
A p(A)
Y
0.3
0.3
Y
O
0.7
O
0.7

E p(E)
E p(E)
L
0.8
L
0.8
H
0.2
0.2
H

A

E

I

I 
A  E 
I 
A  E 
L 
Y 
L 
L 
L 
Y 
H 
Y 
L 
H 
L 
Y 
L 
Y 
H 
H 
Y 
L 
Y 
H  H 
H  H 
Y 
O 
L 
L 
L 
L 
O 
O 
L 
H 
H 
O 
L 
O  H 
L 
O  H 
L 
O  H  H 
O  H  H 
  

  

p(I|A,E)  
p(I|A,E)  
0.9 
0.9 
0.1 
0.1 
0.5 
0.5 
0.5 
0.5 
0.7 
0.7 
0.3 
0.3 
0.2 
0.2 
0.8 
0.8 

HST 951

Learning Probabilities 

� Learning  of  probability  distributions  means  to  update 
a prior belief on the basis of the evidence. 
� Probabilities can be seen as relative frequencies: 
n( xj  | pi )
p( xj  | pi ) = � j n( xj  | pi ) 
� Bayesian estimate includes prior probability: 
aij  + n( xj  | p i )
p( xj  | p i ) = � j aij  + n( xj  | p i) 
aij  /ai  represents our prior as relative frequencies. 

HST 951


Learn the Structure 

� In  principle,  the  process  of  learning  a  Bayesian 
network structure involves: 
Search strategy to explore the possible structures;

Scoring metric to select a structure. 
� In  practice,  it  also  requires  some  smart  heuristic  to 
avoid the combinatorial explosion of all models: 
� Decomposability of the graph; 
�  Finite horizon heuristic search strategies; 
� Methods to limit the risk of ending in local maxima . 

HST 951


Model Selection 

� There are two main approaches to select a model: 
Constraint-based: use conditional independence test 

to check assumptions of independence and then 

encode the assumptions in a Bayesian network.

Bayesian: all models are a stochastic variable, the 
network with maximum posterior probability. 
� Bayesian approach is more popular: 
Probability: it provides the probability of a model.

Model averaging: predictions can use all models and 
weight them with their probabilities. 

HST 951


Constraint Based 

� A network encodes conditional independence (CI). 
� A  DAG  has  an  associated  undirected  graph  which 
explicitly encodes these CI assumptions. 
� Associated  undirected  graph:  the  undirected  graph 
obtained by dropping the direction of links. 
� Moral graph: the undirected graph obtained by. 
� Marring parents of a child. 
� Dropping the directions of the links. 
� How to read this graph? 

A 

B 

C 

HST 951


D 

E 

Learning CI Constraints 

Search strategy: top-down. 
1.  Start with the saturated (undirected) graph. 
2.  Go link by link and test the independence. 
If independence holds, remove the arc. 
3. 
4.  Swing the variables to assess the link direction. 
Scoring metric: independence tests. 
�	 Compute the expected frequencies under the 
assumption that the variables are independent. 
�  Test the hypothesis with some statistics (G2). 
�  Assume no structural 0. 

HST 951


B 

B 

A 

D 

A 

D 

C 

A^B 

B 

A^D‰C 

A 

D 

A 

C 

B^D‰C 

B 

C 

D 
HST 951


Example 

C 

A 

B 

C 

D 

A<B<C<D 

Bayesian Model Selection 

� The set of possible models M  is a stochastic variable 
with a probability distribution p(m ). 
� We  want  to  select  the  model  mi  with  the  highest 
posterior probability given the data D. 
� We  must  search  all  models  and  find  the  one  with 
highest posterior probability. 
� We can use Bayes’ theorem: 
p ( D , M ) 
p ( D ) 

p ( D  | M ) p (M ) 
p ( D ) 

p (M  | D ) =

= 

HST 951


Scoring Metric 

Result: we just need the posterior probability. 
First note: all model use the same data: 
p(mi| D) � p(D  |mi)p(mi). 
Second note: models have equal prior probability: 
p(mi| D) � p(D  |mi). 
Conclusion:  as  we  need  only  a  comparative  measure, 
we need just the marginal likelihood. 
Assumptions:  this  scoring  metric  works  under  certain 
assumptions  (complete  data,  symmetric  Dirichlet 
distributions as priors). 

HST 951


Bayes Factor 

� The  marginal  likelihood  (linear  or  log)  is  a  measure 
proportional to the posterior probability. 
� This is good enough to identify the best model but not 

to say how better is a model compared to another.

� This may be  important  to  take  into account  criteria of 
parsimony or to assess confidence. 
� Bayes  factor  computes  how  many  times  a  model  is 
more  likely  than another as  the  ratio of  their marginal 
likelihood (or marginal log likelihood): 
BF(mi,mj)= p(D  |mi) / p(D |mj) � p(mi| D)/p(mj| D). 

HST 951


Factorization 

the  “global” 
likelihood: 
the 
factorize 
� The  graph 
likelihood is the product of all local likelihood. 

HST 951


Search 

Strategy: Bottom up.

Variables: Xi<Xj, if Xi cannot be parent of  Xi.

Example: A<B<C.  


B 

C 

B 

C 

B 

C 

B 

C 

A 

A 

A 

A 

B 

C 

B 

C 

B 

C 

B 

C 

A 

A 

A 

A 

HST 951


Local Model Selection 

A (possible parents B; C): 
A (possible parents B; C):

B 

C 

B 

C 

B 

C

B 

C 

A 

A 

A 

A 

B  (possible parent C). 
B (possible parent C). 
B 
C 

The model: 
The model:

A 

B 

B 

C

C 

HST 951


Survival Analysis 

Topic: Survival analysis of the Titanic disaster. 
Input: 2022 cases on four variables. 
� Class: first, second, third, crew; 
� Gender: male, female; 
�  Age: adult, child; 
�  Survived: yes, no. 
Output: the model of interactions and its likelihood. 

HST 951


The Titanic


Example 

Database: Breast Cancer Database (UCI Archive).

Source: University of Wisconsin, W .  H. Wolberg.

Topic: Breast cancer malignancy classification.

Cases: 699 cases.

Variables: 10 with 10 states + malignancy class:


1  Clump Thickness 
2  Uniformity of Cell Size 
3  Uniformity of Cell Shape 
4  Marginal Adhesion 
5  Single Epithelial Cell Size 

6  Bare Nuclei 
7  Bland Chromatin 
8  Bland Chromatin 
9  Normal Nucleoli 
10  Mitoses 

HST 951


Breast Cancer


Causality 

� What the arrows in a Bayesian network mean? 
� The  received definition of causal sufficiency  (Suppes, 
1970) states that a relation is causal  if: 
�  There is correlation between the variables; 
�  There is temporal asymmetry (precedence); 
�  There is no hidden variable explaining correlation. 
� Hidden  variables  explain  statisticians’  reluctance  to 
use the word causal . 
� Yule  (1899)  on  the  poverty  causes  in  England.  Note: 
"Strictly speaking, for 'due to' read 'associated with'." 

HST 951


Richard III 

� Naïve (Aristotelian) causality: 
For want of a nail the shoe was lost,

For want of a shoe the horse was lost,

For want of a horse the rider was lost,

For want of a rider the battle was lost,

For want of a battle the kingdom was lost,

And all for want of a horseshoe nail.

� Modern causality among variables not events: 
Galilean equation: d=t2. 
� When we talk causality, we talk Causal Laws. 

HST 951


The Enemies 

� The  critical  problem  here  is  the  Simpson’s  paradox: 
getting stuck in a local maximum. 
� 674 murder  defendants  in  Florida  between  1976  and 
1987. Are capital sentences racially fair? 

No Death  Death 
19 
141 
88.1% 
11.9% 
17 
149 
89.8% 
10.2% 

Total 
160 
49.1% 
166 
50.9% 

White 

Black 

R 

C

HST 951


Lurking Variable: The Victim


Victim 

Defendant 

Non Death 
132 
87.4% 
52 
82.5% 
9 
100% 
97 
94.2% 

Death 
19 
12.6% 
11 
17.5% 
0 
0% 
6 
5.8% 

White 

White 

Black 

Black 

White 

Black 

White 

Black 

V 

R 

C 

HST 951


Hidden Variables 

� Hidden variables can also prevent independence. 
� Consider  a  database  of  children,  reporting  their  T-
shirt size and their running speed. 

T-shirt 

Fast 

Slow 

Small 

0.32 

0.68 

Large 

0.35 

0.65 

A 

T-shirt 

Age 

Fast  Slow 

Small 

Large 

Small 

Large 

<5 

<5 

>5 

>5 

0.3 

0.3 

0.4 

0.4 

0.7 

0.7 

0.6 

0.6 

T 

S

HST 951


Does Smoking Cause Cancer? 

� In  1964,  the Surgeon General  issued  a  report  linking 
cigarette smoking to lung cancer based on correlation 
between smoking and cancer in observational data. 
� Based  on  these  results,  the  report  claimed  causality: 
If  we  ban  smoking,  the  rate  of  cancers  will  be  the 
same as the one in the non-smoking population. 
Note:  Observational  data  are  data  collected  without

design (all St Valentine customers of Stephanie’s).  


S 

C

HST 951


“Of Course Not!” 

� Sir Ronald Fisher said. 
� The correlation can be explained by a model in which 
there  is  no  causal  link  between  smoking  and  cancer 
but  an  unobserved  genotype  simultaneously  causes 
cancer and craving for nicotine. 
� Only  a  controlled  experiment  (once  impossible  now 
also illegal) could have the last word. 

G 

S 

C

HST 951


Auxiliary Variables 

� The  causal  model  rests  on  the  assumption  that 
smoking affects lungs through tar accumulation. 
� This  accumulation  is  a  measurable  quantity  and  can 
be used as a proxy of the causal dependency. 

G 

S 

T 

C

HST 951


Measuring the Immeasurable 

� Not all factors are measurable: 
Measurable: tar concentration, age, income. 
Non measurable: lifestyle, affective state, genotype. 
� Can we  use  only measurable  factors  to  rule  out  both 
measurable  and  non  measurable  factors  and  avoid 
the  appearance  of  hidden  variables  and  Simpson’s 
paradox with them? 
� This  seems  to  be  an  experimental  design  problem 

but it can be used in observational studies as well.

� In statistics it is called the Adjustment Problem. 

HST 951


The Adjustment Problem 

Adjustment:  Which  factors  should  be  measured  (or

which experimental conditions should be kept still?).

Problem: Are factor 6 and 7 enough to avoid paradox? 
Solution: Model the interaction of factors with a BBN. 
4 
3 
2 

Variables of Interest 
Observable 
Not Observable 

1 

8 

S 

5 

T 

6 

7 

C
HST 951


The Adjustment Problem 

Step 1: Build the model.

Note: Measurements should not be children of S and C.

Step 2: Remove all non ancestors of S, C, 6 and 7.


4 

S 

1 

8 

2 

5 

T 

HST 951


6 

3 

7 

C

The Adjustment Problem 

Step 3: Delete all arcs starting from S. 

4 

S 

1 

2 

5 

T 

HST 951


6 

3 

7 

C

The Adjustment Problem 

Step 4: Moralize (marry parents of a common child). 

4 

S 

1 

2 

5 

T 

HST 951


6 

3 

7 

C

The Adjustment Problem 

Step 5: Drop the directionality of the links (arrows). 

Step 6: Remove the factors to measure (6 and 7).


4 

S 

1 

2 

5 

T 

HST 951


6 

3 

7 

C

Solution 

Test:  If  the  variables  of  interest  (S  and  C)  are

disconnected, then measurements are appropriate.

Answer: Yes. 

4 

S 

2 

3 

1 

5 

T 

HST 951


C

Take Home 

� Bayesian  networks  are  a  knowledge  representation 
formalism to reason under uncertainty. 
� They provide a semantics understandable  to humans  

and facilitate the acquisition of modular knowledge.

� Bayesian networks can be learned from data. 
� Hidden  variables  and  not  measurable  quantities  are 
major obstacles to causal discovery. 
� Bayesian  networks  can  be  used  to  model  causality, 
although their arcs aren’t necessarily causal. 

HST 951


