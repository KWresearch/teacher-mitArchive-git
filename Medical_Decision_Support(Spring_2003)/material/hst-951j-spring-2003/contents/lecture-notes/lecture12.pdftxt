Support Vector Machines 

Stephan Dreiseitl 
University of Applied Sciences 
Upper Austria at Hagenberg 

Harvard-MIT Division of Health Sciences and Technology
HST.951J: Medical Decision Support

Overview


•  Motivation 
•  Statistical learning theory 
•  VC dimension 
•  Optimal separating hyperplanes 
•  Kernel functions 
•  Performance evaluation 

Motivation


Given data D = {(xi,ti)} distributed according to 
P(x,t), which is better performance on test set? 
0.7 
0.5
0
-0.5 
0.9
1
1
-1.2
-0.2 
0.3 
0.6
1
-0.2 
0.5
1
0.8 
-0.2
1

0.7 
-0.5 
-0.2 
0.3 
-0.2 
0.8 

0.7 
-0.5 
-0.2 
0.3 
-0.2 
0.8 

0.7 
-0.5 
-0.2 
0.3 
-0.2 
0.8 

0.5
0.9
-1.2
0.6
0.5
-0.2

0.5
0.9
-1.2
0.6
0.5
-0.2

0
1
1
1
0
1

0
1
1
1
0
0

0.5
0.9
-1.2
0.6
0.5
-0.2

0
1
1
1
1
0

Motivation


•  Neural networks model p(t|x) by 
– Topology restriction 
– Early stopping 
– Weight decay 
– Bayesian approach 
•  SVM: capacity control 
•  Based on statistical learning theory 

Statistical learning theory


•	 Given data D = {(xi,ti)}, model output 
y(α,xi) ∈{+1,-1}, class labels ti ∈{+1,-1} 
•	 Fundamental question: Is learning 
consistent? 
•	 Can we infer performance on test set 
(generalization error) from performance 
on training set? 

Statistical learning theory


Average error on a data set D for model  

with parameter α: 
n 
n ∑ | y (α , xi ) − ti  | 
Remp (α ) =  2
1 
i =1 
Expected error of same model given 
unseen data distributed like D: 
R(α ) =  2  ∫ | y (α , x ) − t | dP( x , t )
1

Statistical learning theory 

•  How can we relate Remp  and R? 
•	 Generalization error R(α) depends on 
empirical error Remp(α) and capacity h of 
model 
•  With probability 1-η : 
h(log(2n / h) + 1) − log(η / 4) 
n 
h is VC dimension (Vapnik-Chervonenkis) 

R(α ) ≤ Remp (α ) + 

VC dimension


•	 Capacity measure for classification 
models 
•	 Largest number of points that can be 
shattered by model 
•	 Classifier shatters data points if for any 
labeling, points can be separated 
•	 Not the same as number of parameters 
in model! 

Shattering 
Straight lines can shatter 3 points in 2-space 
Model: sign(α • x) 

Shattering 
Other model: 
sign(x • x - α) 

Still other model: 
sign(βx • x - α) 

VC dimension


•	 Largest number of points for which there 
is arrangement that can be shattered 
•  For straight lines in 2-space, VC dim. is 3 
•	 For hyperplanes in n-space, VC dim. is 
n + 1 
•	 There is model with one parameter and 
infinite VC dimension 

Structural risk minimization


R(α ) ≤ Remp (α ) + 

h(log(2n / h) + 1) − log(η / 4) 
n 
•	 Fix data set and order classifiers 
according to their VC dimension 
•	 For each classifier, train and calculate 
right-hand side 
•  Best classifier minimizes right-hand side 

Structural risk minimization 
R(α ) ≤ Remp (α ) +  h (log( 2n / h ) + 1) − log(η / 4) 
n 

Remp

VC conf.

Upper bound

best


Model 
f1(α) 
f2(α) 
f3(α) 
f4(α) 
f5(α) 

Model selection 

•	 Cross-validation: use test sets to 
estimate error 
•  Penalize model complexity: 
– Akaike information criterion (AIC) 
– Bayesian information criterion 
•  Structural risk minimization 

Support Vector Machines 

•	 Implement hyperplanes, so know VC-
dimension 
•	 Algorithmic representation of concepts 
from statistical learning theory 
•	 Determine hyperplanes that maximize 
margin between classes 

Geometry of hyperplanes


Geometry of hyperplanes


Hyperplanes invariant to scaling of parameters:

{x|w • x + w0  = 0} = {x|c w • x + c w0  = 0}  


3x – y – 4 = 0 

6x – 2y – 8 = 0 

Geometry of hyperplanes


3x – y – 4 = 0 

6x – 2y – 8 = 0


3 * 2 – (-4) – 4  = 6 

6 * 2 – 2*(-4) – 8 = 12


Optimal hyperplanes 
We want 
w • xi  + w0  ≥ 1  for all xi  in class 1 (ti =+1) 
w • xi  + w0  ≤  -1 for all xi  in class 2 (t =-1) 

x 
xx 
+1

x


-1


0 

Optimal hyperplanes 
Points on dashed lines satisfy

g(x) = +1 resp. g(o) = -1


Margin =|g(x)|/||w|| + |g(o)|/||w||

= 2 /||w||


Largest (optimal) margin:

maximize 2 /||w|| equiv. to

minimize ||w||2

subject to ti (w • xi  + w0) –1 ≥ 0


Optimal hyperplanes


•	 Optimal hyperplane has largest margin 
(“large margin classifiers”) 
•	 Parameter estimation problem turned into 
constrained optimization problem 
•  Unique solution w =  Σαixi over all inputs xi 
on the margin (“support vectors”) 
•  Decision function g(x) = sign(Σαixi ·x + w0) 
•  All other cases xj irrelevant to solution! 

Nonseparable data sets 

•  Introduce slack variables ξi ≥ 0 
•	 Constraints are then

w • xi  + w0  ≥ +1 - ξi  for all xi  in class 1 

w • xi  + w0  ≤  -1 + ξi for all xi  in class 2

•  minimize ||w||2 +  CΣξi 

Nonlinear SVM


•	 Idea: Nonlinearly project data into higher 
dimensional space with Φ: Rm→H 
•  Apply optimal hyperplane algorithm in H 

Nonlinear SVM example 

Idea: Project R2→ R3 via 

2, √2 x1 x2,x2
Φ(x1  ,x2) = (x1
2) 

Nonlinear SVM example 

Do the math: 



2
y
1
1 y2 
y 
2 


2 
y2 



 

 
2
x
1
y1  = 
x1  ⋅ Φ  
Φ 
⋅ 
1x2 
2 
x 




 y2 
 x2 
  
 
2 
  x2 
  
2 y2
2  + 2 x1x2 y1 y2  + x2
2 y1
2 
= x1
= ( x1 y1  + x2 y2 ) 2 
2 
 
= 
 x1 
⋅  y1 



  x2   y2   

Nonlinear SVM 

•	 Recall: Input data xi enters calculation 
only via dot products xi ·xj  or Φ(xi)·Φ(xj) 
•	 Kernel trick: 
K(xi ,xj) = Φ(xi)·Φ(xj) 
•  Advantage: no need to calculate Φ 
•  Advantage: no need to know H 
•  What are admissible kernel functions? 

Kernel functions


•  Satisfy Mercer’s condition 
•  Most widely used (+parameters): 
– Polynomials (degree) 
– Gaussians (variance) 
•	 For given kernel function K, projection Φ 
and projection space H not unique 

Kernel function example 

•  Data space R2; x, y ∈ R2 
•  K(x,y) = (x ·y)2 
•  Possible H=R3 
•  Possible Φ(x1  ,x2) = (x1
2, √2 x1 x2,x2

2) 

Kernel function example 

K(x,y) = (x ·y)2 

SVM examples 

Linearly separable 

C=100


SVM examples 

C=100 

C=1


SVM examples


Linear function 

Quad. polynomial 


SVM examples


Quad. poly., C=10 

Quad. poly., C=100 

SVM examples 

Cubic polynomial 

Gaussian, σ = 1


SVM examples


Quad. polynomial 

Cubic polynomial 


SVM examples 

Cubic polynomial 

Degree 4 poly.  


SVM examples 

Gaussian, σ = 1 

Gaussian, σ = 3


Performance comparison 

•  Log. regression ⇔ ANN ⇔ SVM 
•  Real-world data set 
•  1619 lesion images 
•  107 morphometric features: 
– Global (size, shape) 
•  size 
•  shape 
– Local (color distributions) 
•  Use ROC analysis 

CN ⇔ DN + MM 

•  Logistic regression: 0.829 
•  Artificial neural networks: 0.826 
•	 Support vector machines (polynomial 
kernel): 0.738 to 0.813 
•	 Support vector machines (Gaussian 
kernel): 0.786 to 0.831 

MM ⇔ CN + DN 

•  Logistic regression: 0.968 
•  Artificial neural networks: 0.968 
•	 Support vector machines (polynomial 
kernel): 0.854 to 0.918 
•	 Support vector machines (Gaussian 
kernel): 0.947 to 0.970 

Summary


•  SVM based on statistical learning theory 
•  Bounds on generalization performance 
•  Optimal separating hyperplanes 
•  Kernel trick (projection) 
•	 Performs comparable to log. regression 
and neural networks 

Pointers to the literature


•	 Burges C. A tutorial on support vector 
machines for pattern recognition. Data Mining 
and Knowledge Discovery. 1998; 2(2):121-
167. 
•	 Christianini N, Shawe-Taylor J. An 
introduction to support vector machines. 
Cambridge University Press 2000. 
•	 Vapnik V. Statistical learning theory. Wiley 
Interscience 1998. 

