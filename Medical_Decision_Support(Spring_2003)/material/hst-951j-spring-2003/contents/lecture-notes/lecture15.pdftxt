Optimization and Complexity 


Decision Systems Group

Brigham and Women’s Hospital,

Harvard Medical School


HST 951 Spring 2003 
Harvard-MIT Division of Health Sciences and Technology

Aim 

• Give you an intuition of what is meant 
by 
– Optimization 
– P and NP problems 
– NP-completeness 
– NP-hardness 
•  Enable you to recognize formals of 
complexity theory, and its usefulness 

HST 951 Spring 2003 

Overview


• Motivating example 
• Formal definition of a problem 
• Algorithm and problem complexity 
• Problem reductions 
– NP-completeness 
– NP-hardness 
• Glimpse of approximation 
algorithms and their design 

HST 951 Spring 2003 

What is optimization?


• Requires a measure of optimality 
– Usually modeled using a 
mathematical function 
• Finding the solution that yields the 
globally best value of our measure 

HST 951 Spring 2003 

What is the problem?


• Nike: Just do it 
• Not so simple: 
– Even problems that are simple to 
formally describe can be intractable 
– Approximation is necessary 
– Complexity theory is a tool we use to 
describe and recognize (intractable) 
problems 

HST 951 Spring 2003 

Example: Variable Selection


•	 Data tables T and V have n predictor columns and one 
outcome column. We use machine learning method L to
produce predictive model L(T) from data table T. We
can evaluate L(T) on V, producing a measure E(L(T),V). 

•  We want to find a maximal number of predictor columns
in T to delete, producing T’, such that 
E(L(T’),V) = E(L(T), V) 

•	 There is no known method of solving this problem 
optimally (e.g, NP-hardness of determining a minimal 
set of variables that maintains discernibility in training 
data, aka the rough set reduct finding problem). 

HST 951 Spring 2003 

Search for Optimal  

Variable Selection

•  The space of all possible 
selections is huge 
•  43 variables, 243  -1 possibilities 
of selecting a non-empty subset, 
each being a potential solution 
•  one potential solution pr. post-it 
gives a stack of post-its reaching 
more than half way to the moon 

HST 951 Spring 2003 

Search for Optimal  

Variable Selection

•  Search space 
– discrete 
– structure that lends 
itself to stepwise 
search (add a new or 
take away one old) 
– optimal point is not 
known, nor is optimal 
evaluation value 

{a,b} 

{a} 

{b} 

∅ 

{a,b,c}


{a,c} 

HST 951 Spring 2003 

{b,c} 

{c} 

Popular Stepwise Search  

Strategies

• Hill climbing: 
– select starting 
point and always 
step in the 
direction of most 
positive change 
in value 

HST 951 Spring 2003 

Popular Stepwise Search  

Strategies 
•  Simulated 
annealing: 
– select starting point 
and select next 
stepping direction 
stochastically with 
increasing bias 
towards more 
positive change 

HST 951 Spring 2003 

Problems 


•  Exhaustive search: generally intractable 
because of the size of the search space 
(exponential in the size of variables) 
•  Stepwise: no consideration of synergy 
effects 
– Variables a and b considered in isolation 
from each other are excluded, but their 
combination would not be 

HST 951 Spring 2003 

Genetic Algorithm Search 

– population of solutions 
– Stochastic selection of 
parents with bias towards 
“fitter” individuals 
– “mating” and “mutation” 
operations on parents 
– Merging of old population 
with offspring 
– Repeat above until no 
improvement in 
population 

HST 951 Spring 2003 

GA Optimization  

Animation


HST 951 Spring 2003


Addressing the Synergy  

Problem of Stepwise Search

• Genetic algorithm search 
– Non-stepwise, non-exhaustive 
– Pros: 
• Potentially finds synergy effects 
• Does not a priori exclude any parts of the search 
space 
– Cons: 
• Computationally expensive 
• Difficult to analyze, no comprehensive theory for 
parameter specification 

HST 951 Spring 2003 

Variable Selection for Logistic  

Regression using GA

• Data: 
– 43 predictor variables 
– Outcome: MI or not MI (1 or 0) 
– Training (T, 335 cases) and Holdout 
(H, 165 cases) from Sheffield, 
England 
– External validation (V, 1253 cases) 
from Edinburgh, Scotland 

HST 951 Spring 2003 

GA Variable Selection for 

LR: Generational Progress

Fitness value evolution 

Max 

Mean 

 
s
s
e
n
t
i
F

0.9 

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

Min 

0 

20 

40 

60 

80 
Generation 

100 

120 

140 

160 

HST 951 Spring 2003


GA Variable Selection for 

LR: Results

•  Table presenting results on validation 
set E, including SAS built-in variable 
selection methods (removal/entry level 
0.05) 

Selection  Size  
Genetic 
none 
Backward 
Forward 
Stepwise  

ROC AUC 
0.95 
0.92 
0.92 
0.91 
0.91 

6 
43 
11 
13 
12 

P < 0.05 

HST 951 Spring 2003 

Problem Example


•  Boolean formula f (with variables) 
– Is there a truth assignment such that f is 
true? 
– Does this given truth assignment make f 
true? 
– Find a satisfying truth assignment for f 
– Find a satisfying truth assignment for f with 
the minimum number of variables set to 
true 

HST 951 Spring 2003 

Problem Formally Defined


• A problem P is a relation from a 
set I of instances to a set S of 
solutions: P ⊆  I ×  S 
– Recognition: is (x,y) ∈  P ? 
– Construction: for x find y such that 
(x,y) ∈  P 
– Optimization: for x find the best y 
such that (x,y) ∈  P 

HST 951 Spring 2003 

Solving Problems 


• Problems are solved by an 
algorithm, a finite description of 
steps, that compute a result given 
an instance of the problem. 

HST 951 Spring 2003 

Algorithm Cost 

•  Algorithm cost is measured by 
– How many operations (steps) it takes to 
solve the problem (time complexity) 
– How much storage space the algorithm 
requires (space complexity) 
on a particular machine type as a 
function of input length (e.g., the 
number of bits needed to store the 

problem instance). 

HST 951 Spring 2003 

O-Notation 

• O-notation describes an upper 
bound on a function 
• let g,f: N →  N 
f(n) is O(g(n)) 
if there exists constants a,b,m 
such that for all n=m 
f(n) = a * g(n) + b 

HST 951 Spring 2003 

O-Notation Examples 

f(n) = 9999999999999999 
is O(1) 

f(n) = 1000000n + 100000000 
is O(n) 

f(n) = 3n2  + 2n – 3 
is O(n2) 

(Exercise: convince yourselves of this please)


HST 951 Spring 2003 

Worst Case Analysis


• Let t(x) be the running time of 
algorithm A on input x 
• Let T(n) = max{t(x) | |x| = n} 
– I.e., T(n) is the worst running time on 
inputs not longer than n. 
• A is of time complexity O(g(n)) if 
T(n) is O(g(n)) 

HST 951 Spring 2003 

Problem Complexity 


• A problem P has a time complexity 
O(g(n)) if there exists an algorithm 
that has time complexity O(g(n)) 
• Space complexity is defined 
analogously 

HST 951 Spring 2003 

Decision Problems


• A decision problem is a problem P 
where the set of Instances can be 
partitioned into YP  of positive instances 
and NP  of non-positive instances, and 
the problem is to determine whether a 
particular instance is a positive instance 
•  Example: satisifiability of Boolean CNF 
formulae, does a satisfying truth 
assignment exist for a given instance? 

HST 951 Spring 2003 

Two Complexity Classes for 

Decision Problems

• P – all decision problems of time 
complexity O(nk), 0 = k =∞ 
• NP – all decision problems for 
which there exists a non-
deterministic algorithm with time 
complexity O(nk), 0 = k =∞ 

HST 951 Spring 2003 

What is a non-deterministic  

algorithm? 

• Algorithm: finite description 
(program) of steps. 
• Non-deterministic algorithm: an 
algorithm with “guess” steps 
allowed. 

HST 951 Spring 2003 

Computation Tree


•  Each guess step 
results in a 
“branching point” 
in a computation 
tree 
•  Example: 
satisfying a 
Boolean formula 
with 3 variables 

HST 951 Spring 2003 

0 

1 

a 

b 

c 

1 

1 

0 

0 

Y  N Y  Y Y  N Y  N 

((~a ∧ b) ∨ ~c) 


Non-deterministic algorithm 

time complexity 

• A ND algorithm A solves the 
decision problem P in time 
complexity t(n) if, for any instance 
x with |x| = n, A halts for any 
possible guess sequence and x∈YP 
if and only if there exists at least 
one sequence which results in YES 
in time at most t(n) 

HST 951 Spring 2003 

P and NP


• We have that 
– P ⊆  NP 
• If there are problems in NP that 
are not in P is still an open 
problem, but it is strongly believed 
that this is the case. 

HST 951 Spring 2003 

Problem Reduction 

• A reduction from problem P1  to 
problem P2  presents a method for 
solving P1  using an algorithm for 
P2. 
– P2  is then intuitively at least as 
difficult as P1 

HST 951 Spring 2003 

Problem Reduction 
x 

R 

P1(x)

x’  P2(x’)

Oracle 

•	 Problem P1  is reducible to P2 
if there exists an algorithm 
R which solves P1  by 
querying an oracle for P2.  In 
this case, R is said to be a 
reduction from P1  to P2, and 
we write P1 = P2 
•	 If R is of polynomial time 
complexity we write P1 =p  P2 

HST 951 Spring 2003 

NP-completeness


•  A decision problem P is NP-complete if 
–  It is in NP, and 
–  For any other problem P’ in NP we have that P’ =p  P, 
•	 This means that any NP problem can be solved 
in polynomial time if one finds a polynomial 
time algorithm for NP-complete P 
•	 There are problems in NP for which the best 
known algorithms are exponential in time 
usage, meaning that NP-completeness is a 
sign of problem intractability 

HST 951 Spring 2003 

Optimization Problems


•  Problem P is a quadruple (IP, SP, mP, gP) 
–  IP is the set of instances 
–  SP is a function that for an instance x returns the set 
of feasible solutions SP(x) 
–  mP(x,y) is the positive integer measure of solution 
quality of a feasible solution y of a given instance x 
–  gP is either min or max, specifying whether P is a 
maximization or minimization problem 
•	 The optimal value for mP  for x over all 
solutions is denoted mP(x).  A solution y for 
which mP(x,y) = mP(x) is called optimal and is 
denoted y*(x). 

HST 951 Spring 2003 

Optimization Problem 

Example

• Minimum hitting set problem 
– I = { C | C ⊆  2U} 
– S = {H | H ∩  c ≠ ∅, c ∈  C } 
– m(C,H) = |H| 
– g = min 

HST 951 Spring 2003 

Complexity Class NPO


An optimization problem (I, S, m, g) is in NPO if

1.	 An element of I is recognizable as such in
polynomial time 
2.	 Solutions of x are bounded in size by a
polynomial q(|x|), and are recognizable as
such in q(|x|) time 
3.  m is computable in polynomial time 

Theorem: For an NPO problem, the decision 
problem whether m(x) = K (g=min) or m(x) 
= K (g=max) is in NP 

HST 951 Spring 2003 

Complexity Class PO


• An optimization problem P is said 
to be in PO if it is in NPO and there 
exists an algorithm that for each x 
in I computes an element y*(x) 
and its value m(x) in polynomial 
time 

HST 951 Spring 2003 

NP-hardness 

• NP-completeness is defined for 
decision problems 
• An optimization problem P is NP-
hard if we for every NP decision 
problem P’ we have that P’ =p  P 
• Again, NP-hardness is a sign of 
intractability 

HST 951 Spring 2003 

Approximation Algorithms


• An algorithm that for an NPO 
problem P always returns a 
feasible solution is called an 
approximation algorithm for P 
• Even if an NPO problem is 
intractable it might not be difficult 
to design a polynomial time 
approximation algorithm 

HST 951 Spring 2003 

Approximate Solution Quality


•	 Any feasible solution is an approximate 
solution, and is characterized by the distance 
from its value to the optimal one. 
•	 An approximation algorithm is characterized 
by its complexity, and by the ratio of the 
distance above to the optimum, and the 
growth of this performance ratio with input 
size 
•	 An algorithm is a p-approximate algorithm if 
the performance ratio is bounded by the 
function p in input size 

HST 951 Spring 2003 

Some Design Techniques for  

Approximation Algorithms

•  Local search 
–  Given solution, search for better “neighbor” solution 
•  Linear programming 
–  Formulate problem as a linear program 
•  Dynamic Programming 
–  Construct solution from optimal solutions to sub-
problems 
•  Randomized algorithms 
–  Algorithms that include random choices 
•  Heuristics 
–  Exploratory, possibly learning strategies that offer no 
guarantees 

HST 951 Spring 2003 

Thank you 


That’s all folks 


HST 951 Spring 2003


