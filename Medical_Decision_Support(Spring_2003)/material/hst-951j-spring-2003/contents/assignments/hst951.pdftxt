T hi s problem set i s handed out a day  af ter  six th lecture( L E C #6) and due two weeks 
later. 

G oal s 

T hi s  assignment  i s  meant  to  teach  you  about methods  of  reasoning  and  learning 
under  uncertainty.  T he main part focuses  on B ayes Networ ks,  a graphical  formal -
i sm that represents probabi l i stic  relationships among random variables.  T he paper 
i s broken down into 4 sections: 

B ack gr ound:  S ummary of the basic notions and de(cid:2)  ni tions of B ayesian networks. 

R epr esentation:  A  problem requi ring the use of  B ayesian networks  to  represent 
expert medical  knowledge. 

R easoning:  A  problem  requi ring  the  use  of  B ayesian  networks  to  reason  under 
uncertainty. 

L ear ning:  A  problem  requi ring  the  use  of  B ayesian  networks  to  learn  f rom  a 
database. 

B ack gr ound 

M uch of what we know about the world i s known wi th various degrees of  certainty. 
A l though many  di fferent  numerical  model s  have  been  proposed  for  representing 
thi s uncertainty,  many  researchers  today  bel ieve that probabi l i ty  theory—the most 
ex tensively  studied uncertainty calculus—i s an appropriate basi s for bui lding com(cid:173)
puteri zed reasoning systems. 

M athematical  P r el iminar ies 

We take the world of interest to consi st of a set of random variables, X  =  f X  1 ; X  2 ;  :  :  :  ; X  n g.€
E ach  of  the  X  i  can  take  on  one  of  a  di screte  set  of  values,  f x i 1 ; x i 2 ;  :  :  :  ; x i k i  g. 1€
E ach possible combination of  assignments of  values to each of  the variables repre(cid:173)€
Q n
i = 1  k i  such states.  T hi s  i s  clearly€
sents  a possible state of  thi s world.  T here are 

1Formulations using continuous variables are al so possible,  but we wi l l  not pursue them here.  A 
continuous variable can,  of  course,  be approx imated by  a large number of  di screte values. 

1 

Harvard-MIT Division of Health Sciences and Technology
HST.951J: Medical Decision Support

exponential  in  the number of variables;  for example,  if each variable  is binary, all 
of  the  ki  = 2  and  the  number  of  states  is  2n .  Each  state may  be  identi ﬁed  with 
the particular values  that  it assigns  to each variable.  We might have,  for example, 
S12  = {X1  = a, X2  = e, . . . , Xn  = b}. 
A  probability  function,  P ,  assigns  a  probability  to  each  of  these  possible  states. 
The  probability  for  each  state,  P (Si ),  is  the  joint  probability  of  that  particular 
assignment of values to the variables. All states are distinct, and they exhaustively 
� � 
enumerate the possibilities; therefore, 
n 
i=1 ki 
j=1 

P (Sj ) = 1.0 

We will often be interested in the probability not of individual states, but of certain 
combinations of particular variables  taking on particular values.  For  instance, we 
may  be  interested  in  the  probability  that  {X3  =  a, X5  =  b}.  In  such  cases,  we 
wish  to  treat  other  variables  as  “don’ t  cares.”  We  call  such  a  partial  description 
of  the  world  a  circumstance,2  C ,  and  the  variables  that  have  values  assigned  the 
instantiation-set  of  the  circumstance,  I (C ).  The probability  of  a  circumstance C 
can  be  computed  by  summing  the  probabilities  of  all  states  that  assign  the  same 
values  to  the variables  in  the  instantiation set I (C ) as C .  If we  re-order variables 
�  � 
X  so that the ﬁrst m are the instantiation set of C , then (in a shorthand notation): 
P (C ) = 
P (S ). 
. . . 
Xn 
Xm+1 
For example,  if  the world consists of four binary variables, W , X , Y  and Z ,  then 
� 
� 
the circumstance {X  = true, Z  = false} is given by 
P ({W  = u, X  = true, Y  = v , Z  = false}) 
P ({X  = true, Z  = false}) = 
u∈{true,false} v∈{true,false} 

The computational difﬁculty  of calculating the probability of a circumstance comes 
precisely  from  the  need  to  sum  over  a  possibly  vast  number  of  states.  The  num­
ber  of  such  states  to  sum  over  is  exponential  in  the  number  of  “don’ t  cares”  in  a 
circumstance. 

De ﬁning  P  for  each  state  of  the  world  is  a  rather  tedious  and  counter-intuitive 
way  to describe probabilities  in  the world,  although  the ability  to assign an expo­
nentially  large  number  of  independent  probabilities  would  allow  any  probability 

2 “Circumstance” 
is not a commonly-used  term  for partial descriptions of  the world.  People use 
terms such as “partially-speciﬁed state” and oth
er equally unsatisfying terms. 

2 

distribution  to  be  described.  It might  happen  that  the world  really  is  so  complex 
that  only  such  an  exhaustive  enumeration  of  the  probability  of  each  state  is  ad-
equate,  but  fortunately  many  variables  appear  to  be  independent.  For  example, 
in  medical  diagnosis,  the  probability  that  you  get  a  strep  infection  is  essentially 
independent  of  the  probability  that  you  catch  valley  fever  (a  fungal  infection  of 
the  lungs  prevalent  in  California  agricultural  areas).  Formally,  this  means  that 
P (strep, vf )  =  P (strep)P (vf ).  When  two  variables  both  depend  on  the  same 
set  of  variables  but  not  directly  on  each  other,  they  are  conditionally  indepen­
dent.  For example, if an infection causes both a fever and diarrhea, but there is no 
other  correlation  between  these  symptoms,  then  P (fever, diarrhea|infection)  = 
P (fever|infection)P (diarrhea|infection). 
The  independencies  among  variables  in  a  domain  support  a  convenient  graphical 
notation,  called  a  Bayes  network.  In  it,  each  variable  is  a  node,  and  each  proba­
bilistic dependency  is drawn  as  a directed  arc  to  a dependent node  from  the node 
it  depends  on.  This  notion  of  probabilistic  dependency  does  not  necessarily  cor­
respond  to  what  we  think  of  as  causal  dependency,  but  it  is  often  convenient  to 
identify them. When there is no directed arc from one variable to another, then we 
say that the second does not depend on the ﬁrst. 

The probability of a state is a product over all variables of the probability  that the 
variable  takes  on  its  particular  value  (in  that  state)  given  that  its  parents  take  on 
� 
their particular values: 
P (Xi |π(Xi )) 
i=1,...,n 

P (X1  = v1 , . . . , Xn  = vn ) = 
� 
� 
The right hand side is an abbreviation for 
P (Xi  = vi |Xj  = vj , . . . , Xl  = vl ) 
P (Xi |π(Xi )) = 
i=1,...,n 
i=1,...,n 
where  π(X ) =  {Xj , . . . , Xl ).  As  described  above,  to  ﬁnd 
the  probability  of  a 
circumstance,  we must  still  sum  over  all  of  the  states  that  are  consistent with  the 
circumstance—i.e., a number of states e xponential in the number of “don’ t cares.” 

Software 

The  program  to  use  for  the  exercises  is  Bayesware  Discoverer.  A  free  Student 
Edition  is  available  from  http://bayesware.com.  The  data  for  the  last  exercise  are 
available from the course homepage, as well the related paper by Wolberg et al. 

3


Representation 

Question  1.  Your  expert  supplies  you with  the  following  assessment:  ”I  think 
that disease D1  causes symptoms S1  and, together with disease D2 , causes symp­
tom S2 .  80% of patients affected by disease D1  will manifest symptom S1 , which 
is  present  in  the  10%  of  the  patients  without  disease  D1 .  D1  and  D2  cause  the 
occurrence of symptom S2  in 60% of cases, when only D1  is present S2  occur  in 
the 70% of patients, when only D2  is present S2  occur in the 80% of patients, when 
neither D1  or D2  occur symptom S2  occurs in the 10%.  Disease D1  occurs in the 
20% of the population, while disease D2  occurs in the 10%. 

1.  Draw the network capturing the description. 

2.  What kind of graph is this and why? 

3.	 Assume  that  the  variables  can  take  two  values,  1  for  present,  0  for  absent. 
Write the conditional probability tables associated to each dependency in the 
network. 

4.	 Write down the formula to calculate the joint probability distribution induced 
by the network. 

5.  Using these table, calculate the marginal probability p(S1  = 1). 

Reasoning 

Question 2.  Consider the network in Figure 1. 

1.  How are termed the networks with this kind of structure. 

2.  List the assumptions of conditional independence encoded by this network. 

3.  Assume that the distributions of the network as are as follows: 

4


F igure 1:  B ayesian network  for R easoning exerci se. 

( a)€ L i st  the probabi l i ty  di stributions  of  each symptom given  that  the D i s(cid:173)
ease i s T rue. 
(b)€ C ompute the posterior  probabi l i ty  di stribution of  the D i sease variable 
given  that  S ymptom1  i s  T rue,  S ymptom2  i s  T rue  and  S ymptom  3  i s 
F al se. 
( c)€ H as  any  of  the  symptoms  enough  evidential  power  to make  T rue  the 
D i sease variable by  i tsel f ,  or  you  always  need a  combination of  vari (cid:173)
ables to make posi tive your diagnosi s? 

L ear ning 

Q uestion 3.  C onsider the database W i sconsin B reast C ancer.  T he database i s as 
fol lowing: 

5 

1.  Number of Instances:  699 (as of 15 July 1992) 

2.  Number of Attributes:  10 plus the class attribute 

3.  Attribute Information:  (class attribute has been moved to last column) 
Domain

#  Attribute 
id number

1.  Sample code number 
2.  Clump Thickness 
1 - 10

3.  Uniformity of Cell Size 
1 - 10

4.  Uniformity of Cell Shape 
1 - 10

1 - 10

5.  Marginal Adhesion 
1 - 10

6.  Single Epithelial Cell Size 
1 - 10

7.  Bare Nuclei 
8.  Bland Chromatin 
1 - 10

1 - 10

9.  Normal Nucleoli 
1 - 10

10.  Mitoses 
(2 for benign, 4 for malignant)

11.  Class: 

The original description of the database is in: 

Wolberg,  W.  H.,  &  Mangasarian,  O.  L.  (1990).  Multisurface  method  of  pattern 
separation for medical diagnosis applied to breast cytology.  In Proceedings of the 
National Academy of Sciences, 87, 9193 –9196. 

Assess whether the conclusions of the paper are consistent with the structure of the 
learned 

1.  Learn the Bayesian network from the database. 

2.  Encode the model suggested by the original paper as a Bayesian network. 

3.  List the differences between the two models, if any. 

4.	 How  important  are  these  differences  and  how  they  change  the  predictive 
power  of  the model,  i.e.  the  ability  to  predict  the malignancy  of  the  tumor 
given  the  evidence.  hint:  Try  to  propagate  different  instances  of  the  symp­
tom variables and assess how different are  the posterior distributions of  the 
variable Class. 

6


