9.520  Problem  set  1


Pr.  1.1  Reproducing  Kernel  Hilbert  Spaces 

1.  An RKHS  can  be  deﬁned  via  a  kernel K  and  has  the  following  reproducing  property: 
Ft [f ] = �K (t, ·), f (·)�K  = f (t). 

Given  two  functions 

f (x) = 

g (x) = 

�� 
aiK (x, xi ) 
i=1 
�� 
biK (x, xi ) 
i=1 

K ,  and  ||g ||2  ? 
what  is  �f , g �K ,  ||f ||2 
K
2.	 Given Mercer’s  theorem 

K (s, t) = 

� 
∞ 
λnφn (s)φn (t), 
n=1 
where  λn  ≥  0  and  the  following  deﬁnition  of  a  RKHS  norm  for  functions  f (x) = 
� 
n  cnφn (x) 
∞�  c2 
||f ||2  = 
n
k 
n=1  λn 
where  φn (x)  are  the  eigenfunctions  of  the  integral  operator  deﬁned  by  the  kernel. 
Prove  that  the  reproducing  property  holds.

Prove  that  one  gets  the  same  form  for  the  quantities  �f , g �K ,  ||f ||2 
K ,  and  ||g ||2  as 
K

obtained  in  part  1 where 
∞ 
� 
cnφn (·) 
n=1 
∞ 
� 
g (·) = 
dnφn (·) 
n=1 
and  compute  the  relation  between  the  a i  and  the  cn . 

f (·) = 

Pr.  1.2  The  discrete  counterpart  in  IRn  of  the  eigenfunction  equation 
� 
0 
where  K (x, y )  is  a  positive  deﬁnite  function  and  µ  a  suitable  measure,  is 

K (x, y )v (y )dµ(y ) = σv (x), 

1 

with  K  a  positive  deﬁnite  matrix  and  P  a  diagonal  positive  deﬁnite  matrix  (basically  a 
weighting  on  each  data  point). 

K P v = σv 

(1) 

1 

f  = 

with 

1.	 Show  that  there  exists  a  solution  to  equation  (1)  of  n  linearly  independent  vectors  v i 
and corresponding strictly positive values σ i . (Hint:  Consider the matrix P 1/2 K P 1/2 ...) 
2.	 Let  f  ∈ IRn  be  represented  as 
� 
n 
ai vi 
i=1 
ai  = f � P vi . 
n�  a2 
||f ||2  = 
i
k 
i=1  σ 
i 
is  independent  of  P .  What  can  you  conclude  about  the  dependence  on  the  measure 
P  of  the  norm  of  f  in  the  RKHS? 

Prove  that  the  norm  of  f 

Pr  1.3  Given  a  training  set  of  points  not  necessarily  linearly  separable  consider  the  SVM  ob­
tained  by  minimizing 
�
1 � 
1
λ�w�2  + 
2 ,
ξi 
2  i=1 
2
λ > 0,  with  respect  to w,  b,  and  ξ  sub ject  to  the  constraints 
ξ
yi (w · xi  + b) ≥ 1 − ξi ,
1.	 Derive  the  dual  formulation  and  show  that  the  associated  QP  is  equivalent  to  the 
QP  discussed  in  class  in  the  linearly  separable  case  but  with  the  Kernel  matrix  with 
entries  deﬁned  as 

i = 1, ..., �. 

Kij  = 

1 
yiyj xi  · xi  + δij . 
λ
2.	 Set  b  =  0  and  show  that  if  the  �  inequalities  become  equalities  the  learning  scheme 
is  Regularized  Least-Squares  Classiﬁcation.  Derive  the  associated  linear  system  of 
equations  and  compare  the  matrix  which  has  to  be  inverted  with  the  Kernel  matrix 
above. 

i = 1, . . . , � 

Pr  1.4  For  the  SVM  classiﬁcation  problem  we  have  the  following  optimality  conditions 
�
�
� 
� 
yiαj K (xi , xj ) = 0 
cj K (xi , xj ) − 
j=1 
j=1 
�� 
αiyi  = 0 
i=1 
C − αi  − ζi  = 0 
�� 
yj αj K (xi , xj ) + b) − 1 + ξi  ≥ 0 
yi( 
j=1 
�� 
yj αj K (xi , xj ) + b) − 1 + ξi ] = 0 
αi [yi ( 
j=1 
ζiξi  = 0 
ξi , αi , ζi  ≥ 0 

i = 1, . . . , � 
i = 1, . . . , �. 

i = 1, . . . , � 

i = 1, . . . , � 

i = 1, . . . , � 

2 

Derive  the  following  “reduced”  optimality  conditions: 
αi  = 0  “ ⇐⇒ ”  yif (xi ) ≥ 1 
0 < αi  < C  “ ⇐⇒ ”  yif (xi ) = 1 
αi  = C  “ ⇐⇒ ”  yif (xi ) ≤ 1 
and  explain  why  we  put  quotes  around ⇐⇒. 
Pr  1.5  We are given the SVM for regression with the �-insensitive  loss function without a b term 
�
1 � 
(|f (x) − y | − �)+  + λ�f �2 
min 
K . 
f ∈H  � 
i=1 
1.	 Write  the primal  formulation with  slack variables  (Hint:  You need  two  slack variables 
for  the  error  at  each  point  instead  of  one  as  was  the  case  in  classiﬁcation). 
2.  Derive  the  Lagrangian  (Hint:  You  need  a multiplier  for  each  slack  variable). 
3.  Derive  the  dual  formulation. 

Pr  1.6  Consider  any  function  of  one  variable  that  is  continuous,  symmetric  and  periodic  with 
positive Fourier coeﬃcients.  Then K (x) can be expanded in a uniformly convergent Fourier 
series  (all  normalization  factors  set  to  1): 
∞ 
� 
λncos(nx) 
n=0 
∞ 
∞
� 
� 
λn sin(nx) sin(ny ) + 
K (x − y )  =  1 + 
n=1 
n=1 
the  eigenvalues  are  set  to  λn  = 2−n  and  the  eigenfunctions  of K  are 

λn  cos(nx) cos(ny ), 

K (x) = 

(1, sin(x), cos(x), sin(2x), cos(3x), ..., sin(px), cos(px), ...). 

The  RKHS  norm  of  this  function  is 
∞ 
�f �2  �  �f , cos(nx)�2  + �f , sin(nx)�2 
K  ≡ 
λn 
n=0 
1.  Show  that  if we  write  the  functions  as 
∞ 
� 
cn  sin(nx) + dn cos(nx) 
n=1 
then  the  coeﬃcients  cn  and  dn  are  bounded  as  follows 
2 cn  + d2  < 2−n .
n 

f (x) = 

< ∞. 

2.  Prove  that  the  space  of  functions  spanned  by  this  RKHS,  functions  spanned  by 
∞ 
� 
cn  sin(nx) + dn cos(nx) 
n=1 
with  ||f ||K  <  ∞  is  compact.  (Hint:  Look  at  the  proof  in  Mathcamp  1  of  the  com­
pactness  of  the  Hilbert  cube  and  use  the  fact  that  c2 
n  < 2−n .). 
n  + d2

f (x) = 

3 

Pr.  1.7  An  algorithm  is  (β , δ ) Cross-Validation  (CV)  stable  if  for  almost  all S  ∈ Z �  (except  for 
a  set  of measure  δ ),  the  following  holds: 

∀i, u ∈ Z ,  |c(fS , u) − c(fS i,u , u)| ≤ β . 
Assuming  that  our  algorithm  A  has  CV-stability  β  and  assuming  that  the  loss  function 
c(f , z )  (equal  to  V (f (x), y ) ≥ 0)  non-negative  and  bounded  above  my M ,  show  that 

|IES D [fS ]| ≤ β + M δ. 
Here  D [fS ]  is  the  defect  deﬁned  in  lectures  (empirical  error  - expected  error).  Note  that 
bounding the expectation of the defect was one of two things we had to show for application 
of McDiarmid’s  inequality. 

4


