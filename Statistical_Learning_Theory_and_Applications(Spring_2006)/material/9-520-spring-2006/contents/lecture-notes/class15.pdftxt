Stability  of  Tikhonov  Regularization 

9.520  Class  15,  05  April,  2006


Sasha  Rakhlin


Plan

Review  of  Stability  Bounds 
Stability  of  Tikhonov  Regularization  Algorithms 

• 
• 

Uniform  Stability

Review  notation:  S  = {z1 , ..., zn};  S i,z  = {z1 , ..., zi−1 , z , zi+1 , ..., zn} 

An  algorithm  A  has  uniform  stability  β  if 
∀(S, z) ∈ Z n+1 ,  ∀i,  sup  V (fS , u) − V (fS i,z , u) ≤ β . 
|
|
�
� 
u∈Z 
1
Last  class:  Uniform  stability  of  β  =  O 
n 
generalization  bounds. 
�
�
This  class:  Tikhonov  Regularization  has  uniform  stability 
1
of  β 
. n 
= O 

implies  good 

� 
Reminder:  The  Tikhonov  Regularization  algorithm:

n1 
V (f (xi), yi) + λ�f � 2 
fS  = arg min 
f ∈H 
K 
n i=1

Generalization  Bounds  Via  Uniform  Stability


If  β  =  k  for  some  k,  we  have  the  following  bounds  from 
� 
� 
� 
�
n 
the  last  lecture: 
n�2 
k 
P I [fS ] − IS [fS ]
|
+ � ≤ 2 exp 
−
| ≥ 
.
2(k + M )2 
n 
�
Equivalently,  with  probability  1 − δ , 
k 
I [fS ] ≤ IS [fS ] +  + (2k + M ) 
n

2 ln(2/δ)
n 

. 

Lipschitz  Loss  Functions,  I


We  say  that  a  loss  function  (over  a  possibly  bounded  do­
main  X )  is  Lipschitz  with  Lipschitz  constant  L  if 
, V (y1, y �) − V (y2, y �)
∀y1, y2, y � 
≤ L y1 − y2
|
∈ Y |
|

|

.

Put  diﬀerently,  if  we  have  two  functions  f1  and  f2,  under 
an  L-Lipschitz  loss  function, 
sup  V (f1(x), y) − V (f2(x), y)
|
(x,y) 
Yet  another  way  to  write  it: 
|∞ ≤ L|
|
f1(·) − f2(·)
V (f1, ·) − V (f2, ·)

≤ L f1 − f2
|

|∞ 

|

|∞

.

Lipschitz  Loss  Functions,  I I


If  a  loss  function  is L-Lipschitz,  then  closeness  of  two  func­
tions  (in  L∞  norm)  implies  that  they  are  close  in  loss. 

The  converse  is  false  —  it  is  possible  for  the  diﬀerence  in 
loss  of  two  functions  to  be  small,  yet  the  functions  to  be 
far  apart  (in  L∞).  Example:  constant  loss. 

The hinge loss and the �-insensitive loss are both L-Lipschitz 
with  L =  1.  The  square  loss  function  is  L Lipschitz  if  we 
can bound  the  y values and  the  f (x) values generated.  The 
0 − 1  loss  function  is  not  L-Lipschitz  at  all —  an  arbitrarily 
small  change  in  the  function  can  change  the  loss  by  1: 

f1 = 0, f2 =  �, V (f1(x), 0) = 0, V (f2(x), 0) = 1. 


Lipschitz  Loss  Functions  for  Stability


Assuming  L-Lipschitz  loss,  we  transformed  a  problem  of 
bounding 

|
sup  V (fS , u) − V (fS i,z , u)
u∈Z 
|
|
into  a  problem  of  bounding  fS  − fS i,z  ∞.

|

As  the  next  step,  we  bound  the  above  L∞  norm  by  the 
norm  in  the  RKHS  assosiated  with  a  kernel  K . 

For  our  derivations,  we  need  to  make  another  assumption: 
�
there  exists  a  κ  satisfying 
K (x, x) ≤ κ. 
∀x ∈ X , 

Relationship  Between  L∞  and  LK


Using  the  reproducing  property  and  the  Cauchy-Schwartz 
inequality,  we  can  derive  the  following: 
|f (x) = 
| 
), f (·) K
|�K (x, ·
∀x 
| 
�
�
||K (x, ·)||K ||f ||K 
≤ 
�
�K (x, ·), K (x, ·
)�||f ||K 
= 
K (x, x)||f ||K 
= 
≤  κ||f ||K 
Since above  inequality holds  for all x, we have  f|

|∞ ≤  ||f ||K . 

Hence,  if  we  can  bound  the  RKHS  norm,  we  can  bound 
the  L∞  norm.  Note  that  the  converse  is  not  true. 

Note  that  we  now  transformed  the  problem  to  bounding 
||fS  − fS i,z ||K . 

A  Key  Lemma


We  will  prove  the  following  lemma  about  Tikhonov  reg­
ularization: 
L fS  − fS i,z 
K  ≤ |
|∞
||fS  − fS i,z || 2 
λn 

This  theorem  says  that  when  we  replace  a  point  in  the 
training  set,  the  change  in  the  RKHS  norm  (squared)  of 
the  diﬀerence  between  the  two  functions  cannot  be  too 
large  compared  to  the  change  in  L∞. 

We  will  ﬁrst  explore  the  implications  of  this  lemma,  and 
defer  its  proof  until  later. 

Bounding  β ,  I


Using  our  lemma  and  the  relation  between  LK  and  L∞, 
L fS  − fS i,z 
K  ≤ |
|∞
||fS  − fS i,z || 2 
λn 
Lκ||fS  − fS i,z ||K 
≤ 
λn 

Dividing  through  by  ||fS  − fS i,z ||K ,  we  derive 
κL 
||fS  − fS i,z ||K  ≤ 
. 
λn


Bounding  β ,  I I


Using  again  the  relationship  between  LK  and  L∞,  and  the 
L  Lipschitz  condition, 
sup  V (fS (·), ·) − V (fS z ,i (·), ·)
|

L fS  − fS z ,i
| ≤ |
|∞ 
Lκ||fS  − fS z ,i ||K
≤ 
L2κ2 
≤ 
λn 
=  β 

Divergences


Suppose  we  have  a  convex,  diﬀerentiable  function  F ,  and 
we  know  F (f1)  for  some  f1.  We  can  “guess”  F (f2)  by 
considering  a  linear  approximation  to  F  at  f1: 
ˆ
F (f2) = F (f1) +  f2 − f1, �F (f1) . 
�
�

The  Bregman  divergence  is  the  error  in  this  linearized  ap­
proximation: 
dF (f2, f1) = F (f2) − F (f1) −  f2 − f1, �F (f1) . 
�
�

Divergences  Illustrated


(f2, F (f2)) 

dF (f2, f1) 

(f1, F (f1)) 

Divergences  Cont’d


We  will  need  the  following  key  facts  about  divergences: 

•	 dF (f2, f1) ≥ 0 
•	 If f1 minimizes F ,  then the gradient  is zero, and dF (f2, f1) = 
F (f2) − F (f1). 
•

If  F  =  A +  B ,  where  A  and  B  are  also  convex  and 
diﬀerentiable,  then  dF (f2, f1)  =  dA(f2, f1) + dB (f2, f1) 
(the  derivatives  add). 

The  Tikhonov  Functionals

1
 n�

We  shall  consider  the  Tikhonov  functional

V (f (xi), yi) + λ||f || 2 
K , 
n i=1 
n�

as  well  as  the  component  functionals 
1 
V (f (xi), yi) 
n i=1 

VS (f ) = 

TS (f ) = 

and 

N (f ) =  ||f ||K . 
2
Hence,  TS (f ) =  VS (f ) +  λN (f ). 
If  the  loss  function  is 
convex  (in  the  ﬁrst  variable),  then  all  three  functionals  are 
convex. 

A  Picture  of  Tikhonov  Regularization 


RFfsfs’Vs’(f)N(f)Ts’(f)Ts(f)Vs(f)Proving  the  Lemma,  I


Let fS  be  the minimizer of TS ,  and  let fS i,z  be  the minimizer 
of  TS i,z ,  the  perturbed  data  set  with  (xi, yi)  replaced  by  a 
new  point  z = (x, y).  Then 
λ(dN (fS i,z , fS ) + dN (fS , fS i,z ))  ≤ 
dTS (fS i,z , fS ) + dT
(fS , fS i,z ) =
S i,z

1

≤ 
(V (fS i,z , zi) − V (fS , zi) + V (fS , z) − V (fS i,z , z)) 
n 
|∞ 
2L fS  − fS i,z
|
. 
n 

We  conclude  that 
dN (fS i,z , fS ) + dN (fS , fS i,z ) ≤

2L fS  − fS i,z
|
λn


|∞ 

Proving  the  Lemma,  I I


But  what  is  dN (fS i,z , fS )? 

We  will  express  our  functions  as  the  sum  of  orthogonal 
� 
eigenfunctions  in  the  RKHS: 
∞
cnφn(x) 
� 
n=1 ∞
c� φn(x)
n
n=1 
�  2 
Once  we  express  a  function  in  this  form,  we  recall  that 
c
∞
||f || 2 
n 
K
n=1 λn 

fS i,z (x) = 

fS (x) = 

= 

Proving  the  Lemma,  I I I


Using  this  notation,  we  reexpress  the  divergence  in  terms 
�
of  the  ci  and  ci: 
� 
� 
�
K  − �fS i,z  − fS , �||fS || 2  �
dN (fS i,z , fS ) =  ||fS i,z || 2 
K  − ||fS ||
2
K
∞ c�
∞
∞
2 
2 
� − cn)( 
2cn )
− 
− 
c
n 
n 
= 
(cn
� 
n=1 λn 
n=1  λn 
λn
i=1 
∞ c�
� cn 
n + cn − 2cn
2
2
� 
λn
n=1 
∞ �
(cn − cn)2
= 
λn
n=1 
=  ||fS i,z  − fS || 2 
K 
We  conclude  that 
dN (fS i,z , fS ) + dN (fS , fS i,z ) = 2||fS i,z  − fS || 2 
K 

= 

Proving  the  Lemma,  IV


Combining  these  results  proves  our  Lemma:

dN (fS i,z , fS ) + dN (fS , fS i,z )

||fS i,z  − fS || 2 = 
K 
2 
2L fS  − fS i,z |∞
|
≤
λn 

Bounding  the  Loss,  I


We  have  shown  that  Tikhonov  regularization  with  an  L-
Lipschitz  loss  is β -stable with β =  L2κ2
.  If we want  to actu-
λn 
ally  apply  the  theorems  and  get  the  generalization  bound, 
we  need  to  bound  the  loss. 

Let  C0  be  the  maximum  value  of  the  loss  when  we  predict 
and  Y ,  we  can
X
a  value  of  zero.  If  we  have  bounds  on 
ﬁnd  C0. 

Bounding  the  Loss,  I I


Noting  that  the  “all  0”  function  �0  is  always  in  the  RKHS, 
we  see  that 

Therefore, 

λ||fS || 2	 ≤ 
K
≤ 

= 
≤ 

T (fS )
� 
T (�0)
1  n
V (�0(xi), yi) 
n
 i=1 
C0.

�
||fS ||K  ≤ 
C0 
2 
λ 
= ⇒ |fS |∞  ≤ κ||fS ||K  ≤ κ

C0 
λ 

|
|
Since  the  loss  is  L-Lipschitz,  a  bound  on  fS  ∞  implies
boundedness  of  the  loss  function. 


A  Note  on  λ


We  have  shown  that  Tikhonov  regularization  is  uniformly 
stable  with 

L2κ2 
β = 
. 
λn

� 
�
If  we  keep  λ  ﬁxed  as 	 we  increase  n,  the  generalization
bound  will  tighten  as  O  √
1
n  .  However,  keeping  λ  ﬁxed  is 
equivalent  to  keeping  our  hypothesis  space  ﬁxed.  As  we 
get  more  data,  we  want  λ  to  get  smaller.  If  λ  gets  smaller 
too  fast,  the  bounds  become  trivial. 

Tikhonov  vs.  Ivanov

� 
It  is  worth  noting  that  Ivanov  regularization

1  n
fˆH,S  =  arg min 
f ∈H 
n i=1
�f �K  ≤ τ 
2

V (f (xi), yi) 
�
�
1
is  not  uniformly  stable  with  β = O 
,  essentially  because 
n 
the  constraint  bounding  the  RKHS  norm may  not  be  tight. 
This  is  an  important  distinction  between  Tikhonov  and 
Ivanov  regularization. 

s.t. 

