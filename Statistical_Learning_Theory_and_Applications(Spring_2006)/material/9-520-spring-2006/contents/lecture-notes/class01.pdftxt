9.520

Statistical Learning Theory and 
Statistical Learning Theory and 
Applications 
Applications 

Sasha Rakhlin and Andrea Caponnetto and Ryan Rifkin + tomaso poggio

9.520, spring 2006

Learning: Brains and Machines
Learning: Brains and Machines

Learning is the gateway to 
understanding the brain and to 
making intelligent machines. 

Problem of learning: 
a focus for 
o modern math
o computer algorithms
o neuroscience

9.520, spring 2006

Learning: much more than memory

(cid:131) Role of learning (theory and applications 
in many different domains) has grown substantially in CS

(cid:131) Plasticity and learning have a central stage in the 
neurosciences

(cid:131) Until now math and engineering of learning has developed 
independently of neuroscience…but it may begin to change: we 
will see the example of learning+computer vision…

Learning:
math, engineering, neuroscience
1
⎤
⎡
∑l
2
fμ
f x
V y
m i n
(
) )
(
,
+
⎢
⎥
K
f H
⎣
⎦
∈
i
1
=

i

l

i

Theorems on foundations of learning:

Learning theory
+ algorithms

Predictive algorithms

ENGINEERING 
APPLICATIONS

• Bioinformatics
• Computer vision
• Computer graphics,  speech     
synthesis,  creating a virtual actor

Computational
Neuroscience: 
models+experiments

How visual cortex works – and how it 
may suggest better computer vision 
systems

Class
Class

Rules of the game: problem sets (2)
final project (min = review; max = j. paper)
grading
participation!
mathcamps? Monday late afternoon?

Web site: http://www.mit.edu/~9.520/

9.520, spring 2006

9.520 Statistical Learning Theory and Applications 
9.520 Statistical Learning Theory and Applications 
Class 24: Project presentations

2:30—2:45 "Adaboosting SVMs to recover motor behavior from motor 
data", Neville Sanjana
2:45-3:00 "Review of Hierarchical Learning", Yann LeTallec
3:00—3:15 "An analytic comparison between SVMs and Bayes Point 
Machines", Ashis Kapoor
3:15-3:30 "Semi-supervised learning for tree-structured data", Charles 
Kemp
3:30—3:45 “Unsupervised Clustering with Regularized Least Square 
classifiers" - Ben Recht
3:40—3:50 "Multi-modal Human Identification."  Brian Kim
3:50—4:00 "Regret Bounds, Sequential Decision-Making and Online 
Learning", Sanmay Das

9.520, spring 2003

9.520 Statistical Learning Theory and Applications 
9.520 Statistical Learning Theory and Applications 
Class 25: Project presentations

2:35-2:50 "Learning card playing strategies with SVMs", David 
Craft and Timothy Chan
2:50-3:00 "Artificial Markets: Learning to trade using Support 
Vector Machines“, Adlar Kim
3:00-3:10 "Feature selection: literature review and new 
development'‘, Wei Wu
3:10—3:25 "Man vs machines: A computational study on face 
detection" Thomas Serre

9.520, spring 2003

9.520, spring 2006

Overview of overview
Overview of overview

o  The problem of supervised learning: “real” math 
o  The problem of s
behind it

o   Examples of engineering applications (from our 
group)

o   Learning and the brain (example of object 
Learning and the brain (example of object 
recognition)
recognition)

9.520, spring 2006

Learning from examples: goal is not to memorize 
but to generalize, eg predict.
f
x
,
1

a set of llexamples (data)
examples (data)
Given Given a set of 

OUTPUT

,)

...,

INPUT

(),

x

})

,

y

(

x

{
(

,

y

2

2

y

1

l

l

: find function ffsuch that 
such that 
QuestionQuestion: find function 

good predictorof of yyfor a 
for a futurefutureinput 
input x (fitting the data is 
x (fitting the data is not not 
is a good predictor
is a 
enough!):
enough!):
ˆ
xf
y
)
(
=

Reason for you to know theory

We will speak today and later about applications…

they are not simply using a black box. The best ones are about 
the right formulation of the problem (choice of representation 
(inputs, outputs), choice of examples, validate predictivity, do not 
datamine)

x)(…
f

= wx

+

b

Notes

Two strands in learning theory:

(cid:137) Bayes, graphical models…

(cid:137) Statistical learning theory, regularization (closer to classical 
math, functional analysis+probability theory+empirical process 
theory…)

Interesting development: the theoretical foundations of 
Interesting development: the theoretical foundations of 
learning are becoming part of mainstream mathematics
learning are becoming part of mainstream mathematics

Learning from examples: predictive
predictive , multivariate 
, multivariate 
Learning from examples: 
function estimation from sparse data 
function estimation from sparse data 
(not just curve fitting) 
(not just curve fitting) 

= data from f
= function f
= approximation of   f

y

Generalization:  estimating value of function where 
there are no data (good generalization means 
predicting the function well; most important is for        
empirical or validation error to be a good proxy of the 
prediction error)

x

Regression:      function is real valued

Classification:   function is binary
9.520, spring 2006

Thus…… .the key requirement (main focus of learning 
.the key requirement (main focus of learning 
Thus
theory) to solve the problem of learning from 
theory) to solve the problem of learning from 
examples: 
examples: 
(and possibly even consistency)
consistency) ..
generalization (and possibly even 
generalization

A standard way to learn from examples is ERM (empirical risk 
minimization) 

The problem does not have a predictivesolution in general 
(just fitting the data does not work). Choosing an appropriate 
hypothesis space H (for instance a compact set of continuous 
functions) can guarantee generalization (how good depends on 
the problem and other parameters).

9.520, spring 2006

Learning from examples: another goal (from inverse 
Learning from examples: another goal (from inverse 
problems) is to ensure that problem is well--posed (solution 
posed (solution 
problems) is to ensure that problem is well
exists stable)
exists stable)

A problem is well-posed if its solution
exists, unique and 
J. S. Hadamard, 1865-1963
is stable, eg depends continuously on the data 
(here examples) 

9.520, spring 2006

Thus…… .two key requirements to solve the problem 
.two key requirements to solve the problem 
Thus
of learning from examples: 
of learning from examples: 
posedness andand generalization
wellwell --posedness
generalization

Consider the standard learning algorithm, i.e. ERM 

The main focus of learning theory is predictivityof the 
solution eg generalization. The problem is in addition ill-posed. 
It was known that by choosing an appropriate hypothesis space 
H predictivity is ensured. It was also known that appropriate H
provide well-posedness. 
A couple of years ago it was shown that generalization and 
well-posedness are equivalent, eg one implies the other. 
Thus a stablesolution is  predictiveand (for 
ERM) also  viceversa.

9.520, spring 2006

More later…..

9.520, spring 2006

Learning theory and natural sciences

Conditions for generalization in learning theory

have deep, almost philosophical, implications:

they may be regarded as conditions that guarantee a 
theory to be predictive(that is scientific) 

We have used a simple  algorithm 
-- that ensures generalization --
in most of our applications…

1min
⎡
⎢
Hf
⎣
∈
l

l
∑
i
1
=

xfV
(
(
i

)

−

y

i

)

+

λ

f

2
K

⎤
⎥
⎦

implies

f

xx
,(

x ∑= α
l
i K
)(
i
Equation includes Regularization Networks (special cases 
are splines, Radial Basis Functions and Support Vector 
Machines). Function is nonlinear and general approximator…

)

i

For a review, see Poggio and Smale, The Mathematics of Learning, 
Notices of the AMS, 2003

Classical framework but with more general 
Classical framework but with more general 
loss function
loss function

space of functions or  ““hypotheses
quite general space of functions or 
The algorithm uses a quite general
hypotheses ”” : : 
The algorithm uses a 
RKHSs .. n of the classical framework can provide a better measure 
RKHSs
of “loss” (for instance for classification)…

1min
⎡
⎢
Hf
⎣
∈
l

l
∑
i
1
=

xfV
(
(
i

)

−

y

i

)

+

λ

f

2
K

⎤
⎥
⎦

9.520, spring 2006

Girosi, Caprile, Poggio, 1990

Another remark: equivalence to networks

Many different V  lead to the same solution…
= ∑

xx
,(

Kc
i

x
)(

f

l
i

)

+

b

i

1x

…and can be “written” as 
the same type of  network…where the 
value of K corresponds to the “activity”
ic
of the “unit” and the     correspond to 
dx
(synaptic) “weights”

K

K

ic

K

+

f

Theory summary
Theory summary
In the course we will introduce 

• Generalization (predictivity of the solution)
• Stability (well-posedness) 
• RKHSs hypotheses spaces
• Regularization techniques leading to RN and SVMs
• Manifold Regularization (semisupervised learning)
• Unsupervised learning
• Generalization bounds based on stability
• Alternative classical bounds (VC and Vgamma dimensions)

• Related topics

• Applications

9.520, spring 2006

S
y

Syllabus

9.520, spring 2006

Overview of overview
Overview of overview

o  Supervised learning: real math
o  Supervised learning: real math
o  Examples of recent and ongoing in--house engineering 
house engineering 
o  Examples of recent and ongoing in
on applications
on applications
o  Learning and the brain
o  Learning and the brain

9.520, spring 2006

engineering 
Learning from Examples: engineering 
Learning from Examples: 
applications
applications

INPUT
INPUT

OUTPUT
OUTPUT

Bioinformatics 
Artificial Markets
Object categorization
Object identification 
Image analysis
Graphics
Text Classification
…..

9.520, spring 2006

Bioinformatics application: predicting type of 
cancer from DNA chips signals
Learning from examples paradigm

Statistical Learning 
Algorithm

Examples

Prediction  

Prediction

New sample

9.520, spring 2006

Bioinformatics application: predicting type of 
cancer from DNA chips

New feature selection SVM:

Only 38 training examples, 7100 features

AML vs ALL: 40 genes 34/34 correct, 0 rejects.
5 genes 31/31 correct, 3 rejects of which 1 is an error.

Pomeroy, S.L., P. Tamayo, M. Gaasenbeek, L.M. Sturia, M. Angelo, M.E. 
McLaughlin, J.Y.H. Kim, L.C. Goumnerova, P.M. Black, C. Lau, J.C. Allen, D. 
Zagzag, M.M. Olson, T. Curran, C. Wetmore, J.A. Biegel, T. Poggio, S. 
Mukherjee, R. Rifkin, A. Califano, G. Stolovitzky, D.N. Louis, J.P. Mesirov, E.S. 
Lander and T.R. Golub. Prediction of Central Nervous System Embryonal
Tumour Outcome Based on Gene Expression, Nature, 2002. 

9.520, spring 2006

engineering 
Learning from Examples: engineering 
Learning from Examples: 
applications
applications

INPUT
INPUT

OUTPUT
OUTPUT

Bioinformatics 
Artificial Markets
Object categorization
Object identification
Image analysis
Graphics
Text Classification
…..

9.520, spring 2006

Face identification: example
Face identification: example

An old view --based system: 15 views
based system: 15 views
An old view

Performance: 98% on 68 person database
Beymer, 1995

9.520, spring 2006

engineering 
Learning from Examples: engineering 
Learning from Examples: 
applications
applications

INPUT
INPUT

OUTPUT
OUTPUT

Bioinformatics 
Artificial Markets
Object categorization
Object identification 
Image analysis
Graphics
Text Classification
…..

9.520, spring 2006

System Architecture 

Scanning in x,y and 
scale

Preprocessing with 
overcomplete
dictionary of Haar
wavelets 

SVM Classifier

TRAINING

Data Base

QP Solver

9.520, spring 2006

Sung, Poggio 1994; Papageorgiou and Poggio, 1998

People classification/detection: training 
People classification/detection: training 
the system
the system

. . .

. . .

1848 patterns
7189 patterns
Representation: overcomplete dictionary of Haar wavelets;  high
dimensional feature space (>1300 features)

Core learning algorithm:
Support Vector Machine
classifier

pedestrian detection system 

9.520, spring 2006

Trainable System for  Object Detection: 
Pedestrian detection - Results

Papageorgiou and Poggio, 1998

The system was tested in a test car 
(Mercedes)

System installed in 
experimental Mercedes

A fast version, integrated 
with a real-time obstacle 
detection system

MPEG

Constantine Papageorgiou

People classification/detection: training the 
People classification/detection: training the 
system
system

. . .

. . .

1848 patterns
7189 patterns
Representation: overcomplete dictionary of Haar wavelets;  high
dimensional feature space (>1300 features)

Core learning algorithm:
Support Vector Machine
classifier

pedestrian detection system 

9.520, spring 2006

Face classification/detection: training the 
Face classification/detection: training the 
system
system

. . .

. . .

Representation: grey levels (normalized) or overcomplete
dictionary of Haar wavelets

Core learning algorithm:
Support Vector Machine
classifier

face detection system 

9.520, spring 2006

Face identification: training the system
Face identification: training the system

. . .

. . .

Representation: grey levels (normalized) or overcomplete
dictionary of Haar wavelets

Core learning algorithm:
Support Vector Machine
classifier

face identification system 

9.520, spring 2006

Computer vision: new StreetScenes
Project
Learning Algorithms for Scene Understanding

Project Timeline

Construction of 
the StreetScenes 
Database

Automatic 
Learning of object 
specific features 
or parts

Recognition of 
10 Object 
Categories

Automatic Scene 
Description

Lior Wolf, Stan Bileschi, …

Learning from Examples: Applications
Learning from Examples: Applications

INPUT
INPUT

OUTPUT
OUTPUT

Object identification
Object categorization
Image analysis
Graphics
Finance
Bioinformatics
…
9.520, spring 2006

Image Analysis
Image Analysis

IMAGE ANALYSIS: OBJECT RECOGNITION AND POSE 
IMAGE ANALYSIS: OBJECT RECOGNITION AND POSE 
ESTIMATION
ESTIMATION

⇒ Bear (0° view)

⇒ Bear (45° view)

9.520, spring 2006

Computer vision: analysis of facial expressions

12

10

8

6

4

2

0

3
1 7 1

1
5 3
2

9
1

9
3 4
4

7
3

1
5 6
5

7
6

9
3 7
7

5
8

The main goal is to estimate basic facial parameters, e.g. 
degree of mouth openness, through learning. One of the main 
applications is video-speech fusion to improve speech 
recognition systems.

9.520, spring 2002

Kumar, Poggio, 2001

Learning from Examples: engineering 
applications

CBCL

INPUT
INPUT

MIT

OUTPUT
OUTPUT

Bioinformatics 
Artificial Markets
Object categorization
Object identification 
Image analysis
Image synthesis, eg Graphics
Text Classification
…..

9.520, spring 2003

Image Synthesis
Image Synthesis

Metaphor for UNCONVENTIONAL GRAPHICS
Metaphor for UNCONVENTIONAL GRAPHICS

Θ= 0° view ⇒

Θ= 45° view ⇒

9.520, spring 2006

Reconstructed 3D Face Models from 1 image

9.520, spring 2006

Blanz and Vetter,
MPI
SigGraph ‘99

Reconstructed 3D Face Models from 1 
Reconstructed 3D Face Models from 1 
image
image

9.520, spring 2006

Blanz and Vetter,
MPI
SigGraph ‘99

V. Blanz, C. Basso, 
T. Poggio
and 
T. Vetter, 2003

Extending the same basic learning techniques (in 2D): 
Trainable Videorealistic Face Animation
(voice is real, video is synthetic)

Ezzat, Geiger, Poggio, SigGraph 2002

Trainable Videorealistic Face Animation
2. Run Time
1. Learning

For any speech input the system 
provides as output a synthetic 
video stream
Phone Stream
/AE/
/JH/
/AE/
/AE/
/JH/

/SIL/
/B/

/B/

/JH//SIL/

Trajectory 
Synthesis

Phonetic Models

MMM

Image Prototypes

System learns from 4 mins
of video the face appearance 
(Morphable Model) and the 
speech dynamics of the 
person

Tony Ezzat,Geiger, Poggio, SigGraph 2002

A Turing test: what is real and what is 
synthetic?

We  assessed the realism of the talking face  with 
psychophysical experiments.
Data suggest that the system passes a visual  
version of the Turing test.

Overview of overview
Overview of overview

o  Supervised learning: the problem and how to frame 
o  Supervised learning: the problem and how to frame 
it within classical math
it within classical math
o  Examples of in--house applications
house applications
o  Examples of in
o  Learning and the brain
o  Learning and the brain

9.520, spring 2006

Learning to recognize objects and the ventral 
stream in visual cortex

Some numbers
Human Brain

1011… 1012 neurons
1014 + synapses

Neuron
Fine dendrites : 0.1 µ diameter
Lipid bylayer membrane : 5 nm thick
Specific proteins : pumps, channels, receptors, enzymes
Synaptic packet of transmitter opens 2 x 103 channels 
(with 104 AcH molecules)
Each channel: conductance g = 10-11 mho
Fundamental time length : 1 msec

A theory 
of the ventral stream of visual cortex

Thomas Serre, Minjoon Kouh,  Charles Cadieu, Ulf Knoblich
and Tomaso Poggio

The McGovern Institute for Brain Research, 
Department of Brain Sciences 
Massachusetts Institute of Technology

The Ventral Visual Stream: From V1 to IT

modified from Ungerleider and Haxby, 1994

Hubel & Wiesel, 1959

Desimone, 1991

Desimone, 1991

Summary of “basic facts”
Accumulated evidence points to three (mostly accepted) 
properties of the ventral visual stream architecture:

• Hierarchical build-up of invariances (first to 
translation and scale, then to viewpoint etc.) , size of 
the receptive fields and complexity of preferred 
stimuli

• Basic feed-forward processing of information (for 
“immediate” recognition tasks)

• Learning of  an individual object generalizes to scale 
and position

Mapping the ventral stream into a model

Serre, Kouh, Cadieu, Knoblich, Poggio, 2005; 
Riesenhuber et al, Nat. Neuro, 1999,2000

…

The  model
Claims to interpret or predict several existing data in microcircuits and system 
physiology,  and also in cognitive science:

• What some complex cells in V1 and V4  do and why: MAX…

• View-tuning of IT cells (Logothetis)
• Response to pseudomirror views
• Effect of scrambling 
• Multiple objects
• Robustness/sensitivity to clutter
• K. Tanaka’s simplification procedure
• Categorization tasks (cats vs dogs)
• Invariance to translation, scale etc…
• Read-out data…

• Gender classification
• Face inversion effect : experience, viewpoint, other-race, configural
vs. featural representation
• Binding problem, no need for oscillations…

Neural Correlate of Categorization (NCC)

Define categories in morph space
60% Dog
Morphs

60% Cat
Morphs

80% Cat
Morphs

Prototypes
100% Cat

80% Dog
Morphs

Prototypes
100% Dog

9.520, spring 2006

Category 
boundary

Categorization task

Train monkey on categorization task

.

Fixation
500 ms.

.

Sample
600 ms.

.

.

(Match)

.

Delay
1000 
ms.

Test
(Nonmatch)

.

Delay

.

Test
(Match)
After training, record from neurons in IT & PFC

9.520, spring 2006

Single cell example: a “categorical” PFC neuron that 
responds more strongly to DOGS than CATS

F ixation 

Sample 

Delay 

Cho ice

13

10

7

4

)
z
H
(
 
 
e
t
a
R
 
g
n
i
r
i
F

1
-500

Dog 100% 
Dog 80% 
Dog 60% 

Cat 100% 
Cat 80% 
Cat 60% 

2000
1500
1000
500 
0   
T ime f rom sample stimulus onset  (ms)

9.520, spring 2006

D. Freedman + E. Miller + M. 
Riesenhuber+T. Poggio (Science, 
2001)

The model fits many physiological data, 
predicts several new ones…

recently it provided a surprise (for us)…

…when we compared its performance  with 
machine vision…

Sample Results on the CalTech 101-object dataset

The model performs at the level of the best 
computer vision systems

…and another surprise…

… was the comparison with human performance
(Thomas Serre with Aude Oliva) 
on rapid categorization of complex natural images

Experiment: rapid (to avoid backprojections) 
animal detection in natural images

Image

Interval 
Image-Mask
Mask
1/f noise

20 msec

30 msec

80 msec
[Thorpe et al, 1996; Van Rullen & Koch, 2003; 
Oliva & Torralba, in press]

Animal present
or not ?

Targets and distractors

[Serre, Oliva & Poggio, in prep]

Humans achieve model-level performance

Model results obtained without tuning a single parameter!

Human: 80% correct
vs.
Model: 82% correct

[Serre, Oliva & Poggio, in prep]

Theory supported by data in V1, V4, IT; works as well as the best computer vision; mimics human 
performance

Freedman, Science, 2002
Logothetis et al., Cur. Bio., 1995
Gawne et al., J. Neuro., 2002
Lampl et al.,J. Neuro, 2004.

A challenge for learning theory: 

an unusual, hierarchical architecture
with unsupervised and supervised learning
and learning of invariances…

We will see later why this is unusual and interesting for learning 
theory!

