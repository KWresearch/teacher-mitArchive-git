Online  Learning 

9.520  Class   12,  20  March  2006


Andrea  Caponnetto,  Sanmay  Das


About  this  class


Goal  To  introduce  the   general  setting  of  online   learning. 


To  describe  an  online  version   of  the   RLS  algorithm  and  analyze 
its  performance. 

To  discuss  convergence   results  of  the   classical  Perceptron   algo(cid:173)
rithm. 

To  introduce  the   “experts”  framework and  prove  mistake   bounds 
in  that  framework. 

To  show  the   relationship  between  online  learning  and   the   theory 
of  learning  in  games. 

What  is  online   learning?


Sample  data  are  arranged  in  a  sequence.


Each  time  we  get  a  new  input,  the   algorithm  tries  to  predict  the 
corresponding  output. 

As  the  number  of  seen   samples  increases,   hopefully  the  predictions 
improve. 

Assets


1.  does  not  require  storing  all  data   samples


2.  typically  fast  algorithms  

3.  it  is  possible  to  give  formal  guarantees  not  assuming  probabilis(cid:173)
tic  hypotheses  (mistakes  bounds) 

but... 

•	 performance  can   be  worse   than   best   batch  algorithms


•	 generalization  bounds  always  require  some  assumption  on  the 
generation  of  sample   data 

Online  setting


Sequence  of  sample   data  z1,  z2, .   .  .   ,  zn.


Each  sample  is  an  input-output   couple  zi  = (xi,  yi).


xi  ∈ X  ⊂ IRd ,  yi  ∈ Y  ⊂ IR.


In  the  classiﬁcation  case  Y  =  {+1,  −1},  in  the  regression   case  Y  =

[−M ,   M ].


:  IR 
Loss  function  V 
V  (w,   y)  = 1  − yw +). 
|
|

×  Y  → 

IR+  (e.g. 

E (w,  y)  = Θ(−yw)  and


Estimators  fi  :  X Y  constructed  using  the   ﬁrst  i  data  samples. 
→ 

Online  setting  (cont.)


•  initialization  f0 

•  for  i  = 1,  2, .  .  .  ,  n 

•  receive  xi 

•  predict   fi−1(xi) 

•  receive  yi 

•  update  (fi−1,  zi)  →  fi 

Note:  storing  eﬃciently  fi−1  may  require  much  less  memory  than 
storing  all   previous  samples  z1,  z2, .   .  .   ,  zi−1. 

Goals 

Batch  learning:  reducing  expected  loss 

I [fn]  = IEzV  (fn(x),  y) 

Online  learning:  reducing  cumulative   loss  

n 
X 
i=1 

V  (fi−1(xi),  yi) 

Online  implementation  of   RLS  ∗


update  rule:  For   some   choice  of  the  sequences  of  positive  para(cid:173)
meters  γi  and  λi, 

(cid:1) 
(cid:0) 
fi  =  fi−1  − γi  (fi−1(xi) − yi)Kxi  +  λifi−1 
, 
where  K  :  X  × X  →  IR  is  a  pd  kernel  and  for  every  x  ∈ X ,  Kx(x′) = 
K (x,  x′). 

Note:  this  rule   has  a  simple  justiﬁcation  assuming  that   the  sample 
points   (xi,  yi)  are  i.i.d.  from  a  probability  distribution  ρ. 

∗Online  learning  algorithms.  Smale,  Yao.  05 

Interpretation   of  online  RLS


For  sake  of  simplicity,  let  us  set   λt  =  λ >  0. 

We  would  like  to  estimate   the   ideal  regularized  least-squares  esti(cid:173)
λ 
mator  fρ

λ  =  arg  min 
fρ
f ∈HK 

Z 
X×Y

(f (x) − y)2dρ(z) +  �f �K . 
2

λ  also  satisﬁes 
From  the  deﬁnition  above   it   can  be   showed  that   fρ
λ(x) − y)Kx  +  λfρ
λ]  = 0, 
IEz∼ρ[(fρ
therefore,  f λ 
is   also  the  equilibrium  point   of  the  averaged  online  
ρ 
update  equation 

fi  =  fi−1  − γiIEzi∼ρ[(fi−1(xi) − yi)Kxi  +  λfi−1]. 

Generalization  bound  for  online  algorithm  ∗


Theorem:  Let  fρ  be  the  minimizer  of  the  expected  squared  loss 
I [f ]  (i.e.  the  regression  function).   Assume  K (x,  x)  ≤  κ  for  some  
positive  constant  κ,  and  L−rfρ  ∈  L2(X,   ρX )  for  some  r  ∈  [1/2,  1]. 
K 
1 
2r 
Then  letting  γi  =  c1i− 2r+1  and  λi  =  c2i− 2r+1  for  some  constants  c1 
and  c2,  with  probability  greater  than  1  − δ ,  for  all   i  ∈ IN  it  holds 
2r 
I [fi]  ≤ I [fρ] +  C i− 
, 
2r+1
where  C  depends  on  M ,  κ,  r,  �L−rfρ�ρ  and  δ .
K 
2r 
(cid:19) 
(cid:18) 
Note:  the   rates  of  convergence  O i− 2r+1  are  the  best  theoretically  
attainable  under  these  assumptions. 


∗Online  learning  as  stochastic  approximations  of   regularization  paths.  Tarres, 
Yao.  05 

The  Perceptron  Algorithm


We  consider  the  classiﬁcation  problem:  Y  =  {−1,  +1}.


We  deal  with  linear   estimators  fi(x) =  ωi  x,  with   ωi  ∈ IRd .

· 

loss   E (fi(x),   y)  = Θ(−y(ωi  x)) 
The  0-1 
in 
is  the   natural  choice 
·

the   classiﬁcation  context.  We  will  also  consider   the  more  tractable 
hinge-loss 

V  (fi(x),  y)  = 1  − y(ωi  x) +.
|
·
|

Update  rule: 

If  Ei  =  E (fi−1(xi),  yi)  =  0  then  ωi  =  ωi−1,  otherwise 
ωi  =  ωi−1 +  yixi 

The  Perceptron  Algorithm  (cont.) 


Passive-Aggressive  strategy  of  the  update  rule.


If  fi−1  classiﬁes   correctly  xi,  don’t  move.


If  fi−1  classiﬁes   incorrectly,  try  to  increase  the  margin  yi(ω xi).   In

· 
fact, 

yi(ωi  xi) =  yi(ωi−1  xi) +  yi  �xi� 2  >  yi(ωi−1  xi)
2 
·
· 
· 

Perceptron  Convergence  Theorem  ∗


Theorem:  If  the   samples   z1, . . . , zn  are  linearly   separable,  then  pre(cid:173)
senting  them  cyclically   to  the  Perceptron  algorithm,   the  sequence 
of   weight  vectors  ωi  will  eventually  converge. 

We  will  proof a  more  general result encompassing  both the   separable 
and   the  inseparable   cases 

∗Pattern  Classiﬁcation.  Duda,   Hart,  Stork,  01 

Mistakes’  Bound  ∗


Theorem:  Assume  �xi� ≤ R  for  every  i  = 1,  2, .  .  .  ,  n.  Then   for 
every  u  ∈ IRd 
2 
v
 
u  n 
u  X 
R�u� +  t 
M ≤ 
 
i=1 
where  ˆVi  =  V  (u  xi,  yi)  and  M is  the  total   number  of  mistakes: 
·
n
n
M = 
i=1  E (fi−1(xi),  yi). 
i=1  Ei  = 
P
P

2 
ˆ
Vi 

, 

∗Online  Passive-Aggressive   Algorithms.  Crammer   et  al,  03 

Mistakes’  Bound  (cont.)


•	 the  boundedness  conditions  �xi� ≤ R  is  necessary. 

•	 in  the  separable  case,  there   exists  u∗ inducing margins  yi(u∗ xi)  ≥
·
1,  and  therefore  null  “batch” 
loss  over   sample  points.   The 
Mistakes’ Bound  becomes 
M ≤ R2 �

u∗ 2 .
� 

•	 in  the  inseparable   case,  we  can  let  u  be  the  best  possible  linear 
separator.  The  bound  compares  the  online   performance  with 
the  best  batch  performance  over   a   given  class  of  competitors. 

Proof 

The  terms  ωi  u  increase  as  i  increases 
·

1.  If  Ei  = 0  then  ωi  u  =  ωi−1  u
·
·

2.  If  Ei  = 1,  since   ˆVi  = 1  − yi(xi  u) +,
|
·
|
ωi  u  =  ωi−1  u  +  yi(xi  u)  ≥ ωi−1  u  +  1  − ˆVi.
·
·
·
·

3.  Hence,  in  both  cases  ωi  u  ≥ ωi−1  u  +  (1  − ˆVi)Ei
·
·

4.  Summing  up,  ωn  · u  ≥ M − 

n 
ˆ
i=1  ViEi. 
P

Proof  (cont.)  

The  terms  �ωi� do  not  increase  too  quickly 

1.  If  E = 0  then  �ωi�2 =  ωi−1�2 
�
i 

2.  If  E = 1,  since   yi(ωi−1  xi)  ≤ 0, 
·
i 
ωi� 2  =  (ωi−1 +  yixi) · (ωi−1 +  yixi) 
�
=  �ωi−1� 2 +  �xi� 2 +  2yi(ωi−1 

· xi)  ≤ �

ωi−1� 2 +  R2 .

3.  Summing  up,  �ωn�2 = 

.MR2 

Proof   (cont.)


Using  the  estimates  for  ωn  u  and  �ωn�2,  and  applying  Cauchy-
·
Schwartz  inequality 

1.  By  C-S,  ωn  ·

u�,  hence 
u  ≤ �ωn��

n

ˆ
ViEi  ≤ ωn  · u  ≤ �ωn��u� ≤
X 
i=1 

M − 

√

MR�u�

2.  Finally,  by  C-S, 

q 
n
n 
ˆ
i=1  ViEi  ≤ 
P
P
i=1 
v u  X 
M −  u  n 
√
t 
i=1 

V  2 
ˆ
i 

q 
2,  hence  
n
i=1  Ei 
P
2  ≤ R�u�
ˆ
Vi 

.

The  Experts  Framework


We  will  focus   on  the  classiﬁcation  case.


Suppose  we  have  a  pool  of  prediction  strategies,  called  experts. 
Denote  by  E =  {E1, . . . , En}. 

Each  expert  predicts  yi  based  on   xi. 

We  want  to  combine  these   experts  to  produce  a  single  master  al(cid:173)
gorithm  for  classiﬁcation  and  prove  bounds  on  how  much  worse  it 
is  than   the  best  expert. 

The  Halving   Algorithm∗


Suppose  all   the  experts  are  functions  (their   predictions  for  a  point 
in  the  space  do  not  change   over  time)  and  at   least  one   of  them  is 
consistent  with   the   data.  

At  each  step,  predict  what   the   majority   of  experts  that  have  not 
made  a   mistake  so  far  would  predict.  

Note  that  all   inconsistent  experts  get  thrown  away!


Maximum  of  log2( E )  errors. 
|
|

But  what  if  there  is  no  consistent  function  in  the  pool?  (Noise  in  
the   data,  limited  pool,  etc.) 

∗Barzdin  and  Freivald,  On   the  prediction  of  general  recursive  functions,  1972, 
Littlestone  and  Warmuth,  The  Weighted  Majority  Algorithm,  1994 

The  Weighted  Majority  Algorithm∗


Associate  a  weight  wi  with  every  expert.   Initialize  all  weights  to  1.  

At  example  t: 

= 

q−1 

|E |
X 
i=1 
|E |
X 
i=1 
Predict   yt  = 1  if  q1  >  q−1,  else  predict  yt  =  −1


wiI [Ei  predicted  yt  =  −1] 

q1 = 

wiI [Ei  predicted  yt  = 1] 

If  the  prediction  is  wrong,  multiply   the  weights  of  each  expert  that 
made  a   wrong  prediction  by  0  ≤ β  <  1. 

Note  that  for  β  = 0  we   get  the  halving  algorithm.  

∗Littlestone  and  Warmuth,  1994 

Mistake  Bound  for  WM

For  some  example  t  let   Wt  =  P|E | 
i=1  wi 
Then  when  a   mistake  occurs  Wt+1   ≤ uWt  where  u  <  1 

=  q−1 +  q1

Therefore  W0um  Wn≥
Or  m  ≤ log(W0/Wn) 
log(1/u) 

log(W0/Wn)
Then  m  ≤ log(2/(1+β )) (setting  u  = 1+β  
2 ) 

Mistake   Bound  for  WM  (contd.)


Why?  Because  when  a  mistake  is  made,  the   ratio  of   total  weight

after  the   trial  to  total   weight   before  the  trial  is  at   most  (1  +  β )/2.


W.L.o.G.  assume  WM   predicted  −1  and  the  true   outcome   was  +1. 
Then  new  weight   after  trial  is: 
β q−1 +  q1  ≤ β q−1 +  q1 + 1−β 
1+β
2 (q−1  − q1) 
=  2 (q−1 +  q1. 
The  main  theorem  (Littlestone  &  Warmuth):  
Assume  mi  is  the  number  of  mistakes  made  by  the   ith  expert  on  a  
sequence  of  n  instances  and  that  E =  k.  Then  the  WM  algorithm 
|
|
makes  at  most  the   following  number   of  mistakes: 


log(k) +   mi  log(1/β ) 
log(2/(1  +  β )) 

Big  fact: 
Ignoring  leading  constants,   the  number  of   errors  of   the

pooled  predictor  is  bounded  by  the  sum   of  the  number  of  errors  of

the  best  expert  in  the  pool  and  the  log  of   the   number  of  experts!


Finishing  the  Proof


W0 =  k  and   Wn  ≥ βmi


log(W0/Wn)  =  log(W0) − log(Wn)


log(Wn)  >  mi  log β ,  so  − log(Wn)  <  mi  log(1/β )


Therefore  log(W0) − log(Wn)  <  log k  +  mi  log(1/β )


A  Whirlwind  Tour  of   Game  Theory


Players   choose  actions,  receive  rewards  based  on  their   own  actions 
and   those  of  the  other  players. 

A  strategy   is   a  speciﬁcation  for  how  to  play   the  game  for   a  player. 
A  pure  strategy  deﬁnes,  for  every  possible  choice  a   player  could 
make,  which  action  the  player  picks.   A   mixed  strategy  is  a  prob(cid:173)
ability  distribution  over  strategies. 

A  Nash  equilibrium  is   a   proﬁle   of  strategies  for  all   players  such 
that  each  player’s  strategy 
is  an  optimal  response  to   the  other  
players’  strategies.  Formally,  a  mixed-strategy  proﬁle  σ i 
is  a  Nash 
∗ 
equilibrium   if  for  all   players  i: 
∗  )  ≥ i
i(σ i 
i 
u  (s  , σ∗−i)∀s  ∈ S i 
i 
∗, σ−i

u 

Some  Games:  Prisoners’  Dilemma


Cooperate  Defect 
Cooperate  +3,  +3 
0,  +5 
+1,  +1 
+5,  0 
Defect 

Nash  equilibrium:  Both  players  defect!


Some  Games:  Matching  Pennies


H 
T 
−1,  +1  −1,  +1 
+1,  −1 
+1,  −1 
Nash  equilibrium:  Both  players  randomize  half   and  half  between 
actions. 

H 
T 

Learning  in   Games∗


Suppose  I  don’t  know  what   payoﬀs  my   opponent  will  receive.


I  can  try  to  learn  her  actions  when  we  play   repeatedly  (consider 
2-player  games  for  simplicity). 

Fictitious   play  in  two  player  games.  Assumes  stationarity  of  oppo-
nent’s  strategy,  and  that  players  do  not  attempt  to  inﬂuence  each 
others’  future  play.  Learn  weight  functions  

i(s−i) =  κi
t−1(s−i) + 
κt

( 

if  s−t−
i 
1 =  s−i 
1 
0  otherwise 

∗Fudenberg   &  Levine,  The  Theory  of  Learning  in  Games,  1998


Calculate  probabilities  of  the  other  player  playing  various  moves  as:


i(s−i) 
κt
s−i∈S−i κi  s−i) 
t(˜
P 
˜
Then  choose  the  best  response  action.  

i(s−i) = 
γt

Fictitious  Play  (contd.)


If   ﬁctitious  play  converges,  it  converges  to  a  Nash   equilibrium.


If   the  two   players  ever  play  a  (strict) NE  at  time  t,  they  will   play  it 
thereafter. (Proofs  omitted) 

If   empirical   marginal   distributions  converge,   they  converge  to  NE. 
But  this  doesn’t  mean  that  play  is  similar! 

t  Player1  Action  Player2  Action 
1 
T 
T 
H 
T 
2 
3 
T 
H 
H 
H 
4 
5 
H 
H 
H 
H 
6 
7 
H 
T 

κ1 
T 
(1.5,  3) 
(2.5,  3) 
(3.5,  3) 
(4.5,  3) 
(5.5,  3) 
(6.5,  3) 
(6.5,  4) 

κ2 
T 
(2,  2.5)  
(2,  3.5)  
(2,  4.5)  
(3,  4.5)  
(4,  4.5)  
(5,  4.5)  
(6,  4.5)  

Cycling  of  actions  in  ﬁctitious   play  in  the  matching   pennies

game


Universal  Consistency


Persistent  miscoordination:  Players  start  with  weights   of   (1,  √2)


B 
A 
A  0,  0  1,  1 
B  1,  1  0,  0 

A  rule   ρi  is  said  to  be  ǫ-universally  consistent  if  for  any   ρ−i 
1 
lim  sup max  u  (σ i,  γt
X  ui(ρi
i
i) − 
t(ht−1))   ≤ ǫ 
T 
σ i 
T →∞ 
t
almost  surely  under  the  distribution  generated  by   (ρi,  ρ−i),  where 
ht−1  is  the  history  up  to  time   t − 1,  available  for  the  decision-making 
algorithm  at  time  t. 

Back  to  Experts


Bayesian  learning  cannot  give  good  payoﬀ  guarantees. 


Deﬁne  universal  expertise  analogously   to   universal  consistency,   and 
bound  regret  (lost  utility)  with  respect  to  the  best  expert,   which  is 
a  strategy. 

The  best  response  function  is  derived  by   solving  the   optimization 
problem 

I i  I  ut  +  λv i(I  ) 
i
i� i 
max 
� iut  is  the  vector  of  average  payoﬀs  player  i  would   receive  by  using 
each  of  the  experts 

I i  is  a   probability  distribution  over   experts 

λ  is  a   small  positive   number.


Under  technical  conditions  on  v ,  satisﬁed  by   the  negative  entropy: 

X 
−  s 
we   retrieve  the  exponential   weighting  scheme,  and  for   every  ǫ  there 
is  a   λ  such  that  our   procedure  is  ǫ-universally  expert. 

σ(s) log σ(s)

