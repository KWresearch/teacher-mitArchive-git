Regularized   Least  Squares

9.520  Class  04,  21  February  2006


Ryan  Rifkin


Plan 


•  Introduction  to   Regularized  Least  Squares

•  Computation:  General   RLS 
•  Large  Data  Sets:  Subset  of  Regressors 
•  Computation:  Linear  RLS 

Regression


We  have  a  training  set  S  =  {(x1, y1), . . . , (xℓ, yℓ)}.  The  yi 
are  real-valued.  The  goal   is  to  learn  a  function  f  to  predict 
the  y  values  associated  with  new  observed  x  values. 

Our  Friend  Tikhonov  Regularization 


We  pose  our  regression   task  as  the   Tikhonov  minimization 
problem:  

ℓ1 
� 
f  = arg min 
f ∈H  2
i=1  

V  (f (xi),  yi) + 

λ 
2 

�f �2 
K

To  fully  specify   the  problem,  we   need  to  choose  a 
function  V  and  a  kernel  function   K . 

loss 

The   Square  Loss


For  regression,  a  natural  choice   of 
square  loss  V (f (x), y)  = (f (x) −  y)2. 

loss  function  

is  the 

 
s
s
o
l
 
2
L

9 

8 

7 

6 

5 

4 

3 

2 

1 

0 

−3

−2

−1 

0
y−f(x) 

1

2

3 

Substituting  In  The  Square  Loss


Using  the  square   loss,  our  problem  becomes  

ℓ1 
(f (xi) −  yi)2 + 
� 
f  = arg min 
f ∈H  2
i=1 

λ 
2

�f �2
K . 

The  Return  of  the  Representer  Theorem


Theorem.   The  solution   to   the  Tikhonov  regularization 
problem 

ℓ1 
� 
min 
f ∈H  2
i=1  
can  be  written  in  the  form 

V  (yi,  f (xi)) + 

λ 
2 

�f �2 
K

f  = 

ℓ 
� 
i=1 

ciK (xi,  ·). 

This  theorem  is  exceedingly   useful  —  it  says  that  to  solve 
the  Tikhonov   regularization   problem,  we   need  only  ﬁnd  the 
best  function  of the   form  f  =  �ℓ
i=1  ciK(xi).  Put  diﬀerently, 
all  we  have  to   do   is  ﬁnd  the   ci. 

Applying   the  Representer  Theorem,  I


NOTATION   ALERT!!!  We  use  the  symbol  K  for  the 
kernel  function,   and  boldface  K for  the  ℓ-by-ℓ  matrix: 

Kij  ≡  K (xi,  xj ) 

Using  this  deﬁnition,  consider  the  output  of  our   function 


f  = 

ℓ 
� 
i=1 

ciK (xi,  ·). 

at   the  training  point  xj : 

f (xj )  = 

K (xi,  xj )ci 

ℓ 
� 
i=1 
= (Kc)j 

Using  the  Norm  of   a   “Represented”

Function


A  function  in  the  RKHS   with  a  ﬁnite   representation 


f  = 

ℓ 
� 
i=1 

ciK (xi,  ·), 

satisﬁes 

�  ℓ 
ℓ 
� 
�f �2 
k  =  �  ciK (xi,  ·),  �  cjK (xj ,  ·) 
i=1 
j=1 
ℓ 
ℓ 
� 
� 
�  � 
= 
cicj  K (xi,  ·),  K (xj ,  ·) 
i=1  j=1

ℓ
ℓ

�  � 
cicjK (xi,  xj ) 
i=1  j=1 
=  c tKc. 

= 

The  RLS  Problem 


Substituting,  our  Tikhonov  minimization  problem  becomes: 


1

min 
c∈Rℓ  2

�Kc −  y�2 
2 + 

λ T
c  K c.
2


Solving  the  Least  Squares  Problem,  I


We  are  trying   to  minimize  

g(c) =


1 
2

�Kc −  y�2 
2 + 

λ T
c  K c.

2 

This   is  a   convex,  diﬀerentiable  function  of  c,  so  we  can 
minimize  it   simply   by  taking  the   derivative   with  respect  to 
c,  then  setting   this  derivative  to  0. 

∂ g(c) 

∂ c  

=  K(Kc −  y) +  λKc. 

Solving   the  Least  Squares  Problem,   I I 


Setting  the  derivative   to  0, 

∂ g(c) 

=  K(Kc −  y) +  λKc = 0 

∂ c  
→  K(Kc) +  λKc =  Ky 

“  →  ”  Kc +  λc =  y 

→ 

→ 

(K +  λI )c =  y 
c = (K +  λI )−1 y 

The  matrix   K +  λI 
is  positive  deﬁnite  and  will  be  well-
conditioned  if   λ  is  not   too  small. 

Leave-One-Out  Values


Recalling  that  S  =  {(x1,  y1), .  .  .  ,  (xℓ,  yℓ)},  we  deﬁne   fS  to 
be  the  solution   to   the  RLS   problem  with  training   set  S . 
We  deﬁne 

S i	 =  {S \xi} 
=  {(x1,  y1), .  .  .  ,  (xi−1,  yi−1),  (xi+1,  yi+1), .  .  .  ,  (xℓ,  yℓ)}, 

the  training  set  with  the  ith  point  removed. 


We  call  fS i (xi)  the  ith  LOO  value,  and  yi  − fS i (xi)  the  ith 
LOO  error.  Let  LOOV  and  LOOE  be  vectors  whose   ith 
entries  are  the  ith  LOO  value  and  error.  

Key  Intuition: 

if  �LOOE �  is   small,   we  will   generalize   well. 


The  Leave-One-Out  Formula 


Remarkably,  for  RLS,  there  is  a  closed  form  formula   for 
LOOE .  Deﬁning  G(λ)  = (K +  λI )−1,  we  have:  

LOOE  = 

= 

G−1y 
diag(G−1) 
c 
diag(G−1) 

. 

Proof:   Later,  Blackboard. 


Computing:  Naive   Approach


Suppose  I  want  to   minimize  �LOOE �,  testing  p  diﬀerent

values   of  λ.


I  form  K,  which  takes  O(n2d)  time  (I  assume  d-dimensional

data   and  linear-time   kernels  throughout). 


For  each  λ,  I  form  G,  I  form   G−1 (O(n3)  time),  and   com(cid:173)

pute  c =  G−1y  and  diag(G−1).


Over  p  values  of  λ,  I  will  pay  O(pn3)  time.


We  can  do  much  better...


Computing:  Eigendecomposing   K


We  form  the   eigendecomposition  K =  QΛQt,  where  Λ  is 
diagonal  with  Λii  ≥  0  and  QQt  =  I . 

Key  point: 

G  =  K +  λI 
=  QΛQt  +  λI 
=  Q(Λ   +  λI )Qt , 

and  G−1 =  Q(Λ   +  λI )−1Qt . 

Forming the  eigendecomposition  takes  O(n3) time   (in   prac(cid:173)
tice). 

Computing  c  and  LOOE  eﬃciently 


c(λ)  =  G(λ)−1 y

=  Q(Λ  +  λI )−1Qt y. 

G−1 
ij  =  (Q(Λ  +  λI )−1Qt)ij 
n 
QikQj k 
� 
k=1  Λkk  +  λ 

= 

, 

Given  the  eigendecomposition,  I can   compute  c,  diag(G−1), 
and  LOOE  in  O(n2)  time.  Over  p  values  of  λ,   I  pay  only 
O(n3 +  pn2).  If  p <  n,  searching  for  a  good  λ  is   eﬀectively 
free! 

Nonlinear  RLS,  Suggested  Approach


•	 1.  Form  the  eigendecomposition   K =  QΛQt . 

•	 2.  For  each  value  of  λ  over  a  logarithmically  spaced 
grid,  compute   c  =  Q(Λ+λI )−1Qty and  diag(G−1) using 
the  formula  for  the   last  slide.  Form   LOOE ,  a  vector 
ci
. 
whose  ith  entry  is 
diag(G−1)i 

•	 3.  Choose   the  λ  that   minimizes  �LOOE �  in  some  norm 
(I  use  L2). 

•	 4.  Given   that   c,  regress  a  new  test  point  x  with  f (x) =

i  ciK (xi,  x). 
� 

Limits  of  RLS


RLS  has  proved  very  accurate.  There  are  two   computa(cid:173)
tional  problems:  

•	 Training:   We  need  O(n2)  space  (to  store  K),   and 
O(n3)  time  (to  eigendecompose  K) 

•	 Testing:  Testing  a  new  point  x  takes  O(nd)  time  to 

compute  the  n  kernel  products  in   f (x) = 
i K (x,  xi).  
� 
Next  class,  we  will   see  that  an  SVM  has  a  sparse  solu(cid:173)
tion,  which  gives  us  large  constant  factor   (but  important 
in  practice!)  improvements  for  both   the  training   and  test(cid:173)
ing  problems.  

Can  we  do  better,  sticking  with  RLS?


First   Idea:  Throw  Away  Data 


Suppose  that  we  throw  away   all   but  M of  our   data  points, 
where  M << ℓ.  Then  we  only   need  time   M 2d to  form  our 
new,  smaller  kernel  matrix,   and  we  only  need  time   O(M 3) 
to  solve  the  problem.  Great,   isn’t  it? 

Well,  if we  have  too  much  data  to  begin   with  (say   1,000,000 
points   in  3  dimensions)  this  will  work  just  ﬁne.  In  general, 
we  will  lose  accuracy.  

Subset  of  Regressors


instead  of  throwing  away  data,   we  restrict  our 
Suppose, 
hypothesis  space  further.  Instead  of  allowing   functions  of 
the  form  

f (x) =  

ℓ 
� 
i=1 

ciK (xi, x), 

we  only  allow 

f (x) = 

M 
� 
i=1 
In  other  words,  we  only  allow  the   ﬁrst  M 
where  M  <<  ℓ. 
points   to  have  non-zero  ci.  However,  we  still  measure   the 
loss   at  all  ℓ  points. 

ciK (xi, x), 

Subset  of  Regressors,  Cont’d


Suppose  we   deﬁne  KM M  to  be  the  kernel  matrix   on  just 
the  M  points  we’re  using  to   represent  our  function,   and  
KM L  to  be  the  kernel   product  between  those  M  points 
and  the  entire  dataset,  we  can  derive   (homework)   that  the 
minimization  problem  becomes:  

(KM LKLM  +  λKM M )c 
=  KM Ly , 

which  is  again  an  M -by-M  system. 


Various   authors  have  reported   good  results  with   this  or  
similar,   but  the  jury   is  still   out  (class  project!).   Sometimes 
called  Rectangular  Method. 

λ  is  Still  Free


To  solve 

(KM LKLM  +  λKM M )c 
=  KM Ly , 
consider   a  Cholesky  factorization  KM M  =  GGt: 

(KM LKLM  +  λKM M )c 
=  KM Ly
→  (KM LKLM  +  λGGt)c =  KM Ly 
→  (KM LKLM  +  λGGt)G−tGt c =  KM Ly 
→  (KM LKLM G−t  +  λG)Gt c =  KM Ly 
→  G(G−1KM LKLM G−t  +  λI )Gt c =  KM Ly , 

and  we  use  the   “standard”  RLS  free-λ  algorithm  on   an 
eigendecomposition  of  G−1KM LKLM G−t . 

Linear  Kernels 


An  important  special   case  is  the  linear  kernel 

K (xi,  xj ) =   xi  ·  xj . 

The  solution  function  f  simpliﬁes  as: 
� 
cixi  ·  x 
� 
=  ( 
cixi) ·  x 
≡  w t  ·  x. 

f (x)  = 

We  can  evaluate  f  in  time  d  rather  than  ℓd.


This  is   a   general  property  of  Tikhonov  regularization  with 
a  linear   kernel,   not  related  to  the  use  of   the  square   loss. 

Linear  RLS


In  the  linear  case,  K =  XtX  (xi  is   the  ith  column   of  X).  
Note  that  w  =  Xc. 

We  work   with  an  “economy-sized  SVD”   X =  U ΣV t,  where  
U  is  d × d  orthogonal,  Σ  is   d × d  diagonal  spd,   and  V 
is  n × d 
with  orthogonal  columns  (V tV  =  I ). 

w  =  X(XtX +  λI )−1 y 
=  U ΣV  t(V  Σ2V  t  +  λI )−1 y 
=  U Σ(Σ2  +  λI )−1V  t y. 

We  need  O(nd2) time  and  O(nd)  memory   to  form  the  SVD. 
Then  we  can  get  w(λ) in  O(d2)  time.  Very  fast. 

Linear  RLS,  Sparse   Data


Suppose  that  d,   the  number  of  dimensions,   is  enormous,  
and  that  n 
large,   but  the  data  are  sparse:  each 
is   also 
dimension  has  only  a  few  non-zero  entries.  Example:   doc(cid:173)
ument  classiﬁcation.  We  have  dimensions  for   each   word 
in  a   “dictionary”.  Tens  of  thousands  of  words,  but  only  a 
few  hundred  appear  in  a  given  document. 

The  Conjugate  Gradient  Algorithm


The  conjugate  gradient  algorithm   is  a  popular  algorithm  
for  solving  linear  systems.  For  this  class,   we   need  to  know 
that  CG  is  an  iterative   algorithm.   The  major  operation   is 
multiplying  taking  a  matrix-vector  product  Av .  A  need   not 
be  supplied  explicitly. 

CG  is  the  method  of  choice  when  there  is  a  way  to  multiply 
by  A  “quickly”.  

CG  and  Sparse  Linear  RLS


Remember,   we  are  trying  to  solve 

(K +  λI )c =  y 
→  (XtX +  λI )c =  y . 

K  is  too  big  to   write  down.  X  is   “formally”   too  big,  so  
we  can’t  take  its  SVD,  but  it’s  sparse.   We   can  use   CG,  
because  we  can  form   the   matrix  vector-product  (XtX+λI )c 
quickly: 

(XtX +  λI )c =  Xt(Xc) +  λc 

Cost   per  iteration:  2 ¯dℓ,  where  d¯ is   the  average  number  of 
nonzero  entries  per  data  point. 

Square-Loss  Classiﬁcation


There  is   nothing  to  formally  stop  us  for  using   the   above   al(cid:173)
gorithm  for  classiﬁcation.  By  doing  so,  we  are  essentially  
treating  our  classiﬁcation  problem   as  a  regression   problem  
with  y  values  of  1   or  -1. 

How  well  do   you   think  this  will   work?


