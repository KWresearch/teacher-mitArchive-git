Vision: Learning in the Brain

Thomas Serre

Center for Biological and Computational Learning
Brain and Cognitive Sciences Department

Object Recognition by Humans

(cid:190) Some numbers:
(cid:137) Basic-level categories*** ~ thousands
(cid:137) Subordinate-level:
(cid:57)Specific scenes** ~ thousands
(cid:57)Faces*                 > hundreds

*** Biederman (1974)
**  Standing (1973): good memory for 10,000 photos
*   Bahrick et al. (1975): 90% recognition of year-book photos of 
schoolmates, indep. of class size (90 to 900), and of time 
elapsed since graduation (3 mos. - 35 years).

Source: Modified from DiCarlo & Kanwisher (9.916)

M. Potter (1971)

Thorpe et al. (1996)

(cid:190) Measured rate of scene/object recognition using 
RSVP
(cid:190) Subjects able to get the “gist” even at 7 images/s 
from an unpredictable random sequence of 
natural images
(cid:137) No time for eye movements
(cid:137) No top-down / expectations

(cid:190) Rapid animal vs. non-animal categorization
(cid:137) Animal present vs. absent

(cid:190) How fast is object recognition?

Source: Modified from DiCarlo & Kanwisher (9.916)

ERP study: Recognition under 150 ms

Robustness to degradations

[Yip and Sinha, 2002]

(cid:190) Recognition performance:
(cid:137) 7 x 10 pix: more than ½ set of familiar faces
(cid:137) 19 x 27 pix: ceiling level 

(cid:190) Typical computer vision algorithms: 
(cid:137) 60 x 60 pixels [Heisele et al., 2001, 2002, 2004]
(cid:137) Typical face db > 200 x 200 pix

[Thorpe et al, Nature 1996]

FAs by one computer vision system

Pigeons can discriminate between:

(cid:190) Paintings 
(cid:137) Monet vs. Picasso
(cid:137) Van Gogh vs. Chagall

(cid:190) Animal vs. non-animal
(cid:190) Different kind of leaves
(cid:190) Letters of the alphabet
(cid:190) Human artifacts vs. natural 
objects

Source: Modified from Pawan Sinha (9.670)

Understanding how brains 
recognize objects may lead to 
better computer vision systems

Neuron Basics

[Heisele, Serre & Poggio, 2001]

(cid:190) AI systems << primate visual system 
(cid:190) AI systems   ~  birds and insects

Bees can discriminate between flower patterns

Training: One 
pattern reinforced 
with sugar

Test: New patterns

Source: Modified from Pawan Sinha (9.670)

Roadmap

1. Neuroscience 101:
(cid:137) Neuron basics
(cid:137) Primate visual cortex 

2. Computational model 
developed at CBCL

Different shapes and sizes but common structure

Neural Network

Source: http://webvision.med.utah.edu/

Source: Modified from Jody Culham’s web slides

Neuron basics

Computation at the SOMA

spikes

INPUT    
= Digital
COMPUTATION 
= Analog

OUTPUT 
= Digital

X3

X1

Y

Model Neuron

In the real world

X1

X2

X3

W1

W2

W3

Y

)∑=
(
HY
i xw
i

HY
=

∑

xw
i

i

⎛
⎜
⎜
⎝

⎞
⎟
⎟
⎠

∑

x

2
i

Y

=

⎛
⎜
exp
⎜
⎝

−

∑

2

i wx
−
i

⎞
⎟
⎟
⎠

2
σ

(cid:190) 1010 -1012 neurons (105 neurons/mm3)
(cid:190) 103 – 104 synapses/neuron
(cid:190) 1015 synapses

Axon ~ 5μm

Dendrites ~ 1μm

Soma ~ 20μm 

Gross Brain Anatomy

The visual system

50-60% of the brain devoted to vision

The Visual System

The Visual System

The complexity of 
the hardware 
matches that of the 
problem…

Computational 
approaches may help 
(cf. bioinformatics)

[Van Essen & Anderson, 1990]

[Felleman & van Essen, 1991]

The ventral visual stream

dorsal 
stream:
“where”

ventral 
stream:
“what”

[Ungerleider & Haxby, 1994]

Source: Modified from DiCarlo & Kanwisher (9.916)

Feedforward architecture

The Retina

[Thorpe and Fabre-Thorpe, 2001]

Source: http://webvision.med.utah.edu/

Photoreceptors

(cid:190) Back of the eyes
(cid:190) Behind blood vessels
(cid:190) 100 million+ photoreceptors

Rods and cones

(cid:190) Duplicity theory
(cid:190) 2 classes of 
photoreceptors for 2 
different luminance 
regimes:
(cid:137) Scotopic vision: Rods
(cid:137) Photopic vision: Cones

Source: http://webvision.med.utah.edu/

Source: http://webvision.med.utah.edu/

Cone type distrib. varies between ind.

Ganglion cells / LGN

(cid:190) We don’t all see the same thing!!
(cid:190) Human trichromatic cone mosaic

(cid:190) Center / surround 
receptive fields
(cid:190) Convolution / Laplacian:
(cid:137) Enhances local changes / 
boundaries
(cid:137) Disregard smooth surfaces
(cid:137) Computation of zero-
crossings

[Roorda & Williams, Nature 1999]

Ganglion cells / LGN

Illusions: Mach Bands

[Marr, Vision 1982]

Illusions: Mach Bands

V1: Orientation selectivity

V1: hierarchical model
Complex 
Simple 
LGN-type 
cells
cells
cells

V1: Orientation columns

(Hubel & Wiesel, 1959)

V1: Retinotopy

Beyond V1: A gradual increase in RF size

Reproduced from [Kobatake & Tanaka, 1994]

Reproduced from [Rolls, 2004]

Beyond V1: A gradual increase in invariance to 
translation and size

Beyond V1: A gradual increase in the 
complexity of the preferred stimulus

Reproduced from [Logothetis et al, 1995]

See also [Perrett & Oram, 1993; Ito et al, 1995; Logothetis & Sheinberg, 1996; Tanaka 1996]

[Kobatake et al, 1994]
Reproduced from [Kobatake & Tanaka, 1994]

Anterior IT 

AIT: Face cells

(cid:190) Very large receptive fields (several degrees)
(cid:190) Invariance:
(cid:137) Position
(cid:137) Scale

(cid:190) Hand, face, “toilet brush” cells, etc
(cid:190) Broad cells tuning
(cid:137) Population coding
(cid:137) =“grand-mother” cells

[Desimone et al. 1984]

AIT: Hand cells

Is it possible to read out what the monkey is seeing?

[Desimone et al. 1984]

[Hung, Kreiman, Poggio & DiCarlo, 2005]

Source: Courtesy of Gabriel Kreiman

Stimulus presentation

Passive viewing
(fixation task)

100 ms
100 ms
• 10-20 repetitions per stimulus
• presentation order randomized
• 77 stimuli drawn from 8 pre-defined groups

time

5 objects presented
per second

Source: Courtesy of Gabriel Kreiman

[Hung, Kreiman, Poggio & DiCarlo, 2005]

Classification
8 groups

Identification
77 pictures

Input to the classifier

Object category can be decoded quite accurately 
from the population response

0 ms

100 ms

200 ms

300 ms

w

SUA: spike counts in each bin
MUA: spike counts in each bin
LFP: power in each bin
MUA+LFP: concatenation of MUA and LFP

Source: Courtesy of Gabriel Kreiman

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

4
64
16
Number of sites
[Hung, Kreiman, Poggio & DiCarlo, 2005]

1

0

MUA

LFP

256

Object identity

12.5 ms are enough to decode well above chance

Classification

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

MUA

LFP

chance=1/8

0

1

4
64
16
Number of sites
[Hung, Kreiman, Poggio & DiCarlo, 2005]

256

1

Identification

0.8

0.6

0.4

0.2

0

1

MUA

LFP

=1/77

chance
256

4
64
16
Number of sites

[Hung, Kreiman, Poggio & DiCarlo, 2005]

Scale and translation invariance

The classifier extrapolates to new scales and 
positions

+

+

+

+

0.6

0.5

0.4

0.3

0.2

0.1

0

TRAIN

TEST

chance

[Hung, Kreiman, Poggio & DiCarlo, 2005]

Source: Courtesy of Gabriel Kreiman

[Hung, Kreiman, Poggio & DiCarlo, 2005]

Source: Courtesy of Gabriel Kreiman

Object selective region in the human brain: LOC

What about humans?

Source: Modified from DiCarlo & Kanwisher (9.916)

Face selective region in the human brain: FFA

Single neuron recordings in epileptic patients

MRI

Electrodes

[Kanwisher, McDermott & Chun, 1997]

Source: Modified from DiCarlo & Kanwisher (9.916)

A neuron selective to actress Jennifer Aniston

s
u
r
y
g
 
l
a
p
m
a
c
o
p
p
i
h
a
r
a
P
 
L

ROC area=1
Quian Quiroga, Reddy, Kreiman, Koch, Fried. Nature 2005
Quiroga, Reddy, Kreiman, Koch, Fried. Nature, 2005

137/998 (14%) 
selective units, 
52/137 (38%) 
showed 
invariance

Source: 
Courtesy of 
Gabriel 
Kreiman

Roadmap

I.

II.

The model

Comparison with other computer vision 
systems

III. Comparison with human observers

•Patients with 
pharmacologically 
intractable epilepsy

•Multiple electrodes 
implanted to localize 
seizure focus

•Targets include the 
hippocampus, 
entorhinal cortex, 
amygdala and 
parahippocampal 
gyrus

Fried et al J. Neurosurgery 1999

Kreiman, Koch, Fried (2000) Nature 
Neuroscience 3:946-953

Source: Courtesy of Gabriel Kreiman

Computational models and 
work at CBCL

Modified from 
(Ungerleider & 
VanEssen)

[Riesenhuber & Poggio, 1999, 2000;                              
Serre, Kouh, Cadieu, Knoblich, Kreiman & Poggio, 2005]

S1 and C1 units

(cid:190) Tuning properties match V1 
parafoveal simple and 
complex cells 
(cid:190) Assessed with:
(cid:137) Gratings                     
(Devalois et al, 1982a,b)
(cid:137) Bars and edges           
(Hubel & Wiesel, 1965,  Schiller et 
al, 1976a,b,c)

0.4o-1.6o
0.2o-1.1o

S1 and C1 units

(cid:190)Increase in tolerance to 
scale (broadening in 
frequency bandwidth)

C1

S1

MAX

s 17 spatial frequencies (=scales)
n
o
i
t
a
t
n
e
i
r
o
 
4

S2 and C2 units

(cid:190) Features of moderate complexity
(cid:190) Combination of V1-like complex units 
at different orientations
(cid:137) 10 subunits
(cid:137) Synaptic weights w learned from 
natural images

S2 
unit

S1 and C1 units

(cid:190)Increase in tolerance to 
position (and in RF size)

C1

S1

MAX

s 17 spatial frequencies (=scales)
n
o
i
t
a
t
n
e
i
r
o
 
4

S2 and C2 units

(cid:190) Features of moderate complexity
(cid:190) Combination of V1-like complex units 
at different orientations

1.1o-3.0o
0.6o-2.4o

Learning the tuning of units  in the model

(cid:190) Learning is likely to play a key role in the 
recognition ability of primates

(cid:190) From V2 to IT in the model, units are tuned to a 
large number of “patches” from natural images

(cid:190) Details still open-ended (more than the rest of 
the model, i.e., RF sizes, tuning properties) for  
which we have quantitative data

(cid:190) For clarity, I will describe the learning approach 
in a more “algorithmic” way (but see thesis for 
more biological implementation)

…

…

Start with S2 
layer
Units are organized 
in n feature maps

Database ~1,000 
natural images

At each iteration:
(cid:190) Present one image 
(cid:190) Learn k feature maps 

Then pick 1 unit 
from the second 
map at random

Store in unit 
synaptic weights the 
precise pattern of 
subunits activity, i.e. 
w=x

w1
w2

…

…

Image “moves” (looming and shifting)
Weight vector w is copied to  
all units in feature map 1 
(across positions and scales)

S2 and C2 units

(cid:190) n=2,000 feature maps total
(cid:190) Quantitative agreement:
(cid:137) Compatible with tuning for 
boundary conformations in V4

(Pasupathy & Connor, 2001)

w1

…

…

Start with S2 
layer
Pick 1 unit from the 
first map at random
Store in unit 
synaptic weights the 
precise pattern of 
subunits activity, i.e. 
w=x

S2

C1

Image “moves” (looming and shifting)
Weight vector w is copied to  
all units in feature map 1 
(across positions and scales) 

w1
w2
wk
…

…

Iterate until k 
feature maps have 
been learned

Then present 
second image

Learn k 
feature maps

Iterate until 
all maps have 
been trained

One V4 neuron tuning for boundary 
conformations

Most similar model C2 
unit

modified from 
(Pasupathy & 
Connor, 1999)

ρ = 0.78

(Serre, Kouh, Cadieu, Knoblich, Kreiman and Poggio, 2005)

S2 and C2 units

(cid:190) n=2,000 feature maps
(cid:190) Quantitative agreement:
(cid:137) Compatible with two-bar stimulus 
presentation

(Reynolds et al, 1999)

Prediction: Response of the pair is predicted to fall 
between the responses elicited by the stimuli alone

Reference (fixed)
V4 neurons         
C2 units
(with attention directed 
Probe (varying)
away from receptive field)

)
e
c
n
e
r
e
f
e
r
(

p
s
e
r
–
 
)
r
i
a
p
(
p
s
e
r
 
=

1.1o-3.0o

= response(probe) –response(reference)
(Reynolds et al., 1999)
(Serre, Kouh, Cadieu, Knoblich, 
Kreiman and Poggio, 2005)

Read out data [Hung et al, 2005]

From C2 to S4

(cid:190) Units are increasingly complex and 
invariant

(cid:190) 2,000 “features” at the C3 level ~ same 
number of feature columns in IT         
(Fujita et al, 1992)

(cid:190) Tuning and invariance properties at the 
S4 level in quantitative agreement with 
view-tuned units in IT  (Logothetis et al, 1995)

(cid:190) Task-specific circuits (from IT to PFC)
(cid:137) Supervised learning
(cid:137) Linear classifier trained to minimize 
classification error on the training 
set (~ RBF net)
(cid:137) Generalization capability evaluated 
on a distinct set of images (test set)

(cid:190) Features of moderate complexity 
(from V2 to IT) 
(cid:137) Unsupervised learning during 
developmental-like stage
(cid:137) From natural images unrelated to 
any categorization tasks

A loose hierarchy

(cid:190) Bypass routes along with main routes: 
(cid:137) From V2 to TEO (bypassing V4)    (Morel 
and J.Bullier,1990; Baizer et al., 1991; Distler
et al., 1991; Weller and Steele, 1992; 
Nakamura et al.,1993; Buffalo et al., 2005)
(cid:137) From V4 to TE (bypassing TEO)
(Desimone et al., 1980; Saleem et al., 1992)

(cid:190) Some stages are skipped
(cid:190) Richer dictionary of features with 
various levels of selectivity and 
invariance

A neurobiological approach

Successful model predictions

(cid:190) Biophysical implementations
(cid:137) Based on simple properties of cortical circuits 
and synapses [Yu et al, 2002; Knoblich & Poggio, 2005]

(cid:190) Reflects organization of the ventral stream

(cid:190) Predicts several properties of cortical neurons   
[Serre, Kouh, Cadieu, Knoblich, Kreiman, Poggio, 2005]

(cid:190) MAX in V1 (Lampl et al, 2004) and V4 (Gawne et al, 2002)
(cid:190) Differential role of IT and PFC in categ. (Freedman et al, 2001,2002,2003)
(cid:190) Face inversion effect (Riesenhuber et al, 2004)
(cid:190) IT read out data (Hung et al, 2005)
(cid:190) Tuning and invariance properties Of VTUs in AIT (Logothetis et al, 1995)
(cid:190) Average effect in IT (Zoccolan, Cox & DICarlo, 2005)
(cid:190) Tow-spot reverse correlation in V1 (Livingstone and Conway, 2003; Serre et al, 
2005)
(cid:190) Tuning for boundary conformation (Pasupathy & Connor, 2001) in V4
(cid:190) Tuning for Gratings in V4 (Gallant et al, 1996; Serre et al, 2005)
(cid:190) Tuning for two-bar stimuli in V4 (Reynolds et al, 1999; Serre et al, 2005)
(cid:190) Tuning to Cartesian and non-Cartesian gratings in V4 (Serre et al, 2005)
(cid:190) Two-spot interaction in V4  (Freiwald et al, 2005; Cadieu, 2005)

(cid:190) How well does the model perform on different 
object categories?
(cid:190) How does it compare to standard computer 
vision systems?

Roadmap

I.

II.

The model

Comparison with other computer vision 
systems

III. Comparison with human observers

CalTech Vision Group

CalTech Vision Group

(cid:190) Constellation models      [Leung et al, 1995; Burl et al, 1998; 
Weber et al., 2000; Fergus et al, 2003; Fei-Fei et al, 2004]

rear-car

airplane

frontal face motorbike

leaf

Other approaches

Other approaches

(cid:190) Hierarchy of SVM-classifiers                
[Heisele, Serre & poggio, 2001, 2002]
(cid:137) Component experts
(cid:137) Combination classifier

(cid:190) Fragment-based approach    
[Leung, 2004] based on [Ullman et al, 2002; 
Torralba et al, 2004]

Near-profile

Multi-view car

Fragment-based system

CalTech leaf

CalTech face

Weizmann face

Weizmann cow

[Ullman et al, 2005; Epshtein & Ullman, 2005]

[Chikkerur & Wolf, in prep]; courtesy: Chikkerur

CalTech 101 object dataset

CalTech 101 object dataset

(cid:190) 40−800 images per categ. (mode ~  50)
(cid:190) Large variations in shape, clutter, pose, illumination, size, etc.
(cid:190) Unsegmented (objects in clutter)
(cid:190) Color information removed

[Fei-Fei et al., 2004]

[Fei-Fei et al., 2004]

CalTech 101 object dataset

SIFT features [Lowe, 2004]

[Serre, Wolf, Poggio, CVPR 2005]

[Serre, Wolf, Poggio, CVPR 2005]

CalTech 101 object dataset

StreetScene project

(cid:190) Model re-implementation for multi-class
(cid:190) chance < 1%
(cid:190) 15 training examples:
(cid:137) Serre, Wolf & Poggio (2004)      ~ 44%
(cid:137) Wolf & Bileschi (in sub)             ~ 56%
(cid:137) Mutch & Lowe (in sub)              ~ 56%

(cid:190) Others:
(cid:137) Holub, Welling & Perona (2005) ~ 44%
(cid:137) Berg, Berg & Malik (2005)          ~ 45%

Challenge

In-class variability:
(cid:190)Vehicles of different types at 
many poses, illuminations.
(cid:190)Trees in both Summer and 
Winter
(cid:190)City and suburban scenes

Partial labeling:
(cid:190)Rigid objects are only labeled 
if less than 15% occluded.
(cid:190)Some objects are unlabeled.
(cid:190)Bounding boxes overlap and 
contain some background.

The system

Input Image

Segmented Image

Standard Model
classification

Windowing

Standard Model
classification

ped

car

car

Output

Texture-based objects pathway (e.g., trees, road, sky, buildings)
Rigid-objects pathway (e.g., pedestrians, cars)

Rigid objects recognition

Textured-object recognition

(cid:190)Local patch correlation:     
(Torralba et al, 2004) 
(cid:190)Part-based system: 
(Leibe et al, 2004)

(Serre, Wolf, Bileschi, Riesenhuber and Poggio, in sub)

(Serre, Wolf, Bileschi, Riesenhuber and Poggio, in sub)

Examples

(cid:190) The model can handle the recognition of many 
different object categories in complex natural 
images
(cid:190) The model performs surprisingly well at the 
level of some of the best computer vision 
systems
(cid:190) How does it compare to humans?

Roadmap

I.

II.

The model

Comparison with other computer vision 
systems

III. Comparison with human observers

Animal vs. Non-animal categ.

(cid:190) Animals are rich class of stimuli
(cid:190) Variety of shapes, textures
(cid:190) Different depths of view, poses and sizes
(cid:190) Associated with context (natural landscape)

The Stimuli

“Head”

(cid:190) 1,200 stimuli (from Corel database)
(cid:190) 600 animals in 4 categories: 
(cid:137) Head
(cid:137) Close-body
(cid:137) Medium-body
(cid:137) Far-body and groups
(cid:190) 600 matched distractors (½ art., ½ nat.) to prevent 
reliance on low-level cues

“Close-body”

“Medium-body”

“Far-body”

Training and testing the model

(cid:190) Random splits (good estimate of expected error)
(cid:190) Split 1,200 stimuli into two sets

Training 

Test 

Training the model

(cid:190) Repeat 20 times
(cid:190) Average model performance over all

d’ analysis

(cid:190) Signal detection theory
(cid:137) F: false-alarm rate (non-animal images incorrectly 
classified as animals)
(cid:137) H: hit rate (animal images correctly classified)
(cid:137) Z: Z-score, i.e. under Gaussian assumption, how far is 
the rate (F or H) from chance (50%)?

d’

Training 

Test 

0%

F

50%

H

100%

Results: Model

Animal vs. non-animal categ.

Image

Interval 
Image-Mask
Mask
1/f noise

model

20 ms

30 msec ISI

80 ms

(Thorpe et al, 1996; Van Rullen & Koch, 2003; Bacon-
Mace et al, 2005; Oliva & Torralba, in press)

Animal present
or not ?

Results: Human-observers

Results: Image orientation

50 ms SOA (ISI=30 ms)
model

Human observers

(n=14)

upright
90 deg
inverted

Robustness to 
image orientation is 
in agreement with 
previous results 
(Rousselet et al, 2003; 
Guyonneau et al, ECVP 2005)

50 ms SOA (ISI=30 ms)

(Serre, Oliva and Poggio, in prep)

Results: Image orientation

Detailed comparison 

Human observers

Model

(n=14)

(cid:190) For each individual image
(cid:190) How many times image classified as animal:
(cid:137) For humans: across subjects
(cid:137) For model: across 20 runs

upright
90 deg
inverted

50 ms SOA (ISI=30 ms)

(Serre, Oliva and Poggio, in prep)

(cid:190) The model predicts human performance extremely 
well when the delay between the stimulus and the 
mask is ~50 ms
(cid:190) Under the assumption that the model correctly 
accounts for feedforward processing, the discrepancy 
for longer SOAs should be due to the cortical back-
projections
(cid:190) A very important question concerns the precise 
contribution of the feedback loops (Hochstein & Ahissar, 2002)

model

humans
(cid:190) Heads:             ρ=0.71 
(cid:190) Close-body:     ρ=0.84 
(cid:190) Medium-body: ρ=0.71
(cid:190) Far-body:         ρ=0.60

Contributors

(cid:190) Model:
(cid:137) C. Cadieu
(cid:137) U. Knoblich
(cid:137) M. Kouh
(cid:137) G. Kreiman
(cid:137) T. Poggio
(cid:137) M. Riesenhuber

(cid:190) Computer vision:
(cid:137) S. Bileschi
(cid:137) S. Chikkerur
(cid:137) E. Meyers
(cid:137) T. Poggio
(cid:137) L. Wolf 

(cid:190) Learning:
(cid:137) M. Giese
(cid:137) R. Liu
(cid:137) C. Koch
(cid:137) J. Louie
(cid:137) T. Poggio
(cid:137) M. Riesenhuber
(cid:137) R. Sigala
(cid:137) D. Walther

(cid:190) Comparison with human-
observers
(cid:137) A. Oliva
(cid:137) T. Poggio

