Unsupervised   Learning   Techniques

9.520  Class  07,   1  March  2006


Andrea  Caponnetto


About  this  class


Goal  To  introduce  some  methods  for  unsupervised  learn(cid:173)
ing:  Gaussian  Mixtures,  K-Means,  
ISOMAP,   HLLE,  
Laplacian  Eigenmaps. 

Unsupervised   learning


Only   u 
samples  drawn  on  X  from  the  unknown 
i.i.d. 
marginal  distribution   p(x) 

{x1, x2, . . . , xu}. 

The  goal  is  to   infer  properties  of  this  probability  density. 


In  low-dimension  many  nonparametric  methods  allow  di(cid:173)
rect  estimation  of  p(x)  itself.  Owing  to   the  curse  of  di(cid:173)
mensionality,  this  methods   fail  in  high  dimension.  

One   must  settle  for  estimation  of  crude  global  models.


Unsupervised   learning  (cont.)


Diﬀerent  types  of  simple  descriptive  statistics  that  characterize  aspects 
of  p(x) 

•  mixture  modelling 
representation  of  p(x)  by a  mixture  of  simple  densities  representing 
diﬀerent   types  or   classes  of  observations  [eg.   Gaussian   mixtures] 

•  combinatorial  clustering 
attempt  to  ﬁnd  multiple  regions  of  X  that  contain  modes  of  X 
[eg.  K-Means]  

•  dimensionality  reduction 
attempt  to  identify  low-dimensional  manifolds  in  X  that  represent 
high   data  density  [eg.  ISOMAP,HLLE,  Laplacian   Eigenmaps] 

•  manifold  learning 
attempt  to  determine  very  speciﬁc  geometrical  or  topological  in(cid:173)
variants  of  p(x)  [eg.  Homology   learning] 

Limited   formalization


is  a 
learning  there 
With  supervised  and  semi-supervised 
clear  measure  of  eﬀectiveness  of  diﬀerent  methods.   The 
expected  loss  of  various  estimators   I [fS ]  can  be  estimated 
on  validation  set. 

learning, 
In  the   context   of  unsupervised 
ﬁnd   such  a  direct   measure  of  success. 

it 

is  diﬃcult   to 

This   situation  has  led to  proliferation  of  proposed   meth(cid:173)
ods. 

Mixture  Modelling 


Assumption  that  data  is   i.i.d.  sampled  from  some   proba(cid:173)
bility  distribution   p(x). 

p(x) is  modelled  as  a   mixture  of  component  density  func(cid:173)

tions,  each  component   corresponding  to  a  cluster  or  mode. 


The  free  parameters  of  the  model  are  ﬁt  to  the  data  by 
maximum  likelihood. 

Gaussian   Mixtures


We  ﬁrst  choose  a  parametric  model  Pθ  for  the  unknown 
density   p(x),  hence   maximize  the   likelihood   of  our  data 
relative  to  the   parameters  θ. 

Example:  two-component  gaussian   mixture  model with   pa(cid:173)
rameters 

θ  = (π ,  µ1,  Σ1,  µ2,  Σ2). 

The  model: 

Pθ (x)  = (1  − π)GΣ1 (x  − µ1) +  πGΣ2 (x   − µ2) 
Maximize  the  log-likelihood  

�(θ {x1, .   .  .   ,  xu})  = 
|

u 
� 
i=1 

log Pθ (xi)

The  EM  algorithm


Maximization  of  �(θ|{x1 , .  .  .  ,  xu})  is  a  diﬃcult  problem.  Iterative  max(cid:173)
imization  strategies,  as  the  EM  algorithm,  can  be  used  in  practice  to 
get   local  maxima. 

γi  = 

1.  Expectation:  compute   the   responsibilities 
πGΣ2 (xi  − µ2)
(1  − π)GΣ1 (xi  − µ1) +   πGΣ2 (xi  − µ2) 
2.  Maximization:   compute   means  and   variances 
γi(xi  − µ2)(xi  − µ2)T 
� 
� 
γixi
i 
i 
� 
� 
γi
i γi 
i 
γi.
and  the  mixing  probability  π  = 

Σ2  = 

µ2 = 

1
u 

� 
i 

, 

, 

etc 

3.  Iterate  until  convergence 

Combinatorial  Clustering


Algorithms  in  this  class  work  on  the  data   without  any   ref(cid:173)
erence  to  an  underlying  probability   model. 

is  assigning  each  data  point  xi  to  a  cluster  k 
The  goal  
belonging  a  predeﬁned  set  {1,  2, .   .  .  ,  K } 
C (i) =   k,  
i  = 1,   2, .  .  .  ,  u


The  optimal   encoder  C ∗(i)  minimizes   the  overall   dissimi(cid:173)
larities  d(xi,  xj ) between  points  xi,  xj  assigned  to  the   same  
cluster 

W (C ) = 

K1 
� �  � 
2 
k=1  C (i)=k C (j )=k 

d(xi,  xj )

The  simplest choice   for  the  dissimilarity  d(·,  ·)  is  the  squared 
Euclidean  distance  in  X 

Combinatorial   Clustering  (cont.)


The  minimization  of  the  within-cluster  point  scatter  W (C ) 
is  straightforward  in  principle,  but... 

the  number  of   distinct  assignments  grows  exponentially 
with  the  number  of   data  points  u 
1  K 
�  (−1)K−k �K �
K ! 
k
k=1 
already  S (19,   4)  � 1010! 

S (u,  K ) = 

ku

In  practice,  clustering  algorithms  look  for   good  suboptimal 
solutions. 

Most  popular  algorithms  are  based  on 
strategies.  Convergence   to  local  optima. 

iterative   descent 

K-Means


If  d(xi,  xj ) =  �xi − xj �2,  introducing  the  mean   vectors  ¯xk  as(cid:173)
sociated to  the  k-th  cluster,   the  within-cluster  point  scatter 
W (C )  can  be  rewritten  as 

W (C ) = 

1 
2 

K
� � � 
k=1  C (i)=k C (j )=k

�xi−xj � 2 

= 

K
� � 
k=1  C (i)=k 

�xi−¯ 2
xk�

.


Exploiting  this  representation  one   can  easily  verify   that  the 
optimal  encoder  C ∗  is  the  solution  of  the   enlarged  mini(cid:173)
mization  problem 

min 
C,(m1 ,...,mK )

K 
�  �  
k=1  C (i)=k 

�xi  − mk� 2 . 

K-Means  (cont.) 


K-Means  attempts  the  minimization  of  the  enlarged  problem  by  an  it(cid:173)
erative  alternating  procedure.  Each  step  1  and  2  reduces  the  objective 
function,   so  convergence  is   assured. 

1.   minimization  with  respect   to  (m1 , .  .  .  ,   mK ),  getting


mk  = ¯
xk 

2.   minimization  with  respect   to  C ,  getting

C (i)  =  arg  min

1≤k≤K  �xi  − mk� 

3.   do   until  C  does  not  change 

One  should  compare  solutions  derived  from  diﬀerent  initial  random 

means,  and  choose  best   local   minimum. 

Voronoi   tessellation


Dimensionality  reduction


Often  reducing  the  dimensionality   of   a   problem   is  an  ef(cid:173)
fective  preliminary   step  toward  the  actual  solution  of  a  
regression  or  classiﬁcation   problem.  

We  look  for  a   mapping   Φ  from  the   high  dimensional  space 
IRD  to  the   low  dimensional  space   IRd  which  preserves  some 
relevant  geometrical  structure  of  our  problem.  

Dimensionality  reduction


Principal  Component  Analysis   (PCA)


Trying  to  approximate  data  {x1, . . . , xu} 
dimensional  hyperplane  

in 

IRD  by  a  d-

H  =  {c  +  Vθ θ ∈ IRd 
|

} 

IRd  and  V  = 
IRD  ,  θ  coordinates   vector 
in 
in 
c  vector 
(v1, . . . , vd),  D d   matrix  with  {vi}  orthonormal  system 
× 
of  vectors  in  IRD  . 

Problem:  ﬁnd  H  which  minimizes  sum  of  squared  dis(cid:173)
tances   of  data  points  xi  from  H 
u 
� 
i=1 

�xi − PH (xi)� 2

H ∗  = arg min 
H 

Linear  approximation


PCA:  the  algorithm 


1.  center   data  points: 

u 
=1  xi  = 0 
�
i

2.  deﬁne  u × D  matrix  X  = (x1, . . . , xu)T 

3.  construct  singular  value  decomposition  X  =  UΣWT 

•	 D × D  matrix  W  = (w1 , . . . , wD ),  with  {wi} right  eigenvectors

of  X


•	 u × D  matrix  U  = (u1 , . . . , uD ),  with  {ui} left  eigenvectors  of  X

σ1	 ≥ σ2  ≥ · · ·   ≥ σD  ≥ 0 
•	 D × D  matrix  Σ  =   diag(σ1 , . . . , σD ),  with 
singular  eigenvectors  of  X


4.  answer:  V  = (w1, . . . , wd) 

Sketch   of  proof


•  Rewrite  the  minimization  problem 
u 
� 
�xi  − c  − Vθi� 2
min 
c,V,{θi} i=1 

•	 Centering  and  minimizing  with  respect   to  c  and  θi  gives 
θi  =  VT xi 
c  = 0,  

•  Plugging  into  the  minimization  problem 
u	
�	
2	
�xi  − VVT xi�  = arg max 
V 
i=1 

u 
� 
i=1 
d 
� 
j=1 
hence  (v1 , .  .  .  ,   vd)  are  the  ﬁrst  d  eigenvectors  of  XT  X:  (w1 , .  .  .  ,  wd) 

xi  VVT xi 
T

arg min 
V	

= arg max 
V 

T 
vj  XT  Xvj

Mercer’s  Theorem 


Consider   the   pd  kernel   K (x,  x�)  on  X  × X  ,  and   the  proba(cid:173)
bility  distribution   p(x)  on  X . 

Deﬁne  the  integral   operator  LK 
� 
(LK  f )(x) =  K (x,  x�)f (x�)dp(x�). 
X 

Mercer’s  Theorem  states   that 


K (x,  x�) = 

� 
i 
where   (λi,  φi)i  is  the  eigensystem  of  LK . 

λiφi(x)φi(x�) 

Feature  Map


From  Mercer’s  Theorem,   the  mapping   Φ  deﬁned  over  X

� 
�	
Φ(x)  = (  λ1φ1(x), λ2φ2(x), . . . ) 

is  such  that 

K (x, x�)  = Φ(x)T Φ(x). 

�
•  K (x, x )  can  be   interpreted  as  the  dot  product  in  the

“feature  space”.


•	 given  a  mapping  of   X  into  an  Euclidean  space,  we  can 
construct  a  pd  kernel   X × X . 

Kernelization


Algorithms  that   depend   on  the   data,  only  through  the   dot

T
products  xi  xj ,  can  be  easily  kernelized:  

1.  Choose   pd  kernel  K (·,  ·) 

T
2.  Replace  xi  xj  with  K (xi,  xj ) 

Example:  PCA   can  be  kernelized  computing  the  eigen(cid:173)
vectors   of  the  matrix 

Mij  =  K (xi,  xj ) 
instead  of  those  of  the  matrix  XT X. 

ISOMAP  ∗


•	 Assumption:  the   support   of  the  marginal  distribution 
p(x)  is  a   convex  region   of  IRd  (our  manifold  M)  iso(cid:173)
metrically  embedded   in  IRD  . 

•	

Goal:  constructing  a  map  Φ   :  IRD 
forms”  geodesic  distances 
in  M 
tances   in  IRd 

IRd  which  “trans(cid:173)
→ 
into  Euclidean  dis(cid:173)

•	 Construction  1:  approximate   the  matrix  dM  of  pair(cid:173)
wise   geodesic  distances  between  data  points,   estimat(cid:173)
ing  the  shortest  distances   dij  over  the  neighborhood 
graph. 

∗Tenenbaum,  et  al,  00 

•  Construction  2:   compute  the   u  × u  “kernel   matrix”

1
1

H  =  I  −  1111T , 
K  =  −2 
HDH, 
u 
with  11 the  u-dimensional   column  vector  (1,  1, .  .  .  ,  1), 
and  D  the  matrix  of  squared  distances,  that  is:  Dij  = 
d2 
ij . 

let  (λa,  ua)u 
be  the  eigensystem  of   K.  The  
•  Result:  
a=1 
u
embedding  Φ,   of  {xi}i=1 
� 
� 
� 
Φ(xi)  = (  λ1(u1)i,  λ2(u2)i, .  .  .  ,  λd(ud)i), 
is  the  isometry  we  were  looking  for. 

ISOMAP  global  isometry


Explaining  ISOMAP 


Firstly,  we  have  to  verify  that   the  matrix   K  is  a  genuine  
pd  kernel   on  the  data  points.  

1.  Symmetry:  since  both  H  and  D  are  symmetric,  K  = 
−  1HT DH,  hence  KT  =  K.
2

2.  Positivity:  Note  that,  by  assumption,  there  exist  vec(cid:173) 

u
�φi  − φj �. 
For  all  c  =
tors  {φi}i=1,  such  that  dij  = 
1 
(c1, .  .  .  ,  cu),  deﬁning  ing  c� =  c  − u
u
i=1  ci11,  we  get 
�
1 
1 
�T
c Dc� 
(H c)T D(H c) =
cT Kc  =  −2
−2 
1 
�
T φj )cj�
�  ci(φT
i φi  +  φT φj  − 2φi
[Dij  =  �φi  − φj �2]  =
−2 
j
ij 
ci� φi)  ≥ 0. 
ci� φi)T ( 
i c�i  = 0] 
� 
� 
= ( 
[ 
� 
i 
i

Explaining  ISOMAP   (cont.)


•	 We  must  prove  that   the  pd  kernel  Kij  induces  the  cor(cid:173)
rect  pairwise  distances  dij  between  data  points 

d2  = 
ij  Kii   +  Kj j  − 2Kij . 
This  can  be  veriﬁed  by  direct  computation. 

•	 By  Mercer’s  Theorem,  the  feature   map 
Φ0(xi)  = (  λ1(u1)i,  λ2(u2)i, .  .  .  ,  √λu(uu)i), 
�	
� 
is  an 
is  d-dimensional, 
If  the   manifold  M 
isometry.  
λa  = 0  for  a  >  d,   and  we  can  use  the   truncated   mapping  
Φ. 

Hessian  Locally  Linear  Embedding  (HLLE)∗


u
ISOMAP  outputs  an  embedding   of  the  data  points  {xi}i=1 
into  IRd,  attempting  to  preserve  pairwise   distances  on  the 
underlying  manifold  M.  The  method  gives  guarantees  of 
convergence  if  M  is  isometric   to  a   convex  region  in  IRd . 

is  a   very  strong  hypothesis.  Typically,  
Convexity 
combinations   of  images  are  not  reasonable  images! 

linear  

HLLE   gives  guarantees   of  convergence   while  relaxing  the  
convexity   hypothesis.  

∗Hessian Eigenmaps;  Donoho,  Grimes  03 

HLLE  local  isometry 


Core  idea  of  HLLE


For  every  point  x  ∈ M and system  of coordinates  (ξ1, .  .  .  , ξd) 
on  its  tangent  space,   the  Hessian  at  x  of  a  function  f  : 
M → IR  ,  is  the  matrix  of  second  derivatives 
∂  ∂ 
i, j  =  1, .  .  .  , d 
f (x), 
(Hf (x))ij  = 
∂ ξi ∂ ξj 

The  core  idea  of  HLLE   is  that   the  null space   of the  quadratic  
form 

�  � 
ij M 
is  independent  of  the  choice  of  local  coordinates  ξi. 

(Hf (x))2 
ij 

H(f ) = 

The  null space  of  H is  the  d-dimensional  linear  space  spanned  
by  the  global  cartesian  coordinates 

Computing   the   Hessian


In  order   to  implement  this  idea,  HLLE   has  to  evaluate  the 
quadratic   form  H  using  the  data  points  xi. 

1.  construct  proxies  for  the  tangent   spaces  using  the   k-
nearest  neighborhood  graph 

2.  implement   a   ﬁnite  diﬀerences  scheme  to  evaluate  sec(cid:173)
ond  derivatives  

3.  compute  eigensystem  of  approximation  of  H.  Use  d 
eigenvectors  with  smallest  eigenvalues  as  embedding  
coordinates. 

Local  Linear  Neighborhood


Laplacian   based  methods  ∗


Unsupervised  methods  based  on  the  eigensystem  of  the 
Laplacian  on  the  neighborhood  graph  with  weights  Wij . 

•	 Dimensionality  Reduction:  consider  the  solutions  of 
the  eigenvector  problem  (0  =  λ0  ≤ λ1  ≤ · · ·  ≤ λu−1) 
Lfa  =  λaDfa 

where   D  =  diag(D11, .   .  .   ,  Duu).  The  considered  embed(cid:173)
ding  into  the  d-dimensional  Euclidean  space  is 

Φ(xi)  = ((f1)i, .   .  .  ,  (fd)i). 

•	 Spectral   Clustering:  use  sign  of  components  (f1)j  to  
deﬁne  two  clusters:  connection  to  min  cut  problem. 

∗Belkin,  Niyogi,  02 

