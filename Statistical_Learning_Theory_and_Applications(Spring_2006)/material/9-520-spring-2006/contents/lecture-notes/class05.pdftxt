Support   Vector  Machines  For  Classiﬁcation

9.520  Class  05,  22  February  2006


Ryan  Rifkin


Plan 


•  Regularization  derivation  of  SVMs 
•  Geometric   derivation  of  SVMs 
•  Optimality,   Duality  and  Large  Scale   SVMs

•  SVMs   and  RLSC:  Compare  and  Contrast


The  Regularization  Setting  (Again)


We  are  given  n  examples  (x1, y1), .   .  .  , (xn, yn),  with  xi  ∈  IRd 
and  yi  ∈  {−1, 1}  for  all  i.  As  mentioned  last  class,  we  ﬁnd 
a  classiﬁcation   function   by   solving   a  regularization:  

 

n1 
X
min 
f ∈H  n  i=1 

V (yi, f (xi)) +  λ||f ||2 
K . 

In  this  class  we  speciﬁcally   consider   binary  classiﬁcation.


The   Hinge  Loss


The  classical  SVM  arises  by  considering   the  speciﬁc   loss 
function 

where 

V  (f (x),  y)  ≡  (1  −  yf (x))+, 

(k)+  ≡  max(k,  0). 

The   Hinge  Loss


−2

−1 

0 
y * f(x) 

1 

2


3 

 
s
s
o
L
 
e
g
n
i
H

4 

3.5 

3 

2.5 

2 

1.5 

1 

0.5 

0 

−3

Substituting  In  The  Hinge  Loss


With  the  hinge  loss,  our  regularization  problem  becomes


n1 
 
(1  −  yif (xi))+ +  λ||f ||2 
X
min 
K . 
f ∈H  n  i=1  

Slack  Variables


This  problem  is   non-diﬀerentiable   (because  of the  “kink”  in 
V ),  so  we  introduce  slack  variables  ξi,  to  make  the  problem 
easier  to  work  with: 

min 
f ∈H 
subject  to  : 

1 
i=1   ξi +  λ||f ||2 
n  Pn 
K 
yif (xi)  ≥  1  −  ξi 
ξi  ≥  0 

i =  1, . . . , n 

i =  1, . . . , n 

Applying  The  Representer  Theorem


Substituting   in: 

∗ f  (x) = 

 

n 
X
i=1 

ciK (x, xi), 

we  arrive  at  a  constrained  quadratic   programming  problem:


min 
c∈IRn
subject   to  :  yi 

1 
n 
=1  ξi +  λcT K c 
P
i
n 
n
j=1  cjK (xi, xj )  ≥  1  −  ξi 
P
ξi  ≥  0 

i = 1, . . . , n 

i = 1, . . . , n 

Adding  A  Bias  Term 


If  we  add  an   unregularized   bias  term  b,   we  arrive  at  the 
“primal” SVM: 

min 
c∈IRn ,ξ∈IRn 
subject  to  :  yi( 

1 
n

n 
i=1  ξi +  λcT K c 
P
n
j=1  cjK (xi, xj ) +  b)  ≥  1  −  ξi 
P
ξi  ≥  0 

i = 1, . . . , n 

i = 1, . . . , n 

Forming  the  Lagrangian


We  derive  the   Wolfe   dual  quadratic  program  using  La(cid:173)
grange  multiplier  techniques: 

L(c,  ξ ,  b,  α,  ζ )  = 

 

− 

n1 
X
n  i=1 
n 
 
X
i=1 
n 
 
X
i=1 

− 

ξi  +  λc T K c 
 
 
  n 

X
αi  yi 

j=1 
ζiξi 

 



 

 
cjK (xi,  xj ) +  b  −  1  +  ξi
 


We  want  to  minimize   L  with  respect  to  c,  b,   and  ξ ,  and 
maximize  L  with  respect  to   α  and  ζ ,  subject  to  the  con(cid:173)
straints   of the   primal   problem  and nonnegativity constraints 
on  α  and  ζ . 

Eliminating  b  and  ξ


∂L 

∂ b 

∂L 
∂ ξi 

= 0   =⇒ 

= 0   =⇒ 

 

αiyi  = 0 

n
X
i=1  
1 
−  αi  −  ζi  = 0 
n 

=⇒  0  ≤  αi  ≤ 

1 

n 

We  write  a  reduced  Lagrangian   in  terms  of  the  remaining 
variables:  

LR(c,  α) =  λc  K c − 
T

 

n
X
i=1  

αi(yi 

 

n 
X
j=1  

cjK (xi,  xj ) −  1) 

Eliminating  c


Assuming  the  K  matrix  is  invertible, 

∂LR 
∂ c 

= 0   =⇒  2λK c −  K Y  α  = 0 
αiyi 
2λ 

=⇒  ci  = 

Where  Y 
is  a  diagonal  matrix   whose  i’th  diagonal  element 
is   yi;  Y  α  is  a  vector  whose  i’th  element  is  αiyi. 

The  Dual  Program


Substituting  in   our  expression   for   c,  we  are  left  with  the 
following  “dual”  program: 
1 
n
λαT Qα 
i=1  αi  −  4
P
n
= 0
P
i=1   yiαi 
0  ≤  αi  ≤  1 
n 
Here,  Q  is   the  matrix   deﬁned  by 

max 
α∈IRn 
subject   to  : 

i  = 1, .  .  .  ,  n 

Q  =  YKYT ⇐⇒   Qij  =  yiyjK (xi,  xj ). 

Standard   Notation


In  most  of  the   SVM  literature,  instead  of  the  regularization 
parameter  λ,  regularization  is  controlled  via   a  parameter   C , 
deﬁned  using  the  relationship 

C  = 

1 
2λn 
Using  this  deﬁnition   (after  multiplying  our  objective  func(cid:173)
1 
tion  by   the   constant 2
λ  ,  the  basic  regularization  problem  
becomes 

.


min  C
f ∈H 

 

n 
X
i=1  

V   (yi,  f (xi)) + 

1 
2

||f ||2 
K . 

Like  λ,  the  parameter  C  also  controls  the  tradeoﬀ  between 
classiﬁcation   accuracy   and  the   norm  of  the   function.  The 
primal  and  dual  problems  become.  .  . 

The  Reparametrized  Problems


min 
c∈IRn ,ξ∈IRn

subject  to  :  yi( 

cT K c

C i=1  ξi  + 1

n 
P
2
n
j=1  cjK (xi,  xj ) +  b)  ≥  1  −  ξi 
P
ξi  ≥  0 

i  = 1, .  .  .  ,  n 

i  = 1, .  .  .  ,  n 

max 
α∈IRn 
subject  to  : 

1αT Qα 
n 
i=1  αi  −  2
P
n 
P
i=1   yiαi 
0  ≤  αi  ≤  C

= 0

i  = 1, .  .  .  ,  n 

The   Geometric  Approach


The  “traditional”  approach   to   explaining   the   SVM  is  via 
separating  hyperplanes and  margin. 
In  a  linear  space,   a 
perceptron  is  a  linear  hyperplane  that  separates  the   pos(cid:173)
itive  and  the  negative  examples.   Deﬁning   the   margin   as 
the  distance  from  the  hyperplane  to  the  nearest  example,  
intuitively,  we  expect  a  hyperplane   with   larger   margin   to 
generalize better. 

Large  and   Small  Margin  Hyperplanes


(a) 

(b) 


Classiﬁcation  With  Hyperplanes 


We  denote  our  hyperplane  by  w,  and  we  will  classify  a  new 
point  x via  the  function 

f (x)  =   sign  (w ·  x). 

(1)


Given  a  separating  hyperplane  w  we  let  x  be  a  datapoint 
closest  to  w,  and  we  let  xw  be  the  unique  point  on  w that 
is  closest  to  x.   Obviously,  ﬁnding  a  maximum  margin  w is 
equivalent  to  maximizing  ||x −  xw ||.  .  . 

Deriving  the  Maximal   Margin,   I


For  some  k  (assume  k >  0  for  convenience), 


w ·  x =  k 
w ·  xw = 0 
=⇒  w ·  (x −  xw) =  k 

Deriving  the  Maximal  Margin,  I I


Noting  that   the   vector  x −  xw  is  parallel   to  the  normal 
vector  w, 

w ·  (x −  xw) =  w · 

 

!
w 

 
||x −  xw || 
 
||w|| 
xw || 
||w||2 ||x − 
||w|| 
||w||   ||x −  xw || 
||w||   ||(x −  xw)||  =  k 
k 
||w|| 

||x −  xw ||  = 

= 

= 

=⇒ 

=⇒ 

Deriving  the  Maximal  Margin,  I I I


k  is   a  “nuisance  paramter”.  WLOG,  we  ﬁx  k  to  1,  and  see

1 
that  maximizing   ||x − xw ||   is  equivalent  to  maximizing 
, 
||w||
which  in  turn   is  equivalent  to   minimizing  ||w||  or  ||w||2.  We 
can  now   deﬁne   the  margin  as  the   distance  between  the 
hyperplanes  w ·  x = 0   and  w ·  x = 1. 

The  Linear,  Homogeneous,  Separable  SVM


min 
w∈IRn 
subject  to  :   yi(w ·  x)  ≥  1 

||w||2 

i = 1, . . . , n  

Bias  and  Slack


The  SVM   introduced  by  Vapnik   includes  an   unregularized  
bias  term   b,  leading  to  classiﬁcation  via  a  function  of  the 
form: 

f (x)  =  sign  (w ·  x +  b).


In  practice,  we  want  to  work  with  datasets  that  are   not 
linearly  separable,   so  we  introduce  slacks  ξi,  just  as  before.  
We  can  still  deﬁne  the  margin  as  the   distance   between   the 
hyperplanes  w ·  x =  0  and  w ·  x =  1,   but  this  is  no  longer 
particularly  geometrically  satisfying. 

The  New  Primal


With  slack  variables,  the   primal  SVM  problem   becomes

C i=1   ξi + 1 
2 ||w||2
n 
min 
P
w∈IRn ,b∈IR 
subject  to  :  yi(w ·  x +  b)  ≥  1  −  ξi 
ξi  ≥  0 

i = 1, . . . , n 

i = 1, . . . , n 

Historical  Perspective


Historically,   most  developments  begin   with  the   geometric 
form,  derived  a  dual   program  which  was  identical  to  the 
dual  we  derived  above,   and  only  then   observed   that  the 
dual  program   required  only  dot  products  and   that  these  
dot  products  could  be  replaced  with   a  kernel  function. 

More  Historical  Perspective


In  the  linearly  separable   case,  we  can   also  derive  the  sep(cid:173)
arating  hyperplane  as  a  vector  parallel  to  the  vector   con(cid:173)
necting  the  closest  two   points  in  the  positive  and  negative 
classes,  passing  through   the  perpendicular  bisector  of  this 
vector.  This   was  the  “Method  of  Portraits”,   derived   by 
Vapnik   in  the  1970’s,  and  recently   rediscovered  (with  non(cid:173)
separable  extensions)   by  Keerthi.  

The  Primal  and  Dual  Problems  Again 


min 
c∈IRn ,ξ∈IRn

subject  to  :  yi( 

cT K c

C i=1  ξi  + 1

n 
P
2
n
j=1  cjK (xi,  xj ) +  b)  ≥  1  −  ξi 
P
ξi  ≥  0 

i  = 1, .  .  .  ,  n 

i  = 1, .  .  .  ,  n 

max 
α∈IRn 
subject  to  : 

1αT Qα 
n 
i=1  αi  −  2
P
n 
P
i=1   yiαi 
0  ≤  αi  ≤  C

= 0

i  = 1, .  .  .  ,  n 

Optimal  Solutions


The  primal  and  the   dual   are  both   feasible   convex   quadratic  
programs.  Therefore,  they   both  have   optimal  solutions, 
and  optimal  solutons  to  the   primal  and  the   dual  have  the  
same  objective   value. 

The   Reparametrized  Lagrangian 


We  derived  the   dual   from   the  primal  using   the   (now  repa-
rameterized) Lagrangian: 

L(c,  ξ ,  b,  α,  ζ )  =  C 

− 

− 

ξi  +  c T K c 
 
 
  n 

X
αi  yi 

j=1 
ζiξi 

 

 

n 
X
i=1 
n 
 
X
i=1 
n 
 
X
i=1 



 

 
cjK (xi,  xj ) +  b  −  1  +  ξi
 


Complementary  Slackness 


Consider   the  dual   variables  are  associated   with   the   primal 
constraints  as   follows: 

  n 

X

j=1

ζi  =⇒  ξi  ≥  0


 

 

cjK (xi,  xj ) +  b  −  1  +  ξi 
 


αi  =⇒  yi 

 

Complementary  slackness  tells  us   that  at  optimality,   either 
the  primal  inequality  is  satisﬁed   with   equality  or  the  dual 
if  c,  ξ ,  b,  α  and   ζ  are 
variable 
is  zero. 
In  other  words, 
optimal  solutions   to  the  primal  and  dual,  then 
 
 

 

 
  n 


  = 0  
X
cjK (xi,  xj ) +  b  −  1  +  ξi
αi  yi 
 


j=1 
ζiξi  = 0  

 

Optimality  Conditions


All  optimal   solutions  must  satisfy: 

 

n 
X
j=1 

 

n 
X
j=1 

cjK (xi,  xj ) − 

yiαjK (xi,  xj )  =  0 

i  =  1,  .  .  .  ,  n  

 

 

 

 

αiyi  =  0 

n 
X
i=1 
C  −  αi  −  ζi  =  0 
 


n 
 −  1  +  ξi  ≥  0 
X
yj αjK (xi,  xj ) +   b
yi  
j=1 
 
 
 
 




n 
 
 
 −  1  +  ξi
 =  0 
X
yj αjK (xi,  xj ) +   b
αi  yi  
j=1 
ζiξi  =  0 
ξi,  αi,  ζi  ≥  0 

i  =  1,  .  .  .  ,  n  

i  =  1,  .  .  .  ,  n  

i  =  1,  .  .  .  ,  n  

i  =  1,  .  .  .  ,  n  

i  =  1,  .  .  .  ,  n  

Optimality  Conditions,  I I


The  optimality  conditions  are  both  necessary  and   suﬃ(cid:173)
cient. 
If  we  have  c,  ξ ,  b,   α  and  ζ  satisfying  the  above 
conditions,  we   know  that  they   represent  optimal  solutions 
to  the  primal   and  dual  problems.   These  optimality  condi(cid:173)
tions   are  also   known  as   the  Karush-Kuhn-Tucker   (KKT)  
conditons.  

Toward  Simpler  Optimality  Conditions  —

Determining  b


Suppose  we  have  the  optimal   αi’s.  Also  suppose   (this  “al(cid:173)
ways”  happens  in   practice”) that  there  exists  an  i  satisfying 
0  <  αi <  C .  Then 

 

αi <  C  =⇒  ζi >  0 
=⇒  ξi  = 0 
 


n 
 −  1  = 0 
X
=⇒  yi  
yj αjK (xi,  xj ) +  b
j=1 
n 
 
X
=⇒  b  =  yi  − 
yj αjK (xi,  xj ) 
j=1 
So  if  we  know  the  optimal   α’s,  we  can   determine   b.


 

Towards  Simpler  Optimality  Conditions,  I


Deﬁning  our  classiﬁcation  function  f (x)  as 

 

f (x) =  

n 
X
i=1 
we  can  derive  “reduced”  optimality   conditions.   For   exam(cid:173)
ple,  consider  an  i  such  that  yif (xi)  <  1:  

yiαiK (x,  xi) +  b, 

yif (xi)  <  1 =⇒  ξi >  0 
=⇒  ζi  = 0 
=⇒  αi  =  C 

Towards  Simpler  Optimality  Conditions,   I I


Conversely,   suppose  αi  =  C : 

αi  =  C  =⇒  yif (xi) −  1  +  ξi  = 0 
=⇒  yif (xi)  ≤  1 

Reduced  Optimality  Conditions


Proceeding  similarly,  we  can  write  the   following   “reduced” 
optimality  conditions   (full  proof:   homework):  

αi  =  0  ⇐⇒  yif (xi)  ≥  1 
0  <  αi  <  C  ⇐⇒  yif (xi)  =  1 
αi  =  C  ⇐⇒  yif (xi)  ≤  1 

Geometric  Interpretation   of  Reduced 

Optimality  Conditions


SVM  Training


Our  plan  will   be  to  solve   the  dual  problem  to  ﬁnd  the  α’s, 
and use  that  to   ﬁnd   b  and our  function  f .  The  dual  problem 
is  easier  to  solve  the  primal   problem. 
It  has  simple   box  
constraints  and  a  single  inequality  constraint,  even  better, 
we  will  see  that  the  problem  can  be  decomposed  into   a 
sequence  of  smaller  problems. 

Oﬀ-the-shelf  QP  software


We  can  solve  QPs  using  standard  software.   Many   codes 
are  available.  Main  problem  —  the   Q  matrix 
is  dense,  
and  is  n-by-n,  so  we  cannot  write  it  down.  Standard  QP 
software  requires  the  Q  matrix,   so   is  not  suitable   for   large 
problems.  

Decomposition,   I


max  
αW ∈IR|W | ,  αR∈IR|R| 

Partition  the   dataset  into  a  working  set W  and the  remainig 
points  R.  We  can   rewrite  the  dual  problem  as:  
n 
i=1  αi  + 
P
P
i∈W
 
 
#
"
"
αW
QW W  QW R 
− 1 
2 [αW  αR] 
αR 
QRW  QRR 
 
 
i∈W yiαi  + 
i∈R yiαi  = 0 
P
P
0  ≤  αi  ≤  C,  ∀i  

 
i=1  αi

i∈R


subject  to  : 

 

 

#

Decomposition,  I I 


Suppose  we  have   a  feasible   solution  α.  We  can  get  a  
better  solution  by  treating  the   αW  as   variable  and  the  αR 
as   constant.  We   can  solve   the  reduced  dual   problem: 

max 
αW ∈IR|W | 
subject  to  : 

1 
(1 −  QW RαR)αW  −  2αWQW W αW 
 
 
i∈W yiαi  =  − 
P
P
i∈R yiαi 
0  ≤  αi  ≤  C,  ∀i   ∈  W 

Decomposition,  I I I


The  reduced  problems  are  ﬁxed  size,  and   can  be   solved  us(cid:173)
ing  a  standard   QP  code.  Convergence   proofs  are  diﬃcult,  
but  this  approach  seems  to  always  converge  to  an   optimal 
solution  in  practice.  

Selecting   the   Working  Set


There  are  many  diﬀerent  approaches.   The  basic   idea  is  to  
examine  points  not  in   the  working  set,   ﬁnd  points   which 
violate  the  reduced  optimality  conditions,   and   add   them   to  
the  working  set.  Remove  points  which   are  in  the   working  
set  but  are  far  from  violating  the  optimality   conditions.  

Good  Large-Scale   Solvers


•  SVMLight:  http://svmlight.joachims.org 
•  SVMTorch:   http://www.idiap.ch/learning/SVMTorch.html

•  LIBSVM:  http://wws.csie.ntu.edu.tw/~cjlin/libsvm 

