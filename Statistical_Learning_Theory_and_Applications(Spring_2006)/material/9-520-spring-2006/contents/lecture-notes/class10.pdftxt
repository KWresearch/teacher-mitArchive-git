Bagging  and  Boosting 

9.520  Class  10,  13  March  2006


Sasha  Rakhlin


Plan

Bagging  and  sub-sampling  methods

Bias-Variance  and  stability  for  bagging 
Boosting  and  correlations  of  machines

Gradient  descent  view  of  boosting 

• 
• 
• 
• 

Bagging  (Bootstrap  AGGregatING)


Given  a  training  set  D = {(x1, y1), . . . (xn, yn)}, 

• 

• 

sample T  sets of n elements  from D  (with  replacement) 
→ 
T  quasi  replica  training  sets; 
D1, D2, . . . DT 

train  a  machine  on  each  Di,  i  = 1, ..., T  and  obtain  a 
sequence  of  T  outputs  f1(x), . . . fT (x). 

Bagging  (cont.)


for  regression 

f¯(x) = 

• 

• 

The  ﬁnal  aggregate  classiﬁer  can  be 
T�

fi(x), 

i=1 
the  average  of  fi  for  i = 1, ..., T ; 
T�

f¯(x) = sign( 
fi(x)) 
i=1 
T�

f¯(x) = sign( 
i=1 


or  the  majority  vote 

for  classiﬁcation 

sign(fi(x))) 

Variation  I:  Sub-sampling  methods


- “Standard”  bagging:  each  of  the  T  subsamples  has  size 
n  and  created  with  replacement. 

- “Sub-bagging”:  create  T  subsamples  of  size  α  only  (α < 
n). 

- No  replacement:  same  as  bagging  or  sub-bagging,  but 
using  sampling  without  replacement 

- Overlap  vs  non-overlap:  Should  the  T	 subsamples  over-
n
lap?  i.e.  create  T  subsamples  each  with	 T  training  data. 

Bias  - Variance  for  Regression  (Breiman

1996)


Let 

� 
I [f ] =  (f (x) − y)2 p(x, y)dxdy 
be  the  expected  risk  and  f0  the  regression  function.  With 
� 
f¯(x) = ES  fS (x),  if  we  deﬁne  the  bias  as 
(f0(x) − f¯(x))2 p(x)dx 
�� 
and  the  variance  as 

� 
(fS (x) − f¯(x))2 p(x)dx  , 
we  have  the  decomposition 
ES {I [fS ]} = I [f0] + bias + variance. 

ES 

Bagging  reduces  variance  (Intuition)


If  each  single  classiﬁer  is  unstable  –  that  is,  it  has  high 
variance,  the  aggregated  classiﬁer  f¯ has  a  smaller  vari­
ance  than  a  single  original  classiﬁer. 

The  aggregated  classiﬁer  f¯ can  be  thought  of  as  an  ap­
proximation  to  the  true  average  f  obtained  by  replacing 
the  probability  distribution  p  with  the  bootstrap  approxi­
mation  to  p obtained  concentrating mass 1/n  at  each point 
(xi, yi). 

Variation  I I:  weighting  and  combining

alternatives 


- No  subsampling,  but  instead  each  machine  uses  diﬀerent

weights  on  the  training  data.


- Instead  of  equal  voting,  use  weighted  voting. 


- Instead  of  voting,  combine  using  other  schemes.


Weak  and  strong  learners


Kearns  and  Valiant  in  1988/1989  asked  if  there  exist  two 
types  of  hypothesis  spaces  of  classiﬁers. 

• 

• 

Strong  learners:  Given  a  large  enough  dataset  the  clas­
siﬁer  can  arbitrarily  accurately  learn  the  target  function 
1 − τ 

Weak  learners:  Given  a  large  enough  dataset  the  clas­
siﬁer  can  barely  learn  the  target  function  1 
2 + τ 

The  hypothesis  boosting  problem:  are  the  above  equiva­
lent  ? 

The  original  Boosting  (Schapire,  1990):

For  Classiﬁcation  Only


1.  Train  a  ﬁrst  classiﬁer	 f1  on  a  training  set  drawn  from 
a  probability  p(x, y).  Let  �1  be  the  obtained  training 
performance; 

2.  Train a  second classiﬁer  f2  on a  training  set drawn  from 
a  probability  p2(x, y)  such  that  it  has  half  its  measure 
on  the  event  that  h1  makes  a  mistake  and  half  on  the 
rest.  Let  �2  be  the  obtained  performance; 

3.  Train  a  third  classiﬁer	 f3  on  disagreements  of  the  ﬁrst 
two  –  that  is,  drawn  from  a  probability  p3(x, y)  which 
has  its  support  on  the  event  that  h1  and  h2  disagree. 
Let  �3  be  the  obtained  performance. 

Boosting  (cont.)


Main  result:  If  �i  < p  for  all  i,  the  boosted  hypothesis 

g = M aj orityV ote  (f1, f2, f3) 
has  training  performance  no  worse  than  � = 3p2  − 2p3 

00.050.10.150.20.250.30.350.40.450.500.050.10.150.20.250.30.350.40.450.5Adaboost  (Freund  and  Schapire,  1996)


The  idea  is  of  adaptively  resampling  the  data 

• 

• 

• 

Maintain  a  probability  distribution  over  training  set; 

Generate  a  sequence  of  classiﬁers  in  which  the  “next” 
classiﬁer  focuses  on  sample  where  the  “previous”  clas­
siﬁer  failed; 

Weigh  machines  according  to  their  performance. 

Adaboost

Given:  a  class  F  = {f  : X  �→ {−1, 1}}  of  weak  learners  and 
the  data  {(x1, y1), . . . , (xn, yn)}, 
yi  ∈  {−1, 1}. 
Initialize  the 
weights  as  w1(i) = 1/n. 
For  t = 1, . . . T : 

1.  Find  a  weak  learner  ft  based  on  weights  wt(i); 
�

2.  Compute the weighted error �t  = 

=1 wt(i)I (yi  �= ft(xi)); 
n 
�
�
i
1−�t 
3.  Compute  the  importance  of  ft  as  αt  = 1/2 ln 
�t 
�n
4.  Update  the  distribution  wt+1(i) =  wt(i)e−αtyift(xi)
Zt 
i=1 wt(i)e−αtyiht(xi)
Zt  = 
.

;

,

Adopt  as  ﬁnal  hypothesis 

Adaboost  (cont.)

⎛⎝

⎞⎠

T�

αtft(x) 
g(x) = sign 
t=1 

Theory  of  Boosting 


We deﬁne  the margin of (xi, yi) according  to  the  real valued 
function  g  to  be 

margin(xi, yi) = yig(xi). 

Note  that  this  notion  of  margin  is  diﬀerent  from  the  SVM 
margin.  This  deﬁnes  a  margin  for  each  training  point! 

Performance  of  Adaboost


Theorem:  Let  γt  = 1/2 − �t  (how  much  better  ft  is  on  the 
�
T�

1
 n�

weighted  sample  than  tossing  a  coin).  Then 
I (yig(xi) < 0) ≤

n i=1 
t=1


1 − 4γ 2 
t 

Gradient  descent  view  of  boosting

n1  � 
We  would  like  to  minimize 
I (yig(xi) < 0) 
n i=1 
over  the  linear  span  of  some  base  class  F .  Think  of  F  as 
the  weak  learners. 
Two problems:  a)  linear  span of F  can be huge  and  search­
ing  for  the  minimizer  directly  is  intractable.  b)  the  indi­
cator  is  non-convex  and  the  problem  can  be  shown  to  be 
NP-hard  even  for  simple  F . 

Solution  to  b):  replace  the  indicator  I (yg(x)  <  0)  with  a 
convex  upper  bound  φ(yg(x)). 

Solution  to  a)?


Gradient  descent  view  of  boosting

Let’s  search  over  the  linear  span  of  F  step-by-step.  At 
�t−1 
each  step  t,  we  add  a  new  function  ft  ∈ F  to  the  existing 
g = 
i=1 αifi. 
�n
i=1 φ(yig(xi)).  We  wish  to  ﬁnd  ft  ∈  F 
1
Let  Cφ(g) =  n 
to  add  to  g  such  that  Cφ(g + �ft)  decreases.  The  desired 
direction  is  − � Cφ(g).  We  choose  the  new  function  ft  such 
that  it  has  the  greatest  inner  product  with  − � Cφ,  i.e.  it 
maximizes 

− < �Cφ(g), ft  >  . 

Gradient  descent  view  of  boosting

1  n�

One  can  verify  that 
�
2−
− < �Cφ(g), ft  >=

yift(xi)φ
 (yig(xi)).
n i=1 
Hence,  ﬁnding ft  maximizing − < �Cφ(g), ft  >  is equivalent 
n�

to  minimizing  the  weighted  error 
wt(i)I (ft(xi) = yi) 
i=1 
wt(i)  :=  �n 
�
φ (yig(xi)) 
j=1 φ�
(yj g(xj ))

where 

For  φ(yg(x)) = e−yg(x)  this  becomes  Adaboost.


�
