Massachusetts Institute of Technology 
6.042J/18.062J, Spring ’10: Mathematics for Computer Science 
Prof. Albert R. Meyer 

May 7 
revised May 5, 2010, 847 minutes 

In-Class Problems Week 13, Fri. 

Problem 1. 
A herd of cows is stricken by an outbreak of cold cow disease.  The disease lowers the normal body 
temperature of a cow, and a cow will die  if  its  temperature goes below 90 degrees F. The disease 
epidemic  is  so  intense  that  it  lowered  the  average  temperature  of  the  herd  to  85  degrees.  Body 
temperatures as low as 70 degrees, but no lower, were actually found in the herd. 

(a)  Prove that at most 3/4 of the cows could have survived. 
Hint: Let T  be the temperature of a random cow. Make use of Markov’s bound. 

(b)  Suppose  there  are  400  cows  in  the herd.  Show  that  the bound of part  (a)  is best possible by 
giving an example set of temperatures for the cows so that the average herd temperature is 85, and 
with probability 3/4, a randomly chosen cow will have a high enough temperature to survive. 

Problem 2. 
A gambler plays 120 hands of draw poker, 60 hands of black jack, and 20 hands of stud poker per 
day.  He wins  a  hand  of  draw  poker with  probability  1/6,  a  hand  of  black  jack with  probability 
1/2, and a hand of stud poker with probability 1/5. 

(a)  What is the expected number of hands the gambler wins in a day? 

(b)  What would  the Markov bound be on  the probability  that  the gambler will win  at  least  108 
hands on a given day? 

(c)  Assume  the outcomes of  the  card games  are pairwise  independent.  What  is  the variance  in 
the number of hands won per day? 

(d)  What would  the  Chebyshev  bound  be  on  the  probability  that  the  gambler will win  at  least 
108 hands  on  a  given day?  You may  answer with  a numerical  expression  that  is not  completely 
evaluated. 

Problem 3. 
The proof of the Pairwise Independent Sampling Theorem 21.5.1 was given for a sequence R1 , R2 , . . . 
of pairwise independent random variables with the same mean and variance. 

The  theorem  generalizes  straighforwardly  to  sequences  of  pairwise  independent  random  vari­
ables,  possibly  with  different  distributions,  as  long  as  all  their  variances  are  bounded  by  some 
constant. 

Creative Commons 

2010,  Prof. Albert R. Meyer. 

2 

In-Class Problems Week 13, Fri. 

Theorem  (Generalized  Pairwise  Independent  Sampling).  Let X1 , X2 , . . .  be  a  sequence  of  pairwise 
independent random variables such that Var [Xi ] ≤ b for some b ≥ 0 and all i ≥ 1.  Let 
X1  + X2  + · · · + Xn , 
An  ::= 
n 
µn  ::= E [An ] . 

Then for every � > 0, 

b  1
�2  · 
Pr {|An  − µn | > �} ≤ 
n
(a)  Prove the Generalized Pairwise Independent Sampling Theorem. 

. 

(b)  Conclude that the following holds: 
Corollary (Generalized Weak Law of Large Numbers).  For every � > 0, 
lim  Pr {|An  − µn | ≤ �} = 1. 
n→∞ 

Appendix 

Markov’s Theorem 

If R is a nonnegative random variable, then for all x > 0 
[R] 
E 
Pr {R ≥ x} ≤ 
.
x 

Variance 

� 
� 
� 
� 
The variance, Var [R], of a random variable, R, is: 
Var [R] ::= E  (R − E [R])2 = E  R2 − E2  [R] . 
[Variance of an indicator variable], I , with Pr {I  = 1} = p: 
Var [I ] = p(1 − p). 

[Variance and constants] For constants, a, b, 
Var [aR + b] = a 2 Var [R] . 

[Variance Additivity] If R1 , R2 , . . . , Rn  are pairwise independent variables, then 
· · · 
· · · 
Var [R1  + R2  +
+ Rn ] = Var [R1 ] + Var [R2 ] +  + Var [Rn ]

Chebyshev’ s Bound 

Let R be a random variable, and let x be a positive real number. Then 
[R] 
Var 
Pr {|R − E [R]| ≥ x} ≤ 
.
2
x

(1) 

(2) 

In-Class Problems Week 13, Fri. 

Pairwise Independent Sampling 

3 

An  ::= 

, 

Theorem.  Let 

Proof.  By linearity of expectation, 

� 
n
Sn  ::= 
Gi , 
i=1 
Sn
n 
1  � σ �2 
where  G1 , . . . , Gn  are  pairwise  independent  random  variables  with  the  same  mean,  µ,  and  deviation,  σ . 
Then 
Pr {|An  − µ| ≥ x} ≤ 
· 
n 
x
� 
� 
n 
n 
=1 E [Gi ] 
=1 Gi ] 
E [ 
E [An ] = 
= 
i
i
n 
n
� 
� 
� �2 
� 
Since the Gi ’s are pairwise independent, their variances will also add, so 
n
1 
� �2  n
Var [An ] = 
Var 
Gi 
1  � 
n 
i=1 
� �21 
= 
Var [Gi ] 
n 
i=1 
= 
n 
σ2

= 
n 

(additivity) 

=

nµ
n 

= µ. 

nσ2 

.


. 

(3) 

(by (2)) 

Now letting R be An  in Chebyshev’s Bound yields (3), as required. 

� 

MIT OpenCourseWare
http://ocw.mit.edu 

6.042J / 18.062J Mathematics for Computer Science 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

