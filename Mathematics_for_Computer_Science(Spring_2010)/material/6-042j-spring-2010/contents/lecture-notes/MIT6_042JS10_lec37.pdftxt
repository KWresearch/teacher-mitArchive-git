Mathematics for Computer Science
MIT 6.042J/18.062J

Deviation from 
the Mean

Example: IQ 
IQ measure was constructed so
that

average IQ = 100.
What fraction of the people
can possibly have an IQ  300?

Albert R Meyer,                May 7, 2010 

lec 13F.1 

Albert R Meyer,                May 7, 2010 

lec 13F.15 

IQ Higher than 300? 

IQ Higher than 300? 

Fraction f with IQ  300
adds  300f to average, 
so 100 = avg IQ  300f:
f  100/300 = 1/3

At most 1/3 of people 
have IQ  300

Albert R Meyer,                May 7, 2010 

lec 13F.16 

Albert R Meyer,                May 7, 2010 

lec 13F.17 

IQ Higher than x?

Besides mean = 100, 
we used only one fact about 
the distribution  of IQ: 
IQ is always nonnegative

Markov Bound 
If R is nonnegative, then 
E R 
x

Pr{R  x} 

for x  E[R]

Albert R Meyer,                May 7, 2010 

lec 13F.19 

Albert R Meyer,                May 7, 2010 

lec 13F.20 

1

Markov Bound 

•Weak
•Obvious
•Useful anyway 

IQ  300, again 
Suppose we are given that
IQ is always  50?
Get a better bound using 
(IQ – 50)
since this is now  0. 

Albert R Meyer,                May 7, 2010 

lec 13F.22 

Albert R Meyer,                May 7, 2010 

lec 13F.24 

IQ  300, again 
f contributes (300-50)f to
the average of (IQ-50), so 
      50 = E[IQ-50]  250f
f  50/250 = 1/5

Better bound from Markov by 
shifting R to have 0 as minimum 

Albert R Meyer,                May 7, 2010 

lec 13F.25 

Improving the Markov Bound 

Pr{|R|  x}
= Pr{(R)2  x2}
by Markov: 

variance of R 
Albert R Meyer,                May 7, 2010 

lec 13F.26 

Chebyshev Bound 
Pr {|R - μ| x}  Var[R]
x 2
Var[R] ::= E[(R - )2 ]

Albert R Meyer,                May 7, 2010 

lec 13F.28 

Pr{|R - μ| x} 

Standard Deviation 
2
x2
R probably not many ’s from μ: 
further than      Pr  1 
2     Pr  1/4 
3     Pr  1/9
4     Pr  1/16
lec 13F.32 
Albert R Meyer,                May 7, 2010 

2

Variance of an Indicator 
I an indicator with E[I]=p:

Calculating Variance 
Var[aR + b] = a 2 Var[R]

= E I





 2p  p + p2

simple proofs applying linearity 
of E[] to the def of Var[]

Albert R Meyer,                May 7, 2010 

lec 13F.34 

Albert R Meyer,                May 7, 2010 

lec 13F.35 

Calculating Variance 
Pairwise Independent Additivity

Mathematics for Computer Science
MIT 6.042J/18.062J

providing R1,R2,…,Rn are
pairwise independent

again, a simple proof applying 
linearity of E[] to the def of Var[]

Deviation of 
Repeated Trials

Albert R Meyer,                May 7, 2010 

lec 13F.43 

Albert R Meyer,                May 7, 2010 

lec 13F.44 

Jacob D. Bernoulli (16591705)

Jacob D. Bernoulli (16591705)

Even the stupidest man by some instinct of 
nature per se and by no previous instruction 
(this is truly amazing) knows for sure that 
the more observations ...that are taken, the 
less the danger will be of straying from the 
mark.
---Ars Conjectandi (The Art of Guessing), 1713* 
*taken from Grinstead \& Snell, 
http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html 
Introduction to Probability, American Mathematical Society, p. 310. 

It certainly remains to be inquired whether 
after the number of observations has been 
increased, the probability…of obtaining the 
true ratio…finally exceeds any given degree 
of certainty; or whether the problem has, so 
to speak, its own asymptote that is, whether 
some degree of certainty is given which one 
can never exceed. 

Albert R Meyer,                May 7, 2010 

lec 13F.45 

Albert R Meyer,                May 7, 2010 

lec 13F.46 

3

Repeated Trials

Random var R with mean μ
n independent observations 
R1,, Rn

Repeated Trials
take average: 

Bernoulli question:  is it 
probably close to μ if n is big 

{
Pr A

n

 μ  
} =

?

Albert R Meyer,                May 7, 2010 

lec 13F.54 

Albert R Meyer,                May 7, 2010 

lec 13F.55 

Bernoulli answer: 
Weak Law of Large Numbers 
Pr{ A n - μ   } = ?
1

lim
n

Pr{ A n - μ >  } = 0

lim
n

Jacob D. Bernoulli (1659 – 1705) 

Therefore, this is the problem which I 
now set forth and make known after I 
have pondered over it for twenty years.
Both its novelty and its very great
usefulness, coupled with its just as 
great difficulty, can exceed in 
weight and value all the remaining 
chapters of this thesis. 

Albert R Meyer,                May 7, 2010 

lec 13F.57 

Albert R Meyer,                May 7, 2010 

lec 13F.58 

Weak Law of Large Numbers 

will follow easily by Chebyshev
& variance properties

Pr{ A n - μ >  } = 0

lim
n

Repeated Trials 

=

nμ
n

= μ

Albert R Meyer,                May 7, 2010 

lec 13F.59 

Albert R Meyer,                May 7, 2010 

lec 13F.60 

4

Weak Law of Large Numbers 

So by Chebyshev

Pr{ A

n

- μ >  } 





n


Var A


2

need only show 
Var[An]  0 as n  

n


Var A

(


Var R


1

=

1

2




= Var

Repeated Trials 
(

+  + R
+ R
R






 + Var R

2
n2



 +  + Var R


n

n

)






)



n

QED

 0 

Albert R Meyer,                May 7, 2010 

lec 13F.61 

Albert R Meyer,                May 7, 2010 

lec 13F.63 

Analysis of the Proof 
proof only used that R1,…,Rn have
•same mean 
•same variance
•& variances add
 which follows from 
pairwise independence

Pairwise Independent Sampling 
Theorem:
Let R1,…,Rn be pairwise independent 
random vars with the same finite
mean μ and variance 2.  Let 
                                            Then 
2
}  1
{


Pr A


	

n

- μ > 





n

Albert R Meyer,                May 7, 2010 

lec 13F.64 

Albert R Meyer,                May 7, 2010 

lec 13F.65 

Pairwise Independent Sampling 
The punchline:
we now know how big a sample is 
needed to estimate the mean of 
any* random variable within 
any* desired tolerance with 
any* desired probability 
*variance < , tolerance > 0, 
  probability < 1 

Team Problems 

Problems
13

Albert R Meyer,                May 7, 2010 

lec 13F.66 

Albert R Meyer,                May 7, 2010 

lec 13F.67 

5

MIT OpenCourseWare

http://ocw.mit.edu

6.042J / 18.062J Mathematics for Computer Science

Spring 2010

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .

