Chapter 15 

Sums & Asymptotics 

15.1  The Value of an Annuity 

Would you prefer a million dollars today or $50,000 a year for the rest of your life? 
On the one hand,  instant gratiﬁcation is nice.  On the other hand,  the total dollars 
received at $50K per year is much larger if you live long enough. 
Formally, this is a question about the value of an annuity. An annuity is a ﬁnan­
cial  instrument  that pays  out  a ﬁxed  amount  of money  at  the  beginning  of  every 
year  for some speciﬁed number of years.  In particular, an n-year, m-payment an­
nuity pays m dollars at the start of each year for n years.  In some cases, n is ﬁnite, 
but not always.  Examples include lottery payouts, student loans, and home mort­
gages. There are even Wall Street people who specialize in trading annuities. 
A  key  question  is what  an  annuity  is worth.  For  example,  lotteries  often  pay 
out  jackpots over many years.  Intuitively, $50, 000 a year  for 20 years ought  to be 
worth less than a million dollars right now.  If you had all the cash right away, you 
could  invest  it and begin collecting  interest.  But what  if  the choice were between 
$50, 000  a  year  for  20  years  and  a  half  million  dollars  today?  Now  it  is  not  clear 
which option is better. 
In  order  to  answer  such  questions,  we  need  to  know  what  a  dollar  paid  out 
in  the  future  is  worth  today.  To  model  this,  let’s  assume  that  money  can  be  in­
vested at a ﬁxed annual interest rate p. We’ll assume an 8% rate1  for the rest of the 
discussion. 
Here  is why  the  interest  rate  p matters.  Ten  dollars  invested  today  at  interest 
rate p will become (1 + p)  10 = 10.80 dollars in a year, (1 + p)2  10 ≈ 11.66 dollars
·
· 
in two years, and so forth. Looked at another way, ten dollars paid out a year from 
now are only really worth 1/(1+ p) 10 ≈ 9.26 dollars today. The reason is that if we 
·

1U.S.  interest  rates  have dropped  steadily  for  several  years,  and  ordinary  bank deposits  now  earn 
around  1.5%.  But  just  a  few  years  ago  the  rate was  8%;  this  rate makes  some  of  our  examples  a  little 
more dramatic. The rate has been as high as 17% in the past thirty years. 
In  Japan,  the  standard  interest  rate  is near zero%,  and on a  few occasions  in  the past  few years has 
even been slightly negative.  It’s a mystery why the Japanese populace keeps any money in their banks. 

307 

308 

CHAPTER 15.  SUMS & ASYMPTOTICS 

had  the  $9.26  today, we  could  invest  it  and would have  $10.00  in  a year  anyway. 
Therefore, p determines the value of money paid out in the future. 

15.1.1  The Future Value of Money 
So for an n-year, m-payment annuity, the ﬁrst payment of m dollars is truly worth 
m  dollars.  But  the  second  payment  a  year  later  is worth  only m/(1 + p)  dollars. 
Similarly,  the  third payment  is worth m/(1 + p)2 ,  and  the n-th payment  is worth 
only m/(1 + p)n−1 .  The  total  value,  V ,  of  the  annuity  is  equal  to  the  sum  of  the 
n� 
payment values. This gives: 
m 
n−1 � 
1  �j 
(1 + p)i−1 
� 
i=1 
= m · 
� 
1 + p 
j=0 
n−1
· 
xj 
j=0 

(substitute j  ::= i − 1) 

(substitute x =

1 
). 
1 + p 

(15.1)

V  = 

= m

(15.2)

1 − xn 
. 
1 − x 

The  summation  in  (15.1)  is  a  geometric  sum  that  has  a  closed  form,  making  the 
� 
evaluation a lot easier, namely2 , 
n−1
i 
x  = 
i=0 
(The phrase “closed  form”  refers  to a mathematical expression without any  sum­
mation or product notation.) 
Equation  (15.2)  was  proved  by  induction  in  problem  6.2,  but,  as  is  often  the 
case, the proof by induction gave no hint about how the formula was found in the 
ﬁrst place.  So we’ll take this opportunity to explain where it comes from. The trick 
is to let S  be the value of the sum and then observe what −xS  is: 
· · · 
+xn−1 
S  = 1 +x  +x2  +x3  +
−x  −x2  −x3  − 
· · ·  −xn−1  − xn . 
−xS  = 
Adding these two equations gives: 
S − xS  = 1 − x n , 

so 

1 − x
n
1 − x 
We’ll  look  further  into  this  method  of  proof  in  a  few  weeks  when  we  introduce 
generating functions in Chapter 16. 
2 To make this equality hold for x = 0, we adopt the convention that 00  ::= 1. 

S  = 

.

15.1.  THE VALUE OF AN ANNUITY 

309 

15.1.2  Closed Form for the Annuity Value 

So now we have a simple formula for V , the value of an annuity that pays m dollars 
at the start of each year for n years. 
1 − x
n 
V  = m 
1 − x 
1 + p − (1/(1 + p))n−1 
= m 
p 

(by (15.1) and (15.2)) 

(x = 1/(1 + p)). 

(15.4) 

(15.3)

The  formula  (15.4)  is much easier  to use  than a summation with dozens of  terms. 
For  example,  what  is  the  real  value  of  a winning  lottery  ticket  that  pays  $50, 000 
per  year  for  20  years?  Plugging  in  m  =  $50, 000,  n  =  20,  and  p  = 0.08  gives 
V  ≈  $530, 180.  So  because  payments  are  deferred,  the  million  dollar  lottery  is 
really  only worth  about  a half million dollars!  This  is  a  good  trick  for  the  lottery 
advertisers! 

15.1.3  Inﬁnite Geometric Series 

The question we began with was whether you would prefer a million dollars today 
or $50, 000 a year for the rest of your life. Of course, this depends on how long you 
live,  so  optimistically  assume  that  the  second  option  is  to  receive  $50, 000  a  year 
forever.  This  sounds  like  inﬁnite  money!  But  we  can  compute  the  value  of  an 
annuity with an inﬁnite number of payments by taking the limit of our geometric 
sum in (15.2) as n tends to inﬁnity. 
Theorem 15.1.1.  If |x| < 1, then 
� 
∞
i x  = 
i=0 

1
1 − x 
.

Proof. 

� 
�
∞
n−1

x i  ::=  lim 
x i

n→∞ 
i=0

i=0 
1 − x
n

=  lim 
n→∞  1 − x 
1 
.

1 − x

The ﬁnal line follows from that fact that limn→∞ xn  = 0 when |x| < 1. 

(by (15.2)) 

= 

� 

310 

CHAPTER 15.  SUMS & ASYMPTOTICS 

In our annuity problem, x = 1/(1 + p) <  1, so Theorem 15.1.1 applies, and we 
� 
get 
∞
· 
xj 
j=0 
1 
1 − x 
1 + p
p 

· 
= m 
· 
= m 

(by (15.1))

V  = m

(by Theorem 15.1.1)

(x = 1/(1 + p)).

Plugging in m = $50, 000 and p = 0.08, the value, V , is only $675, 000. Amazingly, 
a million dollars today is worth much more than $50, 000 paid every year forever! 
Then again,  if we had a million dollars  today  in  the bank earning 8%  interest, we 
could take out and spend $80, 000 a year forever. So on second thought, this answer 
really isn’t so amazing. 

15.1.4  Problems 
Class Problems 

Problem 15.1. 
You’ve seen this neat trick for evaluating a geometric sum: 

S  = 1 + z + z 2  + . . . + z 
n
zS  = z + z 2  + . . . + z n  + z n+1 
S − zS  = 1 − z n+1 
1 − zn+1 
1 − z 
Use the same approach to ﬁnd a closed-form expression for this sum: 

S  = 

T  = 1z + 2z 2  + 3z 3  + . . . + nz 
n

Homework Problems 

Problem 15.2. 
Is  a  Harvard  degree  really  worth  more  than  an  MIT  degree?!  Let  us  say  that  a 
person  with  a  Harvard  degree  starts  with  $40,000  and  gets  a  $20,000  raise  every 
year  after  graduation,  whereas  a  person with  an MIT  degree  starts with  $30,000, 
but gets a 20% raise every year. Assume inﬂation is a ﬁxed 8% every year.  That is, 
$1.08 a year from now is worth $1.00 today. 
(a)  How much is a Harvard degree worth today if the holder will work for n years 
following graduation? 

(b)  How much is an MIT degree worth in this case? 

15.2.  BOOK STACKING 

311 

(c)  If you plan to retire after twenty years, which degree would be worth more? 

Problem 15.3. 
Suppose you deposit  $100  into your MIT Credit Union  account  today,  $99  in one 
month from now, $98 in two months from now, and so on.  Given that the interest 
rate is constantly 0.3% per month, how long will it take to save $5,000? 

15.2  Book Stacking 

Suppose you have a pile of books and you want  to stack  them on a  table  in some 
off-center  way  so  the  top  book  sticks  out  past  books  below  it.  How  far  past  the 
edge of the table do you think you could get the top book to go without having the 
stack fall over? Could the top book stick out completely beyond the edge of table? 
Most people’s ﬁrst response to this question—sometimes also their second and 
third  responses—is  “No,  the  top  book will never  get  completely past  the  edge  of 
the table.” But in fact, you can get the top book to stick out as far as you want: one 
booklength, two booklengths, any number of booklengths! 

15.2.1  Formalizing the Problem 
We’ll approach this problem recursively.  How far past the end of the table can we 
get one book to stick out?  It won’t tip as long as its center of mass is over the table, 
so we can get it to stick out half its length, as shown in Figure 15.1. 

Figure 15.1: One book can overhang half a book length. 

Now  suppose we have  a  stack  of books  that will  stick  out past  the  table  edge 
without  tipping over—call  that a  stable stack.  Let’s deﬁne  the overhang of a stable 
stack  to  be  the  largest  horizontal  distance  from  the  center  of mass  of  the  stack  to 
the furthest edge of a book.  If we place the center of mass of the stable stack at the 
edge of the table as in Figure 15.2, that’s how far we can get a book in the stack to 
stick out past the edge. 

table12center of massof book312 

CHAPTER 15.  SUMS & ASYMPTOTICS 

Figure 15.2: Overhanging the edge of the table. 

So we want a formula for the maximum possible overhang, Bn , achievable with 
a stack of n books. 
We’ve  already  observed  that  the  overhang  of  one  book  is  1/2  a  book  length. 
That is, 

B1  = 

1 
.2 
Now suppose we have a stable stack of n + 1 books with maximum overhang. 
If  the overhang of  the n books on  top of  the bottom book was not maximum, we 
could  get  a  book  to  stick  out  further  by  replacing  the  top  stack with  a  stack  of  n 
books with larger overhang.  So the maximum overhang, Bn+1 , of a stack of n + 1 
books is obtained by placing a maximum overhang stable stack of n books on top 
of the bottom book.  And we get the biggest overhang for the stack of n + 1 books 
by placing the center of mass of the n books right over the edge of the bottom book 
as in Figure 15.3. 
So we  know where  to  place  the  n + 1st  book  to  get maximum  overhang,  and 
all we  have  to do  is  calculate what  it  is.  The  simplest way  to do  that  is  to  let  the 
center of mass of the top n books be the origin. That way the horizontal coordinate 
of  the  center  of  mass  of  the  whole  stack  of  n + 1  books  will  equal  the  increase 
in  the  overhang.  But  now  the  center  of  mass  of  the  bottom  book  has  horizontal 
coordinate 1/2, so the horizontal coordinate of center of mass of the whole stack of 
n + 1 books is 
·
·  = 
0  n + (1/2) 1 
n + 1 

1
. 
2(n + 1) 

In other words, 

as shown in Figure 15.3. 

Bn+1  = Bn  + 

1 
, 
2(n + 1) 

(15.5)

tablecenter of massof the whole stackoverhang15.2.  BOOK STACKING 

313 

Figure 15.3: Additional overhang with n + 1 books. 

Expanding equation (15.5), we have 

+ 

1 
2(n + 1) 
1
· · · 
+
2n 

+

1
Bn+1  = Bn−1  +
2n 
1 
= B1  + 
� 
· 
2 2 
1 n+1 1 
. 
2 
i 
i=1 

= 

+

1 
2(n + 1) 

(15.6)

Deﬁnition 15.2.1. 

The nth Harmonic number, Hn , is deﬁned to be 
n� 1 
Hn  ::= 
. 
i 
i=1 

So (15.6) means that 

Bn  = 

Hn
.2 
The  ﬁrst  few  Harmonic  numbers  are  easy  to  compute.  For  example,  H4  = 
1 +  1  +  1  +  1  =  25 .  The  fact  that H4  is greater  than  2 has  special  signiﬁcance;  it 
4 
12 
2
3
implies that the total extension of a 4-book stack is greater than one full book! This 
is the situation shown in Figure 15.4. 

15.2.2  Evaluating the Sum—The Integral Method 
It would be nice to answer questions like, “How many books are needed to build a 
stack extending 100 book lengths beyond the table?” One approach to this question 

table}2(n+1)1ntopbooks}center of massof topbooksncenter of massof all+1 booksn314 

CHAPTER 15.  SUMS & ASYMPTOTICS 

Figure 15.4:  Stack of four books with maximum overhang. 

would  be  to  keep  computing  Harmonic  numbers  until  we  found  one  exceeding 
200. However, as we will see, this is not such a keen idea. 
Such questions would be settled if we could express Hn  in a closed form.  Un­
fortunately, no closed form is known, and probably none exists.  As a second best, 
however, we can ﬁnd closed forms for very good approximations to Hn  using the 
Integral Method.  The  idea  of  the  Integral Method  is  to  bound  terms  of  the  sum 
above and below by simple functions as suggested in Figure 15.5.  The integrals of 
these functions then bound the value of the sum above and below. 

�
Figure  15.5:  This  ﬁgure  illustrates  the  Integral  Method  for  bounding  a  sum.  The  area 
n 
under  the  “stairstep”  curve  over  the  interval  [0, n]  is  equal  to  Hn  = 
=1 1/i.  The 
i
function 1/x is everywhere greater than or equal to the stairstep and so the integral of 1/x 
over  this  interval  is an upper bound on  the  sum.  Similarly,  1/(x + 1)  is  everywhere  less 
than or equal to the stairstep and so the integral of 1/(x + 1) is a lower bound on the sum. 

The  Integral Method gives  the  following upper and  lower bounds on  the har­

Table1/21/41/61/810123456781 / x1 / (x + 1)15.2.  BOOK STACKING 

monic number Hn : 

� 
n  1 
�  n 
Hn  ≤  1 + 
1  x 
1
0  x + 1 

Hn  ≥ 

�  n+1
dx = 1 + ln n 
1 

dx = 

1 
x 

315 

(15.7) 

(15.8) 

dx =  ln(n + 1). 

These bounds imply that the harmonic number Hn  is around ln n. 
But ln n grows —slowly —but without bound. That means we can get books to 
overhang any distance past  the edge of  the  table by piling  them high enough!  For 
example, to build a stack extending three book lengths beyond the table, we need 
a number of books n so that Hn  ≥ 6. By inequality (15.8), this means we want 
Hn  ≥  ln(n + 1) ≥ 6, 
so n ≥ e6 − 1 books will work, that is, 403 books will be enough to get a three book 
overhang.  Actual  calculation  of H6  shows  that  227  books  is  the  smallest  number 
that will work. 

15.2.3  More about Harmonic Numbers 
In the preceding section, we showed that Hn  is about ln n. An even better approx­
imation is known: 

Hn  =  ln n + γ +

1
2n 

+ 

1 
12n2 

+

�(n)
120n4 

Here γ  is a value 0.577215664 . . .  called Euler ’s constant, and �(n) is between 0 and 
1 for all n. We will not prove this formula. 

Asymptotic Equality 
The  shorthand  Hn  ∼  ln n  is  used  to  indicate  that  the  leading  term  of  Hn  is  ln n. 
More precisely: 
→ 
Deﬁnition 15.2.2.  For functions f , g  : R
R, we say f  is asymptotically equal to g , 
in symbols, 
f (x) ∼ g(x) 

iff 

lim  f (x)/g(x) = 1. 
x→∞ 
It’s  tempting  to might write Hn  ∼  ln n + γ  to  indicate  the  two  leading  terms, 
but  it  is  not  really  right.  According  to  Deﬁnition  15.2.2,  Hn  ∼  ln n +  c  where  c 
is  any  constant.  The  correct  way  to  indicate  that  γ  is  the  second-largest  term  is 
Hn  − ln n ∼ γ . 

316 

CHAPTER 15.  SUMS & ASYMPTOTICS 

The reason that the ∼ notation is useful is that often we do not care about lower 
order terms. For example, if n = 100, then we can compute H (n) to great precision 
���� 
����
using only the two leading terms: 
1 
1
120  1004  < 
· 
200 

|Hn  − ln n − γ | ≤ 

1 
. 
200 

− 

1 
120000 

+ 

15.2.4  Problems 
Class Problems 

Problem 15.4. 
An  explorer  is  trying  to  reach  the  Holy  Grail,  which  she  believes  is  located  in  a 
desert  shrine  d days walk  from  the  nearest  oasis.  In  the desert  heat,  the  explorer 
must drink continuously.  She can carry at most 1 gallon of water, which is enough 
for 1 day.  However, she is free to make multiple trips carrying up to a gallon each 
time to create water caches out in the desert. 
For  example,  if  the  shrine  were  2/3  of  a  day’s  walk  into  the  desert,  then  she 
could  recover  the  Holy  Grail  after  two  days  using  the  following  strategy.  She 
leaves  the oasis with 1 gallon of water,  travels 1/3 day  into  the desert,  caches 1/3 
gallon,  and  then walks back  to  the oasis— arriving  just as her water  supply  runs 
out. Then she picks up another gallon of water at the oasis, walks 1/3 day into the 
desert,  tops off her water  supply by  taking  the 1/3 gallon  in her cache, walks  the 
remaining 1/3 day to the shrine, grabs the Holy Grail, and then walks for 2/3 of a 
day back to the oasis— again arriving with no water to spare. 
But what if the shrine were located farther away? 
(a)  What  is  the most distant point  that  the explorer can reach and  then return  to 
the oasis if she takes a total of only 1 gallon from the oasis? 

(b)  What  is  the most  distant  point  the  explorer  can  reach  and  still  return  to  the 
oasis if she takes a total of only 2 gallons from the oasis? No proof is required; just 
do the best you can. 

(c)  The explorer will travel using a recursive strategy to go far into the desert and 
back drawing a total of n gallons of water from the oasis.  Her strategy is to build 
up a cache of n − 1 gallons, plus enough to get home, a certain fraction of a day’s 
distance  into  the  desert.  On  the  last  delivery  to  the  cache,  instead  of  returning 
home, she proceeds recursively with her n − 1 gallon strategy to go farther into the 
desert and return  to  the cache.  At  this point,  the cache has  just enough water  left 
to get her home. 
Prove  that  with  n  gallons  of  water,  this  strategy  will  get  her  Hn/2  days  into  the 
desert and back, where Hn  is the nth Harmonic number: 
1 
1 
1
1
· · · 
Hn  ::= + + +  + 
.
1
2
3 
n 
Conclude that she can reach the shrine, however far it is from the oasis. 

15.3.  FINDING SUMMATION FORMULAS 

317 

(d)  Suppose  that  the  shrine  is d  =  10 days walk  into  the desert.  Use  the asymp­
totic approximation Hn  ∼  ln n  to show  that  it will  take more  than a million years 
for the explorer to recover the Holy Grail. 
�
∞
i=1 

ip  converges iff p < a.  What is the value of a?

Problem 15.5. 
There is a number a such that 
Prove it. 

Homework Problems 

Problem 15.6. 
There  is  a  bug  on  the  edge  of  a  1-meter  rug.  The  bug wants  to  cross  to  the  other 
side of the rug.  It crawls at 1 cm per second.  However, at the end of each second, 
a malicious ﬁrst-grader named Mildred Anderson stretches the rug by 1 meter. As­
sume that her action is instantaneous and the rug stretches uniformly. Thus, here’s 
what happens in the ﬁrst few seconds: 

•	 The bug walks 1 cm in the ﬁrst second, so 99 cm remain ahead. 

•	 Mildred stretches the rug by 1 meter, which doubles its length.  So now there 
are 2 cm behind the bug and 198 cm ahead. 

•	 The bug walks another 1 cm in the next second, leaving 3 cm behind and 197 
cm ahead. 

•	 Then Mildred strikes, stretching the rug from 2 meters to 3 meters.  So there 
·	
· 
are now 3  (3/2) = 4.5 cm behind the bug and 197  (3/2) = 295.5 cm ahead. 
•	 The bug walks another 1 cm in the third second, and so on. 

Your job is to determine this poor bug’s fate. 
(a)  During second i, what fraction of the rug does the bug cross? 

(b)  Over  the  ﬁrst  n  seconds,  what  fraction  of  the  rug  does  the  bug  cross  alto­
gether? Express your answer in terms of the Harmonic number Hn . 
· 
(c)  The  known  universe  is  thought  to  be  about  3  1010  light  years  in  diameter. 
How many universe diameters must the bug travel to get to the end of the rug? 

15.3  Finding Summation Formulas 
The  Integral  Method  offers  a  way  to  derive  formulas  like  those  for  the  sum  of 
� 
consecutive integers, 
n
i = n(n + 1)/2, 
i=1 

318 

CHAPTER 15.  SUMS & ASYMPTOTICS 

or for the sum of squares, 

n� 
i2 
i=1 

= 

(2n + 1)(n + 1)n 
6 

=

3
n
3 

+

2
n
2

+ 

n 
. 
6 

(15.9)

These equations appeared in Chapter 2 as equations (2.2) and (2.3) where they 
were  proved  using  the Well-ordering  Principle.  But  those  proofs  did  not  explain 
how someone ﬁgured out in the ﬁrst place that these were the formulas to prove. 
Here’s  how  the  Integral Method  leads  to  the  sum-of-squares  formula,  for  ex­
�  n
�  n 
� 
ample.  First, get a quick estimate of the sum: 
n
i2  ≤ 
x 2  dx ≤ 
(x + 1)2  dx, 
0
0
i=1 
� 
n
i2  ≤ (n + 1)3/3 − 1/3. 
i=1 
� 
and the upper and lower bounds (15.10) imply that 
n
i2  ∼ n 3 /3. 
i=1 

n 3/3 ≤ 

so 

(15.10) 

To get an exact formula, we then guess the general form of the solution. Where we 
are uncertain, we can add parameters a, b, c, . . . .  For example, we might make the 
� 
guess: 
n
i2  = an 3  + bn2  + cn + d. 
i=1 
If  the  guess  is  correct,  then  we  can  determine  the  parameters  a,  b,  c,  and  d  by 
plugging  in  a  few  values  for  n.  Each  such  value  gives  a  linear  equation  in  a,  b, 
c,  and d.  If we plug  in  enough values, we may get a  linear  system with a unique 
solution. Applying this method to our example gives: 

n = 0 
n = 1 
n = 2 
n = 3 

implies  0 = d 
implies  1 = a + b + c + d 
implies  5 = 8a + 4b + 2c + d 
implies  14 = 27a + 9b + 3c + d. 

Solving this system gives the solution a = 1/3, b = 1/2, c = 1/6, d = 0.  Therefore, 
if  our  initial guess at  the  form of  the  solution was  correct,  then  the  summation  is 
equal to n3 /3 + n2/2 + n/6, which matches equation (15.9). 

15.3.  FINDING SUMMATION FORMULAS 

319 

The point is that if the desired formula turns out to be a polynomial, then once 
you  get  an  estimate  of  the  degree  of  the  polynomial —by  the  Integral Method  or 
any other way —all the coefﬁcients of the polynomial can be found automatically. 
Be  careful!  This method  let’s you discover  formulas,  but  it doesn’t guarantee 
they are right! After obtaining a formula by this method, it’s important to go back 
and  prove  it using  induction or  some other method,  because  if  the  initial guess  at 
the  solution  was  not  of  the  right  form,  then  the  resulting  formula  will  be  com­
pletely wrong! 

= 

= 

= 

= 

(geometric sum formula (15.2))

(inﬁnite geometric sum, Theorem 15.1.1) 

15.3.1  Double Sums 
Sometimes  we  have  to  evaluate  sums  of  sums,  otherwise  known  as  double  sum­
mations.  This  can  be  easy:  evaluate  the  inner  sum,  replace  it with  a  closed  form, 
and  then evaluate  the outer  sum which no  longer has a  summation  inside  it.  For 
� 
� 
∞ �� 
example, 
n
∞ � 
� 
i 
n
x 
y
i=0 � 
n=0 
y n 1 − x
n+1 
�
�
1 − x 
n=0 ∞ 
∞ 
�
n 
nxn+1

n=0 y

− 
n=0 y
1 − x 
1 − x 
∞ 
n=0  (xy)n 
1 
x 
− 
1 − x 
(1 − y)(1 − x) 
1 
x 
− 
(1 − y)(1 − x) 
(1 − xy)(1 − x) 
(1 − xy) − x(1 − y)
(1 − xy)(1 − y)(1 − x) 
1 − x 
(1 − xy)(1 − y)(1 − x)

1

(1 − xy)(1 − y) 
.
When  there’s no  obvious  closed  form  for  the  inner  sum,  a  special  trick  that  is 
often  useful  is  to  try  exchanging  the  order  of  summation.  For  example,  suppose we 
�  � � 
want to compute the sum of the harmonic numbers 
n
n
k
1/j 
Hk  =
k=1 j=1 
k=1 
� 
� 
For intuition about this sum, we can try the integral method: 
n
n 
Hk  ≈ 
ln x dx ≈ n ln n − n. 
1
k=1 

(inﬁnite geometric sum, Theorem 15.1.1) 

= 

= 

= 

320 

CHAPTER 15.  SUMS & ASYMPTOTICS 

Now let’s look for an exact answer.  If we think about the pairs (k , j ) over which 
we are summing, they form a triangle: 

j 
1 
1 
1 
1 
1 
. . . 
1 

k  1 
2 
3 
4 

n 

2 

3 

4 

5 

. . . 

n 

1/2 
1/2 
1/2 

1/2 

1/3 
1/3  1/4 

. . . 

1/n 

=

=

The  summation  above  is  summing  each  row  and  then  adding  the  row  sums.  In­
stead,  we  can  sum  the  columns  and  then  add  the  column  sums.  Inspecting  the 
�  � � 
table we see that this double sum can be written as 
n
n
k
Hk  =
1/j 
� � 
k=1 j=1 
k=1 
n
n
1/j 
�  � 
j=1 k=j 
n
n
1/j 
1 
n� 1 
j=1 
k=j 
(n − j + 1) 
n� n − j + 1 
j 
j=1 
� n + 1  � j
j
j=1 
n
n
− 
n� 1  � 
j 
j
j=1 
j=1 
n
− 
1 
= (n + 1) 
j 
j=1 
j=1 
= (n + 1)Hn  − n. 

(15.11) 

= 

= 

= 

15.4  Stirling’s Approximation 
The familiar factorial notation, n!, is an abbreviation for the product 
� 
n
i. 
i=1 

15.4.  STIRLING’S APPROXIMATION 

321 

This is by far the most common product in discrete mathematics.  In this section we 
describe  a good  closed-form  estimate of n!  called Stirling’s Approximation.  Unfor­
tunately, all we can do is estimate:  there is no closed form for n! —though proving 
so would take us beyond the scope of 6.042. 

15.4.1  Products to Sums 
A  good  way  to  handle  a  product  is  often  to  convert  it  into  a  sum  by  taking  the 
logarithm.  In the case of factorial, this gives 
(n − 1)  n)
·
·
· · · 
· 
ln(n!) = ln(1  2  3 
� 
= ln 1 + ln 2 + ln 3 +  + ln(n − 1) + ln n
· · · 
n
ln i. 
i=1 

= 

We’ve not seen a summation containing a  logarithm before!  Fortunately, one tool 
that we  used  in  evaluating  sums  is  still  applicable:  the  Integral Method.  We  can 
bound the terms of this sum with ln x and ln(x + 1) as shown in Figure 15.6.  This 
� 
� 
gives bounds on ln(n!) as follows: 
�
� 
� 
ln x dx ≤ 
n 
n ) + 1 ≤  �n 
ln(x + 1) dx
i=1

0

1
n + 1  + 1 
ln i  ≤ (n + 1) ln 
�n+1 
�
� �
n ln( 
i=1
e 
e
n + 1 
e ≤ 
nn
n! 
e 
e

ln i  ≤ 

≤ 

e. 

n 

n 

The  second  line  follows  from  the  ﬁrst  by  completing  the  integrations.  The  third 
line is obtained by exponentiating. 
So n! behaves  something  like  the  closed  form  formula  (n/e)n .  A more  careful 
analysis yields an unexpected closed form formula that is asymptotically exact: 
� 
�n √
n 
e 

Lemma (Stirling’s Formula). 

(15.12) 

n! ∼ 

2πn, 

Stirling’s  Formula  describes  how  n!  behaves  in  the  limit,  but  to  use  it  effec­
tively, we need  to know how close  it  is  to  the  limit  for different values of n.  That 
information is given by the bounding formulas: 
� n �n 
� n �n 
Fact (Stirling’s Approximation). 
√
2πn 
2πn 
e 
e

1/(12n+1) 
e 

≤ n! ≤

1/12n

√

. 

e

322 

CHAPTER 15.  SUMS & ASYMPTOTICS 

�
n 
Figure 15.6: This ﬁgure illustrates the Integral Method for bounding the sum 
=1  ln i. 
i

Stirling’s Approximation implies the asymptotic formula (15.12), since e1/(12n+1) 
and e1/12n  both approach 1 as n grows large.  These inequalities can be veriﬁed by 
induction, but the details are nasty. 
The bounds  in Stirling’s  formula are very  tight.  For example,  if n  =  100,  then 
�
�100 
Stirling’s bounds are: 
100 
e �
�100 
200π 
100 
200π 
e 

100! ≤

100! ≥

√

√

e 1/1201 

e 1/1200 

The  only  difference  between  the  upper  bound  and  the  lower  bound  is  in  the 
ﬁnal  term.  In  particular  e1/1201  ≈  1.00083299  and  e1/1200  ≈  1.00083368.  As a 
result,  the upper bound  is no more  than 1 + 10−6  times  the  lower bound.  This  is 
amazingly tight! Remember Stirling’s formula; we will use it often. 

15.5  Asymptotic Notation 

Asymptotic notation  is a shorthand used  to give a quick measure of  the behavior 
of a function f (n) as n grows large. 

15.5.1  Little Oh 
The asymptotic notation, ∼, of Deﬁnition 15.2.2 is a binary relation indicating that 
two functions grow at the same rate.  There is also a binary relation indicating that 
one function grows at a signiﬁcantly slower rate than another. Namely, 
→ 
R,  with  g  nonnegative,  we  say  f  is
Deﬁnition  15.5.1.  For  functions  f , g  :  R

ln(x + 1)ln(x)15.5.  ASYMPTOTIC NOTATION 

323 

asymptotically smaller than g , in symbols, 

iff 

f (x) = o(g(x)), 

lim  f (x)/g(x) = 0. 
x→∞ 

For example, 1000x1.9  = o(x2 ), because 1000x1.9 /x2  = 1000/x0.1  and since x0.1 
goes to inﬁnity with x and 1000 is constant, we have limx→∞ 1000x1.9 /x2  = 0. This 
argument generalizes directly to yield 

Lemma 15.5.2.  x = o(xb ) for all nonnegative constants a < b. 
a

Using the familiar fact that log x < x for all x > 1, we can prove 

Lemma 15.5.3.  log x = o(x� ) for all � > 0 and x > 1. 

Proof.  Choose � > δ > 0 and let x = z δ  in the inequality log x < x. This implies 

log z < z δ /δ = o(z � ) 

by Lemma 15.5.2. 

(15.13) 
� 

Corollary 15.5.4.  xb  = o(ax ) for any a, b ∈ R with a > 1. 

Proof.  From (15.13), 

for all z > 1, δ > 0. Hence 

log z < z δ /δ 

�
�z  /δ 
(e b )log z  < (e b )z δ /δ 
δ
z b  < elog a(b/ log a) 
(b/δ log a)z δ 
= a
< az 

for all z  such that 

(b/δ log a)z δ  < z . 

But  choosing  δ <  1, we know  z δ  =  o(z ),  so  this  last  inequality holds  for  all  large 
� 
enough z . 

Lemma  15.5.3  and  Corollary  15.5.4  can  also  be  proved  easily  in  several  other 
ways, for example, using L’Hopital’s Rule or the McLaurin Series for log x and e . 
x
Proofs can be found in most calculus texts. 

324 

CHAPTER 15.  SUMS & ASYMPTOTICS 

15.5.2  Big Oh 
Big Oh is the most frequently used asymptotic notation.  It is used to give an upper 
bound on the growth of a function, such as the running time of an algorithm. 
→ 
Deﬁnition 15.5.5.  Given nonnegative functions f , g  : R
R, we say that 

iff 

f  = O(g) 

lim sup f (x)/g(x) < ∞. 
x→∞ 

This deﬁnition3  makes it clear that 
Lemma 15.5.6.  If f  = o(g) or f  ∼ g , then f  = O(g). 
Proof.  lim f /g = 0 or lim f /g = 1 implies lim f /g < ∞. 
� 
It  is  easy  to  see  that  the  converse  of  Lemma  15.5.6  is  not  true.  For  example, 
2x = O(x), but 2x  �∼ x and 2x =�
o(x). 
The  usual  formulation  of  Big  Oh  spells  out  the  deﬁnition  of  lim sup  without 
mentioning it. Namely, here is an equivalent deﬁnition: 
→ 
Deﬁnition 15.5.7.  Given functions f , g  : R
R, we say that 

f  = O(g) 
iff there exists a constant c ≥ 0 and an x0  such that for all x ≥ x0 , |f (x)| ≤ cg(x). 
This  deﬁnition  is  rather  complicated,  but  the  idea  is  simple:  f (x) =  O(g(x)) 
means f (x) is less than or equal to g(x), except that we’re willing to ignore a con­
stant factor, namely, c, and to allow exceptions for small x, namely, x < x0 . 
We observe, 

Lemma 15.5.8.  If f  = o(g), then it is not true that g = O(f ). 

Proof. 

so g =� O(f ). 

g(x)
lim 
x→∞  f (x) 

= 

1 
limx→∞ f (x)/g(x)

=

1
0 

= ∞, 

� 
3We  can’t  simply  use  the  limit  as  x  → ∞  in  the  deﬁnition  of  O(),  because  if  f (x)/g(x)  oscil­
lates  between,  say,  3  and  5  as  x  grows,  then  f  =  O(g)  because  f  ≤  5g ,  but  limx→∞  f (x)/g(x) 
does  not  exist.  So  instead  of  limit,  we  use  the  technical  notion  of  lim sup.  In  this  oscillating  case, 
f (x)/g(x) = 5.
x→∞
lim sup
The precise deﬁnition of lim sup is 

lim sup h(x) ::=  lim  luby≥x h(y), 
x→∞ 
x→∞ 
where “lub” abbreviates “least upper bound.” 

15.5.  ASYMPTOTIC NOTATION 

325 

Proposition 15.5.9.  100x2  = O(x2 ). 
��
��
Proof.  Choose c =  100 and x0  = 1.  Then the proposition holds, since for all x ≥  1, 
≤ 100x
� 
100x2 
2 . 
Proposition 15.5.10. 

x2  + 100x + 10 = O(x2 ).

Proof.  (x2  + 100x + 10)/x2  =  1 + 100/x + 10/x2  and  so  its  limit  as x  approaches 
inﬁnity is 1+ 0+ 0 = 1. So in fact, x2 + 100x+ 10 ∼ x2 , and therefore x2 + 100x+ 10 = 
� 
O(x2 ).  Indeed, it’s conversely true that x2  = O(x2  + 100x + 10). 

Proposition 15.5.10 generalizes to an arbitrary polynomial: 
Proposition 15.5.11.  For ak  �= 0, ak xk  + ak−1xk−1  + · · · + a1x + a0  = O(xk ). 
We’ll omit the routine proof. 
Big Oh notation is especially useful when describing the running time of an al­
gorithm. For example, the usual algorithm for multiplying n × n matrices requires 
proportional  to  n3  operations  in  the  worst  case.  This  fact  can  be  expressed  con­
cisely by saying that the running time is O(n3 ).  So this asymptotic notation allows 
the  speed  of  the  algorithm  to  be  discussed  without  reference  to  constant  factors 
or  lower-order terms that might be machine speciﬁc.  In this case there  is another, 
ingenious matrix multiplication procedure that requires O(n2.55 ) operations.  This 
procedure  will  therefore  be  much  more  efﬁcient  on  large  enough  matrices.  Un­
fortunately,  the O(n2.55 )-operation multiplication procedure  is almost never used 
because it happens to be less efﬁcient than the usual O(n3 ) procedure on matrices 
of practical size. 

15.5.3  Theta 
Deﬁnition 15.5.12. 

f  = Θ(g) 

iff  f  = O(g) and g = O(f ). 

The statement f  = Θ(g) can be paraphrased intuitively as “f  and g are equal to 
within a constant factor.” 
The value of these notations is that they highlight growth rates and allow sup­
pression  of  distracting  factors  and  low-order  terms.  For  example,  if  the  running 
time of an algorithm is 
T (n) = 10n 3  − 20n 2  + 1, 

then 

T (n) = Θ(n 3 ). 
In this case, we would say that T  is of order n3  or that T (n) grows cubically. 
Another such example is 
(2.7x113  + x9  − 86)4 
√
x 

− 1.083x  = Θ(3x ). 

π23x−7  + 

326 

CHAPTER 15.  SUMS & ASYMPTOTICS 

Just  knowing  that  the  running  time  of  an  algorithm  is  Θ(n3 ),  for  example,  is 
useful, because if n doubles we can predict that the running time will by and large4 
increase by a factor of at most 8 for large n.  In this way, Theta notation preserves in­
formation about the scalability of an algorithm or system.  Scalability is, of course, 
a big issue in the design of algorithms and systems. 

15.5.4  Pitfalls with Big Oh 
There  is  a  long  list  of ways  to make mistakes with  Big Oh  notation.  This  section 
presents some of the ways that Big Oh notation can lead to ruin and despair. 

The Exponential Fiasco 

Sometimes  relationships  involving  Big Oh  are  not  so  obvious.  For  example,  one 
might  guess  that  4x  =  O(2x )  since  4  is  only  a  constant  factor  larger  than  2.  This 
reasoning is incorrect, however; actually 4x  grows much faster than 2x . 
Proposition 15.5.13.  4x  =� O(2x ) 
Proof.  2x /4x  = 2x /(2x 2x ) = 1/2x .  Hence, limx→∞ 2x /4x  = 0, so in fact 2x  =  o(4x ). 
We observed earlier that this implies that 4x  =� O(2x ). 
� 

False Theorem 15.5.14. 

Constant Confusion 
Every  constant  is  O(1).  For  example,  17  =  O(1).  This  is  true  because  if  we  let 
f (x) = 17 and g(x) = 1, then there exists a c > 0 and an x0  such that |f (x)| ≤ cg(x). 
In particular, we could choose  c = 17 and x0  = 1,  since  |17| ≤  17 · 1  for all x  ≥  1. 
We can construct a false theorem that exploits this fact. 
� 
n
i = O(n) 
�
i=1 
· · · 
n 
=1  i = 1 + 2 + 3 +  + n.  Since we have shown that 
False proof.  Deﬁne f (n) = 
�
· · · 
�
i
every constant i is O(1), f (n) = O(1) + O(1) +  + O(1) = O(n). 
i=1  i = n(n + 1)/2 =� O(n). 
n
Of course in reality 
The error stems from confusion over what is meant in the statement i  =  O(1). 
For any constant i  ∈  N  it  is  true  that  i  =  O(1).  More precisely,  if f  is any constant 
function,  then  f  =  O(1).  But  in  this  False  Theorem,  i  is  not  constant  but  ranges 
over a set of values 0,1,. . . ,n that depends on n. 
And  anyway,  we  should  not  be  adding O(1)’s  as  though  they were  numbers. 
We  never  even  deﬁned what  O(g) means  by  itself;  it  should  only  be  used  in  the 
context “f  = O(g)” to describe a relation between functions f  and g . 

4 Since Θ(n3 )  only  implies  that  the  running  time,  T (n),  is between  cn3  and  dn3  for  constants  0  < 
c < d, the time T (2n) could regularly exceed T (n) by a factor as large as 8d/c.  The factor is sure to be 
close to 8 for all large n only if T (n) ∼ n3 . 

15.5.  ASYMPTOTIC NOTATION 

327 

Lower Bound Blunder 

Sometimes  people  incorrectly  use  Big  Oh  in  the  context  of  a  lower  bound.  For 
example,  they might  say,  “The  running  time,  T (n),  is  at  least O(n2 ),”  when  they 
probably mean something like “O(T (n)) = n2 ,” or more properly, “n2  = O(T (n)).” 

Equality Blunder 

The notation f  = O(g) is too ﬁrmly entrenched to avoid, but the use of “=” is really 
regrettable.  For example, if f  = O(g), it seems quite reasonable to write O(g) = f . 
But doing so might tempt us to the following blunder:  because 2n = O(n), we can 
say O(n) = 2n.  But n  =  O(n), so we conclude that n  =  O(n) = 2n, and therefore 
n = 2n. To avoid such nonsense, we will never write “O(f ) = g .” 

15.5.5  Problems 
Practice Problems 

Problem 15.7. 
Let  f (n) =  n3 .  For  each  function  g(n)  in  the  table  below,  indicate  which  of  the 
indicated asymptotic relations hold. 

f  = O(g)  f  = o(g)  g = O(f )  g = o(f ) 

g(n) 
6 − 5n − 4n2  + 3n3 
n3  log n 
(sin (πn/2) + 2) n3 
nsin(πn/2)+2 
log n! 
e0.2n  − 100n3 

Homework Problems 

Problem 15.8.  (a)  Prove that log x < x for all x > 1 (requires elementary calculus). 

(b)  Prove  that the relation, R, on functions such  that f  R g  iff f  =  o(g)  is a strict 
partial order. 
(c)  Prove that f  ∼ g  iff f  = g + h for some function h = o(g). 

Problem 15.9. 
Indicate  which  of  the  following  holds  for  each  pair  of  functions  (f (n), g(n))  in 
the  table  below.  Assume  k  ≥  1,  � >  0,  and  c >  1  are  constants.  Pick  the  four 
table  entries  you  consider  to  be  the  most  challenging  or  interesting  and  justify 
your answers to these. 

328 

CHAPTER 15.  SUMS & ASYMPTOTICS 

f  = O(g)  f  = o(g)  g = O(f )  g = o(f )  f  = Θ(g)  f  ∼ g 

f (n) 
2n 
√
n 
log(n!) 
nk 
logk n 

g(n) 
2n/2 
nsin nπ/2 
log(nn ) 
cn 
n� 

Problem 15.10. 
Let  f ,  g  be  nonnegative  real-valued  functions  such  that  limx→∞ f (x) =  ∞  and 
f  ∼ g . 
(a)  Give an example of f , g such that NO T(2f  ∼ 2g ). 
(b)  Prove that log f  ∼ log g . 
(c)  Use Stirling’s formula to prove that in fact 
log(n!) ∼ n log n 

Class Problems 

Problem 15.11. 
Give an elementary proof (without appealing to Stirling’s formula)  that  log(n!)  = 
Θ(n log n). 

Problem 15.12. 
Recall that for functions f , g on N, f  = O(g) iff 
c · g(n) ≥ |f (n)| . 
∃c ∈ N ∃n0  ∈ N ∀n ≥ n0 
For  each  pair  of  functions  below,  determine  whether  f  =  O(g)  and  whether 
g  =  O(f ).  In  cases  where  one  function  is  O()  of  the  other,  indicate  the  smallest 
nonegative  integer,  c,  and  for  that  smallest  c,  the  smallest  corresponding  nonegative 
integer n0  ensuring that condition (15.14) applies. 
(a)  f (n) = n2 , g(n) = 3n. 
f  = O(g) 
NO 
YES 
g = O(f ) 
NO 
YES 
(b)  f (n) = (3n − 7)/(n + 4), g(n) = 4 
f  = O(g) 
YES 
NO 
g = O(f ) 
NO 
YES 

If YES, c = 
If YES, c = 

If YES, c = 
If YES, c = 

(15.14) 

, n0  = 
, n0  = 

, n0  = 
, n0  = 

15.5.  ASYMPTOTIC NOTATION 

329 

(c)  f (n) = 1 + (n sin(nπ/2))2 , g(n) = 3n 
f  = O(g) 
YES 
NO 
g = O(f ) 
NO 
YES 

If yes, c = 
If yes, c = 

n0  = 
n0  = 

Problem 15.13. 

False Claim. 

(15.15) 
Explain  why  the  claim  is  false.  Then  identify  and  explain  the  mistake  in  the 
following bogus proof. 

2n  = O(1). 

Bogus proof.  The proof by induction on n where the induction hypothesis, P (n), is 
the assertion (15.15). 
base case: P (0) holds trivially. 
inductive  step:  We  may  assume  P (n),  so  there  is  a  constant  c >  0  such  that 
2n  ≤ c  1. Therefore, 
· 
2n+1  = 2  2n  ≤ (2c) 1,
·
· 
which implies that 2n+1  = O(1). That is, P (n + 1) holds, which completes the proof 
of the inductive step. 
We  conclude  by  induction  that  2n  =  O(1)  for  all  n.  That  is,  the  exponential 
function is bounded by a constant. 
� 

Problem 15.14. 
(a)  Deﬁne a function f (n) such that f  = Θ(n2 ) and NO T(f  ∼ n2 ). 
n2 ) and g =�
(b)  Deﬁne a function g(n) such that g = O(n2 ), g = Θ(

o(n2 ). 

�
330 

CHAPTER 15.  SUMS & ASYMPTOTICS 

MIT OpenCourseWare
http://ocw.mit.edu 

6.042J / 18.062J Mathematics for Computer Science 
Spring 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

