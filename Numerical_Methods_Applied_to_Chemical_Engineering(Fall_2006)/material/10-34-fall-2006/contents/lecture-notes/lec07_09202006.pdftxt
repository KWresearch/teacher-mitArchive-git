10.34, Numerical Methods Applied to Chemical Engineering 
Professor William H. Green 
Lecture #7: Introduction to Eigenvalues and Eigenvectors. 
Newton’s Method (Multi-dimensional) 
 
F(xtrue) = 0
If Δx is small. Works well when xguess is close 
Newton: Taylor expansion around xguess  
F(xguess) + J(xguess)Δx ~ 0.0  xtrue ≈ xguess + Δx
 
Select xguess;  usually difficult to get a good guess 
F
∂
m
x
∂

compute F(xguess), J(xguess) 

=

J

mn

guess

 

xn

factorize J (cid:198) L U  

backsub: L V = -F; U Δx = V

solve L U Δx = -F  
xnew = xguess + Δx
if ||xnew – xguess|| < tolx 
if ||F(xnew)|| < atolf  
xguess (cid:197) xnew 
Iterate from compute F(xguess) 
 

rtol doesn’t work for F(x) = 0 

CONVERGENCE 

If J is singular or poorly conditioned, will not be able to solve. 
If Δx is big, method will not work. 
In general, radius of convergence is small 
- 
can bound Δx size 
- 
can stop iteration after a certain number, for example, 20 iterations to see 
Assumption of Newton’s Method is xguess is VERY GOOD 
How close does xguess have to be to guarantee convergence? 
• 
radius of convergence 
Backtrack Line Search 
xnew (Newton)  
FJF
⋅=∇

If you think xnew is too big, you can backtrack by looking at: 
||F(xguess)||- ||F(xnew)|| 
g(λ) = ||F(xguess)+λΔxNEWTON|| 
by minimizing g(λ) using Bisection (etc.) 

ΔxNEWTON 
 
Δx = -J-1F
xguess  

 
Figure 1. Trying to find x between xguess and xnew that gives lower ||F||. 
 
Maybe direction F(xguess) to F(xnew) is wrong 
f(x) = ||F(x)||2 = Σ|Fi(x)|2
=∇ ∑
f
2

Minimize scalar function:  

FJ
⋅

F
i

=

2

F
∂
i
x
∂

m

 works even when J is singular 

Cite as: William Green, Jr., course materials for 10.34 Numerical Methods Applied to Chemical 
Engineering, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu), Massachusetts Institute of 
Technology. Downloaded on [DD Month YYYY]. 

f
J
•=∇

f

 is a move downhill 

If xguess is good, Δx = -J-1F is the best direction but more risky (good if you can see the end)  
f∇  is if “you are lost” (cid:198) Brute force 
1)  Most risky method (Newton) 
2)  Safest method 
3)  Dogleg: compromise 

 
Figure 2. The relationship of Newton’s Method to Dogleg Method. 
 
fsolve implements Dogleg Method using “Trust Region” 
 

If xNewton is within the trust region, 
the function will quickly converge 

Read ‘doc fsolve’ 

x~

 
F(xguess), you can expand the trust region. 
) is close to 

Figure 3. If F(
 
*fsolve has this all built in and is therefore much more powerful than simple Newton’s 
method. 
Optimization 
xf
min
)(
 
x
f∇  = 0 is a bad way to do this (i.e. fsolve(gradf, xguess)) 

The matrix is positive definite and  

2

f
∂
x
x
∂∂
nm

=

2
f
∂
xx
∂∂
mn

. 

 

Strategy: find regions where the problem can be considered optimization 
f = ||F||2  problem is there are local minimums 
f∇  = J·F  can be zero if J is singular and F is in   “BAD DIRECTION” 
(cid:75)
⎛
⎞
v
row
0
  1 
bad
⋅
⎜
⎟
(cid:75)
⎜
⎟
v
  2 
row
0
⋅
⎜⎜
⎟⎟
(cid:35)
0
⎝
⎠

J singular (cid:198) rank(J) < N 
J·vbad = 0 
 
 
 

−−−−−−
−
−−−
−−
−−−−−−

⎛
⎜
⎜
⎜
⎝

(cid:75)
( )
v

=

bad

=

⎞
⎟
⎟
⎟
⎠

 

 

 

⎞
⎟
⎟
⎟
⎠

⎛
⎜
⎜
⎜
⎝

10.34, Numerical Methods Applied to Chemical Engineering 
Prof. William Green 
 
 
Cite as: William Green, Jr., course materials for 10.34 Numerical Methods Applied to Chemical 
Engineering, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu), Massachusetts Institute of 
Technology. Downloaded on [DD Month YYYY]. 

Lecture 7 
Page 2 of 3 

if 

f∇ =0, no way of knowing which direction to go in. 
J·vbad = 0*vbad  
J·vbad = λvbad      λ = 0 

A(x+vbad) = A·x + A·vbad  
        b       δb
 

Poor conditioning 
A·x = b
A·vbad = λvbad   
 
 
 ≈0 
 
Certain linear combinations of values you can determine well.  Other combinations you 
cannot determine. 
A·x=b
 
Sensitivity 
A-1 (cid:198) vbad   
 
 
x = A-1b  

 

00
\
0
0
\
00
\

⎞
⎟
⎟
⎟
⎠

Lecture 8 will discuss when you can 
do this factorization 

⎞
⎟
⎟
⎟
⎠

⎛
⎜
⎜
⎜
⎝

00
\
0
0
\
00
\

( )
0
0 λ
⎛
⎜
A·V = V   
⎜
⎜
⎝

 
 
VT = V-1
usually:  A = V·Λ·VT   
 
 
 
 
If you have large dimensional problem, it is difficult to give good xguess
Look at Ftrue(x): can you change to a different problem? 
Fapprox(xguess)=0 solvable (ideally, linear) 
 
Ftrue=Fapprox + λFperturb
 
You want to solve: 
Ftrue(x) = 0 
Is there an Fapprox(x) = 0 that is soluble through linearization? 
xguess  
 
 
 
Ftrue = Fapprox + λFperturb  
 
linear 
or easy 
 
xguess  
 
 
Ftrue – Fapprox    
 

solve new problem with small λ: 
Fapprox(xguess = xguess
approx) (cid:198) xguess,1
or linear 
Fapprox + 0.001Fperturb(xguess,1) (cid:198) xguess,2
Fapprox + 0.01Fperturb(xguess,2) (cid:198) xguess,3
      Increment λ until λ = 1 

 

If the program crashes, need to step back and choose λ as a smaller value. 

10.34, Numerical Methods Applied to Chemical Engineering 
Prof. William Green 
 
 
Cite as: William Green, Jr., course materials for 10.34 Numerical Methods Applied to Chemical 
Engineering, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu), Massachusetts Institute of 
Technology. Downloaded on [DD Month YYYY]. 

Lecture 7 
Page 3 of 3 

