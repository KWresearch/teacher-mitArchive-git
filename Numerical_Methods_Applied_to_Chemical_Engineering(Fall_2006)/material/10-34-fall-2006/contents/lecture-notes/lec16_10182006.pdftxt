10.34, Numerical Methods Applied to Chemical Engineering 
Professor William H. Green 
Lecture #16: Unconstrained Optimization.  
Unconstrained Optimization 
• 
 
 
 
 
 
minx f(x) 
x[k+1] 
• 
 
 
 
 
 
 
xguess                                 Require: 
 
 
 
 
f(x[k+1])<f(x[k]) 
x[k] 
 
 
 

 

 

 

 

Which direction to move? 

Move Downhill (cid:198) “Steepest Descent” 
•  very robust but poor convergence at the end 

x 

 
Figure 1. Diagram of steepest descent approach to global minimum. 
 

Unless you start on the center line, you will zigzag inefficiently 
•  going down contour lines is easy with this method 

 
fapprox(x) = f(x[k]) +  ∇ f|x[k]·(x–x[k]) + ½(x–x[k])TB·(x – x[k]) 
(x – x[k]) = p = - ∇ f(|| ∇ f||2/ ∇ fTB ∇ f) 

{Cauchy} 

                                                B must be positive definite and not singular. 
pcauchy  

 

 

 

      (- ∇ f/||f||)Δ (cid:197) max step size allowed   

psteepest descent =     pcauchy  
 

 

 

 

 

 

take min ||p|| 

Look at Figures 5.5 and 5.6 in BEERS 

Cite as: William Green, Jr., course materials for 10.34 Numerical Methods Applied to Chemical 
Engineering, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu), Massachusetts Institute of 
Technology. Downloaded on [DD Month YYYY]. 

Figure 2. An example of poor scaling. 
 
If you rescale into circles (2nd derivatives similar), good scaling 

extremely poor 
scaling 

Figure 3. SCALING IS KEY. 

 

 
Newton Step 
If fapprox is correct, 
O =  ∇ fapprox =  ∇ ftrue + B·p   
B·pnewton =  ∇ ftrue|x

[k]   

guess 

If B is accurate and initial guess is close, converges quickly; 

Similar to Newton’s:  (cid:206) {JΔx = F}   

 

 

otherwise, you may step too far 

10.34, Numerical Methods Applied to Chemical Engineering 
Prof. William Green 
 
 
Cite as: William Green, Jr., course materials for 10.34 Numerical Methods Applied to Chemical 
Engineering, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu), Massachusetts Institute of 
Technology. Downloaded on [DD Month YYYY]. 

Lecture 16 
Page 2 of 5 

Dogleg or Trust Region Method 

pcauchy 

∇ f

x[k] 

dogleg

x 

pnewton

intersection of intermediate trust 
region  with line connecting 
pcauchy and pnewton  

 

Figure 4.  Diagram of dogleg method. 
Broyden-Fletcher-Goldfarb-Shanno Algorithm (BFGS) 
1. have x[k] (cid:198) f(x[k]) (cid:198) (cid:86)f(x[k]) (cid:198) B[k]  (cid:198) (cid:198) pold  
x[k+1] = x[k] + p
2. Compute f(x[k+1]), (cid:86)f(x[k+1]) 
 ((cid:86)f(x[k+1])-(cid:86)f(x[k]))((cid:86)f(x[k+1])-(cid:86)f(x[k]))T 
3. 
 
B[k+1] = B[k] +  
 
 
 
 
 
((cid:86)f(x[k+1])-(cid:86)f(x[k]))Tpold  
 
 
 
 

       (B[k]pold)(B[k]pold)T
- 
       (pold)T(B[k]pold) 

 

Most programs use this! 

Always get symmetric matrix but sometimes eigenvalues are negative 
Use Bnew = B[k+1] + τI to guarantee positive eigenvalues. 
In quantum mechanics, use estimates of stretching frequencies, but rest of bond angles are 

are set to be identity matrix.  
If number of variables (Nvariables) large, the total numbe of entries in B (N2
large. Can try sparse matrix storage methods. 

variables) will get too 

Conjugate Gradient Method 
Work like steepest descent but avoid zigzagging by forcing NEW direction to be orthogonal 

to OLD direction. 

10.34, Numerical Methods Applied to Chemical Engineering 
Prof. William Green 
 
 
Cite as: William Green, Jr., course materials for 10.34 Numerical Methods Applied to Chemical 
Engineering, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu), Massachusetts Institute of 
Technology. Downloaded on [DD Month YYYY]. 

Lecture 16 
Page 3 of 5 

forced orthogonality  

 

Figure 5. Conjugate gradient method. 
Polak-Ribiere Formula for Step direction 
(x[k+1])·((cid:86)f(x[k+1])-(cid:86)f(x[k])) 
 
 
 
 
 
      (cid:86)f
(direction only) p[k+1] = -(cid:86)f(x[k+1]) +  
                 ||((cid:86)f(x[k])||2   
 
 
 
 
 
 
 

new 

 

pold,[k]

For quadratic, it takes n steps to find the minimum (n = dimension) no matter what the 

dimension. Use this method for LARGE SYSTEMS. The minima found are local. 

* no matrices (doesn’t require a lot of memory) 

 

 

 

 
 

For 2D quadratic, gives you exact minimum direction 

 Figure 6. 

 
Diagram of search for global minimum. 

 

must do “strong search” at each step to find absolute minimum 

 
B·pnew = -(cid:86)f 
 
f(x) = c + ½xTAx – b·x  
  
(cid:86)f = A·x – b   

(cid:86)f = 0  (cid:198) A·x = b 

The Conjugate Gradient Method can solve linear systems. 

10.34, Numerical Methods Applied to Chemical Engineering 
Prof. William Green 
 
 
Cite as: William Green, Jr., course materials for 10.34 Numerical Methods Applied to Chemical 
Engineering, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu), Massachusetts Institute of 
Technology. Downloaded on [DD Month YYYY]. 

Lecture 16 
Page 4 of 5 

 
ƒ = ½xTBx + (cid:86)f·x 
 
(cid:86)ƒ = B[k+1] P + (cid:86)f(x[k+1]) great for sparse matrices 

(x = p) use conjugate gradient to find p

10.34, Numerical Methods Applied to Chemical Engineering 
Prof. William Green 
 
 
Cite as: William Green, Jr., course materials for 10.34 Numerical Methods Applied to Chemical 
Engineering, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu), Massachusetts Institute of 
Technology. Downloaded on [DD Month YYYY]. 

Lecture 16 
Page 5 of 5 

