• 

A
 = 

Smallest  Eigenvalue  of  M­Matrices

Def:  M­Matrix 

⎤
⎡ 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
. . .  a1n
a11  a12  a13 
. . .  a2n
a21  a22  a23 
. . .  a3n
a31  a32  a33 
.
.
.
.
. . .

.
.
.
.
. 
. 
.
.
an1  an2  an3  . . .  ann 
• Given:  Row  sums  si  and  oﬀ  diagonals  aij , i =  j . 
• Diagonal  elements  computable  accurately,  sum  of  positives

� 
aii  =  si  − 
aij
j=i 

aii  ≥  0 
aij  ≤  0, 
i =  j

Row  Sums  si 

; 

�n
j=1 

= 

aij  ≥  0

�
�
�
GE  on  Weakly  Diagonally  Dominant  M­Matrices


• Pivoting,  if  needed,  is  diagonal,  preserves  structure 

• One  step  of  GE: 
– Oﬀ  diagonals:  aij  =  aij  −  aikakj 
akk 
– Row  sums:  si  =  si  −  aik 
sk 
akk 
• Everything  is  preserved  in  Schur  complementation 

– Weak  diagonal  dominance 
– M­matrix  structure 
– High  relative  accuracy  in  aij  and  si 
• Yields  Cholesky  factors 

Getting  the  inverse


⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 

=


⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 

⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 

b1

b2

b3

b4 

• Again  no  subtractions  in  solving

⎡ ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
⎡⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
x1

c11  c12  c13  c14 

x2

c22  c23  c24 
x3

c33  c44 
c44  x4 
• Think  of  b  as  ei  or  >  0  in  general. 
x4 =  b4/c44 
x3 = (b3  − c34x4)/c33 
x2 = (b2  − c24x4  − c23x3)/c22 
x1 = (b1  − c14x4  − c13x3c12x2)/c11 
• Solving  with  C T  analogous  ⇒ A−1  –  positive. 
• Accurate  (Positive)  Inverse = Accurate  smallest eigenvalue  (even 
in  the  nonsymmetric  case) 

