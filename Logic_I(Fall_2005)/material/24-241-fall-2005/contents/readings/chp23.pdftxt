Function Signs 
 
We are going to liberalize the predicate calculus a little bit by introducing notations for 
functions.  An n-ary function on a set A is a set  of n+1-tuples of elements of A that satisfies this 
condition: 
 

For each n-tuple <a1,a2,...,an> of elements of A, there is one and only one 
element b of A such that the n+1-tuple <a1,a2,...,an,b> is an element of 
the set. 

 
If f is an n-ary function, we write “f(a1,a2,...,an) = b” to indicate that <a1,a2,...,an,b> ∈
 

 f. 

A language for the  predicate calculus with identity and function signs will have as its 
nonlogical symbols individual constants, predicates, and — this is new — n-ary function signs, 
for n a positive integer. The logical symbols will be the variables, the connectives (“∨
,” “ ∧
,” 
“ →
,” and “¬) and quantifiers (“∀ ” and “∃ ”), the punctuation marks (“(,” “),” and “,”), and 
,” “ ↔
the predicate “=.” “=” is considered a logical symbol, because it has a fixed semantic role 
determined by the definition of “interpretation,” rather than having a semantic role that varies 
from one interpretation to another. 
 

The terms constitute the smallest class of expressions that meets the following 
conditions: 
 

Every variable is a term. 
Every individual constant is a term. 
If τ1, τ2,..., τn are terms and f is an n-ary function signs, f(τ1,τ2,...,τn) is a term. 

 
A term with no variables is closed. 
 

If A is to be an interpretation of such a language, it needs to assign a value to “ ∀
,” to 
each of the individual constants, and to each of the predicates, just as before. It also needs 
to assign a value to each of the function signs, subject to the following constraint: 
 

If f is an n-ary function sign, A(f), also written fA, is an n-ary function on   A

. 

 

 

The semantic role of the function signs is given by the following: 

Definition. If A is an interpretation and σ is a variable assignment for A, the 
denotation of a term τ, Denσ,A, is defined as follows: 
 
If τ is a variable, Denσ,A(τ) = σ(τ). 
If τ is an individual constant, Denσ,A(τ) = A(τ). 
If τ has the form f(ρ1,ρ2,...,ρn), Denσ,A(τ) = 
fA(Denσ,A(ρ1),Denσ,A(ρ2),...,Denσ,A(ρn)). 
 

Function Signs, p. 2 
 
If φ is an atomic formula of the form Rτ1τ2...τn, σ satisfies φ under A just in case the n-tuple 
<Denσ,A(τ1),Denσ,A(τ2),...,Denσ,A(τn)> is an element of RA. With this adaptation, the definition 
of satisfaction under an interpretation is unchanged. 
 

The introduction of function signs is gratuitous. Instead of using an n-ary function 
sign f, we could just as well introduce an n+1-place predicate F. Then, instead of writing 
“f(v1,v2,...,vn),” we could write “(ιvn+1)F(v1,v2,...,vn,vn+1).”  
 

The rules of inference are unchanged, except that, here and there, they’ll say 
“closed term” instead of “individual constant.” Let me write them out in full, for reference: 
 
PI 

 ψ) with premise 

 {φ}, you may write (φ →

At any stage of a derivation, you may write down a sentence φ with any set that 
contains φ as  its premise set. 
 
If you have written down sentences ψ1, ψ2,..., ψn in a derivation, and φ is a tauto-
logical consequence of {ψ1,ψ2,...,ψn}, then you may write down sentence ψ, taking the 
premise set to be the union of the premise sets of the ψis. In particular, if φ is a tau-
tology, we can write φ with the empty premise set. 
 
If you have derived ψ with premise set Γ ∪
set Γ. 
 
If you've derived ( ∀ v)φ, you may derive φv/τ with the same premise set, for any 
variable v and closed term τ. 
 
For any variable v, if you've derived φv/c from Γ and if the individual constant c 
doesn't appear in φ or in any of the sentences in Γ, you may derive ( ∀ v)φ with 
premise set Γ 
 
From ¬( ∀ v)¬φ, you may infer ( ∃ v)φ with the same premise set, and vice versa, for 
each variable v. 
From ( ∀ v)¬φ, you may infer ¬( ∃ v)φ with the same premise set, and vice versa, for 
each variable v. 
From ¬( ∀ v)φ, you may infer ( ∃ v)¬φ with the same premise set, and vice versa, for 
each variable v. 
From ( ∀ v)φ, you may infer ¬( ∃ v)¬φ with the same premise set, and vice versa, for 
each variable v. 
 
If you have written φv/τ, for any variable v and closed term τ, you may write ( ∃ v)φ 
with the same premise set. 
 
Suppose that you have derived ( ∃ v)φ with premise set ∆ and that you have derived ψ 
with premise set Γ ∪
 {φv/c}, for some individual constant c and variable v. Suppose 

TC 

CP 

US 

UG 

QE 

EG 

ES 

IR 

SI 

premisses. 

Function Signs, p. 3 
 
further that the constant c does not appear in φ, in ψ, or in any member of Γ. Then 
you may derive ψ with premise set ∆ ∪
 Γ. 
 
You may write any sentence of the form τ=τ, with the empty set of 
 
If you’ve written either τ=ρ or ρ=τ with premiss set Γ and you’ve written φv/τ with 
premiss set ∆, you may write φv/ρ with premiss set Γ ∪ ∆. 
 
 (¬x=z ∧
As an example, let’s derive “( ∃ x)( ∃ y)( ∃ z)(¬x=y ∧
 ¬y=z))” from the premiss 
“(¬( ∃ x)s(x) = 0  ∧
 ( ∀ x)( ∀ y)(s(x) = s(y)  →
 x = y)).” The conclusion says that there are at least 
three individuals. In the same way, we can use the same premiss to derive the sentence that 
says “There are at least four individuals,” the sentence that says “There are at least five 
individuals,” and so on. The premiss is consistent — it’s made true by taking “s” to denote 
the successor function on the natural numbers — but it isn’t true in any finite model. 
 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
 

 ( ∀ x)( ∀ y)(s(x) = s(y)  →
1  (¬( ∃ x)s(x) = 0  ∧
 x = y))   
2 ¬( ∃ x)s(x) = 0 
 
 
 
 
 
3  ( ∀ x)( ∀ y)(s(x) = s(y)  →
 
 x = y) 
 
 
4 ( ∀ x)¬s(x) = 0 
 
 
 
 
 
5 ¬s(0) = 0 
 
 
 
 
 
 
 
6 ¬s(s(0)) = 0   
 
 
 
 
7 ( ∀ y)(s(s(0)) = s(y)  →
 
 s(0) = y) 
 
 
8 (s(s(0)) = s(0)  →
 
 
 s(0) = 0)   
 
9 ¬s(s(0)) = s(0) 
 
 
 
 
 
 (¬s(s(0)) = 0  ∧
10 (¬s(s(0)) = s(0)  ∧
 ¬s(0) = 0)) 
 
 (¬s(s(0)) = z  ∧
11 ( ∃ z)(¬s(s(0)) = s(0)  ∧
 ¬s(0) = z))   
12 ( ∃ y)( ∃ z)(¬s(s(0)) = y  ∧
 (¬s(s(0)) = z  ∧
 ¬y = z)) 
 
 (¬x=z ∧
13 ( ∃ x)( ∃ y)( ∃ z)(¬x = y  ∧
 
 
 ¬y = z)) 

PI 
TC, 1 
TC, 1 
QE, 2 
US, 4 
US, 4 
US, 3 
US, 7 
TC, 5, 8 
TC, 5, 6, 9 
EG, 10 
EG, 11 
EG, 12 

As a second example, let’s formalize the following inference: 

 

 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 

( ∀ x)( ∀ y)( ∀ z)p(x,p(y,z)) = p(p(x,y),z) 
( ∀ x)p(x,e) = x 
( ∀ x)p(x,i(x)) = e 
 ( ∀ x)( ∀ y)( ∀ z)(p(x,z) = p(y,z)  →

 x = y) 

 
Disguised in a quirky notation,* this is the derivation in the theory of groups of the 
cancellation law: 
                                                
 
* The reason for the quaint notation is this: When we’re using a 2-ary function sign, 
like “+” or “ ⋅ ,” it’s convenient to write the function sign between the terms it applies to, 
writing “x+y” or “x⋅ y” instead of “+(x,y)” or “ ⋅ (x,y).” But such a notation doesn’t 
generalize to 3- or more-ary function signs. In practice, unless there’s risk of confusion, 

∴
Function Signs, p. 4 
 

 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 

 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 

 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 

 

PI 
PI 
PI 
PI 
US, 2 
US, 3 
SI, 5, 6 

US× 3, 1 
SI, 7, 10 
SI, 4, 11 

US× 3, 1 
SI, 12, 15 
SI, 6, 16 
US, 2 
SI, 17, 18 
CP, 4, 20 

UG× 3, 20 

(x⋅ z = y⋅ x →

 x = y). 

1  ( ∀ x)( ∀ y)( ∀ z)p(x,p(y,z)) = p(p(x,y),z) 
2  ( ∀ x)p(x,e) = x 
 
 
 
3  ( ∀ x)p(x,i(x)) = e 
 
 
 
 
 
 
4 p(a,c) = p(b,c) 
 
 
 
5 p(a,e) = a 
 
 
 
 
6 p(c,i(c)) = e   
7 p(a,p(c,i(c))) = a 
 
 
 

 
1 
2 
3 
4 
2 
3 
2,3 
 
Applying US to line 1 three times, we continue: 
 
10 p(a,p(c,i(c))) = p(p(a,c),i(c)) 
1 
 
 
1,2,3  11 p(p(a,c),i(c)) = a 
1,2,3,4 12 p(p(b,c),i(c)) = a 
 
 
 
Again, we apply US to line 1 three times: 
 
15 p(b,p(c,i(c))) = p(p(b,c),i(c)) 
1 
 
 
1,2,3,4 16 p(b,p(c,i(c))) = a 
 
 
1,2,3,4 17 p(b,e) = a   
 
2 
18 p(b,e) = b   
 
1,2,3,4 19 a = b 
 
 
 
1,2,3  20 (p(a,c) = p(b,c)  →
 a = b)   
 
We apply UG thrice to complete the proof: 
 
1,2,3  23 ( ∀ x)( ∀ y)( ∀ z)(p(x,z) = p(y,z)  →
 

 x = y) 

 
 
 

 
 
 
 
 
 

Notice what rule UG says is that, if you've derived φv/c from Γ and the individual 
constant c doesn't appear in φ or in any of the sentences in Γ, you may derive ( ∀ v)φ from Γ. 
It doesn’t say that, if you've derived φv/τ from Γ and the closed term τ doesn't appear in φ or 
in any of the sentences in Γ, you may derive ( ∀ v)φ from Γ. The latter rule would give us an 
unsound system, since it would enable us to derive the invalid sentence “( ∀ x)( ∀ y)x=y” from 
the empty set, as follows: 
 
1 
1 a=b   
 
 
 
 
 
 
PI 
                                                                                                                                                            
 
there’s no need to insist on putting the function signs in front. 

Function Signs, p. 5 
 

1 
1 
1 
1 
1 
1 
1 

 
 
 
2 f(a) = f(a) 
 
 
3 f(a) = f(b) 
 
4 ( ∀ x)x = f(b)  
 
 
 
 
 
5 c = f(b) 
 
 
 
6 d = f(b) 
 
 
7 c = d  
 
8 ( ∀ y)c=y 
 
 
 
9 ( ∀ x)( ∀ y)x=y 
 
 
10 (a=b →
 ( ∀ x)( ∀ y)x=y) 
 
 ( ∀ x)( ∀ y)x=y) 
11 ( ∀ z)(a=z →
 ( ∀ x)( ∀ y)x=y) 
12 (a=a →
 
13 a=a   
 
 
 
14 ( ∀ x)( ∀ y)x=y 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

IR 
 
SI, 1, 2 
 
Illegitimate use of UG 
 
US, 4 
 
 
US, 4 
SI, 5, 6 
UG, 7 
 
UG, 8 
 
CP, 1, 9 
 
 
 
UG, 10 
US, 11 
 
 
IR 

 
If we look back at our original proof that UG is sound, we can see what’s gone wrong. The proof 
makes use of the fact that, for any interpretation A and constant c, it is possible to change what 
c denotes while leaving everything else unchanged. The same doesn’t hold for an arbitrary 
closed term. When we change what “f(a)” denotes, we’ll have to change either what “a” denotes 
or what “f” denotes. 
 

Similarly, we would get an unsound system if we changed “individual constant” to 
“closed term” in rule ES. 
 

The proof of the completeness theorem is scarcely changed. We define a Henkin set to be 
a d-consistent set Γ of sentences such that, for each sentence, either the sentence or its negation 
is in Γ, and such that, whenever a existential sentence ( ∃ v)φ is in Γ, there is an individual 
constant c such that φv/c is in Γ. Given a d-consistent set of sentences ∆, we first add infinitely 
many new constants to the language, then we show how to form, within the expanded language, 
a Henkin set that contains ∆. The proof is the same as before. 
 

The proof that, given a Henkin set Γ, there is an interpretation under which all and only 
the sentences in Γ are true is only slightly different from before. The slight changes are to ensure 
that “=” behaves as intended. First, enumerate the individual constants in the language as c0, c1, 
c2, c3,.... Next define, for each j, 
 

cjA = the least natural number i such that the sentence ci = cj is in Γ. 

 
Define 
 

 

 A
 = {cjA: j a natural number}. 
<j1,j2,...,jn> is in RA iff Rcj1cj2...cjn is in Γ 

fA(j1,j2,...,jn) = the least i such that ci = f©j1,cj2,...,cjn) is in Γ. 

Function Signs, p. 6 
 
 

It is straightforward to verify that, “=”A is the identity relation on   A ; that, for any 
closed term τ, τA = the least i such that ci = τ is an element of Γ; and that, for any sentence φ, 
φ is true in A iff φ ∈
 Γ. 
 

The derivation of the Compactness Theorem from Strong Completeness is the same 
as before. 
 

The original version of the Löwenheim-Skolem theorem told us that, for any 
consistent set of sentences Γ, there is a model whose domain is the set of natural numbers in 
which all the members of Γ are true. This result will no longer hold when we introduce 
identity into the language. “( ∀ x)( ∀ y)x=y” is consistent, but it isn’t true under any 
interpretation whose domain is the set of natural numbers. Indeed, it isn’t true under any 
interpretation whose domain contains more than one element. We do, however, have this: 
 

Löwenheim-Skolem Theorem. For any consistent set of sentence Γ, there 
is an interpretation under which all the members of Γ are true whose 
domain consists entirely of natural numbers. Not every natural number has 
to appear in the domain. 
 
 

