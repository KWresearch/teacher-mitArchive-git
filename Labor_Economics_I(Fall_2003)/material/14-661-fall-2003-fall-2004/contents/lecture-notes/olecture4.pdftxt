Empirical Applications to Labor Supply: Lecture 4 
 
Empirical Application: Differences in Differences 
Paper: Eissa and Liebman (QJE 1996) 
 
Difference-in differences strategies are simple panel data methods applied  to sets of group means  in 
cases  when  certain  groups  are  exposed  to  the  causing  variable  of  interest  and  others  are  not.    The 
approach  is  well  suited  to  estimating  the  effect  of  sharp  changes  in  the  economic  environment  or 
changes in government policy when a suitable control group can be found.  We’ll consider the Diff-in-
diff approach with the application of examining the impact of welfare reform in the U.S. 
 
Eissa  and  Liebman  want  to  examine  the  impact  of  the  U.S.  Tax  Reform  Act  of  1986.    In  particular, 
part  of  the  act  increased  an  existing  earned  income  tax-credit  for  single  women  with  children.    The 
earned  income  tax  credit  (EITC)  began  in  1975.    In  general,  a  taxpaying  family  is  eligible  for  the 
subsidy  if  1)  earned  income  is  below  a  particular  amount  (about  $28,000  in  1996),  and  2)  parents 
have a child under 19 years old. 
 
The  credit  works  roughly  the  following  way:  the  full  credit  is  phased  in  at  a  11%  rate  over  the  first 
$5000 of income (the subsidy increases with hours worked, unlike the SSP where it decreases).  The 
maximum credit  is 550 holds until earnings are $6,500, and  is then phased out at 12.22 percent, until 
earnings is $11,000. 
 
<write down figure> 
 
The  1987  change  increased  the  subsidy  phase-in  rate  from  11%  to  14%,  and  the  maximum  credit 
increased  to  $851.    The  phaseout  rate  was  also  lowered,  from  12.33%  to  10%.    Taxpayers  with 
incomes between $11,000 and $15,432 became eligible for the credit for the first time in 1987. 
 
The  tax  reform act also  raised non-work  related deductions  for households with and without children, 
fairly  substantially  relative  to  the  change  in  the  EITC.    This  shifts  the  budget  line  up  –  which  should 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

1 

decrease hours of work  for eligible  taxpayers who are already  in  the workforce and  view  leisure as a 
normal good. 
 
In  contrast,  the EITC  unambiguously  predicts  a  positive  impact  on  labor  force  participation,  because 
income effects from the program for those not participating are zero. 
 
Estimation strategy: 
 
Let 

igtY   be  the  observed  outcome  (hours  worked  in  this  example),  for  individual  i ,  from  group  g ,  at 
−
time  t .    The  effect  of  average  interest  is 
, 
  is  the  outcome  if  the  policy  is 
YE
igt Y
igtY1
(
)
igt
1
0
  is  the  outcome  if  not.    Group  1  is  at  some  time  exposed  to  the  policy  and 

implemented,  and 

igtY0

group 2 is not. 
 
The underlying assumption in the diff-in-diff framework is that  
 
YE
( 0

=),
tg

e
t

+

e

igt

 

|

g

 
that is, in the absence of the new policy, average hours worked can be decomposed into a time effect 
that is common to groups and a group effect that is fixed over time. 
 
<draw trending pattern on board> 
 
Suppose that the average effect of the program is simply a constant, so that: 
 
YE
(
igt
1

β+
 

YE
(
0

tg
),

tg
),

=

igt

|

|

 
If 

 
Y
i

iD  is an indicator for whether group g is exposed to the policy, then we can write: 

= β
+
eD
i

g

++
e
t

e
i

, 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

2 

 
where 

=tgeE i
|
(
0),

. 

 
This is just a regression equation, with fixed effects for time and group. 
 
If we  take  the difference  in outcomes across  time, we  identify  the average effect.   Suppose when  t = 
1,  no  reform  has  taken  place,  and  in  between  t=1  and  t=2  the  policy  changes  and  affects  group  1.  
Then: 
 
=
gYE
(
|
i
+
gYE
(
|
i

−=
=
gYE
t
)1
(
|
i
−
=
gYE
t
(
,1
)2
|
i

  
β=

,1
=

,2
=

=
t
t
,2

)1
=

)2

 
 
In  this  example, Eissa  and Liebman want  to examine  how  the EITC  reforms  impacted  single women 
parents’ labor supply.  Where 
igtY   is  labor supply, t=1 for the year=1985 (before the reform) and t=2  if 
the  year = 1987  (after  the  reform).   Note we  are note examining a  specific  change  in  tax  liability, but 
the  overall  effect  in  the  entire  shift  in  the  budget  constraint.    Individuals  will  be  affected  in  different 
ways.    The  only  thing  we  can  predict  is  that  the  reform  should  raise  overall  employment,  among 
single eligible parents.  
 
Key  here  is, what  is  the  counterfactual??   We’d  like  to  know  the  treatment  effect  relative  to  a  similar 
group  that  was  not  eligible.    Picking  the  control  group  is  crucial,  since  we  assume  that  both  groups 
are affected by time identically. 
 
Eissa  and  Liebman  suggest  using  as  the  control  group  the  population  of  single  women  without 
children.  This group was not eligible for the EITC,  
 
The difference  in differences  strategy makes 2  crucial assumptions: 1)  the  interaction  terms are  zero 
in  the  absence  of  the  intervention.    The  outcomes  are  trending  exactly  the  same  way  without  the 
change.   If hours worked evolves differently across single women with and without children, we have 
a  problem.   We  can  often  test whether  trends  in  outcomes  are  the  same  before  the  policy  break.    A 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

3 

related  assumption  is  the  composition  of  both  groups  remains  stable  before  and  after  the  policy 
change.   We’re  assuming  the  population  background  characteristics  within  groups  (not  correlated  to 
the policy change) remain stable. 
 
 
The smaller the time range examined, the less likely trends will deviate. 
 
Note,  essentially  the way  I’ve  described  this  analysis,  there  are  only  4  observations:  the mean  labor 
supply  for  the  2  groups,  before  and after  the  change.    If we  can  observe  other  factors  for  individuals 
that  could  affect  labor  supply  (that  could  change  between  periods),  we  may  be  able  to  get  more 
efficient  estimates  by  controlling  for  these  observables  and  working  at  a  smaller  level  of  data  than 
means.   
 
Y
i

+
βδ'
+
eD
i
i

++
e
t

e
i

 

=

X

g

 
Controlling for other individual characteristics, 

iX , the estimate of  β only if 

iX  and 

iD  are correlated, 

conditional on group and time main effects. 
 
Also in practice, we can sometimes allow for the effect to vary with time. 
 
A  quick  note  on  regression  discontinuity:  why  do  we  even  need  a  control  group  in  the  first  place?  
Why  not  measure  the  change  in  hours  supplied  the  year  before  and  the  year  after  the  change?  
Under a different set of assumptions, we can identify the causal effect.  I’ll present a good example of 
regression discontinuity  later.   A discontinuity approach doesn’t work well here because we have  few 
observations before and after the reform change, and the policy is likely to take some time to have an 
effect and so there is less likely a discontinuity in outcomes right at the year the policy changes. 
 
Results: 
 
Table  1  shows  that  the  average  characteristics,  such  as  income,  hours  worked,  age,  are  quite 
different  between  single  women  with  and  without  children.    It  should  make  us  nervous  about  the 
identification assumptions – are these groups really likely to experience the same time shocks? 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

4 

 
Eissa and Liebman  try  to address  this by  focussing only on single women with  less  than high school.  
Table  II  shows  main  result:  labor  force  participation  rises  for  the  treatment  group  by  4  percentage 
points after the reform. 
 
Figure  II,  tries  to  convince us  that  there are no underlying  time  trends.   We  can  squint and  see main 
results here: labor force participation going up for females with children, and perhaps slightly down for 
females without. 
 
Table  V  interestingly  shows  no  significant  response  in  annual  hours  and  annual  weeks  from  reform.  
This result got a lot of attention. 
 
One  last  thing:  the analysis  says  nothing  about  the overall  costs  (to  the  taxpayer)  for  introducing  the 
program.   
 
 
 
A note on weighting data 
 
 
Sampling weights are often used to correct for imperfections in the sample that might lead to bias and 
other departures between the sample and reference population. 
 
Be aware of how your data was collected. 
 
e.g.  Census:  no  missing  observations.    Why?    (hot  deck:  allocation  flags  to  catch  this)  Imputed 
observations 
e.g. PSID: over-samples low income families 
 
 
Why weight? 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

5 

 
1)  to compensate for unequal probabilities of selection (non random sample) (known) 
2)  2)  to compensate  for non  response  (missing observations): use known distributions of observable 
observations (e.g. gender) to reweight sample so weighted sample in line with known distribution. 
3)  c)  to  adjust  weighted  sample  distribution  to  make  it  conform  to  a  known  population  distribution 
(make the data ‘add up’ to known population” 

 
 
To compute any counts or means, must use weights 
 
Example: 
There  is  a  population  of  100,000  people,  and  only  enough  money  to  interview  1,000  people.    The 
population  Is  divided  into  2  regions,  A  and  B.    The  percentage  of  low  income  people  in  the  total 
population is 20%.  We want to do some separate analysis for the low income group, and 200 people 
may not generate a  large enough sample.   Suppose we know Region A has 25,000 people, 50%  low 
income  people.    Region  B  has  only  10%  low  income  people.    If  we  sample  500  people  from  each 
region,  we  can  expect  to  sample  500*.5  +  500*.1  =  300,  instead  of  200  from  sampling  a  random 
sample across both regions. 
 
The  chance  of  a  person  in  region  A  being  selected  is  500/25,000=.02.    The  chance  of  a  person  in 
region  B  being  selected  is  500/75,000  =  .00666667.    To  create  weights,  we  assign  the  inverse 
probability of being selected.  People  in region A get a weight of 1/.02 = 50.  Each person  in region A 
represents 50 people.  People in B get a weight of 1/..00666667 = 150. 
 
 
 

∑
wx
i
i
n
∑
w
i
n

=

∑
wNx
g
g
i
g
∑
n

w
i

=

∑
g

x

g

Pr(

g

)

 

=

x
i

 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

6 

For  regression  it’s  less  clear  whether  we  should  use  weights.    If  our  data  is  of  cell  means  and  we 
know  the sample size  in each cell, we would definitely want  to weight  the  regression.    If  the variance 
2σNei
of  each  individual  observation  is  normally  distributed: 
,  then  the  variance  for  cell  mean 
)
,0(
~

observations is 

e

N
,0(~

g

σ
2
n

g

is:   
 

)

, where 

gn  is the number of observations in each cell.  The appropriate correction 

v

g

.   D  is the diagonal matrix whose diagonal elements are the elements of v.  Ng is total 

g

g

=

Let 

nN
g
∑
n
g
number of cells.   Then, regression weights the equations by the observations: 
 
=
DXXXX
'
'
=
DyXyX
'
'
 

 

ˆβ
=

∑
g

y
(
i
∑
n

−

xy
)(

i

−

vx
)

g

−

(

x

i

x

2)

v

g

. 

 
This is the computation made when using aweights in STATA. 
 
Note,  this  is  the equivalent  to multiplying every  variable  in  the  regression by 

unweighted regression of: 

nY
i

g

=

β
0

n

g

+

β
1

nX
i

g

+

e

i

n

g

 

gn   and  carrying out  the 

 
The  justification  for  using  probability  weights  when  the  survey  over  samples  some  groups  is  less  clear.    If 
2σNei
variance  for  each observation  is 
,  the variance  for  each over or under  sampled observation  is  still 
,0(~
)
2σNei
.    There  is  no  heteroskedasticity  problem  like  the  case  with  cell  mean  observations.    One  could 
,0(~
argue  for using  this approach with probability weights  instead  (

iw  as defined above)  for efficiency  reasons.    In 
addition,  if  you  believe  B  is  different  for  different  groups,  you  should  weight  if  you  are  after  the  population 
average effect.    

)

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

7 

 
 
 
In  practice,  doesn’t  seem  to  matter  much  if  proportion  in  population  similar  to  proportion  in  sample.  
See, for example, Angrist and Krueger, table 12.  W ith regression, conditioning on X variables used to 
group and compute weights, don’t require weights.  Some statisticians have even questioned whether 
weights  should  be  used  at  all  with  regression.    I  have  yet  to  see  a  paper  that  rests  on  the  weighting 
assumptions, but the standard practice is to weight. 
 
Further references: 
 
http://www.amstat.org/sections/srms/Proceedings/papers/1981_135.pdf 
 
http://www2.chass.ncsu.edu/garson/pa765/sampling.htm 
 
http://unstats.un.org/unsd/demographic/meetings/egm/Sampling_1203/docs/no_5.pdf 
 
STATA 8 User reference 23.16 
 
 
 
 
 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

8 

=

(

)

'

E
[(

y

yy
E
)(

−
XXX
'
)
1

their means.  The ordinary linear regression estimates are 
 
b
var(
 
b
var(
 
where 

− Ω
XXXXXX
'
)
(
'
1
'

XXXy
E
'
])'
(

Ω=)

0

 and 

−
1

)

 

=iE ε
)
(

iE εε
(
j

−
1

)

 

−

−

=

(

)

A note on the need to ‘cluster’ standard errors 
 
OLS assumes no serial correlation or autocorrelation  in  the error  terms when estimating  the variance 
(and  standard  errors)  of  the  coefficients.    This  can  lead  to  downward  bias  in  the  standard  errors  if, 
instead,  the  errors,  or  at  least  some  of  them,  are  positively  correlated.    The  bias  can  sometimes  be 
severe. 
 
pn ×   design  matrix 
Consider  the  variance  of  the  ordinary  least  squares  regression.    Let    X   be  the 
=
+
1×n
and  y   be  the 
,  so  any  fixed 
  vector  of  dependent  values.    The  regression  model  is: 
eXβ
y
effects are defined as dummy variables contained  in  the  X  matrix, and  y  and  X  are deviations from 
−
yXXX
1)
'
'

, and the variance is: 

(

, (the variance-covariance matrix for all i and j observations) 

 

The standard OLS assumption to estimate the variance is 

I2σ=Ω

, and 

ˆσ
2

1
N
∑=
N 1

ie
2

: 

 
b
var(

)

=

εσ
(ˆ
2

XX
'

)

−

1

 

 
OLS assumes that the variance matrix for the error term is diagonal while in practice  it might be block 
diagonal,  with  a  constant  correlation  coefficient  within  each  group  and  time  cell.   When  we  want  to 
identify  an  aggregate  group/time  effect, within  group/time  correlation  can be  substantial.    In  practice, 
the  correlation  is  often  positive,  which  leads  the  OLS  results  to  underestimate  the  standard  error, 
making  it  more  likely  to  reject  the  null  hypothesis.    It  is  reasonable  to  expect  that  units  sharing 
observable  characteristics  such  as  being  from  the  same  industry,  state,  marital  status,  time  period 
and  location,  also  share  unobservable  characteristics  that  would  lead  the  regression  disturbances  to 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

9 

be positively correlated.  W ith Monte Carlo experiments, several recent papers have suggested using 
OLS  standard  error  estimates  can  bias  standard  errors  downwards  and  lead  to  rejection  that  the 
coefficient is zero, when in fact, it is. 
 
Fortunately,  White  (and  earlier  Eicher  and  Huber)  found  a  way  to  estimate  robust  standard  errors, 
regardless  of  the  form  Ω   takes  (provided  that  Ω   is  well  defined).   White  pointed  out  that we  do  not 
need  to  estimate  every  component  in  the  n  x  n  Ω  matrix,  an  obviously  impossible  task  when  only  n 
observations  are  available.    But  this  way  of  looking  at  the  problem  is  misleading.   What  is  actually 
required is to estimate 
 
 
b
var(
 
(White, 84, Aymptotic Theory for econometricians) 
 
The robust variance-covariance matrix estimator is: 
 

XXXeeX
'
](
'
'

E
[

=

(

XX
'

)

−
1

)

−
1

)

 

b
var(

)

=

(

XX
'

)

−
1





N
∑
1

−

[(

y

i

)ˆ
y
i

x

i

[(]'

y

i

−

)ˆ
y
i

x

i

XX
'

)

−
1

 


(]



 
where 

iyˆ  is the estimated error term, and the sum is over all observations.  This variance is computed 
when  the  ‘robust’  option  is  specified  in  STATA.    When  prior  knowledge  leads  the  researcher  to 
believe  the  error  terms may  be  serially  correlated within  groups,  but  independent  across groups,  the 
variance can be calculated as: 
 

b
var(

)

=

(

XX
'

)

−
1





G
∑
1

k uu
'

k





(

XX
'

)

−
1

=
, where  ∑
u
k
∈
kj

[(

y

j

−

)ˆ
y
j

x

j

[(]'

y

j

−

)ˆ
y
j

x

j

]

 

 
This variance estimate is computed with STATA’s ‘cluster’ command, specifying groups G. 
 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

10 

This  estimator  is  consistent  for  any  arbitrary  heteroskedasticity  or  serial  correlation,  but  it  is  not 
efficient when prior information about the form of the matrix is known. 
 
 
To give  you a  little  intuition  for  the need  to  cluster,  consider  the  following example.   Suppose we are 
isS  
evaluating  the  relationship  between  education  attainment  and  state  compulsory  school  laws.    Let 
SZ   is  the  dropout  age  that  an  individual  faced 
when  in  high  school,  from  state  S.    So  the  independent  variable  is  the  same  for  everyone  from  that 
state.  The OLS regression equation is: 
 
S

be  years  of  schooling  for  individual  i   in  state  S ,  and 

= β
Z

, 

+

is

e
iS

S

 
It’s certainly plausible that  individuals from the same state are related  in other ways.  There could still 
=iS
be  no  omitted  variables  bias: 
,  but  the  error  terms  are  serially  correlated  among 
S eZE
0)
,
(
= S
≠
eE
e
S
(
,
|
iS

individuals from the same state: 

.  

0

)

jS

individuals  from  the  same  state.    Suppose  also  that 

 
One  extreme  example  is  we  have  100  individuals,  2  from  each  state.   
SZ   is  the  same  for  each  two 
isS   is  the  same  for  both.   So  what  we  have  is  2 
( 2 =iSeE
sets  of  the  same  50  values  for  S  and  Z.    Normalize  the  standard  deviation  to  1: 
.    If  the 
1)
IΩ = , as in OLS, the variance of  βˆ  is: 

variance-covariance matrix is 
 

var(

)ˆ
β

=

1
50
∑
1

2

Z

2
i

2

50
∑
1

Z

2
i

1
50
∑
1

2

Z

2
i

=

1
50
∑
1

2

 

Z

2
i

 
If,  instead, 
iSe   is  perfectly  correlated  within  state, 
Recognizing the, the true variance of  βˆ  is 
 
 
 

eE
(
iS

,

e

jS

|

S

= S

=
1)

  and  zero  otherwise.  

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

11 

var(

)ˆ
β

=

1
50
∑
1

2

Z

2
i

4

50
∑
1

Z

2
i

1
50
∑
1

2

Z

2
i

=

 

1
50
∑
Z
1

2
i

 
  
If  the  second  covariance  matrix  is  correct,  we  falsely  underestimate  the  variance  of  βˆ   using  OLS.  
iSe   was  only  partially  correlated 
The  second  individual  in  each  state  adds  no  new  information.    If 
within  state,  the  variance  would  be  smaller,  but  still  larger  than  OLS.    Using  White’s  clustering 
approach leads to a consistent estimate of the variance of  βˆ , no matter what shape underlies  Ω . 
 
One  should  note  that  this  estimator  applies  asymptotically  (as  the  sample  size  and  the  number  of 
groups approaches infinity).  Monte carlo experiments reveal that the estimator works reasonably well 
when  the  sample  size  within  groups  is  not  especially  large  relative  to  the  number  of  groups.  
Unfortunately,  the  number  of  groups  is  very  small,  relying  on  asymptotics  can  be  very  misleading.  
What  is  small?    The  references  below  suggest  even  groups  as  high  as  40  or  50  can  lead  to  poor 
estimates.    A  conservative  solution  is  to  aggregate  the  data  up  to  the  group  level  and  run  the 
regressions using the grouped means, weighted by the sample size.  In our example, this would be: 
 
S

= β
Z

+

s

e
S

, 

S

 
which will generate  the  same estimate  for B and  the  variance of B  in our  simple example.    If  there  is 
no  cluster  effect  (no  serial  correlation  within  groups),  then  aggregating  to  the  group  level  removes 
information  and  increases  the  variance  unnecessarily.    In  practice,  results  are  far more  convincing  if 
you can produce robust and significant results with this aggregated approach (if it’s applicable). 
 
Note,  in  the  diff-in  diff  example  above,  if  we  aggregated,  we  only  would  have  4  observations.    And 
indeed, one criticism that has been put out by some researchers  is that the diff  in diff approach  is  just 
in  essence  comparing  2  groups  over  time  and  we  can’t  be  sure  that  any  observed  significant 
difference in means is due entirely to the policy change. 
 
 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

12 

Useful references for this topic: 
 
Wooldridge, AER, May 2003, p 133, “Cluster Sample methods in Applied Econometrics” 
 
Donald, Stephen, and Kevin Lang, “Inference with Difference in difference and other panel data,’ 
mimeo, 2001 
 
White, Halbert, “Asymptotic Theory for Econometricians,” 1984 
 
Bertrand, Duflo, and Mullainathan, ‘how much should we trust differences-in-differences estimates,’ 
QJE, Feb 2004-09-13 
Arellano, M. “Computing Robust Standard errors for within groups estimators,’ oxford bulletin of 
economics and statistics, 49, 4 (1987) 
 
White, Halbert, “a heteroskedasticity-consistent covariance matrix estimator and a direct test for 
heteroskedasticity,’ econometrica, 1980 
 
Kezdi, Gabor, ‘robust standard error estimation in fixed effects panel models,’ university of Michigan 
mimeo, 2002. 
 
 

Philip Oreopoulos Labor Economics Notes for 14.661 Fall 2004-05 Lecture 4 

13 

