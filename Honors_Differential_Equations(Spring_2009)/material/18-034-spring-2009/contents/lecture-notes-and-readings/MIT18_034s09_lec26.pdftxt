MIT OpenCourseWare
http://ocw.mit.edu 

18.034 Honors Differential Equations 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

LECTURE  26.  EIGENVALUES AND  EIGENVECTORS 

We study the system 
(26.1) 
where A = (aij ) is a constant n × n matrix. 
When n = 1, the above system reduces to the scalar equation y �  = ay , and it has solutions of the 
form ceat .  For n �  2, similarly, we try solutions of the form �veλt , where �v  ∈ Rn  and λ  ∈ C.  Then, 
(26.1) becomes 

�y �  = A�y , 

λ�ve λt  = A�ve λt . 

Subsequently, 
(A − λI )�v = 0. 
A�v = λ�v , 
(26.2) 
In  order  to  ﬁnd  a  solution  (26.1)  in  the  form  of  �veλt  we  want  to  ﬁnd  a  nonzero  vector  �v  and 
λ ∈ C satisfying (26.2).  It leads to the following useful notions in linear algebra. 
Deﬁnition 26.1.  A nonzero vector �v satisfying (26.2) is called an eigenvector of A with the eigenvalue 
λ(∈ C). 
These words are hybrids of English and German, and they follow German usage, “ei” rhymes 
with π . 
We recognize  that (26.2)  is a  linear system of equations for �v .  A well-known result from  linear 
algebra  is  that  it  has  a  nontrivial  solution  �v  if  and  only  if  A − λI  is  singular.  That  is,  pA (λ) = 
|A − λI |  = 0,  where  pA (λ)  is  the  characteristic  polynomial  of  A.  In  this  case,  such  a  nontrivial 
� 
� 
solution �v  is an eigenvector and the corresponding root of pA (λ) = 0 is an eigenvalue. 
Plane systems.  For a 2 × 2 matrix A = 
����

����

a11  a12 
, the characteristic polynomial is 
a21  a22 
a11  − λ
a12 
a22  − λ 
a21 
This quadratic polynomial has two roots, λ1  and λ2  (not necessarily distinct).  Let �v1  and �v2  be the 
eigenvectors  corresponding  to  the  eigenvalues  λ1  and  λ2 ,  respectively.  By  deﬁnition, �v1eλ1 t  and 
�v2eλ2 t  are solutions of (26.1). 
If  λ1  �=  λ2 ,  then  the  functions �v1eλ1 t  and �v2eλ2 t  are  linearly  independent.  Hence,  they  form  a 
basis of solutions of (26.1).The general solution of (26.1) is given as 
�y(t) = c1�v1e λ1 t  + c2�v2e λ2 t , 
�

�

where c1 , c2  are arbitrary constants. This shows one use of eigenvalues in the study of (26.1). 
Let us deﬁne 2 × 2 matrices

0 
λ
1 
V  = (�v1  �v2 )  and  Λ = diag (λ1 , λ2 ) = 
. 
0 
λ
2 
One can verify that AV  =  V Λ.  If �v1  and �v2  are linearly independent, so that |V | �= 0, then we can 
make the (non-singular) change of variables 
�x = V −1�y . 

= λ2  − (tr A)λ + det A.


p(λ) =


1 

Then, (26.1) is transformed into �x �  = Λ�x, that is 
x�
1  = λ1x1 , 
x�
2  = λ2x2 . 
That  is,  �x  solves  a decoupled  system.  The  solution  of  this  system  is  immediate  and x1  =  c1eλ1 t , 
x2  =  c2eλ2 t .  The  new  variables  �x  is  called  the  canonical  variables,  and  Λ =  V −1AV  is  called  the 
diagonalization.  This  is  another  use  of  eigenvalues.  Canonical  variables  play  a major  role  i  engi­
neering, economics, mechanics, and indeed in all ﬁelds that makes intensive use of linear systems 
with constant coefﬁcients. 
Lemma  26.2.  If  the  eigenvalues  of  a  2 × 2  matrix  are  distinct,  then  the  corresponding  eigenvectors  are 
linearly independent. 
Proof.  Suppose that the eigenvalues λ1  =� λ2 , but the eignevectors satisfy 
c1�v1  + c2�v2  = 0.

(26.3) 
We want to show that c1  = c2  = 0. Applying the matrix A to (26.3), we obtain that

c1λ1�v1  + c2λ2�v2  = 0. 
(26.4) 
Subtraction then yields 

c1 (λ2  − λ1 )�v1  = 0. 
� 
� 
This implies c1  = 0. Then, (26.3) implies c2  = 0. 
12  5
Example 26.3.  We consider A =  −6 1  . 
Its characteristic polynomial  is p(λ) =  λ2  − 13λ + 42  =  (λ − 6)(λ − 7), and A has  two distinct 
eigenvalues, λ1  = 6 and λ2 �= 7. 
� 
� � 
� −6  −5�
� −6�

5
6
5 
If λ1  = 6, then A − 6I  = 
, and 
is an eigenvector.

5
1
5 
If λ2  = 7, then A − 7I  =  −6  −6  , and  −1 
is an eigenvector. The general solution of (26.1) 
� � 
� � 
is, thus, 
5 + c2e 7t 
1 
� −y1  − y2 
1  �−1 � 
� 
� 
� 
y(t) = c1e 6t 
−6 
−1 
. 
5
y1 
� 
� 
The canonical variable is �x = 
−6  −1 
. 
6y1  + 5y2 
y2 
� � 
1
6
has only one eigenvalue λ = 7, and the only corresponding 
Exercises.  1.  Show that A = 
−1 8 
� 
� 
1
.  In this case, we can’t construct the general solution of (26.1) from this. 
eigenvector is 
1 
� 
�
1
1
1  has two (complex) eigenvalues 1 ± 2i and the corresponding eigen­
2.  Show that A =
−4 
1
�  � 
�  � 
±2i 
vectors 
, respectively. They leads to the general (complex) solution of (26.1) 
1 
c1e(1+2i)t  1 + c2e(1−2i)t 
−2i 
.
2i 

� 

= 

Lecture 26 

2 

18.034 Spring 2009 

Higher-dimensional systems.  If A is an n × n matrix, where n � 1 is an integer, then 
pA (λ) = |A − λI |
is  a  polynomial  in  λ  of  degree  n  and  has  n  roots,  not  necessarily  distinct.  That means,  A  has  n 
eigenvalues λ1 , . . . , λn ,  not necessarily distinct.  Let �v1 , . . . , �vn  be  the  eigenvectors  corresponding 
to λ1 , . . . , λn , respectively. Let 
· · · 
�vn )  and  Λ = diag (λ1 , . . . , λn ).
V  = (�v1 
By deﬁnition, �v1eλ1 t , . . . , �vneλn t  are solutions of (26.1). 
If |V | = 0
, that means, if �v1 , . . . , �vn  are linearly independent, then they form a basis of solutions 
of (26.1).  Moreover, �x  =  V −1�y  is a canonical variable and �x �  = Λ�x.  In many cases,  the vectors �vj 
can be chosen linearly independent even if λj  are not all distinct. Sometimes the condition |V | = 0 
is met by the following. 
Lemma  26.4.  If  eigenvalues  λ1 , . . . , λn  are  distinct,  then  the  corresponding  eigenvectors  �v1 , . . . , �vn  are 
linearly independent. 
Proof.  Suppose  not.  Let  m >  1  be  the  minimal  number  of  vectors  that  are  linearly  dependent. 
Without loss of generality, we assume that �v1 , . . . , �vm  are linearly dependent, so that 
· · · 
+ cm�vm  = 0 
c1�v1  + c2�v2  +
(26.5) 
and some cj  is nonzero. We further assume that c2  = 0
. 
We now proceed similarly to Lemma 26.2. Applying A to (26.5) we obtain 
· · · 
c1λ1�v1  + c2λ2�v2  +
+ cmλm�vm  = 0.

Multiplying by λ1  then 

Thus, we have 

· · · 
c1λ1�v1  + c2λ1�v2  +
+ cmλ1�vm  = 0.
+ cm (λm  − λ1 )�vm  = 0.
c2 (λ2  − λ1 )�v2  +
· · · 
· · · 
Since  �v2 , . . . , �vm  are  linearly  independent,  c2  =
=  cm  = 0  must  hold.  A  contradiction  then 
� 
proves the assertion. 
We  recall  that  if  A  =  AT  then  the  square  matrix  A  is  called  symmetric.  If  a  complex  matrix 
satisﬁes  A  =  A∗ ,  where  A∗  denotes  the  conjugate  transpose  or  adjoint  of  A,  then  A  is  called 
Hermitian.  A  symmetric  or  a Hermitian matrix has many  important properties pertaining  to  the 
study of (26.1) via eigenvalues. 

(1) All eigenvalues of a symmetric matrix are real and eigenvalues of A corresponding to differ­
ent eignevalues are orthogonal. 
Proof.  Let 
and λ =� µ. Then, 
�u, �v = 0 

T AT �u = ¯  T �
�u T A�u = λ�u  u, 
T �
�u 
λ�u  u. 
Since A = AT , it implies that λ = λ¯ . The second assertion is left as an exercise. 
(2) A has n linearly independent eigenvectors (regardless of the multiplicity of eigenvalues). 

A�u = λ�u, 

A�v = ν�v , 

� 

An immediate consequence of (1) and (2) is the following. 

(3) If eigenvalues are simple (multiplicity = 1) then the corresponding eigenvectors are orthog­
onal. 

Lecture 26 

3 

18.034 Spring 2009 

�
�
�
�
