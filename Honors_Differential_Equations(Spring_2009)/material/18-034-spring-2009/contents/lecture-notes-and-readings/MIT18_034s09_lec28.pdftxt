MIT OpenCourseWare
http://ocw.mit.edu 

18.034 Honors Differential Equations 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

LECTURE  28.  REPEATED  EIGENVALUES AND  THE MATRIX  EXPONENTIAL 

Repeated eigenvalues.  Again, we start with the real n × n system 
�y �  = A�y . 
(28.1)	
We  say  an  eigenvalue  λ or  A  is  repeated  if  it  is  a  multiple  root  of  pA (λ).  That  is,  pA (λ)  has
∗ 
(λ − λ )2  as a factor. We suppose that λ∗  is a double root of pA (λ), so that in principle two linearly 
∗
independent  solutions  of  (28.1)  correspond  to  λ∗ .  If  �v is  the  corresponding  eigenvector,  then 
∗ 
�y  = �v∗eλ∗ t  is a solution.  The problem is to ﬁnd the second solution of (28.1), linearly independent 
of �y∗ . We ﬁrst discuss an easy case. 

Example 28.1 (The complete case).  Still supposing that λ is a double root of pA (λ), we say λ is a 
∗	
∗ 
complete eigenvalue if there are two linearly independent eigenvectors corresponding to λ∗ , say �v1 
and �v2 . Using them, we obtain two linearly independent solutions of (28.1), namely 
�y1 (t) = �v1e λ1 t  and  �y2 (t) = �v2e λ1 t 
� 
� 
Exercise. Let A be a 2 × 2 matrix.  If λ is a repeated and complete eigenvalue of A, show that 
λ  0 
.
0  λ 

A = 

The converse holds true. 
In general, if an eigenvalue λ∗  of A is k-tuply repeated, that is, pA (λ) =  |A − λI | has the power 
(λ− λ )k  as a factor, but no higher, the eigenvalue is called complete if k linearly independent eigen­
∗
∗
vectors correspond to λ .  In this case, these eigenvectors produce k linearly independent solutions 
of (28.1).  An important result in  linear algebra is that if a real square matrix A is symmetric,  that 
is, A = AT , then all its eigenvalues are real and complete. 

However, in general, an eigenvalue of multiplicity k  (> 1) has less than k eigenvectors, and we 
cannot construct the general solution from the techniques of eigenvalues. 

The matrix exponential.  To motivate, the initial value problem 
y �  = ay , 
y(0) = 1 
has the solution y(t) =  eat  in the form of exponential.  We want to deﬁne the expression eAt  for a 
general n × n matrix A, n � 1, so that Y (t) = eAt  is a solution of 
Y �  = AY , 
Y (0) = I 
and moreover eAt  is easy to compute. 
Recall that if a is a real number then 
(at)2 
2! 

e at  = 1 + at + 

(at)n 
n! 

�∞	 (at)n 
,
n! 
n=0 

· · · 

= 

+

· · · 
+ 

+

1 

· · · 

= 

+

.

+

(28.2)	

· · · 
+ 

t2A2 
2! 

tnAn 
n! 

e At  = I + tA + 

where t ∈ R.  A natural way to deﬁne the matrix exponential eAt , for an n × n matrix A, seems to 
� tnAn
use the series expression 
∞
n! 
n=0 
In order to make sense of the above expression, we ﬁrst introduce the matrix norm. 
Deﬁnition 28.2.  For an n × n matrix A, deﬁne the matrix norm as 
|A�y |
�A� = sup 
, 
�=0  y |
|�
y
where |�y | = |�y T �y |1/2  = (y2  + · · · + yn
2 )1/2  and |A�y | = |(A�y)T (A�y)|1/2 .
1 
Exercise.  Show that 
�A + B � � �A� + �B �, 
for any matrices A, B and for any t ∈ R. 

�AB � = �A� �B �, 

�tA|| =  | | �A� 
t

→ 
For a matrix-valued function A(t) = (aij (t))n 
, then A(t)  A(t0 ) means equivalently: 
i,j=1
→
→
t0  for all 1 � i, j  � n.
(i)	 aij (t) 
aij (t0 ) as t
(ii)  ||A(t) − A(t0 )|| → 0 as t → t0 . 

Under the matrix norm, the inﬁnite series (28.2) converges for all A and for all t, and it deﬁnes 
�	
� 
the matrix exponential. 
We now compute the derivative of eAt  by differentiating the right side of (28.2) term by term 
tnAn 
t2A2 
d 
· · · 
· · · 
d
e At  = 
+
+ 
+ 
I + tA +
n! 
2! 
dt 
dt 
tn−1An 
· · · 
(n − 1)! 

=A + tA +

· · · 
+ 

+

=AeAt . 
Moreover, since eA0  = I , by deﬁnition, the matrix exponential eAt  is a solution of 
Y �  = AY , 
Y (0) = I . 
Theorem 28.3.  Let Y (t) be a fundamental matrix of A. Then Y (t) = Y (0)eAt . 
For several classes of A, the inﬁnite series in (28.2) can be summed up exactly. 
� 
� 
� 
� 
Exercise.  1.  Show that exp(diag (λ1  . . .  λn )) = diag (eλ1  . . .  eλn ). 
0	 1 
0 
t
2.  Show that if A = 
then e = 
At 
.
0	 0 
0 0 
Exercise. Prove the exponential law 
e(A+B )t  = e At e B t 
if  AB = BA. 
(28.3)	
� 
� 
We use (28.3) to compute the matrix exponentials of more matrices. 
� 
� 
� 
� 
2	 1
. We write A = B + C , where 
0	 2 
0 1 
2 0 
,
.
0 0 
0 2 

Example 28.4.  Let A = 

B = 

C  = 

Lecture 28	

2 

18.034 Spring 2009 

�
�
� � 
� 
� 
� 
Since BC  = CB , by the results from the previous exercise and by (28.3) 
0
1 
2t  1 
e2t 
t 
t 
.
0 1 
0 1 
0 
e2t 

At 
e  =

= e

Finding  the  fundamental  matrix.  In  general,  it  doesn’t  seem  possible  to  express  eAt  in  closed 
form. Nevertheless, we can ﬁnd n independent vectors for which eAt  can be computed exactly. 
The idea is to write 
e At�v = e(A−λI )t e λI t�v = e(A−λI )t e λt�v . 
If (A − λI )m�v = 0 for some integer m > 0, then (A − λI )m+l�v = 0 for all integers l � 0. Hence, 
−1 
tm
e(A−λI )t�v = �v + t(A − λI )�v + · · · +
(A − λI )m−1�v 
− 
(m
1)!
� 
� 
is a ﬁnite sum, and 
tm−1 
e At�v = e λt  �v + t(A − λI )�v + · · · +
(A − λI )m−1�v 
(m − 1)!
can be computed exactly, although eAt  itself cannot be computed. 
Example 28.5.  Solve the system Y �  = AY , where ⎛
⎞ 
A = ⎝0 1 0⎠ . 
1 1 0 
0 0 2 
SO LU T ION .  Its characteristic polynomial is pA (λ) = (1 − λ)2 (2 − λ), and it has a double root λ = 1 
⎞ 
⎛
and a simple root λ = 2. 
(A − λI )�v = ⎝0 0 0⎠ �v = 0 
If λ = 1, then 
0 1 0 
⎛ ⎞ 
0 0 1 
has  a  solution  �v1  =  ⎝0⎠.  Furthermore,  it  is  the  only  eigenvector  for  λ  = 1  up  to  a  constant 
1 
0 
⎛ ⎞ 
multiple.  (In  fact,  the  well-known  result  from  linear  algebra  tells  us  that  the  solution  space  of 
the above  linear  system of  equations has dimension 1.  )  One  solution of  the  system Y �  =  AY  is
obtained and �y1 (t) = et ⎝0⎠. 
1 
⎛
⎞ 
0 
To ﬁnd the second solution associated to λ = 1, we compute 
(A − λI )2�v = ⎝0 0 0⎠ �v = 0 
0 0 0 
⎛ ⎞ 
0 0 1 
has a nontrivial solution �v2  = ⎝1⎠, linearly independent to v1 . On the other hand, (A − I )�v2  = 0
0 
. 
⎛ ⎞ 
0 
e At�v2  = e t (I + t(A − I )�v2 ) = e t ⎝1⎠ , 
Thus, 
t 
0 

Lecture 28 

3 

18.034 Spring 2009 

�
⎞ 
⎛ 
and it gives an additional solution of the system Y �  = AY  associated to λ = 1. 
Finally, if λ = 2 then 
⎠ �v = 0 
(A − 2I )�v = ⎝−1 
−1 −1 
0 
⎛  ⎞ 
⎛  ⎞ 
0 
0 
0 
0 
0 
has a solution �v3  = ⎝	0⎠. Hence, �y3 (t) = e2t ⎝0⎠. 
0 
0 
1
1 
The general solution of the system Y �  = AY  can be written as 
Y (t) = (c1  + tc2 )e t v�1  + c2e t v�2  + c3e 2t v�3 . 
It is analogous to that for the scalar differential equations with multiple roots. 
We conclude the lecture by the following important result in linear algebra. 
Theorem  28.6  (Cayley-Hamilton  Theorem).  Any  square matrix A  satisﬁes  p(A) = 0,  where  p  is  the 
characteristic polynomial of A. 
Proof.  Recall the formula 
adj (A − λI ) (A − λI ) = p(λ)I . 
· 
Both sides are polynomial expressions for λ ∈ R.  We view them as matrix polynomials, that is to 
� 
say, we replace λ by a matrix. By setting λ = A then proves the assertion. 

Lecture 28	

4 

18.034 Spring 2009 

