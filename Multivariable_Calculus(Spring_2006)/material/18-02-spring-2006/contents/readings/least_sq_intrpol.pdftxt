LS.  Least  Squares Interpolation 

1.  The  least-squares line. 
Suppose you  have a  large number n  of  experimentally  determined points,  through  which 
you want t o  pass a curve.  There is a formula ( the  Lagrange interpolation formula) producing 
a polynomial curve of  degree n - 1which  goes through  the  points exactly.  But  normally one 
wants  t o  find a simple curve, like a line, parabola, o r  exponential, which goes approximately 
through  the  points,  rather  than  a high-degree polynomial which  goes exactly through them. 
Th e  reason  is  t h a t   the   location  of  the  points  is  t o  some extent  determined  by  experimental 
error,  so  one wants  a  smooth-looking curve  which  averages out  these  errors, not  a  wiggly 
polynomial  which  takes  them  seriously. 
I 
In this section, we consider the  most common case -finding a line which 
goes approximately  through  a  set of  d a t a  points. 

. 

Suppose the  d a t a  points  are 

and  we  want  t o  find  the  line 

which  "best"  passes  through  them.  Assuming  our  errors  in  measurement  a r e   distributed 
randomly  according t o  the  usual  bell-shaped  curve  ( the  so-called  "Gaussian  distribution"), 
i t   can  be  shown  th a t   the   right  choice  of  a  and  b  is  the   one  for  which  the   sum  D  of  the 
squares of  the  deviations 
I 

i= l 
is a  minimum.  In  the  formula  (2 ) ,  the  quantities in  parentheses  (shown by 
dotted  lines in  the  picture)  a re  the  deviations between the  observed values 
yi and  the  ones a x i  + b  t h a t  would  be predicted  using th e  line  (1). 
The deviations  a re  squared  for theoretical reasons connected with  th e  assumed  Gaussian 
error  distribution;  note  however  th a t   the   effect  is  t o   ensure  th a t   we  sum  only  positive 
quantities; this is important, since we  do not  want  deviations  of  opposite sign t o  cancel each 
other  out.  I t   also weights more heavily  the  larger  deviations,  keeping experimenters honest, 
since they  tend  t o  ignore large  deviations  ("I  had  a  headache  th a t   day"). 

-

This prescription  for  finding the   line  (1) is  called the   me thod   of  leas t   squares,  and  the  
resulting  line  (1) is  called the  least-squares line or the   regression  line. 

To calculate th e  values of  a and b which make D a minimum, we  see where the  two partial 
derivatives a re  zero: 

2 

18.02  NOTES 

These  give  us  a  pair  of  linear equations  for  determining  a  and   b,  as  we  see  by  collecting 
terms  and   cancelling  t h e   2's: 

(Notice t h a t   it  saves  a  lot  of  work t o  differentiate  (2) using  t h e  chain  rule,  r a th e r  t h a n  first 
expanding  ou t   t h e  squares.) 

Th e  equations  (4) a r e  usually  divided  by  n  t o  make  them   more  expressive: 

where Z  and   a r e  t h e  average of  th e  x i  and  yi, and  % = C xp /n   is t h e  average of  t h e  squares. 
From  th is   point  on  use  linear  algebra  t o  determine  a  and   b.  I t   is  a  good  exercise  t o  see 
t h a t   th e  equations a r e  always solvable unless  all t h e  x i   a r e  t h e  same  (in which  case t h e  best 
line  is  vertical  and   can ' t   be  written  in  t h e  form  (1 ) ) .  

I n   practice,  least-squares  lines  a r e  found  by  pressing  a  calculator  bu t ton ,   or  giving  a 
Ma tLab  command.  Examples  of  calculating  a  least-squares  line  a r e  in  t h e  exercises  in  your 
book  and  these notes.  Do  them  from scratch, s t a r t ing  from  ( 2 ) ,  since t h e  purpose here  is  t o  
get  practice with  max-min  problems  in  several  variables;  don't plug  in to  t h e  equations  (5). 
Remember  t o   differentiate  (2)  using  t h e   chain  rule;  don't  expand ou t   t h e   squares,  which 
leads t o  messy  algebra and  highly  probable  e r ro r .  

2.  Fitting  curves by  least  squares. 
If  t h e  experimental points  seem t o  follow  a  curve r a th e r   t h a n   a  line, i t  might make more 
sense  t o  t r y   t o  fit  a  second-degree  polynomial 

t o  them .   If  the re  a r e  only th ree  points, we  can  do  th is  exactly  (by t h e  Lagrange interpolation 
formula).  For  more  points,  however, we  once  again  seek  t h e  values  of  a o , a l ,  a2  for  which 
t h e  sum  of  t h e  squares of  t h e  deviations 

is a minimum.  Now  the re  a r e  th ree  unknowns, ao, a l ,  a2.  Calculating (remember  t o  use t h e  
chain  rule!)  t h e  th ree  pa r t ia l  derivatives d D / d a i ,   i = 0 , 1 , 2 ,  and  se t t ing  them   equal  t o  zero 
leads  t o  a  square  system  of  th ree   linear  equations; t h e   a i   a re  t h e  th ree   unknowns,  and   t h e  
coefficients depend  on  t h e   d a t a  points  (x i ,  yi).  They   can   be   solved  by  finding  t h e   inverse 
ma t r ix ,  elimination,  or using  a  calculator  or Ma tLab .  
If  th e  points  seem  t o  lie more  and  more  along a  line  a s  x + m, bu t   lie on  one side of  th e  
line for  low values of  x ,  i t  might  be   reasonable  t o  t r y  a  function which has  similar behavior, 
like 

LS.  L E A S T   SQUARE S   I N T E R P O L A T I O N  

3 

and   again  minimize  t h e   sum  of  t h e   squares  of  t h e   deviations,  as  in  (7).  In  general,  this 
method  of  least  squares applies t o  a  t r ia l  expression  of  t h e  form 

where  t h e   f i ( x )   a r e  given functions  (usually  simple ones  like  1,x ,  x 2 ,  l / x ,  e k x ,  e tc .   Such  an  
expression  (9)  is  called  a  linear  combination  of  t h e  functions  f i (x ) .   T h e  method  produces 
a  square  inhomogeneous  system  of  linear  equations  in  t h e   unknowns  a o , .  . . , a ,   which  can 
be  solved by  finding  t h e   inverse ma t r ix   t o  t h e  system, or  by  elimination. 

T h e  method  also applies t o  finding  a  linear  function 

t o  fit  a  set of  d a t a  points 

where  the re   a r e   two  independent  variables  x  and   y  and   a  dependent  variable  z  (this  is 
t h e  quan t i ty  being  experimentally  measured,  for  different  values  of  (x ,  y ) ) .   Th is   t ime   after 
differentiation we  get  a  3  x  3  system  of  linear  equations  for  determining  a l l  a2, a3  . 
Th e   essential  point  in   all  th is   is  t h a t   t h e   unknown  coefficients a i   should  occur  linearly 
in  th e  t r i a l  function.  Try  fitting  a  function  like  cekx  t o  d a t a  points  by  using  least  squares, 
a n d  you'll  see t h e   difficulty  right  away.  (Since  th is   is  a n  impo r tan t   problem  - fitting  a n  
exponential  t o  d a t a  points - one of  t h e  Exercises explains how  t o  a d a p t  t h e  method  t o  th is  
type   of  problem.) 

Exercises:  Section 2G  

