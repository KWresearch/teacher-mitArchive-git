18.02  LECTURE  NOTES  ON  PROBABILITY 

Continuous  Probability 

Discrete  probability  describes  games  of  chance  with  a  list  of  outcomes,  such  as  whether 
a  coin  lands  on  heads  or  tails,  or  a  die  lands  on  one  of  the  values  1  through  6.  In  contrast, 
continuous probability concerns quantities that can take on all possible values in a continuum. 
In  the  discrete  case,  the  probability  of  an  outcome  or  an  average  value  is  expressed  as  a 
sum,  whereas  in  the  continuous  case  these  values  are  described  using  integrals. 
The  basic  equation  of  probability  theory  is 

P (a < x < b) = 

dx = 

PROBABILITY = 

PART 
WHOLE 
Note  that  the  probability  is  a  number  between  0  and  1. 
Example  1.  We  say  that  x  is  uniformly  distributed  on  the  interval  0  ≤  x  ≤  10  if  any 
value  of  x  is  as  likely  as  any  other.  In  this  case,  the  probability  that  1 < x < 7  is 
7 − 1
PART 
6 
=  = 60% 
= 
10 
10 
WHOLE 
�  b
If  0 ≤ a < b ≤ 10,  then  probability  that  a < x < b,  is  given  by  the  formula 
b − a 
1 
10
10 
a 
The  probability  density  of  x  is  1/10  on  0  ≤  x  ≤  10  and  zero  outside  this  interval.  More 
�  b 
generally,  x  can  be  distributed  by  a  nonnegative  function  g(x)  so  that 
a 

P (a < x < b) = 
� 
Because  the  total  probability  is  1,  we  need 
∞ 
−∞ 
In  our  example,  g(x) = 1/10  on  0  ≤  x  ≤  10  and  g(x)  =  0  outside  this  interval.  With 
continuous  variables  like  x  it  does  not matter whether we  include  the  ends  x = a  and  x = b 
or  not.  We  interpret  the  events  x = a  and  x = b  as  happening  with  zero  probability.  Thus, 
P (a < x < b) = P (a ≤ x ≤ b). 
Example  2.  Consider  a  point  (x, y)  distributed  according  to  the  weighting  or  density 
� � 
δ(x, y) = x2 + y2  on  the unit disk D,  x2 + y2  < 1.  The probability  that  (x, y)  is  in  a portion 
R  of  D  is 
1 
mass(R)
PART  = 
� � 
= 
P ((x, y)  in  R) = 
WHOLE  mass(D)  M 
R 
� � 
� � 
δdA  is  the  total mass  of  D.  We  also  write 
D 
δ
dA = 
R  M 
R 

g(x, y)dA;  where  g(x, y) = 

δdA 

δ(x, y)
M

,

where M  = 

P ((x, y)  in  R) = 

g(x)dx 

g(x)dx = 1 

1 

Using  polar  coordinates, 

� � 
In  other  words,  the  probability  density  g(x, y) =  δ(x, y)/M  is  normalized  so  that  the  total 
integral  is  1. 
D 

g(x, y)dxdy = 1. 
�  2π  �  1 
� � 
π 
M  = 
δdA = 
r 2 rdrdθ = 
2
0 
D 
0
�  b 
�  2π 
�  2π  �  b  1
�  b
If  R  is  the  ring  a < r < b,  then 
1 
r 2 rdrdθ = 
r 3dr 
a  M 
a  M 
0 
0 
a 
Thus,  by  integrating  in  the  θ  variable,  we  obtain  the  probability  density  in  the  remaining 
variable  r .  The  probability  density  of  r  in  this  example  is  g(r) = 4r3  for  0  ≤  r  ≤  1  and 
�  1 
g(r) = 0  outside  this  interval.  As  usual,  the  total  probability 
0 

� 
e−x  dx 
2
Example  3.  The  normal  distribution.  The  value  of  the  constant M  = 
−∞
is  very  important  in  probability  theory.  It  gives  us  the  normalizing  factor  to  use  when 
deﬁning 

P (0 ≤ r ≤ 1) = 

P (a < r < b) = 

4r 3dr = 1 

4r 3dr 

dθ =

∞ 

∞

∞ 

1 
e−x 
2 
G(x) = 
M 
� 
as  a  probability  density,  that  is, M  is  the  constant  we  need  in  order  that 
∞ 
−∞ 

G(x)dx = 1 

∞ 

M 2  = 

� 
The  function  G(x)  is  the  well­known  bell  curve  or  normal  distribution. 
∞ 
e−x  dx,  rewrite M 2  in  a  clever  way,  as  in  lecture: 
2
To  compute M  = 
� �
� �� 
�
�� 
�� 
−∞ 
∞ 
� 
� 
� 
�
e−x 2 
e−y  dy  = 
e−x  dx  e−y  dy 
2 
2 
2
dx 
−∞ 
−∞ 
−∞ 
−∞
∞ ∞ 
∞ ∞ 
�  2π  1 
�  2π  ∞ 
−∞ � −∞ 
e−x  e−y  dxdy = 
e−x  −y  dxdy 
2
2 
2 
2
−∞  −∞
e−r  rdrdθ = 
2 
dθ = π
2
0 
0 
0
√
� 
π .  In  all,  we  have 
∞  1 
√
−∞ 
π

Therefore, M 2  = π  and M  = 

1 
G(x) =  √
π

e−x  dx = 1 
2 

e−x  ; 
2 

= 

= 

The importance of this function to probability was discovered by Abraham de Moivre around 
1700.  We  have  used  the  name  G(x)  in  honor  of  Karl  Friedrich  Gauss,  who  laid  the  foun­
dations  of  probability  theory  (along  with  the  method  of  least  squares)  in  the  process  of 

2 

ﬁnding  better  ways  to  make  use  of  measurements  in  astronomy.  Any  function  of  the  form 
e−ax  +bx+c  is  called  a  Gaussian. 
2 
In  order  for  you  to  recognize  the  normal  distribution  in  the  future,  you  will  need  to 
recognize  all  its  scalings,  related  to  the  parameters  a,  b  and  c  above.  The  scaling  of  the 
Gaussian  will  be  discussed  in  the  optional  section  at  the  end  of  these  Notes.  That  section 
is  not  necessary  for  18.02,  but  it will  give  you  a  brief  look  at  some  tools  and  terminology  in 
the  theory  of  probability. 

Conditional  probability 
To  choose  0 ≤ x ≤ 1  and  0 ≤ y ≤ 1  independently  “at  random” means 

dxdy = 

P ((x, y)  in  R) = area(R);  R  in  unit  square 
Thus  P (x ≥ 1/2) = 1/2.  But  the  probability  changes  when  we  add  information: 
P (x ≥ 1/2  |  xy = 1/1000) =? 
This notation means  the probability  that x ≥ 1/2 given  that we already know xy = 1/1000. 
It  is  known  as  a  conditional  probability. 
Computing  conditional  probabilities  of  this  kind  is  very  closely  related  to  computing 
integrals using a  change of variable.  The  conditional probability density  turns out  to be  the 
Jacobian  factor,  renormalized  so  that  the  total  probability  is  one. 
�  1 �  1 
�  1 �  1  du 
Recall  that  if  u = x  and  v = xy ,  then  on  Thursday  we  showed  that 
dv 
v  u 
0 
0 
0
Note  that  the  interesting  parts  of  this  calculation  are  that  the  Jacobian  J  =  1/u  and  that 
the range of u with v  ﬁxed  is v ≤ u ≤ 1.  Consider any ﬁxed value xy = v = v0 , and consider 
�  1  1 
the  inner  integral 
du 
uv0 
The  idea  is  that  if  xy  =  v0 ,  then  v0  ≤  u ≤  1  is  the  full  range  for  u =  x  and  that  Jacobian 
�  1  1 
factor  1/u  is  the  probability  density  on  that  interval.  We  need  to  need  to  normalize  by  this 
total mass. 
du = − ln(v0 ) 
M  = 
uv0 
For  xy  =  v0  ﬁxed,  this  means  that  x >  1  and  x <  v0  never  happen.  In  between,  v0  ≤  a < 
� b  du 
b ≤ 1, 
part 
P (a ≤ x ≤ b  |  xy = v0 ) = 
a u
whole 
M 
� 1  du 
Therefore,  with  a = 1/2,  b = 1,  v0  = 1/210  = 1/1024  (close  enough  to  1/1000) 
P (x ≥ 1/2  |  xy = 1/210 ) =  �  1/2  u
− ln(1/2) 
du  =  − ln(1/210 )
1/210  u 

ln 2  = 
10 ln 2 

= 

= 

1 
10 

Why  this  formula  works.  There  is  a  diﬃculty  with  ﬁxing  xy  =  v0 .  The  condition 
conﬁnes  us  to  a  single  curve,  which  has  zero  area  and  hence  zero  probability.  So  when  we 

3 

Therefore, 

dv 

du 
u

du
u

divide  the part by  the whole, we are dividing zero by zero.  To  repair  this diﬃculty,  consider 
a small  interval v0  ≤ v ≤ v0 + Δv , carry out the computation of the ratio, and take the  limit 
as Δv  tends  to  zero.  This  is  the  correct  way  of  thinking  about  what  it means  to  know  that 
xy = .001 because any computation that reports the value  .001 does  it with a roundoﬀ error 
Δv .  For  practical  purposes,  the  limit  of  the  ratio  as  Δv  tends  to  zero  is  the  same  as  the 
ratio  at  any  ﬁxed  band  of  values  of  v  corresponding  to  ±Δv  = ±10−6  or  smaller.  (Matlab 
is  accurate  to  15  digits.) 
�  v0 +Δv  �  1  du
Consider  the  area  of  the  whole  region, 
area(v0  < xy < v0  + Δv) = 
u
v 
v0 
�  1
�  1 
On  a  very  short  interval  v0  ≤ v ≤ v0  + Δv ,  the  inner  integral  is  nearly  constant: 
du 
du 
≈ 
u 
u
�  1
�  v0 +Δv  �  1 
�  v0 +Δv  �  1  du 
v0 
v
du 
dv ≈ 
dv = Δv 
u 
u
v0 
v0 
v0 
v0 
v
�  v0 +Δv  �  b 
Similarly,  for  v0  + Δv ≤ a < b ≤ 1,  the  area  of  the  part  is 
v0 
a
� b  du 
So  the  factor Δv  cancels  in  the  ratio  of  the  part  to  the  whole, 
� 1  du 
Δv  a u 
Δv 
� b  du 
v0  u
P (a < x < b  |  v = v0 ) =  � 1  du 
a u
v0  u 
There  are  pitfalls  in  dealing  with  zero  area  sets  like  xy  =  v0 .  For  example,  one  may 
be  tempted  to  use  the  ratio  of  the  arclength  of  the  curve  on  the  portion  a  <  x  <  b  to  the 
total  arclength  of  xy  =  v0 .  This  gives  the  wrong  answer.  The  reason  is  that  the  band 
v0  ≤ xy ≤ v0 + Δv  does  not  have  uniform  thickness.  If  it  did,  then  the weighting  or  density 
would  be  equivalent  to  arclength.  Put  another  way,  if  one  tries  to  partition  the  square 
into  subsets  of  uniform  thickness  around  curves  of  the  form  xy  =  v0  this  cannot  be  done 
without  overlapping  bands  or  bands  that  miss  substantial  sections.  By  contrast,  the  bands 
v0  ≤ xy ≤ v0 + Δv  are  compatible with  partitioning without  gaps  or  overlaps:  use  pieces  of 
the  form  kΔv ≤ xy ≤ (k + 1)Δv ,  k = 0, 1, . . . . 

�  b
du 
dv = Δv 
u
a 
� b  du
� 1  du
a u
v0  u 

area(a < x < b  and  v0  < xy < v0  + Δv) = 

P (a < x < b  v0  ≤ v ≤ v0  + Δv) ≈ 
| 

= 

In  the  limit  as  Δv → 0, 

4 

Expected  value,  variance,  and  standard  deviation: 

Rescaling  the  normal  distribution.


This  section  is  optional  reading.  It  introduces  a  few  standard  notions  and  terminology 
�  b 
from  probability  theory.  Recall  that  if  a  variable  x  is  distributed  with  a  density  g(x),  then 
P (a < x < b) = 
a 
� 
Since  the  total  integral  is  1,  the  average  value  or  mean  of  x  is 
∞ 
−∞ 

g(x)dx,  g(x) ≥ 0 

xg(x)dx 

µ = 

E (X ) = 

xg(x)dx = µ 

(µ  for  mean).  In  probability,  the  upper  case  letter  X  denotes  something  called  a  random 
variable,  which  can  be  viewed  as  a  quantity  that  will  vary  depending  on  each  sampling  of 
the  variable.1  The  mean  or  expected  value  of  X ,  E (X ),  is  a  theoretical  value  for  what  one 
would  expect  to  get  if  one  averaged  over  several  samples  of  the  variable  X .  This  quantity 
� 
is  the  same  as  the  average  value  or mean  value  of  x, 
∞ 
−∞ 
� 
One  can  take  the  expected  value  of  any  function  of  f (X ).  The  formula  is 
∞ 
−∞ 
Again,  this  is  a  weighted  average  of  f .  The  expected  value  of  X  is  just  the  special  case 
f (X )  = X .  It  is  also  interesting  to  evaluate  the  expectation  of  functions  like  f (X )  = X k , 
called  the  kth moments  of  X ,  and  f (X ) = e
. 
tX
The  variance  V (X )  is  a  measure  of  the  likelihood  that  X  is  far  from  its  mean.  It  is  the 
� 
average  of  the  square  of  the  distance  from  X  to  its  mean,  (x − µ)2 , 
∞ 
(x − µ)2 g(x)dx 
−∞ 

V (X ) = E ((X − µ)2 ) = 

E (f (X )) = 

f (x)g(x)dx 

Because  the  variance  involves  the  square  of  distance,  it  is  natural  to  take  a  square  root  (as 
� 
in  the  Pythagorean  theorem).  The  standard  deviation  is  deﬁned  by 
σ(X ) =  V (X ) 

We can now explain the scaling of the normal distribution.  There is a probability density 
for  each  σ > 0, 

It  has  three  properties: 

� 
−∞ 
1For  instance,  in  Matlab  the  command  rand(3,4)  produces  a  3 ×  4  matrix  with  each  element  chosen 
uniformly  distributed  between  0  and  1.  The  entries  are  also  independent  of  each  other.  Independence  is  a 
key  concept  in  probability.  But  it  would  take  us  too  far  aﬁeld  to  discuss  it  in  any  detail. 

gσ (x)dx = 1  (total  integral  1), 

(1) 

∞ 

gσ (x) =  √

1 
2
πσ

e−x 2 /2σ2 

5 

∞ 

� 
−∞

∞ 

� 
−∞ 
To  shift  the  mean  to  µ,  take 

xgσ (x)dx = 0  (mean  0), 

2
x gσ (x)dx = σ2 

(variance  σ2 ). 

(2) 

(3) 

π 

π 

√

1
2σ2 , 

√

gσ (x − µ) = 

1 
e−(x−µ)2 /2σ2 
2πσ 
√
The  standard  deviation  σ  or  the  variance  σ2  measures  how  far  the  distribution  is  from  its 
mean  value.  For  larger  σ ,  gσ (x)  is  ﬂatter  and  has  a  smaller  maximum  value  1/
2πσ .  In 
other  words,  it  is  more  weighted  towards  larger  values,  and  X 2  is  more  likely  to  take  on 
larger  values.  The  formula  for  the  variance  above  is  an  exact,  quantitative  expression  of 
this qualitative comparison between the shapes of the graphs of the densities gσ  for diﬀerent 
values  of  σ . 
√
1 
The  function  G(x)  =  √
e−x  considered  in  Example  3  above  equals  gσ  for  σ = 1/
2
√
π
Thus G is the normal distribution with mean 0 and standard deviation 1/
2 (variance 1/2). 
� 
To  conﬁrm  (1),  (2),  and  (3),  recall  that  we  already  showed  that 
√
∞ 
e−x 2 
dx = 
−∞ 
� 
Change  variables  by  x = az ,  dx = adz ,  to  get 
∞ 
e−a 
2 2 z 
adz = 
−∞ 

√

2. 

Putting  a  =
2

e−z 2 /2σ2  √1
2σ

� 
−∞ 
π  gives  (1). 
and  dividing  by 
√
� 
2σ  to  obtain 
Next, multiply  (4)  by 
∞ 
e−z 2 /2σ2 
dz = 
−∞ 
� 
� 
� 
Diﬀerentiating  the  left  side  of  (5)  with  respect  to  σ  gives 
∞  z
d  ∞ 
∞  ∂
e−z 2 /2σ2 
e−z 2 /2σ2 
e−z 2 /2σ2 
dz = 
dz = 
dz 
σ3 
dσ 
−∞ 
−∞ 
−∞
∂σ
� 
On  the  other  hand  the  derivative  of  right  hand  side  of  (5) with  respect  to  σ  is 
√
2∞  z
e−z 2 /2σ2 
dz = 
σ3 
−∞ 
2π  and multiplying  by  σ2  yields  (3). 
Dividing  by 
Finally,  (2)  is  obvious  because  the  integrand  is  odd.  To  conﬁrm  that  the  mean  is  µ  for 
� 
� 
� 
� 
the  density  gσ (x − µ),  change  variables  to  z = x − µ.  Then,  using  (2)  and  (1), 
∞
∞ 
∞
∞ 
xgσ (x − µ)dx =
(z + µ)gσ (z )dz = 
−∞ 
−∞ 
−∞
−∞ 

gσ (z )dz = 0 + µ = µ 

zgσ (z )dz + µ

2π .  Hence, 

dz = 

π 

(4) 

√

2πσ 

(5) 

2

√

2π 

∞ 

√

√

6 

