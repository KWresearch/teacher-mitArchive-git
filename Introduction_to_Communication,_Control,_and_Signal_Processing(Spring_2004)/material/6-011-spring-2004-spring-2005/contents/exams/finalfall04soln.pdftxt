Massachusetts Institute of Technology
Department of Electrical Engineering and Computer Science

6.011: Introduction to Communication, Control
and Signal Processing

Fall 2004 Final Exam SOLUTIONS

YOUR NAME:

Recitation Hour:

• This is a closed book exam, but you may use FOUR 8 1
2 ” × 11” sheets of notes (both sides).
Calculators are not allowed.

• The questions are in two parts. Part I, worth 60% of the exam, comprises several relatively
short questions, which should require only somewhat brief calculations and explanations. We
estimate that this will take you around 100 minutes. Part II comprises two longer problems,
which are worth 20% each, and which we estimate will take you around 40 minutes each.

• We would rather see you do 80% of the exam quite well than 100% of the exam quite poorly!

• Be clear and precise in your reasoning and show all relevant work.

• If we can’t read it, we can’t/won’t grade it! So please write neatly and legibly.

• You are to hand in only this ANSWER booklet. No additional pages will be
considered in the grading. You may want to ﬁrst work things through in the blank areas
of the question booklet or on scratch paper, and then neatly transfer to thisr booklet the
work you would like us to look at. Let us know if you need additional scratch paper.

Your Score

Problem

1 (8 points)

2 (8 points)

3 (8 points)

4 (8 points)

5 (10 points)

6 (8 points)

7 (10 points)

8 (20 points)

9 (20 points)

Total (100 points)

1

Problem 1 (8 points)

The voltage waveform v(t) between the red terminal and ground of a power supply in your
lab is equally likely to be either +5V for all time t, or −5V for all time t, because the power
supply is equally likely to have been manufactured (in the distant past) by the Duraplus or
Everminus companies. For this random process v(t), determine: (i) the mean value of the
process; (ii) its autocorrelation function; (iii) its autocovariance function; (iv) whether it is
wide-sense stationary; (v) whether it is strict-sense stationary; and (vi) whether it is ergodic
in mean value.

2 (5) + 1
(i) E [v(t)] = µv = 1
2 (−5) = 0
(ii) E [v(t + τ )v(t)] = 1
2 (5)(5) + 1
2 (−5)(−5) = 25
(iii) E [(v(t + τ ) − µv )(v(t) − µv )] = 1
2 (5)(5) + 1
2 (−5)(−5) = 25
(iv) YES, it is WSS: Since the mean and autocorrelation are constant, this process is WSS.

(v) YES, it is SSS: Since the time origin is irrelevant to the joint densities of samples, this
process is SSS. In other words, the probabilistic descriptions are time invariant.
(vi) NO, it is not ergodic. The time average is either 5 for all time, with probability 1
2 , or
−5 for all time, with probability 1
2 , while the ensemble average is 0.

(Continue this problem on next side =⇒=⇒)

2

Problem 1 (continued)

(i) Mean = 0

(ii) Autocorrelation function = 25

(iii) Autocovariance function = 25

(iv) Is the process wide-sense stationary? YES

(v) Is the process strict-sense stationary? YES

(vi) Is the process ergodic in mean value? NO

3

Problem 2 (8 points)

Which of the following are valid autocorrelation functions for a continuous-time wide-sense
stationary random process x(t)? Brieﬂy justify your answers. For each case that represents
a valid autocorrelation function, determine the mean value of the process (explaining your
reasoning!), and also compute E [x2 (5)].

(a) Rxx(τ ) = 2, for |τ | < 2, and is zero elsewhere:

The PSD Sxx(jω) is a sinc function (the ampltude and frequency scaling is irrelevant), which
goes negative, making it an invalid PSD for a WSS process.

Is this Rxx(τ ) a valid autocorrelation function? (Justify your answer.)

NO - see explanation above

If it is, what is E [x(t)]? (Explain your reasoning.)

And if it is, then E [x2 (5)] =

(Continue this problem on next side =⇒=⇒)

4

Problem 2 (continued)

(b) Rxx(τ ) = 2 − |τ |, for |τ | < 2, and is zero elsewhere:

The PSD Sxx (jω) = sinc2 , which is real, even, and non-negative for all ω , making it a valid
PSD. This process can be generated by passing white noise through a frequency response of
√
sinc2 .

Is this Rxx(τ ) a valid autocorrelation function? (Justify your answer.)

YES - see explanation above

If it is, what is E [x(t)]? (Explain your reasoning.)

The mean is 0. Proof by contradiction: Assume the mean is not equal to 0, then the trans-
form of the autocovariance would be Dxx (jω) = Sxx (jω) − µ2
xδ(ω). For this to be a valid
autocorrelation, both the PSD and the transform of the autocovariance must be non-negative
for all ω . Since Sxx does not have an impulse at DC to cancel with µ2
x , Dxx would be negative
at DC. This violates our criteria, hence there is a contradiction!

And if it is, then E [x2 (5)] = Rxx (0) = 2

(Continue this problem on next side =⇒=⇒)

5

Problem 2 (continued further)

(c) Rxx(τ ) =

sin(πτ )
πτ

.

The PSD Sxx is a box, which is real, even and non-negative for all ω . Thus this is also a valid
PSD.

Is this Rxx(τ ) a valid autocorrelation function? (Justify your answer.)

YES - see explanation above.

If it is, what is E [x(t)]? (Explain your reasoning.)

The mean is again 0. See the explanation for part b.

And if it is, then E [x2 (5)] = Rxx (0) = 1

6

Problem 3 (8 points)

Suppose v(t) is a CT random process with power spectral density Svv (jω) having the trian-
gular shape shown in the ﬁgure below (so the PSD is ﬁnite at ω = 0). Is it possible to ﬁnd a
T such that the sequence v [n] = v(nT ) is a DT white noise process? If so, determine T and
clearly explain your reasoning; if not, explain why not. (Caution: It is unlikely that your
reasoning is complete if it doesn’t deal with the relation between the autocorrelation function
Rvv (τ ) of a CT process and the autocorrelation function Rvv [m] of the DT process obtained
by sampling!)


Svv (jω)

−2π × 103

-
2π × 103

ω

YES, it is possible.

The PSD shown can also be expressed as the convolution of a box with itself, where the box
is constant over the −π × 103 ≤ ω ≤ π × 103 and 0 otherwise. Therefore the autocorrelation
π×103 = 10−3 . If we sample v(t) every
of the process is a sinc2 , with zero crossings at T = π
T seconds, the autocorrelation of v [n] will be Rvv [m] = Rvv (mT ) = αδ [m], where α is some
constant. The discrete time process is therefore white noise.

(Continue this problem on next side =⇒=⇒)

7

Problem 3 (continued)

Is there a T such that v [n] = v(nT ) is a DT white noise process?

If so, T =

Explanation of whether or not there is a T :

8

y(t)
(cid:1)y(t)

- g(t)

- ?

z (t)
(cid:1)z (t)

-

-

x(t)

- h(t)

Problem 4 (8 points)
Suppose that the random processes x( · ) and y( · ) are zero-mean and jointly wide-sense sta-
estimate (cid:1)y(t) of y(t) for each t. If we denote the impulse response of this Wiener ﬁlter by
tionary, and that you have already designed a non-causal Wiener ﬁlter to use measurements
h( · ), then (cid:1)y(t) = h ∗ x(t), as shown in the ﬁgure here.
of x( · ) for all time in order to produce the linear minimum mean-square error (LMMSE)
Suppose you have computed and stored (cid:1)y(t) for all t but have thrown away the original
ﬁnd the LMMSE estimate (cid:1)z (t) of another process z (t) using the (now discarded) measurements
measurements x( · ). Now you regret having done that, because you realize you also wanted to
x( · ). What saves you, however, is that the z (t) you are interested in happens to be simply a
ﬁltered version of the process y( · ) that you originally estimated: z (t) = g ∗ y(t), where g( · )
Show that you can actually compute (cid:1)z (t) by appropriate LTI ﬁltering of (cid:1)y( · ). In other words,
specify the impulse response of an LTI ﬁlter that will take (cid:1)y( · ) as input and produce (cid:1)z (t) as
is the known impulse response of a stable system.
output. Explicitly verify for this choice that the orthogonality principle is satisﬁed, i.e., that
R(cid:1)zx (τ ) = Rzx(τ ) for all τ .

Guess g(t) is the ﬁlter.

Now verify that this satisﬁes the orthogonality principle:

R ˆzx (τ ) = g ∗ R ˆyx (τ )
g ∗ R ˆyx (τ ) = g ∗ Ryx (τ ) since R ˆyx = Ryx due to the fact that h(t) is optimal
g ∗ Ryx (τ ) = Rzx (τ )

(Continue this problem on next side =⇒=⇒)

9

Problem 4 (continued)

Impulse response of an LTI ﬁlter that will take (cid:1)y( · ) as input and produce (cid:1)z (t) as output:

Explicit veriﬁcation that the orthogonality principle is satisﬁed, i.e., that R(cid:1)zx (τ ) = Rzx(τ )
for all τ :

10

Problem 5 (10 points)

(i) (4 points) Suppose T1 = T2 = T in the ﬁgure below. Determine all values of T for which
the overall system from xd [n] to yd [n] is linear and time-invariant (LTI), and for these
values of T write down the overall transfer function Hd (ejΩ ) (speciﬁed for |Ω| < π),
expressing the transfer function in terms of Hc(jω) and T .

xd[n]

- D/C

- Hc(jω)

- C/D

-

yd [n]


T1


T2

We are assuming we have an ideal D/C converter. The output of the D/C converter,
xc (t), is always bandlimited to |ω | = π
T . As a result, the input to the C/D is always
appropriately bandlimited, and no aliasing occurs.

Yd (ejΩ ) =

Yc jω =

Yd (ejΩ ) =

1
T

1
1
T
T
Hc (jω)T Xd (ejΩ ) = Hc(jω)Xd (ejΩ )|ω= Ω
T

Hc(jω)Xc (jω)

,|Ω|<π

Values of T for which the overall system is LTI: ALL T

Overall transfer function Hd (ejΩ ) for these values of T (specify for |Ω| < π):

Hc(j

Ω
T

) for |Ω| < π

(Continue this problem on next side =⇒=⇒)

11

Problem 5 (continued)
(ii) (6 points) Suppose T1 = 1, T2 = 2, Hc(jω) = e−j 3ω for all ω , and xd [n] = cos(0.2n).
What is yd [n]?

xd[n]

- D/C

- Hc(jω)

- C/D

-

yd [n]


T1


T2

xc (nT ) = xd [n] = cos[0.2n]

xc (t) = cos(0.2t)
yc(t) = cos(0.2(t − 3)) = cos(0.2t − 0.6)
yd [n] = cos[0.2(2n) − 0.6] = cos[.4n − 0.6]

yd [n] = cos[.4n − 0.6]

12

Problem 6 (8 points)

(cid:2)
Consider the PAM communication system shown in the problem statement, where the LTI
channel has frequency response
1 for |ω | < 2π × 103
0 otherwise.

Hch(jω) =

(1)

Let P (jω) denote the CTFT of the basic pulse p(t) used for the PAM system. Determine
the smallest T for which the P (jω) shown will yield b[n] = c a[n], where c is a constant, and
determine c.

P (jω)


1 × 10−3

2 × 10−3
1

(cid:27)
−4π × 103 −3π × 103

−2π × 103

2π × 103

3π × 103

-
ω
4π × 103

First, we must examine how the channel aﬀects the pulse.

Hch(jω)P (jω)


1 × 10−3

2 × 10−3
1

(cid:27)

-

ω

−2π × 103

2π × 103

(Continue this problem on next side =⇒=⇒)

13

Problem 6 (continued)

This has to satisfy the Nyquist zero-ISI condition:

Thus the smallest T is given by:

2π
T

= 2π × 103

−3
T = 10

The Nyquist condition gives:

Integer multiples of T would also work, but the value given above is the smallest.
(cid:5)(cid:5)
(cid:4)
(cid:4)
∞(cid:3)
−∞

ω − 2πn
T

G

j

Evaluating this at ω = 0 shows that c = 1.

Smallest T for which b[n] = c a[n]: 10−3

c = 1

14

Problem 7 (10 points)

Assume we have to decide between hypotheses H0 and H1 based on a measured random
variable X . The conditional densities for H0 and H1 are given below. Fully specify the
decision rule that yields minimum probability of error, assuming the prior probabilities of
the hypotheses satisfy P (H1 ) = 2P (H0 ). Also compute P ((cid:5)H (cid:5)
1 | H0 ) and P ((cid:5)H (cid:5)
1 | H1 ) for this
decision rule (where (cid:5)H (cid:5)
i denotes the event that we decide in favor of Hi ).

fX |H (x|H0 )


1

fX |H (x|H1 )



1/4

−1

1

x
-

−2

x

-
2

We scale the densities by the appropriate probability, and sketch both on the same plot.

P (H0 ) =

1
3

, P (H1 ) =

2
3


1
3

−2

−1

1

1
6

-
2

Thus, we pick our decision rule to be:

‘H0 ’ for |x| < 0.5, ‘H1 ’ othwerwise

(Continue this problem on next side =⇒=⇒)

15

Problem 7 (continued)

We must now ﬁnd the probability of false alarm and the probability of detection.

PF A = P (‘H1 ’|H0 ) = (

1
2

)(

1
2

) =

PD = P (‘H1 ’|H1 ) = (

3
2

)(

1
4

)2 =

1
4
3
4

Decision rule that yields minimum probability of error, assuming P (H1 ) = 2P (H0 ):

P ((cid:5)H (cid:5)
1 | H0 ) =

P ((cid:5)H (cid:5)
1 | H1 ) =

16

PART II

Problem 8 (20 points)

Consider a causal, discrete-time LTI system with input x[n] and output y [n], described by a
second-order state-space model of the form

q[n + 1] = Aq[n] + bx[n]
y [n] = cT q[n] + dx[n]

(so A is a 2 × 2 matrix). You are given the following facts:

Fact 1: With x[n] = 0 for all n ≥ 0 and with q[0] set at some particular value, the output for
n ≥ 0 is

y [n] = 4 (0.5)n + 2 (−0.4)n .

Fact 2: When x[n] for −∞ < n < ∞ is a stationary random process that is equally likely to
take the value +1 or −1 at each instant, independently of the values it takes at other
instants, we ﬁnd that

Ryx [m] = E {y [n + m]x[n]} = δ [m] + 4 (0.5)m−1

u[m − 1]

where E { · } denotes the expected value, δ [m] is the unit sample function, and u[m] is
the unit step function.

(a) What are the eigenvalues of A ? Is the system asymptotically stable ?

We can ﬁnd the eigenvalues using fact 1 and the nature of the modal solution:

λ1 = 0.5, λ2 = −0.4

YES, the system is asymptotically stable, due to the fact that both eigenvalues have magni-
tude less than 1.

Eigenvalues of A are:

Is system asymptotically stable?

(Continue this problem on next side =⇒=⇒)

17

Problem 8 (continued)

(b) Is the system observable ? Explain.

YES, since both modes appear at the output, even for the ZIR.

2 + (−1) 1
(c) For x[n] in Fact 2, mean value µx = (1) 1
2 = 0

Auto-correlation function Rxx [m] = δ [m]

(Continue this problem on next side =⇒=⇒)

18

Problem 8 (continued further)

(d) Unit-sample response h[n] of the system from x[n] to y [n] (with q[0] = 0), and system
function H (z ).

Ryx [m] = h ∗ Rxx [m] = h ∗ δ [m] = h[m]
∞(cid:3)
1
h[m] = δ [m] + 4(
2
m=1

H (z ) = 1 +

u[m − 1]

)m−1

1
2

4(

)m−1

z

−m

H (z ) = 1 +

4z−1
1 − 0.5z−1 = 1 +

4
z − 0.5

=

z + 3.5
z − 0.5

2 )m−1u[m − 1]
h[n] = δ [m] + 4( 1

H (z ) = z+3.5
z−0.5

(Continue this problem on next side =⇒=⇒)

19

Problem 8 (continued still further)

(e) Is the system reachable ? Explain.

NO. There is a hidden mode at z = −0.4. Since both modes are observable, this mode must
be unreachable.

(f ) With x[n] = g1 q1 [n] + g2 q2 [n] for n ≥ 0, specify for each of the following expressions
whether or not it can be the output y [n] for n ≥ 1, for some choice of g1 , g2 and q[0] (and
explain your answer):

(i) 6 (1.2)n + (−0.4)n
(ii) 6 (0.2)n + (−0.4)n
(iii) 4 (0.5)n + 2 (0.8)n
(iv) 4 (1.2)n + 2 (0.8)n
(v) 7 + 4 (0.5)n + 2 (−0.4)n
(vi) 8 + 2 (−0.4)n
(vii) 3 (−0.4)n

(Use this page and the next two)

(Continue this problem on next side =⇒=⇒)

20

Problem 8 (winding down)

The closed loop system is now undriven by any external input, so we only see the ZIR, with
the new modes induced by state feedback. However, the mode at −0.4 is unreachable, and
hence cannot be moved by state feedback.

(i) Fine, reachable mode moved from 0.5 to 1.2

(ii) Fine, reachable mode moved from 0.5 to 0.2
(iii) Not possible, (−0.4)n needs to be one of the terms, if both are excited
(iv) Not possible, (−0.4)n needs to be one of the terms, if both are excited
(v) Not possible, an undriven second order system can only have 2 modes

(vi) Fine, reachable mode is moved to 1

(vii) Fine, reachable mode is either is either moved to 0 by state fedback or it is unexcited

(Continue this problem on next side =⇒=⇒)

21

Problem 8 (conclusion)

(Continue to last problem on next side =⇒=⇒)

22

Problem 9 (20 points)
Suppose you are trying to decide with minimum probability of error between two hypotheses,
H0 and H1 , given measurements of L random variables X1 , X2 , · · · , XL . Under H0 , al l of the
L measurements are governed by the ﬁrst set of relations below, while under H1 al l of them
are governed by the second set of relations:

H0

H1

:

:

Xi = Wi ,

Xi = Si + Wi ,

i = 1, 2, · · · , L
i = 1, 2, · · · , L

All the Wi and Si are zero-mean Gaussian random variables, and all are mutually independent,
but the Wi all have the same variance σ2
W , while the Si all have the same variance σ2
S . Denote
the prior probabilities of H0 and H1 by p0 and p1 (= 1 − p0 ) respectively.

(a) Under hypothesis H1 , is Xi Gaussian (where i is any ﬁxed integer between 1 and L)?
Justify your answer.

YES, the sum of any two INDEPENDENT Gaussian random variables is also a Gaussian
random variable (this can be proven by transform methods, for more information see 6.341)

(Continue this problem on next side =⇒=⇒)

23

Problem 9 (continued)
estimator by (cid:1)Si (Xi ). Also determine the associated minimum mean square error (MMSE).
(b) Under hypothesis H1 , determine the correlation coeﬃcient ρSiXi of Si and Xi , and the lin-
ear minimum-mean-square error (LMMSE) estimator for Si , given Xi — denote this LMMSE
Can your LMMSE estimator of Si under hypothesis H1 be improved by allowing it to use
measurements of not just Xi but also of X(cid:27) for 0 (cid:14)= i ?

µx = 0
2
E (SiXi ) = E (Si (Si + Wi )) = E (S
i + SiWi ) = E (S
(cid:6)
2
2
i ) = E ((Si + Wi )(Si + Wi )) = E (S
i ) + E (W
E (X
σ2
2
s + σ2
σ2
w

σSiXi
σSi σXi

ρSiXi =

σs

=

2
i ) + E (SiWi ) = σ
2
2
2
s + σ
i ) = σ
w

2
s

σ2
√
2
ρSiXi =
s +σ2
σ2
σs
w
(cid:1)Si (Xi ) = µx +
(cid:7)

MMSE = σ2
s

σs

ρSiXi
σx

(Xi ) =
(cid:8)

σ2
s
s +σ2
σ2
w

Xi

σ2
s σ2
w
s +σ2
σ2
w

1 − σ2
s
s +σ2
σ2
w

=

Can your LMMSE estimator of Si under hypothesis H1 be improved by also allowing mea-
surements of X(cid:27) for 0 (cid:14)= i ? Explain.

NO - Since all of the measurements are independent of Si and Wi and hence of each other,
they will yield no additional information.

(Continue this problem on next side =⇒=⇒)

24

Problem 9 (continued further)

(c) Suppose we only had a single measurement, say X1 = x1 . What is the optimal decision
rule?

p0fX1 |H (x1 |H0 )
1(cid:6)
2πσ2
w
(cid:5)

− x2
1
2σ2
e
w
(cid:4)

‘H0 ’
>
<
‘H1 ’

p0
p1

+

ln

1
2

w + σ2
σ2
s
σ2
w

p0
(cid:4)

Take logs:

Rewrite as:

ln

‘H0 ’
>
p1fX1 |H (x1 |H1 )
<
‘H1 ’
1(cid:6)
p1
w + σ2
2π(σ2
s )
(cid:4)
(cid:5) ‘H0 ’
>
<
‘H1 ’
(cid:4)

(cid:5)

(cid:4)

(cid:4)

1
σ2
w

x2
1
2

x2
1
w +σ2
2(σ2
s )

−
e

−

1
s + σ2
σ2
w

(cid:5)

(cid:5)(cid:5)

= γ

x1 ˆS1 (x1 ) =

σ2
s
s + σ2
σ2
w

2
x
i

‘H1 ’
>
<
‘H0 ’

2
w

σ

2 ln

+ ln

p0
p1

s + σ2
σ2
w
σ2
w

(Continue this problem on next side =⇒=⇒)

25

Problem 9 (continued still further)

(c) Optimal decision rule:

Rewrite the rule in a form that involves comparing the product x1
(this is trivial!):

(cid:1)S1 (x1 ) to a ﬁxed threshold

(Continue this problem on next side =⇒=⇒)

26

Problem 9 (winding down)

(d) What is the optimal decision rule in the case where we have measurements of all L random
variables, X1 = x1 , X2 = x2 , · · · , XL = xL?

p0fX|H (x|H0 )

p1fX|H (x|H1 )
(cid:9)
(cid:8)L
(cid:7)
Since we assumed the measurements are all independent, the joint densities above will factor
i will replace x2
x2
to the product of L individual gaussian PDFs. So,
1 from part c, and
σ2
w +σ2
σ2
w +σ2
s
s
from the threshold expression in part c. We thus get the
will replace
σ2
σ2
w
w
(cid:5)(cid:5)
(cid:4)
(cid:5)
(cid:4)
(cid:4)
following expression:
L(cid:3)
i=1

s + σ2
σ2
w
σ2
w

xi ˆSi (xi ) =

p0
p1

‘H1 ’
>
<
‘H0 ’

‘H0 ’
>
<
‘H1 ’

2
w

σ

2 ln

+ L ln

= γ

(Continue this problem on next side =⇒=⇒)

27

Problem 9 (conclusion)

(d) Optimal decision rule:

Rewrite the rule in a form that involves comparing
this rewriting is trivial.)

(cid:9)

(cid:1)Si (xi ) with a ﬁxed threshold. (Again,

xi

28

