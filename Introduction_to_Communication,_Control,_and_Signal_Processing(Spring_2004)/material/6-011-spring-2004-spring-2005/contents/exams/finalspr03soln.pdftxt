Massachusetts  Institute  of  Technology 
Department  of  Electrical  Engineering  and  Computer  Science 
6.011:  Introduction  to  Communication, 
Control,  and  Signal  Processing 

Final  Exam  —  Question  Booklet 

Tuesday,  May  20,  2003 

Your Full  Name: 

Recitation Instructor &  Time: 

•	 Write  your  name  in  the  space  above,  because  we  will  be  collecting  your  question  booklet  in 
addition to you answer booklet but will return it to you when you collect your answer booklet. 
•	 This  is  a  closed  book  exam,  but  you  may  use  6  sheets  of  notes  (both  sides). 
•	 Calculators  are  allowed,  but  probably  will  not  be  useful. 
•	 There  are  ﬁve   problems  on  the  exam,  with  point  distributions  of  15,   20,  25,   15,  25, for 
a  total  of  100  points.  We  have  also  marked  for  each  problem  a  rough  guide  to  the  number 
of minutes  you would  spend  on  the  problem  if  you were  dividing  your  time  according  to  the 
points  on  the  problem.  The  time  you  choose  to  spend  may  diﬀer,  of  course,  based  on  which 
problems  you  are  more  or  less  comfortable  with.  However,  budget  your  time  carefully; 
if  you  are  stuck  on  a  problem,  try  and  move  on  to  another  part  or  another  problem,  then 
return  if  you  have  time  later.  Avoid  spending  inordinately  longer  than  the  prorated  time  on 
any  particular  problem. 
•	 Be  clear  and  precise  in  your  reasoning  and  show   all   relevant   work.   The  grade  on  each 
problem  is  based  on  our  best  assessment  of  your  level  of  understanding,  as  reﬂected  by  what 
you  have written.  A  correct ﬁnal  answer  does not  guarantee  full  credit;  nor does  an  incorrect 
ﬁnal  answer  necessarily mean  loss  of  all  credit. 
•	 If  we  can’t  read   it,  we  can’t/won’t  grade   it!   So  please  write  neatly  and  legibly. 
•	 You  are  to hand  in  only  the  ANSWER  booklet  that  is  supplied.  No  additional   pages  
will  be  considered  in  the  grading.  You  may  want  to  ﬁrst  work  things  through  in  the 
blank  areas  of  this  question  booklet  or  on  scratch  paper,  and  then  neatly  transfer  to  the 
answer  booklet  the  work  you  would  like  us  to  look  at.  Let  us  know  if  you  need  additional 
scratch  paper. 

Problem  1  [15   points,   20   minutes] 

Provide brief answers, with appropriate  justiﬁcations or  computations,  for  each of  the  following 
parts. 

(a)  Can a  causal LTI  state-space  system  that  is not  asymptotically  stable become asymptotically 
stable  under  appropriate  LTI  state  feedback  even  if  some  of  its modes  are  unreachable? 

Answer:  Yes,  as  long  as  unreachable  modes  are  asymptotically  stable.  All  reachable  modes 
can  be  shifted  by  state  feedback. 

(b)  Consider	 a  continuous-time  zero-mean  wide-sense-stationary  (WSS)  random  process  x(t) 
whose power spectral density Sxx (jω) is constant  at the value N0  > 0 for frequencies |ω | < ωm 
and  0  outside  this  band.  By  considering  the  autocorrelation  function  Rxx (τ )  of  this  process, 
one  can  see  that  for  an  appropriately  chosen T  the  samples  x(nT )  of  the  process  constitute  a 
discrete-time  zero-mean  WSS  white   process.  Determine  an  appropriate  T ,  and  specify  what 
the  variance  of  the  corresponding  x(nT ) is. 

ω 
(j  )
S 
XX 

ω  N omπ 

= 
XXR  (  ) 

N o 

τ 
m 

)

ω 
sin( 
τ 

o N 

ω 

m 

ω 

m 

ω 

ω 

ω 

m 

m

m 

τ 

m 

Figure  1:  The  power  spectral  density  and  autocorrelation  function  of  x(t). 

π
Answer:  Figure 1 shows the PSD and autocorrelation function of x(t).  Samples taken T  =  ωm 
apart,  or  at  any  integer  multiple  of  this,  have  zero  mean  and  are  uncorrelated.  Thus,  the 
variance  of  the  corresponding  samples  x(nT ) is  var [x(nT )] = Rxx (0) =  wnNo .
π 

(c)  Consider  a  minimum-error-probability  hypothesis  test  to  select  between  two  Gaussians  of 
the  same  variance  σ2  but  diﬀerent  means  µ1  and  µ2  > µ1 ,  based  on  a  single  measurement 
drawn  from  one  of  these  densities  with  equal  probability.  Use  the  standard Q(x)  function  to 
express  the  conditional  probability  of  correctly  detecting  the  Gaussian  of  mean  µ2 , and  also 
�  ∞ 
the  conditional  probability  of  falsely  detecting  this  Gaussian.  Recall  that 
Q(x) =  √ 
1 
−t2 /2 dt  . 
e 
2π  x 

π
/
ω
2
π
/
ω
π
−
−
2
π
/
−
π
/
τ
Answer:  The  optimum  test  is  a  threshold  test  on  the  measurement  for  this  case  of  2  equal-
variance  Gaussians.  Since  the  a  priori  probabilites  are  equal,  the  threshold  is  just  where  the 
µ2+µ1
.
conditional  densities  cross,  which  is  the  halfway  point  between  their  means: 
2 
The  conditional  probability  of  correctly  detecting  the Gaussian  of mean  µ2  can  be  expressed 
as: 

µ1 +µ2 
2 
PD  = Q( 

− µ2 ) = Q( 
µ1  − µ2 )
2σ

σ 
The  conditional  density  of  falsely  detecting  this  Gaussian  is:

− µ1 ) = Q( 
µ2  − µ1 )
2σ 
σ 

µ1+µ2 
2 
PF A  = Q( 

(d)  If  pulse-amplitude  modulation  is  used  to  communicate  a  general  discrete-time  (DT)  signal 
over  a  continuous-time  lowpass  channel whose  transmission  is bandlimited  to  |ω | < ωm , what 
is the highest symbol repetition rate that can be used before  information about the DT signal 
is  lost  in  the  process? 

Answer:  If  a[n]  is  the  sequence  to  be  transmitted,  p(t)  is  the  pulse  shape  used,  h(t)  is  the 
impulse  response  of  the  channel,  and  r(t)  is  the  received  signal, we  can  express  the  transform 
� � �

of  r(t) as: 
R(jω) = A(ejΩ ) 
P (jω)H (jω) 
Ω=ωT 
Since  A(ejΩ )  has  information  for  |Ω| < π ,  we  need  to  pass  frequencies  in  the  band  |ω | < 
π
T  , 
so  ωm  =  π 
Thus  the  highest  symbol  repetition  rate  that  can  be  used  without  any  loss  of 
.T 
1  =  ωm 
information  of  a[n] is

π  .

T

Problem  2  [20   points,   40   minutes] 

Suppose  the  zero-mean  wide-sense  stationary  (WSS)  process  x[n]  is  obtained  by  applying  a 
zero-mean WSS white  process w[n] with  power  spectral  density  (PSD) Sww (ejΩ ) = σ2  to  the  input 
of  a  (stable,  causal)  ﬁlter  with  system  function M (z ) = 1 − 3z
−1
. 
(a)  If Sxx (ejΩ ) denotes the PSD of x[n], ﬁnd Sxx (z ).  Also ﬁnd the autocovariance function Cxx [m] 
of the process x[n], the variance of the random variable x[n+ 1], and the correlation coeﬃcient 
ρ  between  x[n] and  x[n + 1]. 

Answer:	 Sxx (z )  can  be  expressed  as: 

−1 )Sww (z ) 
Sxx (z ) =  M (z )M (z 
=  σ2 (1 − 3z 
−1 )(1 − 3z ) 
=  σ2 (−3z + 10 − 3z 
−1 )

Taking  the  inverse  transform  of  Sxx (z )  produces  the  autocorrelation  function  Rxx [m].  We 
know  that  E (x[n])  =  0  since  E (w[n])  =  0;  this  gives  Cxx [m] =  Rxx [m].  Thus,  Cxx [m] = 
σ2 (−3δ [m + 1] + 10δ [m] − 3δ [m − 1]). 
The variance of x[n + 1]  is  the value of  the autocovariance  function  evaluated at a  lag of  zero. 
Thus,  var(x[n + 1]) = 10σ2 . 
Cxx [1]
The  correlation  coeﬃcient  ρ  between  x[n] and  x[n + 1]  can  be  expressed  as  ρ  =  Cxx [0]  = 
−3σ2  = −0.3.
10σ2 
(b)  Specify  the  linear  minimum  mean-square-error  (LMMSE)  estimator  of  x[n + 1]  based  on  a 
measurement of x[n], and compute  the associated mean-square-error  (MSE).  Is  this MSE  less 
than  the  variance  of  x[n + 1]  that  you  computed  in  (a)? 

Answer:  Since x[n + 1] and x[n] are both zero-mean and have the same variance, The LMMSE 
estimate  can  be  expressed  as: 

xˆ[n + 1]	 =  ρx[n] 
=  −0.3x[n] 

Thus,  the  associated MSE  is  just: 
M SE  =  E ((x[n + 1] − ˆx[n + 1])2 ) 
= 10σ2 (1 − ρ2 ) 
= 10σ2 (1 − 
9
100
= 9.1σ2 

) 

The  MSE  is  less  than  the  variance  computed  in  2(a)  since  we  have  taken  advantage  of  the 
knowledge  of  the  correlated  variable  x[n]. 

(c)  Find the system  function F (z ) of a stable and causal ﬁlter whose  inverse 1/F (z )  is also stable 
and  causal,  and  such  that 
−1 ) .
Sxx (z ) = F (z )F (z

Be  careful  with  this  part,  because  errors  here  will  propagate  to  the  next  part!


Answer:  From  the  result  in  part  (a),  we  can  factor  Sxx (z ) as: 
−1 )(1 − 3z ) 
Sxx (z ) =  σ2 (1 − 3z 
=  σ2 z (1 − 3z 
−1 (1 − 3z ) 
−1 )z 
=  σ2 (z − 3)(z 
−1  − 3) 
Having  written  it  in  this  form,  we  notice  that  it  has  poles  at  0  and ∞,  and  zeros  at  3  and  1 . 
3 
Picking F (z )  to  be  stable  and  causal  (and  have  a  stable  and  causal  inverse), we  need  its  pole 
3 . Thus,  F (z ) = ±σ(z
−1  − 3). 
at  0  and  its  zero  at  1 

(d)  Find  the  system  function  of  the  causal  Wiener  ﬁlter  that  generates  an  estimate  of  x[n + 1] 
based  on  al l   past   x[k ],  k  ≤  n,  i.e.,  ﬁnd  the  system  function  of  the  one-step  predictor.  Do 
you  expect  that  the MSE  for  this  case  will  be  less  than  or  equal  to  or  greater  than  what  you 
computed  in  (b)?  (You  don’t  need  to  actually  determine  the MSE;  we  are  only  interested  in 
what  your  intuition  is  for  the  situation.) 

Answer:  We  thus  choose  y [n] =  x[n + 1],  so  that  the  causal  Wiener  ﬁlter  will  produce  an 
estimate  ˆy [n]  based  on  all  past  values  of  x[n].  We  note  that: 

Ryx [m] =  E (y [n + m]x[n]) 
=  E (x[n + m + 1]x[n]) 
=  Rxx [m + 1] 

Taking  the  z-transform  of both  sides  results  in Syx (z ) = zSxx (z ).  The  transfer  function H (z ) 
� 
� 
of  the  causal Wiener  ﬁlter  is  given  by: 
Syx (z )
1 
F (z )  F (z−1 )  + 

H (z ) = 

H (z ) = 

�

�

Substituting  the  corresponding  values  results  in:

zSxx (z )
1

� 
�

−1 ) 
F (z )  F (z
+ 
−1  − 3) 
± zσ(z 
1 
=  ±σ(z−1  − 3) 
1 
z−1  − 3 
= 
H (z ) =  − 
1  z 
3 z −  1 
3 
Since  more  information  is  being  used  in  this  case  than  in  2(b),  we  can  expected  the  MSE  to 
be less.  We can recall from Equation (L10-8) from Lecture 10 that the MSE increases from its 
�
 � � � � 
� � � �

� 
�  π 
value  in  the  noncausal  case  (which,  for  a  prediction  problem  is  0!)  by  the  following  amount: 
2
Syx (ejΩ ) 
F ∗ (ejΩ ) 
�

�

−π 
− 
which  is  the  energy  of  the  strictly  anticausal  part.  In  this  case,  we  have  Syx (ejΩ )  = 3σz ,
F ∗ (ejΩ )  − 
or  3σejΩ  for  z  on  the  unit  circle  (which  is  what  the  integral  above  considers).  Hence,  we  get 
M M SE  = 9σ2 ,  which  is  not  a  big  improvement  over  9.1σ2  obtained  in  2(b). 

∆M M SE  = 

+ 

1
2π 

dΩ 

Problem  3  [25   points,   45   minutes] 

A  particular  causal  ﬁrst-order  discrete-time  LTI  system  is  governed  by  a  model  in  state-space 
form: 

q [n + 1] = 3q [n] + x[n] + d[n] 
where  x[n] is a  known  control  input  and  d[n] is an  unknown  wide-sense-stationary  (WSS),  zero-
construct  an  estimate  q�[n] of  q [n],  using  the  noisy  output  measurements 
d .
 We  would  like  to  use  an  observer   to 
mean,  white-noise  disturbance  input  with  E ( d2 [n] )  =  σ
2 
y [n] = 2q [n] + v [n] , 
where  the  measurement  noise  v [n] is also an  unknown  WSS,  zero-mean,  white-noise  process  with 
E ( v
2 [n] )
 =  σ
2 
v .
 Assume  the  measurement  noise  is  uncorrelated  with  the  system  disturbance: 
E ( v [n]d[k ] ) = 0  for  all  n,  k . 

(a)  Specify  which  of  the  following  equations  you  would  implement  as  your  (causal)  observer, 
(i)  q�[n + 1] = 3q�[n] + x[n] + d[n] − * ( y [n] − 2q�[n] − v [n] ) . 
explaining  your  reasoning.  In  each  case,  *  denotes  the  observer  gain. 
(ii)  q�[n + 1] = 3q�[n] + x[n] − * ( y [n] − 2q�[n] − v [n] ) . 
(iii)  q�[n + 1] = 3q�[n] + x[n] − * ( y [n] − 2q�[n] ) . 
(iv)  q�[n + 1] = 3q�[n] − * ( y [n] − 2q�[n] ) . 
(v)  q�[n + 1] = 3q�[n] − * ( y [n] − 2q�[n] − v [n] ) . 
(vi)  Something  other  than  the  above  (specify). 

Answer:  We  pick  choice  (iii),  since  we  do  not  know  d[n] nor  v [n],  but  know  x[n]. 
(b)  Obtain  a  state-space  model  for  the  observer  error,  q�[n] = q [n] − q�[n],  writing  it  in  the  form 
q�[n + 1] = α q�[n] + p[n] , 
statement  (but  with  p[n]  not  involving  q�[n],  of  course).  Check:  If  you  have  done  things 
with  α  and  p[n]  expressed  in  terms  of  the  parameters  and  signals  speciﬁed  in  the  problem 
correctly,  you  should  ﬁnd  that  α = 0  when  * = −
 3 
2 .


Answer:  Substituting  the  expression  for  q [n]  above  and  our  choice  for  the  observer  in  3(a), 
we  can  express  the  state-space  model  for  the  observer  error  as: 
q�[n + 1]	 =  3q�[n] + d[n] + *(2q�[n] + v [n]) 
= (3 + 2*)q�[n] + d[n] + *v [n] 
We  can  easily  see  that  α = 3 + 2*  and  p[n] = d[n] + *v [n].  Substituting  * = −
 3 
2 in  the  former 
gives  α = 0. 

response,  i.e.,  ﬁnd  the  system  function  and  corresponding  impulse  response  that  relate  q�[n] 
(c)  Determine  the  system  function  of  the  error  system  in  (b)  and  the  corresponding  impulse 
to  the  input  p[n]. 

Answer:  Taking  the  transform  of  both  sides  of  the  error  system,  we  get: 

zQ(z ) =  αQ(z ) + P (z ) 
Q(z )(z − α) =  P (z ) 
1 
Q(z )
z − α 
= 
H (z ) = 
P (z ) 
−1z
1 − αz−1 
We  can  see  that  the  system  is  causal.  Taking  the  inverse  Z-transform  of  the  system  function, 
we  get  the  impulse  response  as  h[n] = αn−1u[n − 1]. 

= 

(d)  Note  that  the  input  process  p[n]  in  (b)  is WSS  and  zero-mean.  Determine  its  autocovariance 
function  Cpp [m]  in  terms  of  parameters  speciﬁed  in  the  problem  statement. 

Answer:  We  recall  that  p[n] = d[n] + *v [n],  and  that  d[n] and  v [n]  are  uncorrelated.  Thus,  it 
directly  follows  from  evaluating  Cpp [m] = E (p[n + m]p[n])  that: 

Cpp [m] =  Cdd [m] + *2Cvv [m] 
=  δ [m](σ2 
d  + *2σv 
2 ) 
2 ,  determine  the mean E ( q�[n] )  of  the  observer  error,  its  second moment E ( q� [n] ), 
(e)  For  * = − 3 
2
and  its  variance. 
q�[n + 1] = p[n].  Solving  for  the  moments  of  q�[n]: 
Answer:  For  *  =  − 3 
2 ,  α  =  0.  Evaluating  the  error  model  at  this  value  of  α  reduces  it  to 
E (q�[n])  =  E (p[n − 1]) = E (d[n]) + *E (v [n]) = 0 + 0 = 0 
E (q� [n])  =  E (p  [n − 1]) = Cpp [0] = σ2 
9
2
d  + *2σ2  = σ2
2
d  +  σ2 
var(q�[n])  =  σ2 
v
v4 
9 
2 
d  +  σv 
4 
We  note  that  the  variance  is  the  same  as  the  second  moment  in  this  case  since  q�[n]  is  zero-
mean. 

observer error q�[n] is to be a zero-mean WSS process (assuming the observer has been running 
(f )  If  we  no  longer  ﬁx  *  to  have  the  value  speciﬁed  in  (e),  what  constraints  must  *  satisfy  if  the 
since  the  beginning  of  time,  i.e.,  starting  inﬁnitely  far  in  the  past) ?  Verify  that  the  choice  of 
*  in  (e)  satisﬁes  the  constraints  that  you  specify  here. 

Answer:  We  need  to  choose  *  such  that  the  error  system  is  stable.  That  is,  we  want  the 
(single)  eigenvalue  λ  to  satisfy  |λ| < 1, where  λ = α  for  this  error  system.  This  translates  to: 
−1 < 3 + 2* < 1 
−2 < * < −1 
We  note  that  * = − 3  satisﬁes  this  constraint. 
2 
been  running  since  the  beginning  of  time.  Find  a  general  expression  for  the mean E ( q�[n] )  of 
(g)  Assume  the  constraints  on  *  that  you  speciﬁed  in  (f )  are  satisﬁed  and  that  the  observer  has 
the  observer  error,  its  second  moment  E ( q� [n] ),  and  its  variance.  You  might  ﬁnd  it  helpful 
2
∞� 
to  recall  that 
1 
α2k  = 
1 − α2 
k=0 
process p[n] going  through a  stable LTI ﬁlter with unit  sample  response h[n] and output q�[n]. 
Answer:  With  *  satisfying  the  preceding  constraints,  what  we  have  is  a  zero-mean  WSS 
Hence,  it follows that the output is also zero-mean, that is, E (q�[n]) = 0.  Since it is zero-mean, 
its  second moment  and  variance  are  equal.  Solving  for  the  two: 
E (q� [n])  =  var(q�[n]) = Req [0] = h ∗ h ∗ Rpp [0]
� 
� 
¯ 
2
q e
� 
� 
∞
−2(k 1)
(σ2 
d  + *2σv 
2 )
= 
Σ

αk=1
∞ 2(k)αk=0
d  + *2σv 
(σ2 
2 )
Σ

(σ2  + *2σv 
2 )
d 
1 − α2 
(σ2  + *2σv 
2 )
d 
1 − (3 + 2*)2 
= 
2 , we get  var(q�[n]) = σ2  + *2σ2
We  can  check  that  for  * = − 3 
v ,  as  in  3(e). 
d

= 

.

=


(h)  Given  your  error  variance  expression  in  (g),  you  could  in  principle  try  and  choose  the  value 
of  *  that  minimizes   this  error  variance,  but  the  calculations  are  messy  in  the  general  case. 
Instead,  to  simply  get  an  idea  of  the  possibilities,  evaluate  your  expression  here  for  the  case 
2σd  =  0  and  *  =  − 4 
3 ,  and  show  that  the  error  variance  in  this  case  is  smaller  that  what  you 
get  (still  for  σd = 0)  with  the  earlier  choice  in  (e)  of  * = − 3 .2 
2
Answer:  Substituting  σd = 0  and  * = − 4 :3 
2
var(q�[n])  =

=


2(σd
2 2
+ * σv 
) 
1 − (3 + 2*)2
2 2
* σv 
1 − (3 + 2*)2 
16 
2σv 
9 
1 − (3 −  8 
3 )2 
2
= 2σv 

= 

For  the  case  where  σd = 0  and  * = − 3 :2 
2
var(q�[n])  = 
9 
2σv 
4
1 − (3 − 3)2 
2
= 2 25σ.
v 
Comparing  the  two,  we  can  see  that  the  variance  is  less  for  * = − 4 :3 
2
22σv 
2 25σ
< .
v 

Problem  4  [15   points,   25   minutes] 

A  discrete-time  LTI  system  has  frequency  response 
|Ω| < π . 
−jΩ/4
H (ejΩ ) = e
, 
(a)  Determine  the  phase  angle  ∠H (ejΩ )  of  the  system. 
Answer:  The phase  angle  is ∠H (ejΩ ) = − Ω  for  |Ω| < π ,  and  repeats periodically with period 
4 
2π  outside  this  interval. 

(b)  If h[n] denotes the unit-sample response of the system, evaluate the following three expressions 
∞� 
∞�
(you  should  be  able  to  do  this  without  ﬁrst  explicitly  computing  h[n]): 
(h[n])2  . 
n=−∞ 
n=−∞ 

h[n] ; 

h[0] ; 

= 

dΩ


π 
4

h[0] 

= 

Answer:  Given  the  system  function,  we  can  determine  the  value  for  h[0]  by  evaluating  the 
�  π
synthesis  equation  for  h[n] at  n = 0: 
1 
Ω−j
� � � �

e
4
2π  −pi 
−j
Ω π
1  e 
4
2π  − j 
4  −π 
−j+ e

π 
+j
4 e 
4 
2j 
π 
4 
π 
)
sin(
4
π 
h[n] by evaluating the analysis equation at Ω = 0: 

= 
�∞ 
= 
� 
Similarly, we can ﬁnd the value for 
n=−∞ 
∞
n=−∞ 
�∞ 
Finally,  we  can  use  Parseval’s  theorem  to  compute  for  the  energy  of  the  impulse  response  of 
�  π
� � �

� � �

� 
[n])2 : 
n=−∞ (h

the  ﬁlter, 
∞

1

−j 
2 
Ω
(h[n])2  =

dΩ
e
4
2π  −π
n=−∞ 
1 
= 
2π
= 1 

−j
[n] = H (ej 0 ) = e 
h

(2π)

0 
4

= 1

(c)  Determine  h[n],  and  check  that  your  answer  yields  the  value  of  h[0]  you  computed  in  (b).  Is 
the  system  causal  or  not? 

Answer:  The  system  is  a  quarter-sample  delay,  so  the  impulse  response  is:

sin(π(n −  1

4 ))
π(n −  1 
4 ) 
The  system  is  not  causal  since  h[n]  (cid:9)= 0  for  n <  0.  We  can  check  that  at  n  = 0,  h[0]  = 
sin(− −
π )  =  4 sin( π 
4 ),  which  is  the  same  as  in  4(b). 
4
π 
π 
4

h[n] = 

(d)  If  the  input  sequence  x[n]  to  the  above  system  is  given  by 
for  n  (cid:9)= 0  ,

x[n] = 2

sin(πn/2) 
πn 

x[0] = 1 , 

(1)

ﬁnd  a  simple  and  explicit  expression  for  the  output  y [n]  of  the  system.  (Just  expressing  y [n] 
via  a  convolution  sum  will  not  do!) 

Answer:  Again,  since  the  system  is  a  quarter-sample  delay,  the  output  y [n]  will  just  be: 
sin(π(n −  1 
4 )/2) 
π(n −  1 
4 ) 

y [n] = 2

Problem  5  [25   points,   45   minutes] 

A signal X [n]  that we will  be measuring  for n = 1, 2  is  known  to  be  generated  according  to  one 
of  the  following  two  hypotheses: 

Hno  :  X [n] =  W [n] 
Hyes  :  X [n] =  s[n] + W [n] 
where  s[1],  s[2]  are  speciﬁed  (deterministic)  numbers,  with  0  < s[i]  ≤  1 for  i  = 1, 2,  and  where 
W [1], W [2]  are  i.i.d.  random  variables  uniformly   distributed   in  the  interval  [−1, 1]  (and  hence with 
mean 0 and variance  1 
3 ).  Given measurements x[1] and x[2] of X [1] and X [2] respectively, we would 
like  to  decide  between  the  hypotheses  Hno  and Hyes . 
Most  of  part  (c)  below  can  be  done  independently  of  parts  (a)  and  (b). 

(a)  One  strategy  for  processing  the  measurements  is  to  only  look  at  a  linear  combination  of  the 
measurements,  of  the  form


r = g [1]x[1] + g [2]x[2] .

To  analyze  decision  schemes  that  are  based  on  consideration  of  the  number  r ,  consider  the 
random  variable 

R = g [1]X [1] + g [2]X [2] . 
Determine  the  mean  and  variance  of  R  under  each  of  the  hypotheses,  and  note  that  the 
variance  does  not  depend  on  which  hypothesis  applies.  (Note:  you  do  not  need  to  ﬁnd 
the  densities  of  R  under  the  two  hypotheses  in  order  to  ﬁnd  these  conditional  means  and 
variances!) 
Now  choose  g [1]  and  g [2]  to  maximize  the  relative  distance  between  these  means,  where 
“relative”  signiﬁes  that  the  distance  is  to  be  measured  relative  to  the  standard  deviation  of 
R  under  hypothesis Hno  (or,  equivalently,  under Hyes ).  Equivalently, maximize  the  following 
�2 
“signal-to-noise”  ratio  (SNR):  �	
E [R|Hyes ] − E [R|Hno ] 
variance(R|Hno ) 

. 

Answer:  Under Hno , we get  R = g [1]W [1] + g [2]W [2].  Thus,  it  follows  that: 

E (R|Hno ) =	 g [1]E (W [1]) + g [2]E (W [2]) = 0 
var(R|Hno ) = 
1 
(g 2 [1] + g 2 [2]) 
3

Under  Hyes , we have  R = g [1](s[1] + W [1]) + g [2](s[2] + W [2]).  Evaluating  its  mean,  we  get: 
E (R|Hyes ) =  g [1](s[1] + E (W [1])) + g [2](s[2] + E (W [2])) 
=  g [1]s[1] + g [2]s[2] 

SN R  = 

= 

The  variance  in  this  case  is  the  same  as  in  Hno  since  the  noise  components  are  the  same. 
Substituting  the  values  we  found  above  in  the  SNR  expression  results  in: 
�2 
�	
g [1]s[1] + g [2]s[2] − 0 
1 
3 (g2 [1] + g2 [2]) 
< g, s  >2 
1 
3 < g, g  > 

� 
� 
� 
s[1]
g [1]  ,  s  = 
s[2] 
g [2] 
From  the  Cauchy-Schwartz  inequality,  it  can  be  shown  that: 
< g, s  >2 ≤  < g, g  >< s, s  > 
where  equality  holds  if  and  only  if  g  =  cs  for  some  constant  c  (and  we  need  c  (cid:9)=  0  to  have 
nontrivial processing).  Essentially,  as  the  result  shows,  the vector g  is a matched ﬁlter  for  s. 

and  <  (cid:2), (cid:2) >  is  the  inner  product  operation. 

�	

where  g  = 

(b)  In  the  particular  case  where	 s[1]  =  s[2]  =  1,  which  we  shall  focus  on  for  the  rest  of  this 
problem,  it  turns  out  that  the  choice  g [1]  =  g [2]  =  c  will  serve,  for  any  nonzero  constant  c, 
to  maximize  the  SNR  in  (a).  Taking  c  =  3,  draw  fully  labeled  sketches  of  the  conditional 
densities  of  R  under  each  of  the  hypotheses,  i.e.,  sketches  of  fR|H (r |Hno ) and  fR|H (r |Hyes ). 
Suppose  now  that  the  prior  probabilities  on  the  two  hypotheses  are  p(Hno ) = 2 
3 and  hence 
p(Hyes ) = 1 
3 .  Specify  a  decision  rule  that,  on  the  basis  of  knowledge  that  R  =  r ,  decides 

between  Hno  and  Hyes  with  minimum  probability  of  error.  Also  compute  the  probability  of 
error associated with this decision rule.  (It will probably help you to shade on the appropriate 
sketch  the  regions  corresponding  to  the  conditional  probability  of  a  false  “yes”  and  to  the 
conditional  probability  of  a  false  “no”.) 

Answer:  Under  Hno , we  have  R  = 3W [1] + 3W [2].  Thus,  the  (conditional)  density  of  R  is 
derived  by  convolving  the  densities  of  the  scaled  random  variables  3W [1]  and  3W [2]. 

f 
3W[1]

(3w[1]) 

1/6 

-3 

* 
(3w[1]) 

3 

R|Hf 

(r | H  )no 

= 

-6 

-3 

1/6 

0 

f 
(3w[2])
3W[2] 

1/6 

3 

(3w[2]) 

6

r 

Figure  2:  The  probability  density  function  of  R  under Hno . 

Under hypothesis Hyes , we have R = 3(1 + W [1]) + 3(1 + W [2]) = 6 + (3W [1] + 3W [2]).  Thus, 
the  pdf  of  R  will  be  the  same  as  in  Hno  except  that  its  mean  is  shifted  by  6. 

R|Hf 

(r | H  ) 
yes

1/6 

0 

6

12 

r

Figure  3:  The  probability  density  function  of R  under  Hyes . 

1
2
With  the  a  priori  probabilities  speciﬁed  as  p(Hno ) =  3  and  p(Hyes ) =  3 ,  we  can  use  the 
likelihood  ratio  test  and  compare  p(Hno )fR|H (r |Hno ) with  p(Hyes )fR|H (r |Hyes ).  The  super-
imposed  densities  (normalized  by  the  a  priori  probabilities)  are  plotted  in  Figure  3. 
(r−6)
r  and  −

1 
1 
In  order  to  ﬁnd  the  decision  rule,  we  need  to  ﬁnd  the  r  where  the  lines 
,
18 
9
which represent p(Hyes )fR|H (r |Hyes ) with p(Hno )fR|H (r |Hno ), respectively, intersect.  Solving 
6 
6 

p(H  )f 
R|H
no 

(r | H  )
no 

1/9 

1/18 

p(H  )f 
R|H 
yes 

(r | H  )
yes 

-6 

0 

’H  ’ 
no 

6

’H 

’ 
yes 

12

r

 

Figure  4:  The  superimposed  (scaled)  densities  for  R  under  Hyes  and Hno . 

for  this  r  gives  us: 

9 (r − 6)
1
1 
18  =  −
r 
6
6 
r  = 6 − r
1 
2 
r  = 4 

Formalizing  this  result,  the  decision  rule  is  as  follows: 

(cid:2)H (cid:2) 
r ≷(cid:2)H (cid:2)  4 
yes 
no 

The  associated  probability  of  error  with  this  decision  rule  can  be  expressed  as: 
(cid:5) yes 
(cid:5)
(cid:5) no 
(cid:5)

) + p(Hyes )p(f alse 

) 

+

=	

Pe  =	 p(Hno )p(f alse 
2 1 
1 2 
3 18 
3 9 
1 
9 

= 

Graphically,  the  probability  of  error  can  be  shown  as  the  sum  of  the  shaded  areas  under  the 
conditional  densities  in  Figure  4. 

(c)  If  we  did  not  hastily  commit  ourselves  to  working  with  a  scalar  measurement  obtained  by 
taking  a  linear  combination  of  the  measurements  x[1]  and  x[2],  we  might  perhaps  have  done 
� 
� 
�	
� 
better.  Accordingly,  ﬁrst  sketch  or  fully  describe  the  conditional  densities 
fX [1],X [2]|H  x[1], x[2] | Hyes 
fX [1],X [2]|H  x[1], x[2] | Hno 

and 

1/6 

. . . 

. . .

0 

4 

6 

r

’H 
’ 
no 

’H 

’ 
yes 

- p(H yes)p(false ’yes’) 
- p(H no)p(false ’no ’) 

Figure  5:  The  probability  of  error  under  the  decision  rule  is  the  sum  of  the  shaded  areas. 

for  the case where s[1] = s[2] = 1.  Then specify a decision rule that will pick between the two 
hypotheses  with  minimum  probability  of  error,  on  the  basis  of  knowledge  that  X [1]  =  x[1] 
2
and  X [2]  =  x[2],  and  still  with  the  prior  probabilities  speciﬁed  in  (b),  namely  p(Hno ) =  3 
and  p(Hyes ) =  1 .  Determine  the  probability  of  error  associated  with  this  decision  rule,  and 
3 
compare  with  your  result  in  (b).


Answer:  Since  X [1]  and  X [2]  under  each  hypothesis  are  independent  and  have  the  same

1
uniform  distribution,  their  (conditional)  joint  densities  are  also  uniform  with  a  height  of  4 , 
as  shown  in  Figure  5. 
� �
� �
� �
� �

3  and  p(Hyes ) =  1 
With  the  a  priori  probabilities  as  p(Hno ) =  2 
3  as  in  5(b),  we  now  compare 
the scaled densities  2 
(x[1], x[2]
 Hno ) and  1 
(x[1], x[2]
Hyes ).  Proceeding 
f 
f 
3 
3 
X [1],X [2]  H 
X [1],X [2]  H 
in  the  same manner  as  in  (b),  the decision  rule with  the minimum probability  of  error  can be 
derived  as: 
(cid:2)H (cid:2) 
max(x[1], x[2]) ≷(cid:2)H (cid:2)  1 
yes 
no 
(cid:5)
(cid:5) )  =  0,  while  P (f alse 
With  this  decision  rule,  we  get  P (f alse 
yes
1 1  =  1
associated  probability  of  error  is  Pe  =
.
12 
3 4 
Comparing  this  result  with  the  one  in  5(b),  we  can  see  that  the  probability  of  error  has 
dropped  signiﬁcantly,  as  expected,  since we  have  not  carried  out  the  preliminary  (and  in  this 
case,  suboptimal)  matched  ﬁltering. 

(cid:5) ) =  1 .  Thus,  the 
4 

(cid:5)

no

      

x[2]
      

      

      

      


                             

                             
f              (x[1],x[2] | H)

                             

 
X[1],X[2]|H

                             

 

                             

 

                             

 

                  

                           

                             

 

                  

                           

                             

 

                  

                           

                             

 

                  

                           

                             

 

                  

                           

                             

 

                  

                           

                             

 

                  

                           

                             

 

 

                           

 

                           
                  


       

 

                           

       
2
1

                           

       

                           

       

                           

       
   

                           

       

       

       

x[1]

1/4

          
   

          
-1

          

          

          

          

          

          

          

          

          

   


   
   

   

   


- f              (x[1],x[2] | H    )yes
X[1],X[2]|H

- f              (x[1],x[2] | H   )
X[1],X[2]|H
no

Figure 6: The joint distributions of X [1] and X [2] udner each hypothesis.

