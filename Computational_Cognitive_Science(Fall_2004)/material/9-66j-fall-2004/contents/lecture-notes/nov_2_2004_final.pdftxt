Outline

• Non-parametric models for categorization: 
exemplars, neural networks
• Controlling complexity in statistical models

Bayesian classification  

Image removed due to copyright considerations.

The task: Observe x generated from c1 or c2, compute: 
cpcxp
|
)
)
(
(
1
1
cpcxp
cxp
(
(
(
)
|
)
|
+
1
1
2
2
Different approaches vary in how they represent p(x|cj). 

cp
(
1

cp
(

=

|

x

)

)

)

Non-parametric approaches

• Allow more complex form for p(x|cj), to be 
determined by the data. 
• E.g., kernel density estimation:

)

∝

j

k

2

||

1
n

2/(
σ
2

)

xx
||
−−

cxp
(
|

n
∑
e
k
1
=
• Equivalent to exemplar model
– Observe n examples: x1, …, xk
– Smoothness (“specificity”): σ

Image removed due to 
copyright considerations.

)

j

k

2

||

∝

2/(
σ
2

)

xx
||
−−

cxp
(
|

n
1
∑
e
n
k
1
=
• Equivalent to exemplar model
– Observe n examples: x1, …, xk
– Smoothness (“specificity”): 
σ

Image removed due to 
copyright considerations.

• Learning: Bayesian framework
– Hypothesis space is all smooth density functions f.
– Maximize p( f | x1, …, xk).
σ
• Prior p( f ) favors larger    .
• Likelihood  p(x1, …, xk | f ) favors smaller    .
σ

Nearest neighbor classification

Image removed due to copyright considerations.

Theorem: In the limit of infinite data, classification according
to nearest neighbor is approximately Bayes-optimal.

Nosofsky’s exemplar model

• Motivating example:

Before learning

After learning:
selective 
attention

Nosofsky’s exemplar model
• Math:
– Probability of responding category J to 
stimulus i:
Image removed due to copyright considerations.
– Similarity of stimulus i to exemplar j:
Image removed due to copyright considerations.
– Distance from stimulus i to exemplar j:
Image removed due to copyright considerations.

wm: attentional weight for dimension m ~ inverse variance

Concept learning experiments
• Simple artificial stimuli, e.g.

• Learn to discriminate members of two mutually     
exclusive categories, with repeated presentations of a  
small set of stimuli.

Six types of classifications:

Image removed due to copyright considerations.

Accurate fits to human data

Image removed due to copyright considerations.

How are attentional weights 
determined? 
• By the modeler: tune to fit behavioral data

How are attentional weights 
determined? 
• By the modeler: tune to fit behavioral data
• By the learner: discriminative learning
– Maximize discriminability 
of the training set.

Image removed due to copyright considerations.

Generative vs. Discriminative 
Models
• Generative approach: 
– Separately model class-conditional densities 
p(x | cj) and priors p(cj).
– Use Bayes’ rule to compute posterior probabilities:
cpcxp
(
|
)
)
(
1
1
cp
x
(
1
cxp
cpcxp
(
|
)
(
(
)
|
+
1
1
• Discriminative approach: 
– Directly model posterior probabilities p(cj | x) 

cp
(

2

)

=

|

)

)

2

Generative vs. Discriminative

Image removed due to copyright considerations.

Discriminative methods based on 
function approximation
• Perceptrons

• Neural networks

• Support vector machines

Image removed due to copyright considerations.

Discriminative methods based on 
function approximation
• Perceptrons

Image removed due to copyright considerations.

y

w2

x2

w1

x1

xwxw
y
(
)
=θ
+
11
22
z
z
1/(1)(
exp(
))
+
−
=θ

Weight space

Image removed due to copyright considerations.

The perceptron hypothesis space

• Linearly separable classes

Image removed due to copyright considerations.

Perceptron learning 
• Gradient descent on error surface in weight 
space:

Image removed due to copyright considerations.

Perceptron learning 
• Gradient descent on error surface in weight 
space:
= *
Error
y
y
* =y
−
output
correct 
 
(∑=
j xw
y θ
1 Error
)
j
2
w
×−← α
j

E =

2

j

w

j

E
∂
w
∂
j

E
∂
w
∂
j

=

Error

×

Error
∂
w
∂
j

−=

Error

×

y
∂
jw
∂

−=

Error

′×
(θ

∑
j

xw
j

j

)

×

x

j

−=

( *
y

−

y

)

×

x

j

∑′×
(
θ
j

xw
j

)

j

Discriminative methods based on 
function approximation
• Neural networks

ahid
j

whid
j1

ain
1

whid
j2

ain
2

Image removed due to copyright considerations.

The benefit of hidden units 

ridge = θ (sigmoid+sigmoid)        bump = θ (ridge+ridge)

Image removed due to copyright considerations.

Neural network learning 
• Gradient descent on error surface in weight 
space (“backpropagation”):

Image removed due to copyright considerations.

Neural network learning 
• Gradient descent on error surface in weight 
space (“backpropagation”):

E(w)

w

Backpropagation as a model of 
human learning?
• Kruschke: Are neural networks trained with 
backpropagation a good model of human 
category learning?
• Originally, backpropagation was not 
intended as a precise model of learning.
– Rather, a tool for learning representations.
– But does it learn the right kind of 
representations?

Two learning tasks
“Filtration”: easy to learn

Image removed due to copyright considerations.

“Condensation”: hard to learn

Image removed due to copyright considerations.

Human learning data

Image removed due to copyright considerations.

Conventional neural network

Image removed due to copyright considerations.

Model versus Data

Image removed due to copyright considerations.

People

Backprop

ALCOVE network

Image removed due to copyright considerations.

Differences between the models

• Dimension-specific attentional weights
• Hidden unit activation functions

Image removed due to copyright considerations.

Model versus Data

Image removed due to copyright considerations.

People

ALCOVE

Model versus Data

Image removed due to copyright considerations.

People

Backprop + attentional
weights (c.f. ARD)

“Catastrophic forgetting”

• Stimuli for category-learning experiment

Image removed due to copyright considerations.

Human learning data

Image removed due to copyright considerations.

ALCOVE

Image removed due to copyright considerations.

Conventional neural network

Image removed due to copyright considerations.

Questions about neural networks

• Why do they have such a bad rap?
• To what extent are neural networks brain-
like?
• They take a long time to train.  Is that a 
good thing or a bad thing from the 
standpoint of cognitive modeling?

