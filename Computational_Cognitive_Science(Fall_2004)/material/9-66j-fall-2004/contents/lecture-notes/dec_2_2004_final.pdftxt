Approaches to structure learning
• Constraint-based learning (Pearl, Glymour, Gopnik):
– Assume structure is unknown, no knowledge of 
parameterization or parameters
• Bayesian learning (Heckerman, Friedman/Koller):
– Assume structure is unknown, arbitrary parameterization.
• Theory-based Bayesian inference (T & G):
– Assume structure is partially unknown, parameterization is 
known but parameters may not be.  Prior knowledge about 
structure and parameterization depends on domain theories 
(derived from ontology and mechanisms). 

Advantages/Disadvantages of the 
constraint-based approach
• Deductive
• Domain-general
• No essential role for domain knowledge:
– Knowledge of possible causal structures not 
needed.
– Knowledge of possible causal mechanisms not 
used.
• Requires large sample sizes to make reliable 
inferences. 

The Blicket detector

Image removed due to copyright considerations. Please see:
Gopnick, A., and D. M. Sobel. “Detecting Blickets: How Young 
Children use Information about Novel Causal Powers in Categorization 
and Induction.” Child Development 71 (2000): 1205-1222.

Image removed due to copyright considerations. Please see:
Gopnick, A., and D. M. Sobel. “Detecting Blickets: How Young 
Children use Information about Novel Causal Powers in Categorization 
and Induction.” Child Development 71 (2000): 1205-1222.

The Blicket detector

• Can we explain these inferences using 
constraint-based learning?
• What other explanations can we come up 
with? 

Constraint-based model

Image removed due to copyright considerations. Please see:
Gopnick, A., and D. M. Sobel. “Detecting Blickets: How Young 
Children use Information about Novel Causal Powers in 
Categorization and Induction.” Child Development 71 (2000): 
1205-1222.

•

• Data: 
– d0: A=0, B=0, E=0
– d1: A=1, B=1, E=1
– d2: A=1, B=0, E=1
Constraints: 
– A, B not independent
– A, E not independent
– B, E not independent
– B, E independent conditional on the presence of A
– A, E not independent conditional on the absence of B
– Unknown whether B, E independent conditional on the absence of A.
• Graph structures consistent with constraints:

A

B

A

B

E

E

NOTE: Also have A, B independent conditional on the presence of 
E.  Does that eliminate the hypothesis that B is a blicket?

Imagine sample sizes 
multiplied by 100….
(Gopnik, Glymour et al., 2002)

Constraint-based inference
• Data:
– d1: A=1, B=1, E=1
– d2: A=1, B=0, E=1
– d0: A=0, B=0, E=0
• Conditional independence constraints:
– B, E independent conditional on A
– B, A independent conditional on E
– A, E correlated, unconditionally or conditional on B
• Inferred causal structure:
B
A
– B is not a blicket. 
– A is a blicket.

E

Why not use constraint-based 
methods + fictional sample sizes?
• No degrees of confidence.
• No principled interaction between data and 
prior knowledge.
• Reliability becomes questionable.        
– “The prospect of being able to do psychological 
research without recruiting more than 3 subjects 
is so attractive that we know there must be a 
catch in it.”

A deductive inference?

• Causal law: detector activates if and only if 
one or more objects on top of it are blickets. 
• Premises:
– Trial 1: A B on detector – detector active
– Trial 2: A on detector – detector active
• Conclusions deduced from premises and 
causal law:
– A: a blicket
– B: can’t tell

(Occam’s razor     not a blicket?)

What kind of Occam’s razor?

• Classical all-or-none form: 
– “Causes should not be multiplied without 
necessity.” 
• Constraint-based: faithfulness
• Bayesian: probability

For next time

• Come up with slides on Theory-based 
Bayesian causal inference.
• Combine current teaching slides, which 
emphasize Bayes versus constraint-based, 
with Leuven slides, which emphasize a 
systematic development of the theory. 
• Incorporate (if time) cross-domains, plus 
AB-AC. 

Approaches to structure learning
• Constraint-based learning (Pearl, Glymour, Gopnik):
– Assume structure is unknown, no knowledge of 
parameterization or parameters
• Bayesian learning (Heckerman, Friedman/Koller):
– Assume structure is unknown, arbitrary parameterization.
• Theory-based Bayesian inference (T & G):
– Assume structure is partially unknown, parameterization is 
known but parameters may not be. Prior knowledge about 
structure and parameterization depends on domain theories 
(derived from ontology and mechanisms). 

For next year

• Include deductive causal reasoning as one 
of the methods.  It goes back a long time…. 

Critical differences between Bayesian 
and Constraint-based learning
• Basis for inferences:
– Constraint-based inference based on just 
qualitative independence constraints.
– Bayesian inference based on full probabilistic 
models (generated by domain theory). 
• Nature of inferences:
– Constraint-based inferences are deductive.
– Bayesian inferences are probabilistic.

Bayesian causal inference
Causal hypotheses h
Data X
A
B
E
D
C
1
,1
,1
,1
,1
=
=
=
=
=
A
B
E
D
C
,0
,1
,0
,1
=
=
=
=
=
A
B
C
D
E
,1
,0
,1
,0
=
=
=
=
=
A
B
E
0
,0
,1
=
=
=
C
E
1
,1
=
=

1
0

D

C

B

E

A

C

A

B

E

D

2

x
1
x
x
3
x
x
5

4

=
=
=
=
=

Bayes:

XhP
(
|

)

∝

hPhXP
(
|
)
)(

Why be Bayesian?

• Explain how people can reliably acquire 
true causal beliefs given very limited data:
– Prior causal knowledge: Domain theory
– Causal inference procedure: Bayes

• Understand how symbolic domain theory 
interacts with rational statistical inference:  
– Theory generates the hypothesis space of 
candidate causal structures. 

Role of domain theory

• Determines prior over models, P(h)
– Causally relevant attributes of objects and 
relations between objects: variables
– Viable causal relations: edges

• Determines likelihood function for each 
model, P(X|h), via (perhaps abstract or 
“light”) mechanism knowledge:
– How each effect depends functionally on its 
causes:
V
f
V
parents[
(
])
VP
V
[
(
|
parents
θ⇐

])

Bayesian causal inference
Causal hypotheses h
Data X
A
B
E
D
C
1
,1
,1
,1
,1
=
=
=
=
=
A
B
E
D
C
,0
,1
,0
,1
=
=
=
=
=
A
B
C
D
E
,1
,0
,1
,0
=
=
=
=
=
A
B
E
0
,0
,1
=
=
=
C
E
1
,1
=
=

1
0

D

C

B

E

A

C

A

B

E

D

2

x
1
x
x
3
x
x
5

4

=
=
=
=
=

Bayes:

XhP
(
|

)

∝

hPhXP
(
|
)
)(

EDCBAP
(
,
,
,
,

|

causal
model
)
 

=

∏
VP
(
EDCBAV
},
,
,
,{
∈

|

V
[
parents

])

(Bottom-up) Bayesian causal 
learning in AI
• Typical goal is data mining, with no strong 
domain theory. 
– Uninformative prior over models P(h)
– Arbitrary parameterization (because no 
knowledge of mechanism), with no strong 
expectations of likelihoods P(X|h).
• Results not that different from constraint-
based approaches, other than more precise 
probabilistic representation of uncertainty. 

“Backwards blocking” 
(Sobel, Tenenbaum & Gopnik, 2004)

Image removed due to copyright considerations. Please see:
Gopnick, A., and D. M. Sobel. “Detecting Blickets: How Young 
Children use Information about Novel Causal Powers in Categorization 
and Induction.” Child Development 71 (2000): 1205-1222.

– Two objects: A and B
– Trial 1: A B on detector – detector active
– Trial 2: A on detector – detector active
– 4-year-olds judge whether each object is a blicket
• A: a blicket (100% of judgments)
• B: probably not a blicket (66% of judgments)

Theory

• Ontology
– Types: Block, Detector, Trial
– Predicates:
Contact(Block, Detector, Trial)
Active(Detector, Trial)
• Constraints on causal relations
– For any Block b and Detector d, with probability q :  
Cause(Contact(b,d,t), Active(d,t))
• Functional form of causal relations
– Causes of Active(d,t) are independent mechanisms, with 
causal strengths wi. A background cause has strength w0. 
Assume a near-deterministic mechanism: wi ~ 1, w0 ~ 0. 

Theory

• Ontology
– Types: Block, Detector, Trial
– Predicates:
Contact(Block, Detector, Trial)
Active(Detector, Trial)

A

B

E

Theory

• Ontology
– Types: Block, Detector, Trial
– Predicates:
Contact(Block, Detector, Trial)
Active(Detector, Trial)

A

B

E

A = 1 if Contact(block A, detector, trial), else 0
B = 1 if Contact(block B, detector, trial), else 0
E = 1 if Active(detector, trial), else 0

Theory
• Constraints on causal relations
– For any Block b and Detector d, with probability q :  
Cause(Contact(b,d,t), Active(d,t))
P(h10) = q(1 – q)
P(h00) = (1 – q)2
B
A
B
h00 :                        h10 : 
A
E

No hypotheses with 
E       B,  E      A, 
A       B, etc.

E

A

=  “A is a blicket”
E

P(h01) = (1 – q) q
B
A
h01 :                        h11 : 
E

P(h11) = q2
B
A

E

Theory
• Functional form of causal relations
– Causes of Active(d,t) are independent mechanisms, with 
causal strengths wb. A background cause has strength w0. 
Assume a near-deterministic mechanism: wb ~ 1, w0 ~ 0. 
P(h00) = (1 – q)2
P(h11) = q2
P(h01) = (1 – q) q
P(h10) = q(1 – q)
BA
A B
BA
BA

E
E
E
E
P(E=1 | A=0, B=0):     0                      0                              0
0
P(E=1 | A=1, B=0):     0                      0                              1
1
P(E=1 | A=0, B=1):     0                      1                              0
1
P(E=1 | A=1, B=1):     0                      1                              1
1
“Activation law”: E=1 if and only if A=1 or B=1.

Theory
• Functional form of causal relations
– Causes of Active(d,t) are independent mechanisms, with 
causal strengths wb. A background cause has strength w0. 
Assume a near-deterministic mechanism: wb ~ 1, w0 ~ 0. 
P(h00) = (1 – q)2
P(h01) = (1 – q) q
P(h11) = q2
P(h10) = q(1 – q)
BA
BA
A
BA
B

wb

wb

E
E
E
E
P(E=1 | A=0, B=0):     w0                             w0                                     w0                                                w0
wb + (1 – wb) w0         wb + (1 – wb) w0
P(E=1 | A=1, B=0):     w0
w0
wb + (1 – wb) w0
P(E=1 | A=0, B=1):     w0
w0
wb + (1 – wb) w0
wb + (1 – wb) w0        wb + (1 – wb) w0       1 – (1 – wb)2 (1 – wo)
P(E=1 | A=1, B=1):     w0
“Noisy-OR law”

wb

wb

Bayesian inference
• Evaluating causal network hypotheses in 
light of data:

)

=

dhP
(
|
i

hPhdP
(
|
(
)
)
i
i
∑
hPhdP
(
|
(
)
j
Hjh
∈
• Inferring a particular causal relation:   
∑
AP
dE
AP
hPhE
|
(
)
(
|
(
j
Hjh
∈

→

)

=

→

)

j

|

d

)

j

Modeling backwards blocking

P(h00) = (1 – q)2
A B

P(h01) = (1 – q) q
BA

P(h10) = q(1 – q)
BA

P(h11) = q2
BA

E
E
E
P(E=1 | A=0, B=0):     0                      0                              0
P(E=1 | A=1, B=0):     0                      0                              1
P(E=1 | A=0, B=1):     0                      1                              0
P(E=1 | A=1, B=1):     0                      1                              1
hP
BP
dE
(
)
(
|
→
01
hP
BP
dE
)
(
|
(
00

=

E
0
1
1
1

)
)

+
+

hP
(
11
hP
(
10

)
)

=

q
−

1

q

Modeling backwards blocking

P(h00) = (1 – q)2
BA

P(h01) = (1 – q) q
BA

P(h10) = q(1 – q)
BA

P(h11) = q2
BA

E

E

E

P(E=1 | A=1, B=1):     0                      1                              1
hP
hP
BP
dE
(
)
(
)
(
|
+
→
01
11
hP
BP
dE
)
)
(
|
(
10

=

)

E

1

=

1
−

1

q

Modeling backwards blocking

P(h01) = (1 – q) q
BA

P(h10) = q(1 – q)
BA

P(h11) = q2
BA

E

E

E

P(E=1 | A=1, B=0):                             0                              1                              1

P(E=1 | A=1, B=1):                             1                              1                              1
BP
dE
hP
)
)
(
|
(
→
11
hP
BP
dE
)
)
(
|
(
10

=

=

q
−

q

1

Manipulating the prior
I. Pre-training phase: Blickets are rare . . . . 

II. Backwards blocking phase: 

Trial 2
A
B
Trial 1
After each trial, adults judge the probability that each 
object is a blicket.

• “Rare” condition: First observe 12 objects 
on detector, of which 2 set it off. 

7

6

5

4

3

2

1 AB
Baseline

AB
After AB trial

A
B
After A trial

Figure by MIT OCW.

PEOPLE
 (N = 12)
BAYES

• “Common” condition: First observe 12 
objects on detector, of which 10 set it off. 

7

6

5

4

3

2

1

AB

AB

Baseline

After AB trial

A

B
   After A trial

Figure by MIT OCW.

PEOPLE
 (N = 12)
BAYES

Manipulating the priors of 
4-year-olds
(Sobel, Tenenbaum & Gopnik, 2004)
I. Pre-training phase: Blickets are rare.
II. Backwards blocking phase: 

A

B

Rare condition:
A: 100% say “a blicket” 
B: 25% say “a blicket”

Trial 2

Trial 1
Common condition:
A: 100% say “a blicket” 
B: 81% say “a blicket”

Inferences from ambiguous data
I. Pre-training phase: Blickets are rare . . . . 

II. Two trials: A B       detector,   B C      detector 

Trial 2
A
B
C
Trial 1
After each trial, adults judge the probability that each 
object is a blicket.

Same domain theory generates hypothesis 
space for 3 objects: 
B
C
A
• Hypotheses:          h000 =                          h100 = 
E
B
A
C
h010 =                          h001 = 
E
B
A
C
h110 =                          h011 = 
E
B
A
C
h101 =                          h111 = 
E
P(E=1| A, B, C; h) = 1

• Likelihoods:

E
B

E
B

E

B

E
B

C

C

A

A

C

C

A

A

if A = 1 and A
or B = 1 and B
or C = 1 and C
else 0.

E exists,     
E exists, 
E exists, 

• “Rare” condition: First observe 12 objects 
on detector, of which 2 set it off. 

10

9

8

7

6

5

4

3

2

1

0

PEOPLE
 (N = 20)
BAYES

ABC
Baseline

AB
C
After AB trial

BC
A
After AC trial

Figure by MIT OCW.

Ambiguous data with 4-year-olds
I. Pre-training phase: Blickets are rare.

II. Two trials: A B       detector,   B C      detector 

A

B

C

Trial 1

Trial 2

Final judgments:
A: 87% say “a blicket” 
B or C: 56% say “a blicket”

Ambiguous data with 4-year-olds
I. Pre-training phase: Blickets are rare.

II. Two trials: A B       detector,   B C      detector 

A

B

C

Trial 1

Trial 2

Final judgments:
A: 87% say “a blicket” 
B or C: 56% say “a blicket”

Backwards blocking (rare)
A: 100% say “a blicket” 
B: 25% say “a blicket”

The role of causal mechanism 
knowledge
• Is mechanism knowledge necessary?
– Constraint-based learning using χ2 tests of 
conditional independence. 
• How important is the deterministic functional 
form of causal relations?
– Bayes with “probabilistic independent generative 
causes” theory (i.e., noisy-OR parameterization 
with unknown strength parameters; c.f., Cheng’s 
causal power). 

Bayes with correct theory:
PEOPLE (N=12)
PEOPLE (N=12)
BAYES
BAYES

PEOPLE (N=20)
BAYES

7
6

5

4
3

7
6

5

4
3

10
9
8
7
6
5
4
3
2
1
0

2
1

2
1
B
A
AB
AB
AB
A
ABC
BC
C
B
A
AB
AB
Baseline
After AB trial
After AC trial
Figure by MIT OCW.
Independence test with fictional sample sizes:

7
6

5

4
3

2
1

AB

AB

A

B

7
6

5

4
3

2
1

AB

AB

A

B

Figure by MIT OCW.

10
9
8
7
6
5
4
3
2
1
0

ABC

AB

C

A

BC

Baseline

After AB trial

After AC trial

Bayes with correct theory:
PEOPLE (N=12)
PEOPLE (N=12)
BAYES
BAYES

PEOPLE (N=20)
BAYES

7
6

5

4
3

7
6

5

4
3

2
1

10
9
8
7
6
5
4
3
2
1
0

2
1

AB

AB

A

B

AB
A
ABC
BC
C
After AC trial
After AB trial
Baseline
Figure by MIT OCW.
Bayes with “noisy sufficient causes” theory:

AB

AB

A

B

7
6

5

4
3

2
1

AB

AB

A

B

7
6

5

4
3

2
1

AB

AB

A

B

Figure by MIT OCW.

10
9
8
7
6
5
4
3
2
1
0

ABC

Basline

AB

C

A

BC

After AB trial

After AC trial

Blicket studies: summary
• Theory-based Bayesian approach explains 
one-shot causal inferences in physical 
systems.
• Captures a spectrum of inference:
– Unambiguous data: adults and children make 
all-or-none inferences
– Ambiguous data: adults and children make 
more graded inferences
• Extends to more complex cases with hidden 
variables, dynamic systems, ….

