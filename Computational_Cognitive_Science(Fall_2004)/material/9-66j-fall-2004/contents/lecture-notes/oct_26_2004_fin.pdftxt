Outline


•	 Bayesian concept learning: Discussion


•	 Probabilistic models for unsupervised and 
semi-supervised category learning 

Discussion points


•	 Relation to “Bayesian classification”?


•	 Relation to debate between rules / logic / 
symbols and similarity / connections / 
statistics? 

•	 Where do the hypothesis space and prior 
probability distribution come from? 

Discussion points


•	 Relation to “Bayesian classification”? 

– Causal attribution versus referential inference. 

– Which is more suited to natural concept 

learning?


•	 Relation to debate between rules / logic / 
symbols and similarity / connections / 
statistics? 

•	 Where do the hypothesis space and prior 
probability distribution come from? 

Hierarchical priors


FH,FT 

T~ Beta(FH,FT) 

Coin 1 

T 

Coin 2 

T

...

Coin 200 

T 

d

1 

d
2 

d

3 

d

4 

d

1 

d

2 

d

3 

d

4 

d

1 

d

2 

d

3 

d

4 

•  Latent structure captures what is common to all coins, 
and also their individual variability 

Hierarchical priors


P(h) 

Concept 1 

h

 

Concept 2 

h

 

... 

Concept 200 

 

h

x
1 

x
2 

x
3 

x
4 

x
1 

x
2 

x
3 

x
4 

x
1 

x
2 

x
3 

x
4 

•  Latent structure captures what is common to all 

concepts, and also their individual variability

•  Is this all we need?


number  knowledge


social knowledge


math/magnitude? 

P(h)


Concept 1 

h

	

Concept 2	

h

 

... 

Concept 200 

 

h

x
1 

x
2 

x
3 

x
4 

x
1 

x
2 

x
3 

x
4 

x
1 

x
2 

x
3 

x
4 

•	 Hypothesis space is not just an arbitrary collection of 

hypotheses, but a principled system.


•	 Far more structured than our experience with specific 

number concepts. 


Outline


•	 Bayesian concept learning: Discussion


•	 Probabilistic models for unsupervised and 
semi-supervised category learning 

Simple model of concept learning


“This is a blicket.” 

“Can you show me the 
other blickets?” 

Simple model of concept learning


Other blickets.


The objects of planet Gazoob


Image removed due to copyright considerations.


Simple model of concept learning


“This is a blicket.” 

“Can you show me the 
other blickets?” 

Learning from just one positive example is possible if:

–  Assume concepts refer to clusters in the world. 

–  Observe enough unlabeled data to identify clear clusters. 

Complications


Complications


•  Outliers 

“This is a blicket.”


Complications


•  Overlapping clusters 

“This is a blicket.” 

Complications


•  How many clusters? 

“This is a blicket.”


Complications


•  Clusters that are not simple blobs


“This is a blicket.” 

Complications


•  Concept labels inconsistent with clusters


“This is a blicket.” 

“This is a gazzer.” 

Simple model of concept learning


•	 Can infer a concept from just one positive 
example if: 
–	 Assume concepts refer to clusters in the world.


–	 Observe lots of unlabeled data, in order to identify 
clusters. 

•	 How do we identify the clusters? 
–	 With no labeled data (“unsupervised learning”) 

–	 With sparsely labeled data (“semi-supervised learning”) 

Unsupervised clustering with 

probabilistic models


•	 Assume a simple parametric probabilistic 

model for clusters, e.g., Gaussian.


C 

x
1 

x
2 

1 | c j ) u p ( x
p ( x
p ( x | c
j ) 
p ( x  | c j ) v e  ( x P 2 
)  Vij  )	
2
2 
/( 
i	
ij 
i 

2 | c j )

11V 

P 
21 

c 
2

c 
1

V21

P11

Unsupervised clustering with 

probabilistic models


•	 Assume a simple parametric probabilistic 
model for clusters, e.g., Gaussian. 

•	 Two ways to characterize jth cluster:

– Parameters:  Pij ,Vij
– Assignments: zj 
(k)  = 1 if kth point belongs to 
cluster j, else 0. 

Unsupervised clustering with 

probabilistic models


•  Chicken-and-egg problem:


– Given assignments we could solve for 

maximum likelihood parameters:

(  Pij 2
(  xi 
¦ z 
k
k
) 
) 
j 
k
¦ z 
k 
) 
( 
j 
k 

¦ z j 
k
( 
k 
¦ z j
k

k 
) 
( 
) 
xi 

Pij 

k
( 

) 

2 
V  ij

Unsupervised clustering with 

probabilistic models


•  Chicken-and-egg problem:


– Given parameters we could solve for 

assignments zj 
(k ):

k
( 
z 
j 

k 
j c  | x ) 
(  ) 
c p 
(

,1  j 

) 

max 
arg 
j c 

,0 

else 

k
k
j  | x ) 
(  ) v  p (x ) 
( 
(
c p 

| c j )  c p 
(
j ) 

Solve for “base
rate” parameters: 
j )  ¦ z 
( k 
) 
c p 
( 
j 
k

 
i 

1 
2 VS 
ij 

 ( x ( k ) P 
)  V ij  )  c p 
2
2 
2 
/( 
e 
i 
ij 
( 
j )

Alternating optimization 

algorithm


0. Guess initial parameter values.

1. Given parameter estimates, solve for maximum a 
posteriori assignments zj 
(k):

k
z ) 
( 
j 

,1  j 

max 
arg 
jc 

k 
jc  | x ) 
(  )
c p 
( 

 ( x ( k ) P 
)  V ij  ) 
2 
2
2 
/( 
c p j ) 
e 
i 
ij 
( 

(  ) v  
k 
j  | x ) 
c p 
( 
i 

1 
2 VS  ij
else 
,0 
2. Given assignments zj 
(k), solve for maximum 
likelihood parameter estimates: 
(   x 
(  P ij  2
¦ z j 
¦ z 
k 
k
k
k
) 
( 
( 
) 
) 
) 
xi 
j
i 
k 
k 
¦ z j 
¦ z 
k 
k
) 
( 
( 
j 
k 
k

V 2 
ij 

P ij 

j )  ¦ z k 
) 
( 
c p 
( 
j 
k

) 

3. Go to step 1. 

Alternating optimization 

algorithm


x

z: assignments to cluster
P V p(cj): cluster parameters 

[For simplicity, assume V p(cj) fixed.] 

Alternating optimization 

algorithm


Step 0: initial parameter values


Alternating optimization 

algorithm


Step 1: update assignments


Alternating optimization 

algorithm


Step 2: update parameters


Alternating optimization 

algorithm


Step 1: update assignments


Alternating optimization 

algorithm


Step 2: update parameters


Alternating optimization 

algorithm


0. Guess initial parameter values.

1. Given parameter estimates, solve for maximum a 
posteriori assignments zj 
(k):

k
z ) 
( 
j 

,1  j 

max 
arg 
jc 

k 
jc  | x ) 
(  )
c p 
( 

 ( x ( k ) P 
)  V ij  ) 
2 
2
2 
/( 
c p 
e 
i 
ij 
( 
j ) 

(  ) v  
k 
j  | x ) 
c p 
( 
i 

1 
2 VS  ij 
,0 
else 
(k), solve for maximumWhy hard
2. Given assignments zj 
assignments? 
likelihood parameter estimates: 
(   x 
(  P ij  2
¦ z j 
¦ z 
k 
k
k
k
) 
( 
( 
) 
) 
) 
xi 
j
i 
k 
k 
¦ z j 
¦ z 
k 
k
) 
( 
( 
j 
k 
k

j )  ¦ z k 
) 
( 
c p 
( 
j 
k

V 2 
ij 

P ij 

) 

3. Go to step 1. 

EM algorithm


e

k 
) 
( 
h j

0. Guess initial parameter values T = {P , V , p(cj)}. 
1. “Expectation” step: Given parameter estimates, 

compute expected values of assignments zj 
(k)
1 
 ( x ( k ) P 
)  V ij  )  c p 
;T ) v  
2
2 
k 
2 
/( 
j  | x ) 
( 
c p 
i 
ij 
( 
j )
( 
2 VS  ij
i 
2. “Maximization” step: Given expected 
assignments, solve for maximum likelihood 
parameter estimates: 
¦ h j  xi 
k 
k
) 
) 
( 
( 
k 
P ij  ¦ h ) 
k
( 
j 
k

(   x ) 
(  P ij  2
¦ h ) 
k
k
i 
j
k
¦ h ) 
k 
( 
j
k 

2 
V  ij 

j )  ¦ h j
k
( 
c p 
( 
k 

) 

What EM is really about


•	 Define a single probabilistic model for the 
whole data set: 
p(X |T)   p(x ) 
k 
( 
k 
 ¦ p(x ) 
k 
( 
j 
k

| c j ;T)  c p 
j ;T) 
(


|T) 

“mixture 
model” 

 ¦ 
j
k 
i 

1	
2 VS  ij 

( x ( k )
e 
i 

)  Vij  )  c p 
P 2 
2
2
/( 
ij	
j )
( 

What EM is really about


•	 Define a single probabilistic model for the 
whole data set: 
p(X |T)   p(x ) 
k 
( 
k 
 ¦ p(x ) 
k 
( 
j 
k

| c j ;T)  c p 
j ;T) 
(


|T) 

“mixture 
model” 

1	
 ¦ 
2 VS  ij 
j
k 
i 
•	 How do we maximize w.r.t. T? 

( x ( k )
e 
i 

)  Vij  )  c p 
P 2 
2
2
/( 
ij	
j )
( 

What EM is really about


•	 Maximization would be simpler if we 

introduced new labeling variables Z = {zj 
(k )}:

|T)   p (x ) 
;T) 
k 
( 
| c j ;T)  c p 
p ( Z X 
,	
(  j 
j 
k
(  ¦ log
|T)  ¦ ¦ z 
k	
) 
log p ( Z X 
,	
j 
k
j 
i 
(  ¦ ( x  Pij ) 
¦ ¦ z 
k 
k
) 
( 
) 
2
i 
j
k
j 
i 
•  Problem: we don’t know Z = {zj 
(k )}!

;T) 
| c j ;T)  c p 
(  j 

k
( 
x p 
(  i 

2
/(  V  )  log 
2 
ij 

c p 
j )
( 

( k ) 
z j 

) 

What EM is really about


•	 Maximization expected value of the 
“complete data” loglikelihood, log p(X, Z|T ): 
–	E-step: Compute expectation 
)  ¦ p (Z | X,T  ) 
Q( T T  t 
t 
) 
( 
( 
log 
) 
|	
Z 

|T ) 
p ( Z X 
,

–	M-step: Maximize 
T (t   )1 

( t
Q( T T 
arg 
max 
|	
T 

) 
) 

