Outline

• Probabilistic models for unsupervised and 
semi-supervised category learning
• Nonparametric models for categorization: 
exemplars, neural networks

EM algorithm
0. Guess initial parameter values θ= {µ, σ, p(cj)}. 
1. “Expectation” step: Given parameter estimates, 
compute expected values of assignments zj
(k)
1
x
k
(
)
)
2/(
)
2
(
2
−
−
µ
σ
∏
k
k
)(
)(
θx
cp
cp
h
e
|
;
)
(
(
=
∝
i
ij
ij
j
2
σπ
i
ij
2. “Maximization” step: Given expected 
assignments, solve for maximum likelihood 
parameter estimates: 
∑
k
k
)(
)(
h
x
j
i
k
∑
k

(
k
)(
x
i
∑
k

∑
k

h

k
)(
j

h

k
)(
j

h

k
)(
j

−

µ
ij

2
σ

µ
ij

cp
(

)

j

=

ij

)
2

j

)

j

=

h

k
)(
j

∑=
k

What EM is really about
• Want to maximize log p(X|θ), e.g. 
∏ ∑=
k
)(
X
x
cp
p
p
c
)
)
)
(
|
(
|
;
;
(
θ
θ
θ
j
j
j
k

What EM is really about
• Want to maximize log p(X|θ), e.g. 
∑ ∑
k
)(
X
x
cp
p
p
c
)
)
log
(
|
(
|
;
log
(
=
θ
θ
j
j
k

;
)
θ

j

;
)
θ

What EM is really about
• Want to maximize log p(X|θ), e.g. 
∑ ∑
k
)(
X
x
cp
p
p
c
)
)
log
(
|
(
|
;
log
(
=
θ
θ
j
j
k
j
• Instead, maximize expected value of the 
“complete data” loglikelihood, log p(X, Z|θ):
∑ ∑
k
)(
k
)(
x
ZX
cp
p
z
p
c
,
)
;
)
log
(
|
log
(
|
)
log
;
(
θ
=
+
θ
θ
j
j
j
k
j
– E-step: Compute expectation 
t ∑=
t
)(
)(
XZ
Q
p
|
|
,
(
)
(
log)
θθ
θ
Z
– M-step: Maximize

t
)(
Q θθ
(
|

t
)1(
=+
θ

ZX
,

p

(

|

)
θ

arg
max
θ

)

Good features of EM

• Convergence
– Guaranteed to converge to at least a local maximum of 
the likelihood.  
– Likelihood is non-decreasing across iterations (useful 
for debugging).  
• Efficiency
– Convergence usually occurs within a few iterations 
(super-linear). 
• Generality
– Can be defined for many simple probabilistic models.

Limitations of EM

• Local minima
– E.g., one component poorly fits two clusters, while two 
components split up a single cluster.  
• Degeneracies
– Two components may merge.  
– A component may lock onto just one data point, with 
variance going to zero. 
• How do you choose number of clusters?
• May be intractable for complex models.  

Mixture models for binary data
k
k
k
)(
)(
)(
• Data:                              ,
)( ∈k
x
x K
x
{
,
,
}
=
ix
}1,0{
D
1
• Probabilistic model: mixture of Bernoulli 
distributions (coin flips). 
C
∏=
k
)( θ
X
x
p
p
(
|
)
(
|
)
θ
k
∏ ∑=
j
k
∏ ∑ ∏
cp
(
)
j
j
k
i

...  xi  ...

cp
(

)
;
θ

)
;
θ

µ
ij

1(

−

1(
−

x
(
i

p

(

x

µ
ij

|

c

k

)

x
(
i

k
)(

j

k

)

)

)

=

x1

xD

j

EM algorithm
0. Guess initial parameter values θ= {µ, p(cj)}. 
1. “Expectation” step: Given parameter estimates, 
compute expected values of assignments zj
(k)
∏
x
k
)
(
x
k
k
)
(
)
1(
−
)(
k
)(
θx
cp
cp
h
|
;
)
1(
)
(
)
(
µ
µ
=
∝
−
i
i
ij
ij
j
i
2. “Maximization” step: Given expected 
assignments, solve for maximum likelihood 
parameter estimates: 
∑
k
k
)(
)(
h
x
i
j
k
∑
k

∑=
k

cp
(

h

k
)(
j

h

k
)(
j

)

j

µ
ij

=

j

j

Applications of EM to human 
learning
• Chicken and egg problems
– Categories, prototypes
– Categories, similarity metric (feature weights)

Additive clustering for the integers 0-9: 
∑=
k

fw
k

s
ij

f

jk

ik

Rank

Weight

1
2
3
4
5
6
7
8

.444
.345
.331
.291
.255
.216
.214
.172

Stimuli in cluster
0   1   2   3   4   5   6   7   8   9
*        *                  *
* * * 

*     *      * 
* * * * 
* * * * * 
*   *   *    *   * 
* * * * 
* * *  * * 

Interpretation

powers of two
small numbers
multiples of three
large numbers
middle numbers
odd numbers
smallish numbers
largish numbers

Applications of EM to human 
learning
• Chicken and egg problems
– Categories, prototypes
– Categories, similarity metric (feature weights)
– Categories, outliers 
– Categories, unobserved features

Learning as interpolation of  
missing data
• Interpolating a sparse binary matrix:

Objects/
People/
Entities

P1
P2
P3
P4
P5
P6
P7
P8
P9
P10

?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
F1 F2 F3 F4  F5 F6 F7 F8 F9 F10 F11 F12 F13 F14
Features/Concepts/Attributes

Objects/
People/
Entities

• Interpolating a sparse binary matrix:
?
?
?
?
?
P1
?
?
?
?
?
P2
?
?
?
?
?
P3
?
?
?
?
?
P4
?
?
?
?
?
P5
?
?
?
?
?
P6
?
?
?
?
?
P7
?
?
?
?
?
P8
?
?
?
?
?
P9
?
?
?
?
?
P10
F1 F2 F3 F4  F5 F6 F7 F8 F9 F10 F11 F12 F13 F14
Features/Concepts/Attributes
C

– Assume mixture of Bernoulli 
distributions for objects Pk:

...  Fi  ...
F1
F14
– Learn with EM, treating both class labels and 
unobserved features as missing data.  

Applications of EM to human 
learning
• Chicken and egg problems
– Categories, prototypes
– Categories, similarity metric (feature weights)
– Categories, outliers 
– Categories, unobserved features
– Theories, similarity metric (feature weights) 

Is B or C more “similar” to A?

B

A

C

Is B or C more “similar” to A?

B

A

C

EM for factor analysis
• A simple causal theory
– Generate points at random positions z on a line 
segment.  (Unobserved “latent” data)
– Linearly embed these points (with slope a, 
intercept b) in two dimensions. (Observed data)
– Add Gaussian noise to one of the two observed 
dimensions (x-dim or y-dim). 
• Examples:
– Sensory integration
– Weighing the advice of experts 

EM for factor analysis
• A simple causal theory
– Generate points at random positions z on a line 
segment.  (Unobserved “latent” data)
– Linearly embed these points (with slope a, 
intercept b) in two dimensions. (Observed data)
– Add Gaussian noise to one of the two observed 
dimensions (x-dim or y-dim). 
• Goal of learning:
– Estimate parameters: a, b, dimension of noise 
(x-dim or y-dim)
– Infer unobserved data: z

Applications of EM to human 
learning
• Chicken and egg problems
– Categories, prototypes
– Categories, similarity metric (feature weights)
– Categories, unobserved features
– Categories, outliers
– Theories, similarity metric (feature weights)
– Learning in Bayes nets with hidden variables
– Others?  

Fried and Holyoak (1984)

• Can people learn probabilistic categories 
without labels?
• How does learning with labels differ from 
learning without labels?
• What kind of concept is learned?
– Prototype (mean)
– Prototype + variability (mean + variances)
• Is categorization close to ideal* of a 
Gaussian mixture model? 

Fried and Holyoak stimuli

Image removed due to copyright considerations. Please see:
Fried, L. S., and K. J. Holyoak. “Induction of Category Distributions: A Framework for Classification Learning.” 
Journal of Experimental Psychology: Learning, Memory and Cognition 10 (1984): 234-257.

Fried and Holyoak, Exp. 4

Image removed due to copyright considerations. Please see:
Fried, L. S., and K. J. Holyoak. “Induction of Category Distributions: A Framework for Classification Learning.” 
Journal of Experimental Psychology: Learning, Memory and Cognition 10 (1984): 234-257.

Fried and Holyoak, Exp. 2

Image removed due to copyright considerations. Please see:
Fried, L. S., and K. J. Holyoak. “Induction of Category Distributions: A Framework for Classification Learning.” 
Journal of Experimental Psychology: Learning, Memory and Cognition 10 (1984): 234-257.

Fried and Holyoak (1984)

• Can people learn probabilistic categories 
without labels?  Yes.
• How does learning with labels differ from 
learning without labels?  It’s better. 
• What kind of concept is learned?
– Prototype (mean)
– Prototype + variability (mean + variances)
• Is categorization close to ideal* of a 
Gaussian mixture model?  Yes.

Relevance for human cognition

• How important are these three paradigms 
for human category learning?
– Labeled examples
– Unlabeled examples
– Unlabeled examples but known # of classes
• Other ways of combining labeled and 
unlabeled examples that are worth 
pursuing?

Semi-supervised learning

• Learning with many unlabeled examples 
and a small number of labeled examples.
• Important area of current work in machine 
learning. 
– E.g., learning about the web (or any large 
corpus)
• Natural situation in human learning.
– E.g., word learning
– Not much research here though…. 

The benefit of unlabelled data

Class 2

Test point

Class 1

The benefit of unlabelled data

Class 2

Test point

Class 1

Is this really a new problem?

• Why not just do unsupervised clustering 
first and then label the clusters? 

Complications

• Concept labels inconsistent with clusters

“This is a blicket.”

“This is a gazzer.”

Complications

• Outliers

“This is a blicket.”

“This is a gazzer.”

“This is a wug.”

Complications

• Overlapping clusters

“This is a blicket.”

“This is a blicket.”

“This is a blicket.”

Complications

• How many clusters?

“This is a blicket.”

“These are also blickets.”

Complications

• How many clusters?

“This is a blicket.”

“These are also blickets.”

Semi-supervised learning

• Learning with many unlabeled examples 
and a small number of labeled examples. 
• Approaches based on EM with mixtures 
– Identify each concept with one mixture 
component.  
• Labels serve to anchor class assignments in E-step.

C

x1

...  xD  

L

Deterministic (1-to-1) link

(Could also be many-to-1.)

Semi-supervised learning

• Learning with many unlabeled examples 
and a small number of labeled examples. 
• Approaches based on EM with mixtures
– Treat concept labels as separate features, 
conditionally independent of observed features 
given classes.  
• e.g., Ghahramani and Jordan (cf. Anderson).
C

Probabilistic (Bernoulli) link

x1

...  xD  

L

Other approaches to semi-
supervised learning
• Graph-based
– Szummer & Jaakkola
– Zhu, Ghahramani & Lafferty
– Belkin & Niyogi
– Blum & Chawla
• Tree-based
– Tenenbaum and Xu; Kemp, Tenenbaum, et al.

Graph-based semi-supervised 
learning
E.g., Class labeling function is smooth over 
a graph of k-nearest neighbors:

Image removed due to copyright considerations.

Tree-based semi-supervised learning
• A motivating problem: learning words for kinds of objects

Image removed due to 
copyright considerations.

What does the word “dog” refer to? 
• All (and only) dogs?
• All mammals?
• All animals? 
• All labradors? 
• All yellow labradors?

• Undetached dog parts?
• All dogs plus Silver? 
• All yellow things? 
• All running things? 
• . . .

Image removed due to copyright considerations.

Images removed due to copyright considerations.

Bayesian model of word learning
• H: Hypotheses correspond to taxonomic clusters
– h1 = “all (and only) dogs”
– h3 = “all animals”
– h4 = “all labradors”
– h2 = “all mammals”
– . . .
• Same model as for learning number concepts, but 
with two new features specific to this task:
– Prior favors more distinctive taxonomic clusters.
– Prior favors naming categories at a privileged (basic) level.

Image removed due to copyright considerations.

Bayes (with basic-level bias)

Bayes (without basic-level bias)

Image removed due to copyright considerations.

The objects of planet Gazoob

Image removed due to copyright considerations.

Image removed due to copyright considerations.

Adults:

Image removed due to copyright considerations.

Bayes:  (with basic-level bias)

Image removed due to copyright considerations.

Objects

Semi-supervised learning?
• Interpolating a sparse binary matrix:
?
?
?
?
?
P1
?
?
?
?
?
P2
?
?
?
?
?
P3
?
?
?
?
?
P4
?
?
?
?
?
P5
?
?
?
?
?
P6
?
?
?
?
?
P7
?
?
?
?
?
P8
?
?
?
?
?
P9
?
?
?
?
?
P10
F1 F2 F3 F4  F5 F6 F7 F8 F9 F10 F11 F12 F13 F14
Words             Features
• Use features to infer tree or graph over objects. 
• Use tree or graph to generate priors for the 
extensions of words.

