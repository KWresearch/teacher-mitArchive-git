Compression in Bayes nets
• A Bayes net compresses the joint 
probability distribution over a set of 
variables in two ways:
– Dependency structure
– Parameterization
• Both kinds of compression derive from 
causal structure:
– Causal locality
– Independent causal mechanisms

Dependency structure
MJAEBP
(
=)
,
,
,
,
AMPAJPEBAPEPBP
(
)
(
)
(
|
,
)
(
|
)
(
|
)

Burglary

Earthquake

Alarm

Graphical model asserts:
VP K
nV
( 1
,
=)
,
n
∏
V
VP
|
parents
[
(
i
i
i
1
=

])

JohnCalls

MaryCalls

Dependency structure
MJAEBP
(
=)
,
,
,
,
JAEBMPAEBJPEBAPBEPBP
)
(
)
(
|
(
|
,
)
(
|
,
,
)
(
|
,
,
,

)

Burglary

Earthquake

Alarm

For any distribution:
VP K
nV
( 1
=)
,
,
n
∏
VVP
,
(
|
i
1
i
1
=

V
i

,

K

)

1
−

JohnCalls

MaryCalls

Parameterization

P(B)
0.001

Burglary

Alarm

P(E)
0.002

Earthquake

B    E     P(A|B,E)
0     0      0.001
0     1      0.29
1     0      0.94
1     1      0.95

Full CPT

JohnCalls

A     P(J|A)
0     0.05
1     0.90

MaryCalls

A    P(M|A)
0     0.01
1     0.70

Parameterization

P(B)
0.001

Burglary

Alarm

JohnCalls

A     P(J|A)
0     0.05
1     0.90

P(E)
0.002
Noisy-OR

Earthquake
B    E     P(A|B,E)
0     0      0
0     1      wB  = 0.29
1     0      wE  = 0.94
1     1      wB +(1-wB )wE
FIX FOR NEXT
YEAR Wb and We
A    P(M|A)
0     0.01
1     0.70

MaryCalls

Outline

• The semantics of Bayes nets
– role of causality in structural compression
• Explaining away revisited
– role of causality in probabilistic inference
• Sampling algorithms for approximate 
inference in graphical models

Outline

• The semantics of Bayes nets
– role of causality in structural compression
• Explaining away revisited
– role of causality in probabilistic inference
• Sampling algorithms for approximate 
inference in graphical models

Global semantics
Joint probability distribution factorizes into product 
of local conditional probabilities:
n
∏
VP
VP
V
V
,
( K
,
)
|
parents
[
(
=
i
n
i
1
i
1
=

])

Burglary

Earthquake

Alarm

JohnCalls
MJAEBP
,
=)
(
,
,
,
AMPAJPEBAPEPBP
)
(
|
,
)
(
|
)
(
|
)
(
(
)

MaryCalls

Global semantics
Joint probability distribution factorizes into product 
of local conditional probabilities:
n
∏
VP
VP
V
V
,
( K
,
)
|
parents
[
(
=
i
n
i
1
i
1
=

])

Burglary

Earthquake

Alarm

MaryCalls
JohnCalls
Necessary to assign a probability to any possible world, e.g.
bP
aPePbPmjae
b
amPajPe
,
(
,
,
)
,
(
)
)
(
(
,
|
)
|
(
(
)
)
|
¬¬
¬=
¬
¬¬

Local semantics
Global factorization is equivalent to a set of constraints 
on pairwise relationships between variables. 

“Markov property”: Each node is conditionally 
independent of its non-descendants given its parents.   

U1

Um

Z1j

X

Znj

Y1

Yn

Figure by MIT OCW.

Local semantics
Global factorization is equivalent to a set of constraints 
on pairwise relationships between variables. 

“Markov property”: Each node is conditionally 
independent of its non-descendants given its parents. 

Also: Each node is marginally 
(a priori) independent of any 
non-descendant unless they 
share a common ancestor.

U1

Um

Z1j

X

Znj

Y1

Yn

Figure by MIT OCW.

Local semantics
Global factorization is equivalent to a set of constraints 
on pairwise relationships between variables. 

Each node is conditionally independent of all others 
given its “Markov blanket”: parents, children, 
children’s parents. 

U1

Um

Z1j

X

Znj

Y1

Yn

Figure by MIT OCW.

Example

Burglary

Earthquake

Alarm

JohnCalls

MaryCalls

JohnCalls and MaryCalls are marginally (a priori) dependent, but 
conditionally independent given Alarm.  [“Common cause”]

Burglary and Earthquake are marginally (a priori) independent, 
but conditionally dependent given Alarm. [“Common effect”]

Constructing a Bayes net

• Model reduces all pairwise dependence and 
independence relations down to a basic set 
of pairwise dependencies: graph edges. 

• An analogy to learning kinship relations
– Many possible bases, some better than others
– A basis corresponding to direct causal 
mechanisms seems to compress best. 

An alternative basis
Suppose we get the direction of causality wrong...

Burglary

Earthquake

Alarm

JohnCalls

MaryCalls

• Does not capture the dependence between callers: 
falsely believes P(JohnCalls, MaryCalls) = 
P(JohnCalls) P(MaryCalls).

An alternative basis
Suppose we get the direction of causality wrong...

Burglary

Earthquake

Alarm

JohnCalls

MaryCalls

• Inserting a new arrow captures this correlation.
• This model is too complex: does not believe that 
P(JohnCalls, MaryCalls|Alarm) =   
P(JohnCalls|Alarm) P(MaryCalls|Alarm)

An alternative basis
Suppose we get the direction of causality wrong...

Burglary

Earthquake

Alarm

JohnCalls

MaryCalls

• Does not capture conditional dependence of causes 
(“explaining away”): falsely believes that 
P(Burglary, Earthquake|Alarm) =   
P(Burglary|Alarm) P(Earthquake|Alarm)

An alternative basis
Suppose we get the direction of causality wrong...

Burglary

Earthquake

Alarm

JohnCalls

MaryCalls

• Another new arrow captures this dependence.
• But again too complex: does not believe that  
P(Burglary, Earthquake) = 
P(Burglary)P(Earthquake)

Suppose we get the direction of causality wrong...

PowerSurge

Burglary

Earthquake

Alarm

JohnCalls

MaryCalls

BillsCalls
• Adding more causes or effects requires a 
combinatorial proliferation of extra arrows.  Too 
general, not modular, too many parameters….

Constructing a Bayes net

• Model reduces all pairwise dependence and 
independence relations down to a basic set of 
pairwise dependencies: graph edges. 
• An analogy to learning kinship relations
– Many possible bases, some better than others
– A basis corresponding to direct causal 
mechanisms seems to compress best. 
• Finding the minimal dependence structure 
suggests a basis for learning causal models.

Outline

• The semantics of Bayes nets
– role of causality in structural compression
• Explaining away revisited
– role of causality in probabilistic inference
• Sampling algorithms for approximate 
inference in graphical models

Explaining away

• Logical OR: Independent deterministic causes

Burglary

Earthquake

Alarm

B    E     P(A|B,E)
0     0      0
0     1      1
1     0      1
1     1      1

Explaining away

• Logical OR: Independent deterministic causes

Burglary

Earthquake

Alarm

A priori, no correlation between B and E:

ebP
),(

=

ePbP
)(
)(

B    E     P(A|B,E)
0     0      0
0     1      1
1     0      1
1     1      1

Explaining away

• Logical OR: Independent deterministic causes

Burglary

Earthquake

Alarm

B    E     P(A|B,E)
0     0      0
0     1      1
1     0      1
1     1      1

After observing A = a … 
= 1
bPbaP
)
(
|
)(
aP
)(

abP
(
|

)

=

Explaining away

• Logical OR: Independent deterministic causes

Burglary

Earthquake

Alarm

B    E     P(A|B,E)
0     0      0
0     1      1
1     0      1
1     1      1

After observing A = a … 
bP
)(
aP
)(

abP
|(

)

=

)(bP>

May be a big increase if P(a) is small.

Explaining away

• Logical OR: Independent deterministic causes

Burglary

Earthquake

Alarm

After observing A = a … 

B    E     P(A|B,E)
0     0      0
0     1      1
1     0      1
1     1      1

abP
|
(

)

=

bP
)(
ePbP
eP
bP
)(
)(
)(
)(
−
+
May be a big increase if P(b), P(e) are small.

)(bP>

Explaining away

• Logical OR: Independent deterministic causes

Burglary

Earthquake

Alarm

B    E     P(A|B,E)
0     0      0
0     1      1
1     0      1
1     1      1

After observing A = a, E= e, … 
ebPebaP
(
|
)|
(
),
eaP
(
)|
Both terms = 1

eabP
(
|
),

=

Explaining away

• Logical OR: Independent deterministic causes

Burglary

Earthquake

Alarm

B    E     P(A|B,E)
0     0      0
0     1      1
1     0      1
1     1      1

After observing A = a, E= e, … 
ebPebaP
(
|
)|
),
(
eaP
)|
(
ebP
bP
(
)|
)(
=

eabP
(
|
),

=

=

“Explaining away” or 
“Causal discounting”

Explaining away

• Depends on the functional form (the 
parameterization) of the CPT
– OR or Noisy-OR: Discounting
– AND: No Discounting 
– Logistic: Discounting from parents with 
positive weight; augmenting from parents with 
negative weight.
– Generic CPT:  Parents become dependent when 
conditioning on a common child.

Parameterizing the CPT

• Logistic: Independent probabilistic causes 
with varying strengths wi and a threshold θ
C1   C2     P(Pa|C1,C2)
Child 2 upset
Child 1 upset
]
[
0      0       
)
exp(
1/1
+
θ
[
0      1      
exp(
1/1
+
−
θ
[
1      0      
exp(
1/1
+
−
θ
[
1      1      
exp(
1/1
+
−
θ

Parent upset

]
w
)
1
]
w
)
2
ww
−
1
2

])

P(Pa|C1,C2)

Threshold θ

Annoyance = C1* w1 +C2* w2

Contrast w/ conditional reasoning
Sprinkler
Rain

Grass Wet

• Formulate IF-THEN rules:
– IF Rain THEN Wet
– IF Wet THEN Rain

IF Wet AND NOT Sprinkler
THEN Rain
• Rules do not distinguish directions of inference
• Requires combinatorial explosion of rules

Spreading activation or recurrent 
neural networks
Earthquake
Burglary

Alarm

Alarm, Earthquake

• Excitatory links: Burglary
Alarm
• Observing earthquake, Alarm becomes more active. 
• Observing alarm, Burglary and Earthquake become 
more active.
• Observing alarm and earthquake, Burglary cannot 
become less active.  No explaining away!   

Spreading activation or recurrent 
neural networks
Earthquake
Burglary

Alarm

Alarm, Earthquake

• Excitatory links: Burglary
Alarm
• Inhibitory link: Burglar
Earthquake
• Observing alarm, Burglary and Earthquake become 
more active. 
• Observing alarm and earthquake, Burglary becomes 
less active: explaining away.

Spreading activation or recurrent 
neural networks
Power surge
Burglary

Earthquake

Alarm

• Each new variable requires more inhibitory 
connections.
• Interactions between variables are not causal.
• Not modular.
– Whether a connection exists depends on what other 
connections exist, in non-transparent ways.  
– Combinatorial explosion of connections

The relation between PDP and 
Bayes nets
• To what extent does Bayes net inference 
capture insights of the PDP approach?
• To what extent do PDP networks capture or 
approximate Bayes nets? 

Summary

Bayes nets, or directed graphical models, offer 
a powerful representation for large 
probability distributions:
– Ensure tractable storage, inference, and 
learning
– Capture causal structure in the world and 
canonical patterns of causal reasoning. 
– This combination is not a coincidence.

Still to come
• Applications to models of categorization
• More on the relation between causality and 
probability: 

Causal structure

Statistical dependencies
• Learning causal graph structures.
• Learning causal abstractions (“diseases 
cause symptoms”)
• What’s missing from graphical models

Outline

• The semantics of Bayes nets
– role of causality in structural compression
• Explaining away revisited
– role of causality in probabilistic inference
• Sampling algorithms for approximate 
inference in graphical models

Motivation
• What is the problem of inference?  
– Reasoning from observed variables to 
unobserved variables
• Effects to causes (diagnosis):
P(Burglary = 1|JohnCalls = 1, MaryCalls = 0)
• Causes to effects (prediction):
P(JohnCalls = 1|Burglary = 1) 
P(JohnCalls = 0, MaryCalls = 0|Burglary = 1)

Motivation
• What is the problem of inference?  
– Reasoning from observed variables to 
unobserved variables.
– Learning, where hypotheses are 
represented by unobserved variables.
• e.g., Parameter estimation in coin flipping:
P(H) = θ

θ

d1

d2

d3

d4

Motivation
• What is the problem of inference?  
– Reasoning from observed variables to 
unobserved variables.
– Learning, where hypotheses are 
represented by unobserved variables. 
• Why is it hard?
– In principle, must consider all possible 
states of all variables connecting input 
and output variables.

A more complex system
Battery

Radio

Ignition

Gas

Starts

On time to work

• Joint distribution sufficient for any inference:
SOPGISPGPBIPBRPBPOSGIRBP
)
(
,
,
,
,
,
)
(
(
|
)
(
|
)
(
)
(
|
,
)
(
|
=
∑
OSGIRBP
(
,
,
,
,
,
SIRB
,
,
,

)

)

)

GOP
(
|

)

=

)

AP
)
(

BAP
,
(

∑=
B
“marginalization”

GOP
(
,
GP
(
)

=

GP
(

)

A more complex system
Battery

Radio

Ignition

Gas

Starts

On time to work

• Joint distribution sufficient for any inference:
SOPGISPGPBIPBRPBPOSGIRBP
(
(
,
,
,
,
,
)
)
(
|
)
(
|
)
(
)
(
|
,
)
(
|
=
∑
SOPGISPGPBIPBRPBP
)
(
(
|
)
|(
)
(
)
(
|
,
)
(
|
SIRB
,
,
,

)

)

)

GOP
|
(

)

=

GOP
,
(
GP
(
)

=

GP
(

)

A more complex system
Battery

Radio

Ignition

Gas

Starts

On time to work

• Joint distribution sufficient for any inference:
SOPGISPGPBIPBRPBPOSGIRBP
(
,
,
,
,
,
)
(
)
(
|
)
(
|
)
(
)
(
|
,
)
(
|
=

)

GOP
(
|

)

=

)

GOP
,
(
GP
)
(

=

⎛
⎜
∑ ∑
⎜
S
IB
⎝
,

⎞
⎟
SOPGISPBIPBP
(
)
(
|
)
(
|
,
)
(
|
⎟
⎠

)

A more complex system
Battery

Radio

Ignition

Gas

Starts

On time to work

• Joint distribution sufficient for any inference:
SOPGISPGPBIPBRPBPOSGIRBP
(
,
,
,
,
,
)
(
)
(
|
)
(
|
)
(
)
(
|
,
)
(
|
=

)

• Exact inference algorithms via local computations
– for graphs without loops: belief propagation 
– in general: variable elimination or junction tree, but these   
will still take exponential time for complex graphs.

Sampling possible worlds

sprinkler
rain
wet
,
,
>
sprinkler
rain
wet
,
,
>
¬
sprinkler
wet
rain
,
,
>
¬
sprinkler
wet
rain
,
,
>
¬
sprinkler
wet
rain
,
,
¬
¬
sprinkler
rain
wet
,
,
>
sprinkler
rain
wet
,
,
¬
¬

¬

cloudy
,
<
¬
cloudy
,
¬<
cloudy
,
¬<
cloudy
,
¬<
cloudy
,
<
¬
cloudy
,
¬
<
cloudy
,
¬<
. . .

As the sample gets larger, 
the frequency of each
possible world approaches
its true prior probability
under the model.

How do we use these 
samples for inference?

>

>

Summary
• Exact inference methods do not scale well to 
large, complex networks
• Sampling-based approximation algorithms can 
solve inference and learning problems in arbitrary 
networks, and may have some cognitive reality. 
– Rejection sampling, Likelihood weighting
• Cognitive correlate: imagining possible worlds
– Gibbs sampling
• Neural correlate: Parallel local message-passing 
dynamical system
• Cognitive correlate: “Two steps forward, one step 
back” model of cognitive development

