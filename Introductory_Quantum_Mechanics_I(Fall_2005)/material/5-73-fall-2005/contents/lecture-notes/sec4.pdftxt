IV.  Matrix Mechanics 
We now turn to the a pragmatic aspect of QM: given a particular 
problem, how can we translate the Dirac notation into a form that 
might be interpretable by a computer?  As hinted at previously, we do 
this by mapping Dirac notation onto a complex vector space.  The 
operations in Hilbert space then reduce to linear algebra that can 
easily be done on a computer.  This formalism is completely 
equivalent to the Dirac notation we’ve already covered; in different 
contexts, one will prove more useful than the other. 

1.  States can be represented by vectors 
First, we will begin with an arbitrary complete orthonormal basis of 
{  } 
φ  .
Then, we know that we can write any other state as: 
states 
i
φ 
φ 
+ ...  ƒ 
 ci 
= 
φ 
+
 c
3 
φ 
+
 c
2 
ψ = c 1 
3 
2 
1 
i
i 
How are these coefficient determined?  Here, we follow a common 
trick and take the inner product with the j th  state: 
( 
( φ 
(	
)	
) 
)

φ φ 
= 
= 
=
φ 
ψ φ 
ƒ

ƒ

ƒ

c 
c 
j
i
i 
i 
i
j
j 
i 
i 
i
Since the Kronecker delta is only non-zero when i=j, the sum 
collapses to one term: 

δ 
c
i 
ij 

ψ φ  = c j 
j
The simple conclusion of these equations is that knowing the 
coefficients is equivalent to knowing the wavefunction. If we know 
ψ , we can determine the coefficients through the second relation. 
ψ
by 
Vice versa,  If we know the coeffi
cients, we can reconstruct 
φ .
 Thus, if we fix this arbitrary basis, we 
performing the sum  ƒ 
ci
i
i 
can throw away all the basis state and just keep track of the 
coefficients of the ket state: 

c 1 
’ 
≈
÷
Δ 
c 2→ 
÷
Δ
÷
Δ 
c 3 
÷÷
ΔΔ
...  φ 
◊ 
« 
In harmony with the intuitive arguments made previously, here we 
associate the ket states with column vectors.  Notice the small 

ψ 

subscript  “ φ ”, which reminds us that this vector of coefficients 
{  } 
φ
ψ
n the 
basis.  If we were really careful, we would 
represents 
i
i
keep this subscript at all times; however, in practice we will typically 
know what basis we are working in, and the subscript will be dropped. 

ψ

=


c 
i 

φ  * 
c  .
i
i 

ng bra state 

as a vector? 

How to we represent the correspondi
Well, we know that 
) †ψ  = ≈
= ( 
† 
’
φ 
ψ 
ƒ

ƒ
÷ 
Δ 
i
◊ 
«

i 
i 
Now, as noted before, we expect to associate bra states with row 
vectors, and the above relation shows us that the elements of this 
row vector should be the complex conjugates of the column vector: 
... ) φ 
ψ → ( c 1 
*
*
*
c 2 
c 3 
Noting that bra states and ket states were defined to be Hermitian 
conjugates of one another, we see that Hermitian conjugation in state 
space corresponds to taking the complex conjugate transpose of the 
coefficient vector. 

Now, the vector notation is totally equivalent to Dirac notation; thus, 
anything we compute in one representation should be exactly the 
same if computed in the other. As one illustration of this point, it is 
useful to check that this association of states with vectors preserves 
the inner product: 
≈

’

=
 ≈
'ψψ 
φ  ƒ

ƒ i
* 
ΔΔ
ci 
÷
Δ 
◊ 
«

« 
i 
j 

’

= 
ƒ

÷÷ 
◊ 
ij 

δ 
* ' 
c c i
j 
ij

= 
ƒ

ij 

φ φ 
j
i 

ƒ

i 

* ' 
c c i 
j 

* ' c c  i
i

φ 
j

' 
c 
j 

=

c

*
2 

( c 
*
1 

’
c 1 
≈

÷ 
Δ 
... ) φ
c 2 
÷
Δ
⋅ 
÷ 
Δ 
c 
3 
÷÷

ΔΔ
... ◊φ 
«

So, the two ways of writing the inner product give identical results, 
and overlaps between the vectors have the same quantitative (and 
qualitative) meaning as their bra-ket counterparts. 

ƒ

i

* ' c ci 
i 

=

* 
3 

c

2.  Operators Become Matrices 
In order to complete our transition to linear algebra we need to 
determine how operators act on the coefficient vectors described in 

the previous section.  Before we do this, we need to consider the 
operator: 

ˆO = ƒ  φ φ 
i 
i 
i 
Acting this operator on an arbitrary state: 
Oˆ  ψ = ƒ 
ψ φ φ 
i 
i 
i 
However, we showed above that  ψ φ  = c j , the coefficients of the 
j 
state  ψ .  Thus, 

= ψ

= ƒ φ ci 
Oˆ  ψ = ƒ 
ψ φ φ 
i 
i 
i 
i 
i
Thus,  Oˆ  acting on any state gives the same state back.  The operator 
that accomplishes this is the identity operator, 1ˆ  , and so we write: 
ˆ1 = ƒ  φ φ 
i 
i 
i 
and say that this is a resolution of the identity. 

W ith this in hand, we manipulate the expression for the bra-ket 
sandwich of and arbitrary operator  Aˆ  : 
ˆ  ψψ  A 
'

1 = ƒ  φ φ 
1 = ƒ  φ φ 
ˆ 
j 
j 
i 
i 
j 
i
ψ φ φ 
φ φ ψ 
Aˆ 
Ω ƒ 
' 
i 
i 
j 
j 
ij 
Ω ƒ c  φ Aˆ  φ  c j
' 
* 
i 
i 
j 
ij 
We can re-write this last line as a standard (row)x(matrix)x(column) 
multiplication if we define the matrix  A , whose elements are 
= φ Aˆ  φ 
j  .  This association is so common, in fact, that even in
Aij 
i 
Dirac notation, these objects are typically referred to as  “matrix 
elements”.  So, to summarize, we can write: 

ˆ
≡ A

φ Aˆ  φ 
1 
3 
... 

φ Aˆ  φ φ Aˆ  φ 
’ 
≈
... 
÷
Δ 
1 
1
1 
2 
2  Aˆ  φ φ 
φ 
2  Aˆ  φ 
÷
Δ 
Aˆ  → 
1
2 
÷
Δ 
3  A φ 
ˆφ 
... 
÷
Δ
1 
÷
Δ
... 
◊
« 
φ 
This matrix has all the same action on row and column vectors that 
the operator  Aˆ  has on bras and kets: 
⋅ A↔ c 
ψ Aˆ  ↔
 A
 ⋅
 c
 ψ 
Aˆ
It is also easy to show that the product of two operators is correctly 
represented by the product of their matrix representations.  Further, 
similar to the case of vectors, the adjoint of  Aˆ 
is the complex 
conjugate transpose of  A .  Using the above associations, we can 
write every operation in Hilbert space in terms of matrix-vector 
multiplications, which are easily handled by a computer. 

3.  Some Interesting Matrix Properties 
We are now going to prove a number of interesting things about 
particular classes of matrices which will prove useful later.  These 
same identities can be proven for the raw operators, but the results 
are somewhat more familiar when one has the matrix formulation in 
hand. 

Now, since each Hermitian operator defines its own orthonormal 
basis, we often be interested in making a change of basis from the 
eigenbasis of one Hermitian operator,  Aˆ  , to that of another,  Bˆ  . 
{ 
} αϕ
Aˆ 
and the eigenbasis of  Bˆ  by 
Denote the ei
genbasis of 
by 
{ 
} αχ  . 
Then according to what we know about Hermitian operators 
} αχ
{ 
{ 
} αϕ 
and 
are both  orthonormal bases.  Thus, we can write 
any state as 

=

=
aαϕ 
bα χ
ψ
ψ 
ƒ

ƒ

or 
α 
α 
α 
α 
Our task is to get  { aα}  from  { bα} (or vice versa).  This is accomplished 
using our favorite trick; take the inner product of each equation with 
βχ  : 

β ψ χ 

= 
ƒ

α 

aα  ϕ χ 
β α 

or 

ψ χ 
β 

=

χ χ  α
βα b 

ƒ

α 

†
Equating the r.h.s. and making use of the orthonormali
ty of the 
gives: 

{ 
} αχ

ƒ

α 

=


=
 bβ 

aα  ϕ χ 
β α 

= 
ƒ
 δ α  α β 
bα χ χ  α
ƒ

b

β 
α 
α 
= bβ 
aα ϕ χ  αβ 
Ω ƒ 
α 
This leads to the definition of the Transformation matrix: 
ϕ χ 
ϕ χ 
ϕ χ 
... 

’

≈

1
3
1 
1
1
2
Δ
÷ 
ϕ χ 
ϕ χ 
... 
... 
÷ 
Δ
T ≡ 
2 
2
1
2
ϕ χ 
Δ
÷
... 
... 
÷÷

ΔΔ

3 
1
... 
... 
«

◊

The columns of this matrix are the coefficients of the  “new” basis 
} αϕ
{ 
{ 
} αχ  )
d” ones (
) and  it allows us to 
states (
in terms of  “ol
transform from one basis to another using simple matrix algebra: 
b = Ta 

on (
i.e.  b  to  a )?  W ell, our 
Now, what about the reverse transformati { 
{ 
} αϕd” (
} αχ 
designation of the  “ol
) and  “new ” (
) bases was 
completely arbitrary; we can change the direction of the 
{ 
} αϕ  .
{ 
} αχ
For 
and 
es of 
transformation by simply switching the rol
example, we can obtain the  b  to  a  transformation matrix by simply 
swapping the letters ϕ and  χ in our definition of  T 
χ ϕ 
χ ϕ 
χ ϕ 
...

≈

’ 
1
3
1 
1
1
2
Δ
÷ 
χ ϕ 
χ ϕ 
... 
... 
Δ
÷ 
S ≡ 
2
2 
1
2
χ ϕ 
Δ
÷
... 
... 
÷÷
ΔΔ
3 
1
... 
... 
◊

«


This matrix satisfies: 

a = Sb . 
However, looking at our definition of  S , we see that it is just the 
Hermitian conjugate of  T !  This leads to an important result: 
a = Sb =  b T  = T  ( Ta ) = T  Ta 
† 
Reading from right to left, this shows that  T † T  acting on any vector  a 
gives back the same vector.  Thus, we conclude that: 
T † T = 1 
Matrices that satisfy this special property are called unitary matrices. 
Any property we are interested in will be invariant to unitary 

†
†
transformations.  From a physical perspective, this is because unitary 
transforms correspond to a change of basis and we know that the 
basis we choose to represent things is arbitrary and should not 
matter.  From a mathematical point of view, this results from the fact 
that unitary matrices will always occur in Hermitian conjugate pairs in 
our results (because of the bra-ket structure of the inner product) and 
† T  is the identity. 
T

The transformation matrix also allows us to change the basis for an 
operator.  Denote the matrix representation of  Hˆ 
in the eigenbasis of 
Aˆ  by: 


 ϕ
Hˆ
2 
Hˆ  ϕ
2 
... 

ϕϕ 
1 
1
ϕϕ 
1 
2
ϕ 
1 

ϕ Hˆ 
≈
Δ 
1 
ϕ 
Hˆ 
Δ 
2 
Δ 
ϕ 
Hˆ 
Δ
3 
Δ

... 
«

ix element of  Hˆ 
Then we have that a matr
Aˆ  basis by: 

ϕ  ...

ϕ 
Hˆ
3
1
... 

’

÷ 
÷ 
≡ H A 
÷ 
÷
÷

◊

can be represented in the 

ψ 
Hˆ 

'ψ 

= 
a 

a H 
' 
A 

and in the  Bˆ  basis by: 

= 
'ψ 
ψ 
Hˆ 
b H b 
' 
B 
Now, using the fact that  a' = T † b' (and the Herm itian conjugate 
relation  a †  = b T ), 

=
'ψ  =

Ω
 ψ Hˆ 
'  b † TH A b T  ' 
a H 
a 
A 
leads to the 
Comparing this last equation with the definition of 
H B 
conclusion that under a change of basis from  Aˆ 
to  Bˆ  , an arbitrary 
matrix transforms as: 

= TH AT † 
H B 
One important special case of this relation is when  Hˆ  = Aˆ  .  Then, we 
know the matrix elements: 
Aˆ  ϕ
ϕϕ 
ϕ 
Aˆ 
≈
Δ 
2 
1
1 
1 
Aˆ  ϕ
ϕϕ 
ϕ 
Aˆ 
Δ 
2 
1 
2
2 
Δ 
ϕ 
ϕ 
Aˆ 
... 
Δ
1 
31 
Δ

...

«


0
 ’ 
÷ 
... 
÷ 
= A
÷ 
... 
÷÷

...
 ◊


’ 
a1 
≈
÷ 
Δ 
0
÷ 
Δ 
= 
÷ 
Δ 
0
÷
ΔΔ
÷ 
0 
«

◊


ϕ  ...

ϕ 
Aˆ
3
1
... 

0 
a 
2 
0
... 

0
0
a 
3 
... 

A



†
†
†
†
†


That is, the matrix that represents  Aˆ 
is diagonal in the eigenbasis of 
Aˆ  .  This allows us to very easily represent  Aˆ 
in any other eigenbasis: 
= TA AT † 
A B 
Where we recall that  A A  is just a diagonal matrix.  In practice, we will 
very often want to work with a matrix in its eigenbasis and only use 
the transformation rules to move to other bases when necessary. 

As an example, consider a function of a matrix.  This is defined by 
the power series expansion of the function: 
AA +  f  ' ' ' (0 ) 
f (A ) ≡ 1 +  f ' (0 )A +  f  ' ' (0 ) 
AAA + . . . 
2!
3!
One important example of this is the exponential of a matrix, which 
we will use quite frequently: 
AA  AAA 
+ 
+ . . . 
e  ≡ 1 + A +
A 
2!
3!
If we transform from whatever arbitrary basis we are in into the 
eigenbasis, we can write  A  in diagonal form ( A = TA  T †  ) and the 
A 
function becomes: 
TA  T †  TA AT  +  f  ' ' ' (0 ) 
f (A ) = 1 +  f ' (0 )TA  T  +  f  ' ' (0 ) 
TA  T TA  T †  TA AT  + . . . 
† 
† 
A 
A 
A 
A
2!
3!
(0 )
(0 )
1 
1 
1
' (0 )TA  T 
) 
(
' ' 
' ' ' 
+  f 
= 
+ 
+ 
+
† 
T A 
TA 
T A A A
TA A
1 
Ω
 A 
. . . 
f 
A 
A
A 
A
!2 
3!
and noting that  1 = TT †  and that the coefficients (which are numbers) 
commute with the transformation matrix: 
f  ' ' ' (0 ) 
f  ' ' (0 ) 
f (A ) = TT  + Tf ' (0 ) T A 
+ T 
†  + T 
† 
T A A A 
T A A 
A 
A
A 
A
A
A
2!
3!
f  ' ' ' (0 ) 
f  ' ' (0 ) 
) 
(
(0 )A A 
≈
+ 
+
=

1 T  +  f '

A A 
A A A A
A 
Ω
 f 
Δ 
A 
A
A 
A 
!2 
!3 
« 
f (A ) = Tf (A  )T † 
Ω

A
Thus, functions of matrices transform just like matrices when we 
change basis. W hy is this important?  In its eigenbasis, we know that 
Aˆ 
is represented by a diagonal matrix, and it is trivial to apply a 
function to a diagonal matrix; the result is a diagonal matrix, with the 
diagonal elements given by  f (x )  evaluated at each of the 
eigenvalues: 

+ . . . 

Ω


f 

f

+
 . . .


T


’

÷
◊


†
†
†
†
†
†
†
)

)

f 

( e 
0 
0
0 
0
0 
0
f 
e 1 
’

≈
’
≈

1
( e 
÷
Δ
÷
Δ
( A  ) 
0
0
0
0
0 
0
f 
e
÷ 
Δ 
÷ 
Δ 
=

=  f 
2
2 
( e 
)
÷ 
Δ 
÷ 
Δ 
A 
0 
0 
0
0 
0
0
f 
e 3 
3 
÷÷

ΔΔ
÷÷
ΔΔ

... 
0 
0
0
... 
0
0
0
◊

«

◊

«

Thus, we can apply a function to a matrix in three steps: 1) Change 
)  to the 
(
basis to the eigenbasis of  A ˆ  ( A = TA  T †  ) 2) Apply 
x 
f 
A 
diagonal elements of  A A  to obtain  f ( A  )  3) Transform back to the 
A 
original basis ( f ( A ) = Tf ( A  ) T †  )
A 

B.  Discrete Variable Representation (DVR) 
For one dimensional systems, we are left with a puzzle: on the one 
hand, it is natural to work in the (continuous) position representation 
(as in wave mechanics) but on the other hand, we need a discrete 
representation to translate things on to a computer.  The discrete 
variable representation (DVR) is one solution to this problem.  The 
DVR works in the harmonic oscillator basis.  To refresh our memory, 
recall that the matrix elements of  p ˆ 2  are: 
a ) 
( a
( 
) n 
−
− = 
− 
− 
ˆ + 
n  = 
2
† 
† 
† 
2ˆ
ˆ 
ˆ 
a ˆ 
ˆ 
ˆ aa 
ˆ 
ˆ 
ˆ 
ˆ 
a a 
a a 
1  m
1  m 
n 
p m 
a 
( 
2 
2 
− ( 2n + 1)δ 
n  1  n δ 
− = 
+ 
+ 
−
+ 
1δ 
2  n
n 
1 
− 
n m  + 2 
,n m 
,n m 
2 
2 
, 
while the matrix elements of  q ˆ  are: 
)

(  n 
(  †  +

) 
++ 1
+ 
= 
n δ
1δ

= 
a ˆ 
ˆ n q m 
ˆ n 
1  m 
a 
1
− 1 
,n m 
,n m 
2 
2 
As a result, we can write both of these operators as matrices in the 
harmonic oscillator basis: 
−
 2 
’
’

≈
≈ 
0 
0 
... 
... 
1 
1
÷ 
÷ 
Δ 
Δ
... 
0 
2 
0
0
6
1 
÷
÷
Δ 
Δ
÷
÷
Δ
Δ 
2−
0 
0 
5
0 
2 
3 
÷
÷
Δ
Δ
÷
÷
Δ
Δ
−

... 
«
 ... 
... 
... 
0
6 
...  ◊

3 
◊
«
Thus far, we have just been applying the rules of matrix mechanics. 
( ˆ
)  then 
Now, if our Hamiltonian takes the standard form  H ˆ  =  p ˆ 2  +  q V 
2m 
we can easily translate this into matrix mechanics:  H =  P 2  + V ( Q ) . 
2m 
The only tricky part is how we evaluate V ( Q ) ; in DVR we follow the 

Q ≡ 

≡  1 
2

0

3
0

P 2

)

−

1 
2 

†
†
prescription above and diagonalize  Q , evaluate V (Q )  in the 
eigenbasis and then transform back to the harmonic oscillator basis: 
V (Q) = TV (Q  )T †  .  As mentioned above, evaluating V (Q  )  is simple; it
q
q 
is just a diagonal matrix with diagonal elements given by the function 
V  evaluated at the eigenvalues of  Q . 

( ˆ
)  as a matrix in 
Therefore, we know how to represent  H ˆ  =  p ˆ 2  +  q V 
2m 
the harmonic oscillator basis.  W e have made no approximations 
(yet).  However, we have a bit of a problem, because the matrices  Q 
and  P 2  are infinite-dimensional.  This is a bit of a problem on a (finite) 
computer. The key approximation we make is that of a truncated 
basis: we select some large but finite number of basis functions,  N . 
For example, we might choose the  N  lowest energy states of the 
harmonic oscillator if we are interested in the low energy states of 
( ˆ
) .  Once we have selected our truncated basis, we 
H  =  p ˆ 2  +  q V 
ˆ 
2m 
perform the entire problem as if these functions form a complete 
basis.  As a result,  Q  and  P 2  go from being infinite dimensional 
matrices to being  N × N  matrices that we can store on a computer. 
The key observation is that, as I make  N  larger and larger, my 
truncated basis becomes closer to the complete basis and all the 
results I compute in the truncated basis (e.g. energy eigenvalues, 
expectation values, etc.) must approach the corresponding complete 
basis results.  Since one can easily deal with  N  up to several 
thousand on a computer, this makes it possible to obtain very precise 
answers for one-dimensional problems. 

So, to summarize, here are the major steps in DVR: 

1)  Choose a harmonic oscillator frequency, ω , and number of 
basis functions,  N .  In principle any ω will work if  N  is large 
enough, because every harmonic oscillator defines its own 
complete basis.  However, a well-chosen ω will approach the 
exact result more rapidly with  N . 
2)  Build  Q  and  P 2  in the harmonic oscillator basis.  These will be 
N × N  matrices. 

3)  Diagonalize  Q  to obtain the diagonal matrix  Q q  and the unitary 
matrix  T .

4)  Compute the potential using V (Q) = TV (Q  )T †  .

q 
5)  Construct  H =  P 2  + V (Q )  and compute whatever you need (e.g. 
2 m 
eigenvalues, eigenstates, average values) using matrix 
mechanics. 

C.  Variational method 
One important point is that the DVR representation is not unique. 
The ambiguity comes because we have chosen to compute the 
matrix representation of certain  “basic” operators (  p ˆ 2  and  q ˆ  ) directly, 
while other operators (  q V  ˆ  ) ) have been represented as functions of 
(
the  “basic” matrices.  Toe see where the ambiguity arises, let’s 
pretend we’ve chosen a very small basis:  N =3. There are two ways 
we could compute  P 2 .  The first is to compute its matrix elements 
directly: 

P 2

1≡ 
2 

1
0

0  −

’

≈
2 
Δ 
÷
3
0

Δ 
÷
÷

Δ

−

2  0
5
◊

«

The second way is to compute the matrix representation of  p ˆ  and 
then square the matrix: 
0
 −
’

≈
2 
0
1
0 
0
0

i 
i

’

≈
’
≈ 
÷
Δ 
÷ 
÷ 
Δ 
Δ
= 
−
− 
0 
0

0
3 
0
2i
2i 
i 
i 
÷
Δ 
÷
÷
Δ
Δ
÷

Δ

÷
− 
÷
Δ 
Δ
0 
2  0
2
0 
0 
0 
2i 
2i 
«◊
«

◊

◊

«

And the results are clearly different.  Note that if we had chosen a 
larger  N , these two routes would have given (essentially) the same 
answer.  However, it is clear that, for finite  N  the way we choose to 
represent things in terms of matrices matters. 

⋅ P P  =


1 
2 

1
2 

−

The foundation of variational approaches is the Raleigh-Ritz 
variational principle: Given any state  ψ  (called the trial state) and 
T
any Hamiltonian H ˆ  , the average energy for the trial state is always 
greater than or equal to the ground state energy of the Hamiltonian: 

ψψ 
ˆH 
≥ E  .0 
T 
T
E
T 
ψ ψ 
T 
T
To prove that this is the case, let us expand  ψ  in terms of the 
T
eigenstates of  H ˆ  : 

≡ 

where each 

=ψ 
φ α 
ƒ i 
T 
i 
i 
φ  satisfies the eigenvalue equation: 
i 
φ  = E  φ 
H ˆ 
. 
i 
i 
i 
Now, we plug this form for  ψ  into the expression for the average 
T
energy: 

= 

.

=


� 

ij 

= 

= 

= 

E  ≡ 
T 

T  H ψψ  ˆ 
T
ψ ψ 
T 
T

φ 
ˆα H 
i 
i 
φ α 
i 
i 

* φ α 
ƒ

j 
j
i
φ α 
ƒ

*
j 
j 
i 

φ α 
α 
φ *

 ƒ i 
jƒ

H ˆ
ƒ
i 
j 
j 
j 
i 
φ α 
φ 
α 
ƒ

ƒ

ƒ

* 
i 
j 
i 
j 
j 
i
j 
*α α  E i 
φ 
ƒ
α E 
φ φ 
α 
φ * 
ƒ 
jƒ

j 
i 
i 
i 
i 
j 
i
j 
j 
i
i j 
φ α 
* φ α 
φ φ α α 
ƒ

ƒ

ƒ 
*
j 
j 
i 
i 
j 
i
j
i
j 
ij 
i 
α α  Ei 
ƒ
* 
i 
i 
i 
*α α 
ƒ

i 
i
i 
Now, we note that  α α  ≡ wi 
*
is a real, positive number. Thus the trial
i
i 
energy is just a weighted sum of the eigenvalues of  H ˆ  : 
ƒ
E w 
i 
i 
i
ƒ

wi 
i 
Now, we make use of the fact that every eigenvalue,  Ei , of  H ˆ 
is by 
definition greater than or equal to the lowest eigenvalue,  E 0 , of  H ˆ  . 
Thus, 
ƒ
 E w  ≥ 
i  ƒ
 E w i 
0 
i 
i 
i
with equality holding only if  wi  is zero for every state except the 
ground state (or ground states if 
is degenerate).  Making use of 
E 0 
this for the trial energy: 

E  = 
T 

wi 

= E 0 

≥E T 

ƒ
ƒ 
wi 
E 
w
E 
0 
0  = 
i 
i 
i 
ƒ

ƒ 
w 
i 
i 
i 
Thus we see that the average energy for any trial state must be 
greater than or equal to the ground state energy.  This gives us a 
clear-cut way to determine the  “best ” approximation to the ground 
state out of a set of trial functions: whichever  ψ 
T has the lowest 
average energy is physically the best approximation to the ground 
state. 

E T 

To formulate this in practical terms, assume that  ψ 
T depends on a 
set of adjustable parameters,  c .  Then, consider the possible trial 
energies that can be obtained by varying  c .  W e write 
ψ ( c ) H ˆ  ψ ( cT 
) 
( c ) = 
T 
( c ) 
( c
)
ψ

ψ 
T
T 
And we see that the trial energy becomes a (many dimensional) 
function of the variable parameters.  According to the variational 
theorem, the  “best ” choice of the parameters for describing the 
( c ) .  This implies that the 
ground state are the ones that minimize  ET 
( c )  is zero at the variationally optimal choice of  c .  If we 
gradient of  ET 
call the optimal set of parameters  c 0 , then we can find these 
parameters as the solution of the equation: 
( c ) = 0
∇  ET 
c 
c0 
To make the connection with DVR, consider a large (but not 
{ 
} 
φ
comp
.  Most commonly, this set i
states 
s 
lete) set of orthonormal 
i 
obtained by truncation of some known complete orthonormal set (e.g. 
the harmonic oscillator energy eigenstates) to a set containing only  N 
{ 
} 
φ
states.  The 
are not eigenstates of the Hamiltonian we are 
i 
interested in; the eigenstates are the objects we want to approximate, 
so it would be foolish to assume they were known from the outset. 
We will be interested in trial states that are linear combinations of the 
{ 
} 
φ  : 
i 

ψ ( cT 
)  ƒ
N 
=

c  φ 
i 
i 
i = 1 
Clearly, as the number of basis functions becomes large ( N  → ∞ ), the 
basis becomes complete and we can write any state (including the 

eigenstates of  H  as a linear combination of the  {φ }.  Thus, in the 
i 
N  → ∞ limit, the choice of basis does not matter.  W e will get exactly 
the same answer (and in particular the same answer as obtained with 
DVR) for any choice of the  {φ }.  However, different choices of  {φ }
i 
i 
will give different answers for finite  N  .  In this respect, the variational 
principle also gives one an idea of which basis is  “best”: the one that 
gives the lowest average energy rigorously gives a better 
approximation to the ground state.  This is somewhat magical, since 
we do not know the ground state, but we can tell when we are close 
to it. 

Given the definition of the ket trial state, the bra state is just the 
hermitian conjugate: 

*
i 

ψ (c ) = ƒ φ c
N 
T 
i 
i =1 
And the trial energy is given by 
N 
N
c  ƒ c  φ
c  ƒ φ H 
* ˆ 
ψ (c *) H ψ ( )
ˆ 
j 
j 
i 
i 
=  i =1 
j =1 
T 
T 
ψ (c *) ψ (c 
) 
N 
N 
ƒ φ c  ƒ c  φ
* 
T 
T 
i 
i 
j 
j 
i =1 
j =1 

( c c  *) = 
, 

ET 

φ φ 
H ˆ 
j  c j
i

N 
*ƒ c i 
=  i , j =1
N
ƒ φ φ
*
j  c i  c j
i 
i , j =1 

N 
*ƒ c i 
i , j =1


φ φ 
H ˆ 
j  c j
i


= 

i 

N

* cƒ c 
i 
i =1 
Thus, the energy is a function of both  c  and  c * .  From complex 
variable calculus, we know that these are to be treated as 
independent variables, so the variational theorem gives us two 
conditions for the optimal choices of  c  and  c * : 
N 
N 
N 
φ 
φ 
φ φ 
H ˆ 
H ˆ 
j  c j  ƒ ci 
j  c j  ƒ 
ƒ ci 
*
* 
( c c  *)
∂ET 
i 
k 
, 
= j =1 
− i , j =1
i , j =1 
∂c k 
* 

∂ 
∂c k 
* 

φ φ 
H ˆ 
j  c j
i

0 = 

= 

N 
ƒ c 
* 
c 
i 
i 
i =1 

N 
ƒ c 
* 
c
i 
i =1 

i 

c k

2 
*  ’ 
≈ 
N
Δ ƒ c c  ÷i 
i 
« i =1 
◊ 

N 
φ φ 
kƒ 
H ˆ 
( c c  *)c
j  c j  ET 
, 
= j =1 
− 
N
N 
ƒ c 
ƒ c 
* 
*
c
i 
i 
i =1 
i =1 

c

i 

i 

k 

ˆ
0 = 

(  c c  *)
∂ ET 
, 
∂ c k 

= 

∂ 
∂ c k 

N 
ƒ c i 
* 
i , j = 1 

φ φ 
H ˆ
i 
k

N
φ φ 
H ˆ 
j  c j  ƒ c i 
*
i 
=  i = 1 
N 
ƒ c 
* 
c
i 
i = 1 

N 
ƒ c 
*
i 
i = 1 

c 
i 

i 

N 
ƒ c i 
*
−  i , j = 1

φ φ 
H ˆ
j  c j
i

2 
≈ 
*  ’ 
N
Δ ƒ c 
÷ 
c 
i 
i 
«  i = 1 
◊ 

* 
c k 

N 
*ƒ c i 
=  i = 1 

φ φ 
kH ˆ 
i 

c

N
ƒ c 
*
i 
i = 1 
Collapsing each equation 
N 
Ω 0 = ƒ 
j = 1 

i 

− 

*

(  c c  *) c 
, 
ET 
k

N

ƒ c 
* 
c
i 
i = 1 

i 

φ φ 
H ˆ 
j  c j 
k 

− ET 

(  c c  *) c
, 

k 

and 

k  . 
* 

and 

φ φ 
H ˆ
i 
k 

(  c c  *) c
N 
− ET 
Ω 0 = ƒ ci 
* 
,
i = 1 
We can write these two equations in matrix notation: 
(  c c  *) c 
= 
− 
H c 
Ω 0 
,
ET 
(  c c  *) c
Ω H c = ET 
, 
(  c c  *) c 
Ω 0 = H c †  − ET 
† 
,
(  c c  *) 
Ω c H = c  ET 
† 
,
Clearly, however, these two equations are hermitian conjugates of 
one another, and so we will typically just write the ket equation. 
These equations are just the eigenvalue equations for the matrix  H ; 
thus finding the variationally linear combination of an orthonormal set 
simply corresponds to finding the eigenvalues and eigenvectors of  H . 
The lowest eigenvalue approximates the ground state energy, the 
second lowest approximates the first excited state, etc.  One can 
actually prove a modified variational principle for the excited states 
defined in this manner: for any finite  N  , the mth  eigenvalue of  H  is 
always greater than or equal to the mth  eigenvalue of  H ˆ  . 

In practice, we can apply the variational method the same way DVR 
is used: First, we need to choose an appropriate orthonormal basis 
{φ } .  Second, we need to build the matrix representation of the 
i 
Hamiltonian in the HO basis: 

†
n 

x V ( )
2 
p  + 
H  ≡  m 
m n 
2m 
This is usually the most tedious step of the calculation, as it involves 
a good deal of algebra deriving the general matrix elements and a 
nontrivial amount of computer work to make sure the matrix is built 
properly.  Finally, we diagonalize the matrix  H  to obtain the 
approximate eigenvalues.  As an additional check, we should 
increase  N  and re-run the calculation to see that the result is 
converged. 

Thus, the only difference between DVR and variational mechanics is 
the way one builds  H : in the former case, one builds it from the 
P 2
constituent matrices  Q  and 
while in the latter, one builds  H 
directly. The variational principle is unique among matrix 
representation methods because it guarantees that the approximate 
energy is an upper bound to the true energy.  This gives us a 
rigorous way of evaluating “how close” we are to the correct answer 
and which basis is  “best ” for a given problem: one always seeks the 
lowest energy.  Because of its unique properties, the variational 
method is the technique of choice for many applications. 

