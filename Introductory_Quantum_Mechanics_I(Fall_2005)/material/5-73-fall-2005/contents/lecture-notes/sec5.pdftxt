V.  Time Dependence 
Up to this point, the systems we have dealt with have been time 
independent.  The variable  t  has not appeared in any of our 
equations and thus the formalism we have developed is not yet able 
to describe any kind of motion.  This is at odds with our 
understanding of reality, where things clearly move.  In order to 
describe quantum motion, we need one additional rule of quantum 
mechanics: The time evolution of any state is governed by 
Schrö dinger ’ s equation 

∂
iZ  ψ(t ) = Hˆ  ψ(t )  .
∂t 
Note that in quantum mechanics time,  t , plays a fundamentally 
different role than the position,  qˆ  , or momentum,  pˆ  .  The latter two 
are represented by operators that act on the states, while the time is 
treated as a parameter.  The state of the system depends on the 
parameter,  t , but it makes no sense to have a state that depends on 
an operator like  qˆ  .  That is to say,  ψ(t )  is well-defined but  ψ(qˆ  )  is 
not. 

In most cases, the dependence on  t  is understood and we can write 
the short-hand version of Schr ö dinger’ s equation: 
iZψ = H ψ . 
"
Time dependent quantum systems are the primary focus of the 
second half of this course.  However, it is appropriate at this point to 
at least introduce the basic principles of quantum dynamics, 
especially focusing on how it relates to  the time independent 
framework we’ ve developed. 

A. Energy Eigenstates Are Stationary States 

First, we want to address the very important question of how 
eigenstates of the Hamiltonian (i.e. energy eigenstates) evolve with 
time.  Applying Schrö dinger’ s equation, 
iZψ  = H ψ  = E  ψ
"
n 
n 
n 
n 
This is just a simple first order differential equation for  ψ (t )  and it is
n
easily verified that the general solution is: 

ˆ
ˆ
ψ (t ) = e  ψ (0 )
− i En t / Z
n 
n 
Thus, if the system starts in an energy eigenstate, it will remain in this 
eigenstate.  The only effect of the time evolution is to multiply the 
state by a time-dependent phase factor ( e − i Ent / Z ).  Since an overall 
phase factor cannot influence the outcome of an observation, from an 
experimental perspective, energy eigenstates do not change with 
time.  It is therefore termed a  “stationary state”.  This motivates our 
interest in finding energy eigenstates for arbitrary Hamiltonians; any 
other state has the potential to change between observations, but a 
stationary state lives forever if we don’t disturb it. 

B. The Propagator Governs Time Evolution 
So it is trivial to determine  ψ(t )  if the system begins in a stationary 
state.  But what if the initial state is not an eigenfunction of the 
Hamiltonian?  How do we evolve an arbitrary  ψ(t )  ?  As we show 
below, time evolution is governed by the propagator, 
K ˆ  (t ) ≡ e 
ˆ− tH 
/ Z
i 
in terms of which the time evolved state is given by 
ψ(t ) = K ˆ  (t )ψ(0 )  . 
In order to prove that this is so, we merely take the derivative of the 
propagator ansatz and verify it satisfies Schr ödinger’s equation: 
∂ 
∂ 
e  ψ(0 )
i Z  ψ(t ) = i Z 
ˆ− tHi 
/ Z
∂t 
∂t 
∂ (1 −  i 
tH  −  1  H  tH 2  +  i  HH tH 3  + . . . )ψ(0 )
= i Z 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
t ∂ 
Z 
2 Z2 
Z
= i Z(0 −  i H −  1  H  tH +  i  HH  tH 2  + . . . )ψ(0 )
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
= (H −  i H  tH  −  1  HH t H 2  + . . . )ψ(0 )
2 Z3 
Z2 
Z
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
= H ˆ  (1 −  i 
tH  −  1  H  tH  2  + . . . )ψ(0 )
Z 
2 Z 2 
ˆ 
ˆ 
ˆ 
ˆ= H (e  ψ(0 ) ) 
Z 
2 Z 2 
ˆ− tHi 
/ Z
= H ˆ  ψ(0 ) 
Since this wavefunction satisfies Schrödinger’s equation, we 
conclude that the propagator is, indeed, the time evolution operator. 
Physically, this operator performs a very impressive task:  K ˆ  (t )  takes 

any state and evolves that state forward to time  t  according to the 
Schrödinger equation . 

C. Important Properties of the Propagator 
There are a number of very important properties of the propagator, of 
which we mention only a few: 
The Propagator is Unitary:  K ˆ  ( t ) K ˆ  ( t ) = 1 
To see this, expand each propagator in a power series: 
†K ˆ  ( t ) K	
ˆ  ( t ) =

tH  −  1  H  tH 2  −  i  HH tH  3  + . . . )( 1 −  i 
( 1 +  i	
tH  −  1  H  tH 2  +  i  HH tH 3  + . . . )
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
Z	
Z 
2 Z 2 
2 Z2 
Z
Z
Expanding the product on the right and collecting terms, one obtains: 
t 0  terms: 1 
tH  −  i 
tH  = 0
ˆ 
t 1 
terms:  i 
terms:  −  1  H  tH 2  −  1  H  tH 2  − ( 
tH  )( 
tH  )  − = 
Z	
Z 
1  H  tH 2  +  1  H  tH 2  = 0
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
t 2	
i 
i 
Z 
Z 
Z2 
2 Z 2	
2 Z 2
2
Z 
… 
Thus, the only non-zero contribution is the  t 0  term, and  K ˆ  ( t ) K ˆ  ( t ) = 1 . 
This point gains significance when we realize that unitary operators 
correspond to a change of basis.  Thus time evolution corresponds 
to a change from the basis appropriate at time 0 to the basis 
appropriate at time  t . 
The Propagator obeys Time Reversal:  K ˆ  ( t ) = K ˆ  (− t ) 
Proof:  K  ( t ) = ( e 
) = e 
− Hi  ˆ ( − t ) / Z  = K ˆ  (− t )
= e 
/ Z  † 
− 
/ Z 
† 
tHi 
t Hi 
Thus, the Hermitian conjugate of the Propagator is also a propagator, 
but one that evolves the system backward in time.  This gives 
another physical justification for the Unitarity of the propagator, since 
†K ˆ  ( t ) K ˆ  ( t ) = K ˆ  (− t ) K ˆ  ( t ) .  On the right, we propagate the system forward 
in time by  t  and then immediately propagate backward by the same 
time interval.  Clearly, this operation cannot affect the state of the 
system and the unitarity and time reversal qualities of the propagator 
are related. 
The Product Rule:  K ˆ  ( t  ) K ˆ  ( t  ) = K ˆ  ( t  + t 2 
)
1
2 
1 
First, we note that this makes perfect sense from a physical 
perspective: on the left we have a product of operators that evolve 
the system for a time  t 1 , followed by another evolution for a time 
.  It 
t 2 

†
ˆ


ˆ
†
†
ˆ
ˆ
ˆ
)+ ... 

is clear that this should be the same as propagating once for a 
+ t 2  .  To verify that this is true, we expand out the 
combined time of  t 1 
propagators yet again: 
2  + ... )( 1 −  i 
K ( t  ) K ( t  ) = ( 1 −  i 
2  + ... )
−  1  H  t H 2
−  1  H  t H 1
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
tH  2 
tH 1 
)− (  1  H  tH 1
tH  ) +  1  H  tH 2 
= 1 −  i  (  tH  + 
)( 
2  − ( 
Z 
Z 
1
2 
2 Z 2 
2 Z 2 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
2
t H 1 
t H  2 
i 
i 
) −  1  HH ( t  + 2 t t  + t 2 
) + ... 
Z 
Z 
Z 
1
2
2 Z 2 
2 Z 2 
= 1 −  i H ˆ  ( t  + t 2 
ˆ 
ˆ 
2
2
Z 
1 
1
2 1 
2 Z 2
) −  1  HH ( t  + t 2 
= 1 −  i H ˆ  ( t  + t 2 
) + ... 
ˆ 
ˆ 
2
Z 
1 
1
2 Z 2
= K ˆ  ( t  + t 2 
)
1 
D. Time Dependence of Average Values 
We define the time dependent average of an operator by 
) 
O ˆ  ( t  ≡  ψ( t ) O ˆ  ψ( t )  .
We want to obtain an equation that governs the evolution of this 
average value.  W e do this by taking the time derivative and using  : 
O ˆ  ( t )  = ∂ ψ( t ) O ψ( t )  = ψ( t ) O ψ( t )  + ψ( t ) O ψ( t )
∂ 
ˆ 
ˆ 
" 
"
∂ t 
∂ t 
O ˆ  ψ( t )  + ψ( t ) O ˆ  H ˆ  ψ( t )
=  ψ( t )  H ˆ 
"
− i Z 
i Z 
where the second line makes use of the Schrödinger equation and its 
Hermitian conjugate: 
− i Z ψ = ψ H 
i Zψ = H ψ
ˆ 
" 
"
Simplifying the above expression, we obtain a powerful equation of 
motion for the evolution of this average: 
∂ 
i  ψ( t ) OH − HO ψ( t )
O ˆ  ( t )  − = 
ˆ 
∂ t 
Z 
Or, if we suppress the explicit dependence on time: 
i  [ O ˆ , H ˆ  ]  . 
" O ˆ 
− = 
Z 
This leads to the important conclusion that if an observable 
commutes with the Hamiltonian, its average value does not 
change with time.  This is clear, because in this case 
i  [ O ˆ , H ˆ  ]  − = 
i
" O ˆ 
0  = 0 . 
− = 
Z 
Z 

ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
’

÷ 
÷
◊


q ˆ " 

"ˆ 
p 

= 

ÿ
Ÿ
⁄

’

÷ 
÷
◊


i 
− = 
Z 

[  V q 
) ] 
(

ˆ ,  q ˆ

[  V p 
) ] 
(
ˆ ,  q ˆ

i

− = 
m Z 
2 
]
[ 
H ˆ
p ˆ ,


If we consider the specific case of a Hamiltonian  H ˆ  =  p ˆ 2  + V ( q ˆ  ) , we 
2 m 
obtain two important equations of motion: 
[  H ˆ  ]  − = 
≈
p ˆ 2  ÿ

» 
i 
i 
− = 
+
Ÿ
Δ
q ˆ , 
q ˆ , 
…
Δ

Z 
2
Z 
m 
⁄

«

p ˆ
2p ˆ
» 
q ˆ , 
…
2

m 
m

≈
p ˆ 2  ÿ

» 
i 
− = 
+
Ÿ
Δ
p ˆ , 
…
Δ

Z 
2
m 
⁄

«

) ] 
i  [ 
( 
'
(
q ˆ

 )
− = 
V− = 
q ˆ 
ˆ ,V p 
Z 
where a prime indicates differentiation with respect to the argument 
and we have used the fact that 
)] = −  g 
[ 
)] = 
[ 
ˆ ,  ( ˆ 
ˆ ,  ( ˆ 
′( p ˆ  )
′( q ˆ  ) 
Z 
f 
p 
f q 
i 
q g p 
i 
Z
for any functions  f  and  g .  Thus for any potential, the average 
values of position and momentum obey: 
(

) 
p ˆ
= 
− =  V 
" q ˆ 
q ˆ'
m 
These two equations are the quantum analog of Hamilton’s 
equations; the time derivative of the average position is the average 
velocity (momentum divided by mass) and the time derivative of the 
average momentum is the average force (the derivative of the 
potential).  W hat’s even more amazing is that  Planck’s constant 
� 
appears nowhere in these equations of motion!  Hence, these 
equations are almost entirely classical. 

and 

"ˆ
p 

It is useful to work these out for a specific example.  Consider a 
harmonic oscillator, in which case V ( q ˆ  ) =  1 m ω q ˆ 
2
2
.  Then the equations 
2 
of motion for the average position and momentum are: 
(

) 
p ˆ
= 
− = 
m ω q ˆ 
− =  V 
" q ˆ 
q ˆ'
m 
These equations should be compared with Hamilton’s equations for 
the Harmonic oscillator: 
q " =  p 
m 

p "  = − V ' ( q ) = − m ω q 

and 

and 

"ˆ 
p 





Thus, for the harmonic oscillator, the quantum equations of motion 
for the average values of  q  and  p  are precisely the classical 
equations for the variables  q  and  p !  For example, we can 
immediately write down the solution to the equations of motion 
because they are first order in time: 
p ( 0 )
t q (  =  q ( 0)  cos ω t + 
) 
sin ω t
ω 
( t p  )  = m  p ( 0)  cos ω t − m  qω  ( 0 )  sin ω t . 
These should be compared to the classical trajectory for a particle 
with initial position  q ( 0)  and initial momentum  p ( 0 ) : 
( t q  ) = q ( 0) cos ω t +  p ( 0) 
sin ω tω 
( t p  ) = m p ( 0) cos ω t − m ω q ( 0) sin ω t . 
Thus, if one starts the quantum system  in a state with an initial 
average position  q ( 0)  and initial average momentum  p ( 0 )  and the 
potential is harmonic, then the average values evolve exactly as if the 
system were following the classical trajectory. 

Is there then no difference between quantum dynamics and classical 
dynamics?  Quite the contrary.  The equivalence above holds only for 
the specific case of position and momentum operators in a Harmonic 
potential.  If we change either the operators involved or the potential, 
the quantum results become unique.  If we change our attention to 
2
, we immediately run into a problem because 
the operator  q 
classically q  ( 0 ) = q ( 0) ⋅ q ( 0 ) , whereas the uncertainty principle dictates 
2
that (in general)  q  ( 0) ≠  q ( 0 )  2 .  Meanwhile if we consider evolution 
2
in an anharmonic potential (like V ( q ˆ  ) =  q k ˆ 4 ) then the equation of 
' ( q ˆ  )  − =  4 q 
motion for  p ˆ  will involve the average force ( −  V 
ˆ 3  )
k 
.  That is to say that: 
which is not simply a function of 
q ˆ 
− =  V ' ( 
) q ˆ 
' ( q ˆ 
) 
− ≠  4 q 
− 
− =  4 q 
3
3
k  ˆ 
k  ˆ 
V 
p ˆ  are not even closed, 
and 
Thus, the equations of motion for 
q ˆ 
except in special cases, and an analytic solution is not typically 
possible. However, the similarity between the quantum equations of 
motion and the classical counterparts is the basis for a number of 
semiclassical approximations we may discuss later in the course. 

E. Matrix Representations of the Propagator 
Often, one is interested in the time evolution of a particular initial 
wavefunction.  That is, given some  φ(0)  one wants to know  φ(t )  . 
Now, formally we know that the solution to this problem is given by 
t K  )φ(0 )  . But how does one go about 
the propagator:  φ(t ) = 
ˆ  (
computing the propagator?  More specifically, how should one make 
ˆ  (
t K )  that is amenable to computation? 
a matrix representation of 
There are two routes to solving this problem. 

) 

First, let us assume we know the eigenstates and eigenvalues of the 
Hamiltonian: 

ˆH ψ  = E  ψ
n 
n 
n 
and let us try to work out the action of the propagator in this basis. 
We can re-write  φ(t )  as: 
∞ 
∞
(0
(0) = ƒ e n 
t K  )
t K  ) φ(0) = ƒ 
ˆ  (
ˆ  ( 
φ(t ) = 
φ ψ ψ 
φ ψ ψ 
−iE  t / Z
n 
n 
n 
n 
n=1 
n =1 
where in the first equality, we have inserted the identity in terms of 
the eigenstates of  Hˆ  and in the second equality we have used the 
fact that the eigenstates of  Hˆ  are stationary states.  W e now perform 
our favorite trick and multiply both sides by  ψ  :m
∞ 
φ ψ  (0)
(0) = e 
Ω  φ ψ  (t ) = ƒ e n 
φ ψ ψ ψ 
−iEm t / Z
−iE  t / Z 
� 
mn 
n 
m 
m 
n
m 
n=1 
If we expand  φ(0)  and  φ(t )  in terms of the eigenstates of the 
Hamiltonian: 

∞ 
∞
n c  (0)
(0) ≡ ƒ ψ
φ(0) = ƒ
φ ψ ψ 
n 
n 
n 
n =1 
n =1 
∞ 
∞ 
(t ) ≡ ƒ ψ  c  (t )
φ(t ) = ƒ
φ ψ ψ 
n 
n 
n 
n 
n=1 
n=1 
Then, our previous result reduces to 
φ ψ  (0 )
φ ψ  (t ) = e 
− iEm t / Z
m 
m 
c  (0 ) .
Ω c  (t ) = e
− iEmt / Z
m 
m
Thus, if we expand the wavefunction in terms of the eigenfunctions of 
the Hamiltonian, then the expansion coefficients (the  c  ) are 
m 
independent of one another (i.e.  c  does not depend on  c  ) and the 
m
n 

evolution of each coefficient is just a phase factor.  Motion arises in 
this picture from the fact that each  c  has a different phase.  Thus, at 
n 
different times we can get complicated constructive or destructive 
interference between different contributions. 

Now, if we assume that we do not know the eigenfunctions of  H ˆ  , we 
can also work out the action of the propagator on  φ( 0 )  using an 
arbitrary basis,  χ  .  Note that in the present picture, the basis is
n
fixed in time and all of the time dependence will be carried by the 
) =  φ χ ( t )  .  W e can re-write  φ( t )  as: 
(
coefficients 
t 
d 
n 
n 
∞
∞ 
t K  ) χ  d  ( 0 )
t K  ) χ  φ χ ( 0 ) = ƒ 
t K  ) φ( 0 ) = ƒ 
φ( t ) = 
ˆ  ( 
ˆ  ( 
ˆ  (
n 
n 
n 
n 
n = 1 
n = 1 
Again, we multiply both sides by  χ  :m
∞ 
φ χ  ( t ) = ƒ χ  t K  ) χ  d  ( 0 ) .
ˆ  (
m 
m 
n 
n 
n = 1 
If we then define the matrix representation of the propagator: 
K  ( t ) ≡  χ  t K  ) χ
ˆ  (
m n 
m 
n 
we can re-write the above equation using the convenient matrix short ­
hand: 
d( t ) = K ( t ) d( 0 ) . 
This expression is the natural counterpart of the Dirac expression 
ˆ  (  φ( 0 )  .  But we are still left with the difficulty of computing 
t K  ) 
φ( t ) = 
ˆ  ( χ  t K  ) χ  .  To do this, we will first assume that we know the matrix
m 
n 
elements of the Hamiltonian in this basis:  χ  H  χ  .  Then, if we 
m 
n 
expand the exponential in a power series, 
ˆ  ( χ  t K  ) χ  = χ  e  χ
ˆ− 
/ Z
t H 
i 
+ . . . ) χ
= χ ( 1 − it  H  −  t  HH  +  it 3 
m 
n 
m 
n 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
2
HHH 
Z 
2 Z 2
6 Z3 
m 
n
H H H  χ  + . . . 
χ  HH  χ 
t  χ  H ˆ  χ  − 
χ 
= δ  − i
+  it 3
ˆ 
ˆ 
ˆ 
ˆ 
ˆ 
2 
t 
m 
m n 
m 
n 
m 
n 
n
h
6 Z3 
Z 2 
2 
In order to evaluate the third and fourth terms, we insert the identity 
(once in the third term and twice in the fourth): 

ˆ
H ˆ 

χ  H ˆ 
m 

∞ 
χ  t K  ) χ  = δ  − i
ˆ  ( 
k H  χ
t  χ  H ˆ  χ  −  t  ƒ  χ  H  χ χ 
ˆ 
2
m 
n 
m n 
m 
n 
m 
k 
n 
h
2 Z 2 
k = 1 
∞ 
l H ˆ  χ  + ... 
χ χ 
χ χ 
6 Z 3  ƒ 
it 3 
l 
n 
k 
k 
l k  = 1 
, 
We now recognize that in the second and third terms the products of 
the operators ( H ˆ  ) can be replaced by products of the associated 
matrix ( H ).  Thus, if we use matrix shorthand 
K ( t ) = 1 
t  HH +  it 3  HHH + ... 
− 
− 
2
it  H 
h 
2 Z2
6 Z3 
However, this is just the power series expansion for the exponential 
of a matrix and we can therefore write 
K ( t ) = e 
H− 
i 
which allows us to succinctly express the time evolution of an 
arbitrary state in matrix notation: 
d( t ) = e 
d( 0) . 
H− 
i 

+ 

/ Z
t 

/ Z
t 

Once we have determined the time evolved states  (either in the 
energy eigenbasis or some other basis) we can easily compute any 
expectation value we like.  For example, the average value of an 
observable  O ˆ  at time  t  can be computed using the standard rules of 
matrix mechanics: 
ˆ  φ( t ) → d †  ( t ) Od ( t ) 
) φ(  O 
t 

In order to apply these equations in practice, one can follow a simple 
ansatz.  First, pick a basis  χ  , preferably one in which the matrix
n
elements of  H ˆ  can be easily computed.  As shown above, the 
extreme example of this is the energy eigenbasis, where  H  is just a 
diagonal matrix.  However, in practice any basis where  H  can be 
computed is acceptable.  The second step is to build the matrix 
K ( t ) = e − 
H
/ Z .  This can be done in several ways.  However, most 
i 
t 
precompiled programs have a utility for computing the exponential of 
a matrix and this is probably the easiest route to follow, in practice. 
Third, one applies  K ( t )  to the vectors that correspond to the state(s) 
of interest to obtain  d( t ) .  Fourth, one computes the desired 
t O ˆ  (  )  .  Note 
properties of the system, such as average values like 
that these average values could also, in principle, be obtained by 

ˆ
integrating the equations of motion discussed in the previous section. 
The result is the same, but the present method is typically simpler. 
Finally, one is only rarely interested in the state of the system at an 
isolated time; rather one is interested in tracing the behavior as a 
function of time.  Thus, one typically repeats this process for many 
times to get an idea of what the system is doing. 

It is important to note that, in doing this on a computer, one 
immediately makes an approximation. All the matrix representations 
above assume a complete (infinite) basis, whereas any computation 
will clearly have to use a finite number of basis functions (N ).  As 
before we will need to be sure to choose N  large enough that our 
calculations are converged. 

F. Inversion of the Ammonia Molecule 
Let’s do a simple application of the time evolution formalism we’ve 
developed. Suppose we have an ammonia molecule where each of 
the hydrogens is labeled (i.e. one is a proton, the other deuterium and 
the other tritium).  Then, the molecule is chiral and has two possible 
enantiomers (left-handed and right-handed). Let us represent these 
two states by  L  and  R  , respectively.  W e are interested in the 
imescale of the  “umbrella motion” that converts 
into  R  : if the 
t
L 
system begins i
at time 0, what is the probability of finding the 
n  L
lecule in  R  at time t? 
mo

To answer this question, we follow the steps outlined above: 

Build a Matrix Representation of  H ˆ  .  Since there are only two 
states,  H ˆ  will be represented by a 2x2 matrix: 
ˆ 
≈
’ 
R H L 
L H L 
H =

Δ
÷
Δ
÷
ˆ 
L H R 
R H R 
« 
◊ 
The diagonal elements are just the average energies of the two 
enantiomers, which must be the same (since they are just mirror 
images of one another).  So, we define a variable for this: 
=

= ε

ˆ R H R 
ˆ L H L 

ˆ
ˆ
=
 V


H =


We don’t know what the off-diagonal term is, so we’ll define another 
variable (which we assume is real) 
= 
LHR  ˆ 
RH L  ˆ 
Which allows us to write: 

ε  V ’
≈
.ε ÷÷
ΔΔ
V 
◊ 
« 
Build the time evolution operator  K ( t ) .  On a computer, this can be 
done in one step (by building the exponential of a matrix), but if we’re 
doing it by hand, there’s two steps: diagonalize  H  and then build the 
exponential in the eigenbasis.  To diagonalize  H , we obtain the 
eigenvalues and eigenvectors: 
− 
λε 
(  λε  ) − V
V
0 = det H = 
= 
−
2 
λε − 
V 
= ελ ± 
± 
± V Ω

= 
− λε 
Ω

V 
It’s easiest to guess the eigenvectors: 
1 
≈
 ’

1 
c h 
= 
c h
÷÷

ΔΔ
− 
+ 
1 
2 
«

◊


1
≈

ΔΔ
− 1
« 

1 
2 

’

÷÷

◊


= 

2

It is easy to verify that: 
ε±

1 ≈
 ε  V 
≈
 1
≈’ 1

V 
1
 ≈

’

’

’ 
1 
h
λ
h

= 
= 
± 
= ε 
Hc 
ΔΔ
ΔΔ
ΔΔ
÷
÷÷

÷÷

ΔΔ
÷
V 
c ± 
◊ε ÷ ± 
÷
± 
±
ε 
± 
±
1
1 
V 
V 
«2
2
2 
«

«

◊

◊

« 
◊ 
Further, once we have the eigenvectors, we can obtain the 
transformation matrix for the Hamiltonian: its columns are just the 
eigenvectors, so 

= 

T  ≡ 

1

1
’ 
≈

1
2 ΔΔ
÷÷

.
1  − 1
 ◊

«

We can verify that we’ve got everything right at this point by checking 
that 
? 
= 
H T

λ+ 
1 
0  1
1
1 
≈’
’

≈

≈’
1 
ΔΔ
÷÷
ΔΔ
ΔΔ
÷÷

÷÷
λ−
− 1 
1
 −
 1

0
1 
2 
«

◊
«

◊ 
◊ 
«

λ+ λ−  λ+ λ− 
ε
− 
+ 
V
’

≈

≈
’
1

= 
ΔΔ
ΔΔ
÷÷
÷÷

◊ε 
λ+ λ−  λ+ λ−
− 
+ 
V
2 
«

«

◊

Now that we’ve successfully diagonalized  H , we proceed to build the 
propagator: 

λ+  λ+ 
λ−  − λ− 

1
1
≈

≈’
ΔΔ
ΔΔ
÷÷

1  −
 1

« 
◊

«


’

T †
÷÷

◊ 

λ+ 
0 

0 
λ− 

1
2

’

÷÷

◊


= 

= 

≈

ΔΔ
«

= 

i λ+
− 
t

i λ+ 
− 
t
e 
0 

e

0

= 

≈

ΔΔ

«


0 
i λ− 
− 
t
e 

≈

1

=  ΔΔ
2
«

λ+ 
− 
i 
t

K ( t ) 
≈

? 
= 
ΔΔ

T 
«

1
1 
1
1
’

’

0
≈

’

’

T÷÷

÷÷
ΔΔ
÷÷
÷÷

1  − 1 
1  − 1
i λ−
−
t
e 
«

◊
◊

◊

◊

+
λ+ 
λ+
λ+
λ− 
− 
− 
− 
− 
1
1
 ≈

i
t
t
i
i
t
i 
t
≈

≈
’

e
e 
e 
e 
e
1 
1
ΔΔ
ΔΔ

ΔΔ
÷÷
1  − 1 
−
− 
λ− 
λ− 
λ+
− 
− 
− 
− 
i 
t
i
t
i
i
t
2 
2 
e 
e 
e 
e
e 
«

«◊
«

We can simplify this if we note that 
) = 1 
1 ( e − 
tV  ) = 1  −  t 
− iVt  )
( e  ± e 
( e 
i (ε  )
i (ε  ) ± e 
λ+  ± e 
i ε − iVt 
λ− 
+
− 
− 
− 
−
t
t
i 
i 
tV 
e 
2 
2
2 
if  + 
i εe 
−  t 
cosVt 
if  − 
− ie −  t 
i ε sinVt 

’

÷÷
◊


λ− 
− 
i 
t

= 

λ+
t

= À
Ã
Õ


−−
i
e 
+
−
e 

λ−
t

λ−
t
i

’
÷÷
◊


Thus, 

K ( t ) = e 
i ε
−  t 

− i sinVt 
cosVt 
’ 
≈

.
ΔΔ
÷÷

− i sinVt 
cosVt  ◊

«

We can check that this is correct by checking that this matrix is 
unitary: 
( t ) K ( t )= e 
≈

i ε
t 
ΔΔ
«


−
 i
 sin
Vt

cos
Vt

cosVt

−
 i
 sin
Vt 

cosVt 
i sin 
Vt 

sin
Vt

i 
cos 
Vt 

≈

i ε 
−  t 
ΔΔ
e 
«


1 0
0 1

’
÷÷

◊


≈
ΔΔ
«


’

÷÷

◊


K


=

† 

’

÷÷

◊


K


Apply the time evolution operator to the initial state. This involves 
significantly less algebra: 
− i sin
Vt  ≈’ 1
 ’

) 
(  c 
t  ( 0) = e 
cos
Vt 
cosVt

≈

≈

’

h 
i ε − 
i ε
−  t 
t 
ΔΔ
÷÷

÷÷

ΔΔ
÷÷

ΔΔ
−
 i
 sin

−
 i
 sin
Vt

cos

0
Vt 
Vt
«

◊

«

◊

«

◊

Hence, we see that the state oscillates with time.  This is because our 
initial state is not a stationary state, but a superposition of different 
stationary states.  The oscillation comes about because of the 
interference between the two states as a function of time. 

=
 e


Make measurements at time t. Here, we want to measure the 
: 
probability of being found in state 
R
( 
) =  cR 
V t
 = 
= − ie  ε 
2
−  t 
2 
2 V
t

i
sin 
sin
tP 
R 
Thus, the probability of the molecule being right-handed oscillates 
with time.  In particular, for  t = ( n +  )π /V  there is a 100% probability
1
2 
of the molecule being right-handed.  Thus, the timescale of the 
umbrella motion in ammonia is governed by the magnitude of the off-
=

= V  : a large value of V  leads to
LHR  ˆ
RHL  ˆ 
diagonal element 

†
fast interconversion and a small value leads to slow interconversion. 
In ammonia the off-diagonal coupling is such that the interconversion 
occurs with microwave frequency and by resonantly enhancing this 
motion, one can create a very efficient microwave laser (or maser). 

