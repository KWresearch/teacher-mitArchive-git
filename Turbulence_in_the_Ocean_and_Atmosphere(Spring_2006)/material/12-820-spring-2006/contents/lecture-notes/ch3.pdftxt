Chapter  3 

A  statistical  description  of 
turbulence 

The  evolution  of  turbulent  ﬂows  is  very  complex.  Turbulent  ﬂows  appear  highly  dis­
organized  with  structure  at  all  scales.  Signals  appear  unpredictable  in  their  detailed 
behavior.  However  some  statistical  properties  of  the  ﬂow  can  be  quite  reproducible. 
This  suggests  that  it  can be useful  to  seek  a  statistical  description  of  turbulent  ﬂows. 
A  statistical  measure  is  an  average  of  some  kind:  over  the  symmetry  coordinates, 
if  any  are  available  (e.g,  a  time  average  for  stationary  ﬂows);  over  multiple  realiza­
tions  (e.g,  an  ensemble);  or  over  the  phase  space  of  solutions  if  the  dynamics  are 
homogeneous. 

In  lecture 2 we have shown  that a similar behavior  is observed  in simple deterministic 
maps.  These  maps  are  chaotic  and  not  predictable  in  their  detailed  properties,  but 
their  statistical  properties  are  reproducible,  just  like  for  turbulent  ﬂows. 

Thus it seems quite appropriate to introduce a probabilistic description of turbulence. 
However  we  know  that  the  basic  Boussinesq  equations  are  deterministic.  How  can 
chance  and  chaos  arise  in  a  purely  deterministic  context?  A  nice  discussion  of  this 
issue  can  be  found,  in  chapter  3  of  Frisch’s  book. 

In  this  lecture we  ﬁrst  introduce  the  statistical  tools used  in  the analysis  of  turbulent 
ﬂows.  Then  we  show  how  to  apply  these  tools  to  the  study  of  turbulence. 

3.1  Probability  density  functions  and  moments 

A  complete  description  of  a  turbulent  variable  v  at  a  given  location  and  instant  in 
time  is  given  by  the  probability  density  function  (PDF), P (v ), where  P (v )dv  is 

1


the  probability  of  the  variable  v taking  a  value  between  v and  v + dv , and 
�  +∞ 
−∞ 

P (v )dv = 1. 

(3.1) 

The  PDF  can  be  estimated  by  constructing  a  histogram  of  an  ensemble  of  measure­
ments  of  v  at  the  speciﬁed  location,  repeating  the  experiment  time  and  time  again. 
The  larger  the  ensemble  (i.e.  the  more  times  the  experiment  is  repeated),  the  more 
closely  the  histogram  will  approximate  the  PDF. 

v nP (v )dv 

The moments of the variable v are derived from the PDF. The n-th moment < vm  > 
�  +∞ 
is  deﬁned  as 
< vn  >= 
−∞ 
The  ﬁrst  moment  < v > is  the mean:  �  +∞ 
< v >= 
vP (v )dv 
(3.3) 
−∞ 
The  variance  is  the  second  moment  of  the  perturbation  quantity  v (cid:2)  =  v−  < v >, 
and  describes  the  level  of  variability  about  this mean. 
�  +∞ 
(v− < v >)2P (v )dv 
< v (cid:2)2  >=
−∞ 
The  skewness  is  the  third moment  of  v (cid:2)

(3.2) 

,  normalized  by  the  variance: 
< v (cid:2)3  > 
skewness =  < v (cid:2)2  >3/2 

(3.4) 

(3.5) 

A PDF which  is symmetric about the mean < v > will have zero skewness.  All higher 
odd  moments  of  such  a  symmetric  PDF  will  also  be  identically  zero.  The  skewness 
reveals  information  about  the  asymmetry  of  the  PDF.  Positive  skewness  indicates 
that  the  PDF  has  a  longer  tail  for  v − (cid:1)v (cid:2) > 0 than for  v − (cid:1)v(cid:2) < 0.  Hence  a  positive 
skewness  means  that  variable  v (cid:2)  is  more  likely  to  take  on  large  positive  values  than 
large  negative  values.  A  time  series  with  long  stretches  of  small  negative  values  and 
a  few  instances  of  large  positive  values,  with  zero  time  mean,  has  positive  skewness 
(Fig.  3.1). 
The  kurtosis  (or  ﬂatness)  is  the  fourth moment  of  v (cid:2)
< v (cid:2)4  > 
kurtosis =  < v (cid:2)2  >2 

,  normalized  by  the  variance: 

(3.6) 

A  PDF  with  longer  tails  will  have  a  larger  kurtosis  than  a  PDF  with  narrower  tails. 
A  time  series  with  most  measurements  clustered  around  the  mean  has  low  kurtosis, 
a  time  series  dominated  by  intermittent  extreme  events  has  high kurtosis  (Fig.  3.2). 

2


Figure  3.1:  Signal  with  a  positive  skewness. 

This figure is copyrighted by the MIT Press, 1972. Tennekes, H. and J. L. Lumley. 
A first course in turbulence.  Used with permission. 

The  characteristic  function  Pˆ (k)  is  the  Fourier  transform  of  the  PDF, 
�  +∞ 
−∞ 

e ikvP (v )dv =< eikv  > . 

Pˆ (k) = 

(3.7) 

The  characteristic  function  of  the  sum  of  two  independent  variables  is  the  product  of 
their  individual  characteristic  functions. 

If  we  take  the  derivative  of  the  above  equation  with  respect  to  k  and  evaluate  it  at 
the  origin  (k  =  0)  we  see  that  the  n-th  moment  is  related  to  the  derivatives  of  the 
characteristic  function  by, 
dnPˆ (k) ��� �  = in  < vn  >, 
� 
dkn 
k=0 
√ 
where  i =  −1.  Then  the  characteristic  function  can  be written  as a Taylor  series  of 
the moments, 
�
∞  1  d P (k) ��� 
� 
n  ˆ
� 
n!  dkn 
n=0 
k=0 

� 
∞  (ik)n 
n! 
n=0 

< vn  > . 

(3.9) 

(3.8)

Pˆ (k) = 

kn  = 

3


Figure  3.2:  Signal  with  a  large  kurtosis. 

This figure is copyrighted by the MIT Press, 1972. Tennekes, H. and J. L. Lumley. 
A first course in turbulence. Used with permission. 

4


3.2  Joint  probability  density  functions 

Turbulence,  of  course,  involves  not  one,  but  several  random  variables  dependent  on 
each other.  Therefore,  it is necessary to deﬁne joint probability density  functions 
(JPDF).  For  example  the  JPDF  PJ (u, v ) of variables  u  and  v  is  the  probability  of 
ﬁnding  the  ﬁrst  random  variable  between  u  and  u + du,  and  the  second  one  between 
v  and  v  +  dv .  The  integral  of  PJ  over  the  u,v  two-dimensional  space  is  unity  by 
� 
�  ∞ ∞ 
deﬁnition, 
−∞ 
0 
and  PJ (u, v )  is  positive  deﬁnite.  We  recover  the  PDF  of  u  by  integrating  PJ  over  all 
values  of  v , and  the PDF  of  v  by  integrating  over  all  values  of  u, 
�  ∞ 
�  ∞ 
−∞ 
−∞ 

PJ (u, v ) du dv = 1, 

PJ (u, v ) du. 

P (u) = 

PJ (u, v ) dv , 

P (v ) = 

(3.10) 

(3.11) 

The  moments  of  u  and  v  can  therefore  be  obtained  from  PJ  as  well.  The  joint  ﬁrst 
moment  of  u  and  v ,  (cid:1)uv (cid:2),  is  deﬁned  as, 
�  ∞ ∞ 
� 
(cid:1)uv (cid:2) = 
−∞  −∞ 

uv PJ (u, v ) du dv . 

(3.12) 

The  covariance  of  u  and  v  is  deﬁned  as, 
(cid:2)(cid:2). 
C (u, v ) = (cid:1)uv (cid:2) − (cid:1)u(cid:2)(cid:1)v (cid:2) = (cid:1)u 
(cid:2) v 

(3.13) 

The covariance of u  and  v  normalized  by  the  rms  values  of u  and  v  (the  square  roots 
of  their  variances)  is  called  the  correlation  function  r(u, v ) and  is  used  to  quantify 
the  degree  of  correlation  between  u  and  v , 
(cid:1)u(cid:2)v (cid:2) (cid:2) 
r(u, v ) =  �
� 
. 
(cid:1)v (cid:2)2 (cid:2) 
(cid:1)u(cid:2)2(cid:2)
For perfectly  correlated  variables,  the  correlation  function  is ±1.  The  covariance  is  a 
measure  of  the  asymmetry  of  the  JPDF. 

(3.14) 

Two  variables  whose  covariance  is  zero,  or  equivalently  whose  correlation  is  zero, 
are  said  to  be  uncorrelated.  Notice  that  two  uncorrelated  variables  need  not  to  be 
independent.  Statistical  independence  of  two  variables  requires  that  the  JPDF  can 
be  expressed  as  the  product  of  the  individual  PDFs, 

PJ (u, v ) = P (u)P (v ). 

(3.15) 

Thus  independent  variables  are  uncorrelated,  but  the  reverse  is  not  true. 

5 

These  concepts  can  be  extended  to  three  or  more  variables,  but  it  is  rarely  the  case 
that  one  has  enough  information  about  the  statistics  of  a  ﬂow  to  measure  quantities 
like  PJ (u, v , w). 

While  PDFs  and  JPDFs  are  fundamental  to  theories  of  turbulence,  one  seldom mea­
sures  or  use  such  quantities.  Most  often,  only  the  ﬁrst  and  the  second  moments  are 
used to characterize the turbulent ﬂow.  Particular importance is given to covariances, 
which  appear  in  the  problem  of  Reynolds  averaging  and will  be  discussed  at  the  end 
of  this  lecture. 

3.3  Ergodicity  and  statistical  symmetries 

Computing  ensemble  means  is  a  daunting  task  when  dealing  with  laboratory  experi­
ments or numerical simulations, because it requires to repeat experiments or numerical 
runs  over  and  over.  In  the  case  of  geophysical  ﬂows,  the  situations  is  even  worse;  we 
cannot  ask  nature  to  repeat  weather  patterns  in  order  to  compute  our  means.  Does 
it  mean  that  a  statistical  description  of  turbulence  is  a  nice  theoretical  idea  with 
no  practical  relevance?  Fortunately  not,  because  we  can  estimate  ensemble  means 
through  time  and  spatial means,  under  certain  assumptions. 

•	 Stationarity 
Turbulence  is  stationary  if  all  mean  quantities  (e.g.  < v >,  < vn  > etc)  are 
invariant  under  a  translation  in  time.  A  stationary  variable  v is  ergodic  if  the 
time  average  of  v converges  to  the mean  < v > as  the  time  interval  extends  to 
� 
inﬁnity, 
v (t)dt =< v >  as  T  → ∞.	
1 
T 
T  0 
(See  Frisch,  4.4  for  a  detailed  discussion  of  ergodicity).  In  this  case  a  time 
average  is  equivalent  to  an  ensemble  average. 
•	 Homogeneity 
Turbulence  is homogeneous  if all  the mean quantities  are  invariant under  any 
spatial  translation.  Then  an  ergodic  hypothesis  allows  an  ensemble  average  to 
be  calculated  as  a  spatial  average, 
� 
v (x)dx =< v >  as  L → ∞.	
1  L 
L  0 

(3.16)

(3.17)

•	 Isotropy 
Turbulence  is  isotropic  if  all  the  mean  quantities  are  invariant  under  any  ar­
bitrary  rotation  of  coordinates. 

6 

•	 Axisymmetry 
Turbulence  is  axisymmetric  if  all  the  mean  quantities  are  invariant  under  a 
rotation about one particular axis only (e.g.  the z axis for stratiﬁed turbulence). 

Stationary,  isotropic,  homogeneous  statistics  are  frequently  assumed  in  theories  of 
turbulence,  but  are  in  fact  rare  in  geophysical  contexts where  stratiﬁcation  and Cori­
olis  break  the  symmetry  with  respect  to  rotation,  boundaries  break  the  symmetry 
with  respect  to  translation,  and  seasonal  and  diurnal  forcing  break  the  symmetry 
with  respect  to  time-shifts.  However  less  restrictive  assumptions,  to  be  described  in 
the  rest  of  this  class,  allow  progress  to  be made. 

3.4  Central  Limit  Theorem 

Many  quantities  in  turbulence  can  be  thought  of  as  a  sum  of  random  variables.  For 
example  a  net  particle  displacement  in  a  turbulent  ﬂow  may  be  thought  as  the  sum 
of  N  random  displacements  by  small  eddies;  a  net  velocity  may  be  thought  as  the 
sum  of  N  random  velocity  increments.  If  we  assume  the  incremental  changes  xn  all 
have  the  same  unknown  PDF  and  zero  mean  value,  then  the  net  change  in  variable 
is  given  by, 
� 
N 
xn . 
n=1 
If all  the  individual  incremental  changes are  independent  and uncorrelated with PDF 
P (x),  the  variance  of  z  is  given  by, 
�  � 
N N 
n=1 m=1 

< xnxm  >= N σ 2 , 

< z 2  >= 

z = 

(3.18) 

(3.19) 

where  σ2  is  the  variance  of  all  the  xn .  (Notice  that we  are  assuming  that  σ2  is  ﬁnite. 
This  is  not  always  the  case  as  for  example  for  power-law  PDFs). 

The variance of  z  increases  with  the  size  of  the  ensemble  N .  As  it  turns  out,  it  is 
more  convenient  to  work  in  terms  of  the  variable  u = N −1/2 z ,  which  has  variance  σ2 
for  all N .  We  can  now  determine  the  PDF  of  u. 
The  characteristic  function  of  P (u) is  given  by 
�N  xn (cid:2) = (Pˆ (kN −1/2 ))N . 
Pˆu (k) =< eiku  >= (cid:1)e ik/N 1/2 
n=1
We  can  expand  Pˆ (kN −1/2 )  as  shown  in  (3.9), 
Pˆ (kN −1/2 ) = 1 − 
k2σ2 
2N 

+ O(k3N −3/2 ). 

(3.20) 

(3.21)

7 

For  large N  we  obtain  the  characteristic  function  of  the  sum: 
Pˆu(k) =  lim  (1 − k2σ2/2N )N  = e 
−k2σ2 /2 . 
N→∞
The  corresponding  PDF  is 

(3.22)

P (u) = 

(3.23)

−u2 /2σ2 
e
, 

1 
(2πσ 2)1/2 
i.e.  a  Gaussian  PDF.  Therefore  the  sum  of  a  large  number  of  identically  distributed 
independent  variables has a Gaussian PDF, regardless of the shape of the PDF of the 
variables  themselves.  This  result  is  known  as  the  central  limit  theorem. 
√ 
The sum, normalized by 1/ N , of N  random,  independent and  identical ly 
distributed  variables  of  zero  mean  and  ﬁnite  variance,  σ 2 ,  is  a  random 
variable  with  a  PDF  converging  to  a  Gaussian  distribution  with  variance 
σ2 . 

It  is  important  to  remember  the  conditions  under  which  this  statement  holds:  the 
individual  variables  must  be  independent;  N  must  be  large;  the  variance  σ 2  of  the 
PDFs  of  the  individual  variables  must  be  ﬁnite. 

The  central  limit  theorem  is most  useful  as  a  starting  point,  because  deviations  from 
Gaussian  PDFs  indicate  that  some  of  the  assumptions  discussed  above  are  broken. 
This  happens  often  in  turbulent  ﬂows. 

3.5 

Intermittency 

A  signal  is  said  to  be  intermittent  if  rare  events  of  large magnitude  are  separated  by 
long  periods  with  events  of  low  magnitude.  Spatial  intermittency  implies  that  the 
signal  displays  localized  regions with  events  of  large magnitude,  and wide  areas with 
events  of  low magnitude.  PDFs  of  intermittent  ﬂows  are not Gaussian.  If  turbulence 
is  dominated  by  coherent  structures  localized  in  space  and  time,  then  PDFs  are  not 
Gaussian.  Flows  characterized  by  intermittency  are  not  self-similar. 

(Further reading:  Tennekes  and Lumley, Ch 6; Lesieur, Ch 5; Frisch, Ch 3, Ch 4, and 
Ch  8  for more  on  intermittency). 

3.6  The  Closure  Problem 

Although  it  is  impossible  to  predict  the  detailed  motion  of  each  eddy  in  a  turbulent 
ﬂow,  the  mean  state  may  not  be  changing.  For  example,  consider  the  weather  sys­

8


tem,  in which  the  storms,  anti-cyclones,  hurricanes,  fronts  etc.  constitute  the  eddies. 
Although  we  cannot  predict  these  very  well,  we  certainly  have  some  skill  at  predict­
ing  their  mean  state,  the  climate.  For  example,  we  know  that  next  summer  will  be 
warmer  than  next  winter,  and  that  in  California  summer  will  be  drier  than  winter. 
We  know  that  next  year  it  will  be  colder  in  Canada  than  in  Mexico,  although  there 
might  be  an  occasional  day  when  this  is  not  so.  We  would  obviously  like  to  be  able 
to  predict  the  mean  climate  without  necessarily  trying  to  predict  or  even  simulate 
all  the  eddies.  We  might  like  to  know  what  the  climate  will  be  like  one  hundred 
years  from  now,  without  trying  to  know  what  the  weather  will  be  like  on  February 
9,  2056, plainly  an  impossible  task.  Even  though we  know what  equations  determine 
the  system,  this  task  proves  to  be  very  diﬃcult  because  the  equations  are  nonlinear. 
This  is  the  same  problem  we  discussed  at  the  beginning  of  the  lecture.  We  seek  a 
statistical  description  of  the  turbulent  ﬂow,  because  a  detailed  description  is  beyond 
our  grasp.  The  simplest  statistical  quantity  we  might  try  to  predict  is  the  mean 
ﬂow.  However,  because of the nonlinearities  in  the equations, we come up against the 
closure  problem. 

The  program  is  to  ﬁrst  decompose  the  velocity  ﬁeld  into  mean  and  ﬂuctuating  com­
ponents, 
(cid:2) . 
u = u¯ + u 
(3.24) 
(cid:2)  is  the  deviation  from  that mean.  The mean 
Here u  is  the mean  velocity  ﬁeld,  and u
may  be  a  time  average,  in  which  case  u¯ is  a  function  only  of  space  and  not  time. 
It  might  be  a  time  mean  over  a  ﬁnite  period  (e.g  a  season  if  we  are  dealing  with 
the  weather).  Most  generally  it  is  an  ensemble  mean.  Note  that  the  average  of  the 
(cid:2)  = 0.  We then substitute into the momentum 
deviation is, by deﬁnition, zero; that is u
equation  and  try  to  obtain  a  closed  equation  for  u.  To  visualize  the  problem  most 
simply,  we  carry  out  this  program  for  a  model  nonlinear  system  proposed  by  Geoﬀ 
Vallis  (http://www.princeton.edu/  gkv/aofd.pdf )  which  obeys, 
du 
dt 

+ uu + ν u = 0. 

(3.25) 

The  average  of  this  equation  is, 

+ uu + ν ¯u = 0. 

d ¯u 
dt 
The  value  of  the  term  uu  is  not  deducible  simply  by  knowing  u¯,  since  it  involves 
(cid:6) uu¯.  We can go 
correlations  between  eddy  quantities  u(cid:2)u(cid:2) . That  is,  uu = ¯uu¯ + u(cid:2)u(cid:2)  = ¯
to  next  order  to  try  to  obtain  a  value  for  uu.  First multiply  (3.25)  by  u  to  obtain  an 
equation  for  u2  ,  and  then  average  it  to  yield, 
1 duu 
dt 
2 
This  equation  contains  the  undetermined  cubic  term  uuu.  An  equation  determining 
this  would  contain  a  quartic  term,  and  so  on  in  an  unclosed  hierarchy.  Most  current 

+ uuu + ν uu = 0. 

(3.26) 

(3.27)

9


methods of closing the hierarchy make assumptions about the relationship of (n+1)-th 
order  terms  to  n-th  order  terms,  for  example  by  supposing  that, 
uuuu = uu  uu − αuuu, 

(3.28) 

where  α  is  some  parameter.  Such  assumptions  require  additional,  and  sometimes 
dubious,  reasoning.  Nobody  has  been  able  to  close  the  system  without  introducing 
physical  assumptions  not  directly  deducible  from  the  equations  of motion. 

3.7  The  Reynolds  equations 

Let  us  repeat  the  averaging  procedure  for  the  full  Boussinesq  equations.  We  start 
with  the momentum  equations, 
+ (u¯ · ∇)u¯ + f zˆ × u¯ = ¯bzˆ − ∇p¯ + ν∇2 u¯ − (u(cid:2)  · ∇)u(cid:2) , 
1 
ρ0 

∂u¯
∂ t 

(3.29)

The  extra  term  on  the  right  hand  side  represent  the  eﬀect  of  eddy  motions  on  the 
mean  ﬂow.  If  the  average  operator  is  a  time  average  over  some  time  T ,  then  eddy 
motions are  those motions with  time scales  shorter  than T .  If  the average operator  is 
a spatial average over some scale L, then eddy motions are those motions with spatial 
scales  shorter  than  L.  If  the  average  operator  is  an  ensemble  mean,  then  the  eddy 
motions  are  those motions  that  change  in  every  realizations,  regardless  of  their  scale, 
i.e.  they  represent  the  unpredictable  or  turbulent  part  of  the  ﬂow. 

Using  the  continuity  equation, 
∇ · u = 0  ⇒ 

∇ · u¯ = 0  and  ∇ · u 
(cid:2)  = 0, 

we  can  rewrite  the  averaged  momentum  equation  as, 
� 
� 
+ (u¯ · ∇)u¯ + f zˆ × u¯ = ¯bzˆ −  ∇ ·  p¯I − ρ0ν∇u¯ + ρ0u(cid:2)u(cid:2)  . 
1 
ρ0 

∂u¯
∂ t 

(3.30) 

(3.31)

I  is  th  unit  matrix.  These  are  the  so-called  Reynolds  momentum  equation  and 
the  eddy  ﬂux  ρ0u(cid:2)u(cid:2)  represent  the  Reynolds  stress  tensor  due  to  ﬂuctuations  in 
velocity  ﬁled. 

We  can  similarly  decompose  the  buoyancy  equation  into  a  mean  and  a  ﬂuctuating 
component, b = ¯b + b(cid:2)
, and write an equation for the mean component by substituting 
back  into  the  buoyancy  equation, 
�
� 
+ (u¯ · ∇)¯b = −∇ ·  −κ∇¯b + u(cid:2)b(cid:2)  . 

(3.32)

∂¯b 
∂ t 

10


The  eddy  term  u(cid:2)b(cid:2)  represent  the Reynolds  eddy  ﬂux  of  buoyancy. 

The  problem  of  turbulence  is  often  that  of  ﬁnding  a  representation  of  such  Reynolds 
stress  and  ﬂux  terms  in  terms  of mean  ﬂow  quantities.  However,  it  is  not  at  all  clear 
that  a  general  solution  (or  parameterization)  exists,  short  of  computing  the  terms 
explicitly. 

3.8  Eddy  viscosity  and  eddy  diﬀusivity 

The  simplest  closure  for  the  Reynolds  stress  terms  is  one  which  relates  u(cid:2)u(cid:2)  to  the 
mean  ﬂow,  by  assuming  a  relation  of  the  form, 
u(cid:2)u(cid:2)  = −νT ∇u¯ , 
where  νT  is  the  eddy  viscosity.  With  such  a  closure  the  Reynolds  stress  term 
takes  the  same  form  as  the  mean  viscosity  term,  but  with  a  diﬀerent  viscosity.  In 
essence,  this  closures  states  that  turbulent  eddies  are  similar  to  molecular  motions 
that  constantly  act  to  redistribute  and  homogenize  momentum.  Similarly,  for  the 
tracer  ﬂux  term  we  can  deﬁne  an  eddy  diﬀusivity 
u(cid:2)b(cid:2)  = −κT ∇¯b. 
This  eddy  viscosity/diﬀusivity  closure  is  the  most  commonly  used  in  modeling  and 
interpretation of geophysical observations.  At the crudest level κT  and νT  are assumed 
to  be  constants;  in  more  sophisticated  models  they  are  functions  of  the  large  scale 
ﬂow.  However,  an  eddy  viscosity/diﬀusivity  closure  is  rarely  appropriate. 

(3.34) 

(3.33) 

(Further  reading:  Chapter  1  of McComb) 

11


