9.520: Class 20

Bayesian Interpretations

Tomaso Poggio and Sayan Mukherjee

Plan

(cid:15) Bayesian interpretation of Regularization
(cid:15) Bayesian interpretation of the regularizer
(cid:15) Bayesian interpretation of quadratic loss
(cid:15) Bayesian interpretation of SVM loss
(cid:15) Consistency check of MAP and mean solutions for quadratic loss
(cid:15) Synthesizing kernels from data: bayesian foundations
(cid:15) Selection (called \alignment") as a special case of kernel synthesis

Bayesian Interpretation of RN, SVM, and
BPD in Regression

Consider

1
‘

‘
(yi (cid:0) f (xi))2 + (cid:21)kf k2
Xi=1
min
K
f 2H
We will show that there is a Bayesian interpretation of RN
in which the data term { that is the term with the loss
function { is a model of the noise and the stabilizer is a
prior on the hypothesis space of functions f .

De(cid:12)nitions

1. D‘ = f(xi; yi)g for i = 1; (cid:1) (cid:1) (cid:1) ; ‘ is the set of training
examples

2. P [f jD‘] is the conditional probability of the function f
given the examples g .

3. P [D‘jf ] is the conditional probability of g given f , i.e.
a model of the noise.

4. P [f ] is the a priori probability of the random (cid:12)eld f .

Posterior Probability

The posterior distribution P [f jg ] can be computed by ap-
plying Bayes rule:
P [f jD‘] = P [D‘jf ] P [f ]
P (D‘)

:

If the noise is normally distributed with variance (cid:27) , then
the probability P [D‘jf ] is
2(cid:27)2 P‘
1
e(cid:0) 1
i=1(yi(cid:0)f (xi))2
P [D‘jf ] =
ZL
where ZL is a normalization constant.

Posterior Probability

Informally (we will make it precise later), if

e(cid:0)kf k2
K

1
P [f ] =
Zr
where Zr is another normalization constant, then
K (cid:17)
e(cid:0)(cid:16) 1
2(cid:27)2 P‘
i=1(yi(cid:0)f (xi))2+kf k2

1
ZDZLZr

P [f jD‘] =

MAP Estimate

One of the several possible estimates of f from P [f jD‘] is
the so called MAP estimate, that is

‘
(yi (cid:0) f (xi))2 + 2(cid:27)2kf k2
Xi=1
max P [f jD‘] = min
K :
which is the same as the regularization functional if

(cid:21) = 2(cid:27)2=‘:

Bayesian Interpretation of the Data Term
(quadratic loss)

As we just showed, the quadratic loss (the standard RN
case) corresponds in the Bayesian interpretation to as-
suming that the data yi are a(cid:11)ected by additive indepen-
dent Gaussian noise processes, i.e. yi = f (xi) + (cid:15)i with
E [(cid:15)j (cid:15)j ] = 2(cid:14)i;j

P (yjf ) / exp((cid:0) X(yi (cid:0) f (xi))2)

Bayesian Interpretation of the Data Term
(nonquadratic loss)

To (cid:12)nd the Bayesian interpretation of the SVM loss, we
now assume a more general form of noise. We assume that
the data are a(cid:11)ected by additive independent noise sam-
pled form a continuous mixture of Gaussian distributions
with variance (cid:12) and mean (cid:22) according to
P (yjf ) / exp (cid:18)(cid:0) Z 1
d(cid:12) Z 1
P ((cid:12) ; (cid:22))(cid:19) ;
d(cid:22)q(cid:12)e(cid:0)(cid:12) (y(cid:0)f (x)(cid:0)(cid:22))2
0
(cid:0)1
The previous case of quadratic loss corresponds to
1
2(cid:27)2 (cid:19) (cid:14)((cid:22)):
P ((cid:12) ; (cid:22)) = (cid:14) (cid:18)(cid:12) (cid:0)

Bayesian Interpretation of the Data Term
(absolute loss)

To (cid:12)nd P ((cid:12) ; (cid:22)) that yields a given loss function V ((cid:13) ) we
have to solve
V ((cid:13) ) = (cid:0) log Z 1
0
where (cid:13) = y (cid:0) f (x).

d(cid:22)q(cid:12)e(cid:0)(cid:12) ((cid:13)(cid:0)(cid:22))2

d(cid:12) Z 1
(cid:0)1

P ((cid:12) ; (cid:22));

For the absolute loss function V ((cid:13) ) = j(cid:13) j. Then
P ((cid:12) ; (cid:22)) = (cid:12)(cid:0)2e(cid:0) 1
4(cid:12) (cid:14)((cid:22)):
For unbiased noise distributions the above derivation can
be obtained via the inverse Laplace transform.

Bayesian Interpretation of the Data Term
(SVM loss)

Consider now the case of the SVM loss function V(cid:15)((cid:13) ) =
maxfj(cid:13) j (cid:0) (cid:15); 0g. To solve for P(cid:15)((cid:12) ; (cid:22)) we assume indepen-
dence

P(cid:15)((cid:12) ; (cid:22)) = P ((cid:12) )P(cid:15)((cid:22)):

d(cid:12) Z 1
(cid:0)1

P ((cid:12) )P(cid:15)((cid:22))

Solving
V(cid:15)((cid:13) ) = (cid:0) log Z 1
0
results in
P ((cid:12) ) = (cid:12)(cid:0)2e(cid:0) 1
4(cid:12) ;
1
2((cid:15) + 1) (cid:16)(cid:31)[(cid:0)(cid:15);(cid:15)]((cid:22)) + (cid:14)((cid:22) (cid:0) (cid:15)) + (cid:14)((cid:22) + (cid:15))(cid:17) :
P(cid:15)((cid:22)) =

d(cid:22)q(cid:12)e(cid:0)(cid:12) ((cid:13)(cid:0)(cid:22))2

Bayesian Interpretation of the Data Term
(SVM)

Bayesian Interpretation of the Data Term
(SVM loss and absolute loss)

Note lim(cid:15)!0 V(cid:15) = j(cid:13) j

So

and

P0((cid:22)) =

1
2 (cid:16)(cid:31)[(cid:0)0;0]((cid:22)) + (cid:14)((cid:22)) + (cid:14)((cid:22))(cid:17) = (cid:14)((cid:22))
P ((cid:12) ; (cid:22)) = (cid:12)(cid:0)2e(cid:0) 1
4(cid:12) (cid:14)((cid:22));
as is the case for absolute loss.

Bayesian Interpretation of the Stabilizer

P (f ) =

The stabilizer kf k2
K is the same for RN and SVM. Let us
consider the corresponding prior in a Bayesian interpreta-
tion within the framework of RKHS:
1
exp((cid:0)kf k2
) = exp((cid:0)c>(cid:3)(cid:0)1c):
Xn=1
K ) / exp((cid:0)
Thus, the stabilizer can be thought of as measuring a Ma-
halanobis \norm" with the positive de(cid:12)nite matrix (cid:3) play-
ing the role of a (diagonal) covariance matrix. The most
likely hypotheses are the ones with small RKHS norm.

1
Zr

c2
n
(cid:21)n

Bayesian Interpretation of RN and SVM.

(cid:15) For SVM the prior is the same Gaussian prior, but the
noise model is di(cid:11)erent and is NOT Gaussian additive
as in RN.

(cid:15) Thus also for SVM (regression) the prior P (f ) gives a
probability measure to f in terms of the Mahalanobis
\norm" or equivalently by the norm in the RKHS de-
(cid:12)ned by R, which is a covariance function (positive
de(cid:12)nite!)

Why a Bayesian Interpretation can be
Misleading

Minimization of functionals such as HRN (f ) and HSV M (f ) can be inter-
preted as corresponding to the MAP estimate of the posterior prob-
ability of f given the data, for certain models of the noise and for a
speci(cid:12)c Gaussian prior on the space of functions f .

Notice that a Bayesian interpretation of this type is inconsistent with
Structural Risk Minimization and more generally with Vapnik’s analysis
of the learning problem. Let us see why (Vapnik).

Why a Bayesian Interpretation can be
Misleading

Consider regularization (including SVM). The Bayesian interpretation
with a MAP estimates leads to
‘
Xi=1
Regularization (in general and as implied by VC theory) corresponds
to

(yi (cid:0) f (xi))2 +

2(cid:27) 2kf k2
K :

min H [f ] =

1
‘

1
‘

‘
Xi=1
where (cid:21) is found by solving the Ivanov problem

min HRN [f ] =

1
‘

(yi (cid:0) f (xi))2 + (cid:21)kf k2
K :

subject to

min

1
‘

‘
Xi=1

(yi (cid:0) f (xi))2

K (cid:20) A
kf k2

Why a Bayesian Interpretation can be
Misleading

The parameter (cid:21) in regularization and SVM is a function of the data
(through the SRM principle) and in particular is (cid:21)(‘).
In the Bayes
interpretation ~(cid:21) depends on the data as 2(cid:27) 2
: notice that (cid:27) has to be
‘
part of the prior and therefore has to be independent of the size ‘ of
the training data. It seems unlikely that (cid:21) could simply depend on 1
as
‘
the Bayesian interpretation requires for consistency. For instance note
that in the statistical interpretation of classical regularization (Ivanov,
Tikhonov, Arsenin) the asymptotic dependence of (cid:21) on ‘ is di(cid:11)erent
from the one dictated by the Bayesian interpretation. In fact (Vapnik,
1995, 1998)

(cid:21)(‘) = 0

lim
‘!1
‘(cid:21)(‘) = 1
lim
‘!1
implying a dependence of the type (cid:21)(‘) = O(log‘=‘). A similar de-
pendence is probably implied by results of Cucker and Smale, 2002.
Notice that this is a su(cid:14)cient and not a necessary condition. Here an
interesting question (a project?): which (cid:21) dependence does stability
imply?

Why a Bayesian Interpretation can be
Misleading: another point

The Bayesian interpretation forces one to interpret the loss function
in the usual regularization functional (this could be modi(cid:12)ed but this
is another story) as a model of the noise. This seems a somewhat
unnatural constraint: one would expect to have a choice of cost in-
dependently of the noise type. Conjecture: prove that a probablistic
model of the SVMC loss cannot be interpreted in a natural way in
terms of a noise model’: project?

The argument is that j1 (cid:0) f y j+ cannot be \naturally" interpreted as
additive or multiplicative noise. It is a noise that a(cid:11)ects real-valued f
to give (cid:0)1; +1 with probability that depends on f y . However, we may
think of taking sign(f ): in this case then the noise (cid:13)ips the true sign
with probability ??

From Last Year Class Project...

Consistency check of MAP and mean
solutions for quadratic loss (from
Pontil-Poggio)

D‘ : the set of i.i.d. examples f(xi; yi) 2 X (cid:2) Y g‘
i=1, etc.
Introduce the new basis functions ’n = p(cid:21)n(cid:30)n. A function
f 2 HK has a unique representation, f = Pn bn’n, with
K = Pn b2
kf k2
n.

Bayesian Average

(cid:22)f = Z P (f jD‘)d(f )
where P (f jD‘) = P (D‘jf )P (f )
.
P (D‘)

In the ’n’s representation, Eq. 1 can be written as
(cid:22)f = Z Z 1
dbnbT (cid:30) expf(cid:0)H (b)g
Yn=1
with Z a normalization constant and
1
‘
1
Xn=1
Xi=1
‘

bn’n(xi)) + (cid:21)

V (yi (cid:0)

H (b) =

1
Xn=1

b2
n:

(1)

(2)

Bayesian Average (cont.)
The integral is not well de(cid:12)ned (it’s not clear what Q1n=1 dcn means).
We de(cid:12)ne the average function (cid:22)fN and study the limit for N going to
in(cid:12)nite afterwards. Thus we de(cid:12)ne
bn’n(xi)!2
N
N
‘
Xi=1  yi (cid:0)
1
Xn=1
Xn=1
‘
N
(cid:22)fN := ZN Z
Yn=1

dbn(bT ’) expf(cid:0)HN (b)g

HN (b) =

+ (cid:21)

b2
n

(3)

Bayesian Average (cont.)

We write:

bn(

’n(xi)yi) +

b2
n

HN (b) =

‘
N
1
1
Xi=1
Xn=1
‘ Xi
y2
i (cid:0) 2
‘
1
Xn;m
‘ Xi
’n(xi)’m(xi) + (cid:21)
bnbm
1
‘ Xi
y2
i (cid:0) 2bT ~y + bT ((cid:21)I + M )b
where we have de(cid:12)ned ~yn = 1
‘ Pi ’n(xi)yi and Mnm = 1
‘ Pi ’n(xi)’m(xi).
The integral in Eq. 3 can be rewritten as

N
Xn=1

=

(cid:22)fN = ZN expf(cid:0)

1
‘ Xi

i g Z
y2

N
Yn=1

dbn(b0’) expf(cid:0)bT ((cid:21)I + M )b + 2bT ~yg (4)

Bayesian Average (cont.)

Using the appropriate integral in Appendix A we have

(cid:22)fN (x) =

’n(x)

((cid:21)I + M )(cid:0)1
nm~ym

N
N
Xn=1
Xm=1
which is the same at the MAP solution of regularization networks when
the kernel function is the truncated series, K N (x; t) = PN
i=1 ’n(x)’n(t).
We write
‘
Xi=1
(cid:22)fN (x) =
i K N (xi ; x)
(cid:11)N

(5)

with

i =
(cid:11)N

‘
Xj=1
(K + (cid:21)I )(cid:0)1
ij yj
Now study the limit N ! 1. We hope that (cid:22)fN indeed converges to (cid:22)f
in the RKHS. Then from the property of this space we hope to deduce
that the convergence also holds in the norm of C (X ). Finishing this
proof is a 2003 class project!

We compute the variance of the solution:

Correlation

C (x; y) = E (cid:2)(cid:0)f (x) (cid:0) (cid:22)f (x)(cid:1) (cid:0)f (y) (cid:0) (cid:22)f (y)(cid:1)(cid:3)
(6)
where E denotes the average w.r.t P (f jDm). Again, we study this
quantity as the limit of a well de(cid:12)ned one,
CN (x; y) = E (cid:2)(cid:0)fN (x) (cid:0) (cid:22)fN (x)(cid:1) (cid:0)fN (y) (cid:0) (cid:22)fN (y)(cid:1)(cid:3)
= E [fN (x)fN (y)] + E [fN (x)] E [fN (y)] :
Using the gaussian integral in Appendix we obtain:

CN (x; y) =

1
2

N
Xn;m=1
Note that when (cid:21) ! 1 we get KN (x; y), so when no data term is
present the best guess for the correlation function is just the kernel
itself.

’n(x)((cid:21)I + M )(cid:0)1
nm’m(y)

A Priori Information and \kernel synthesis"

in
Consider a special case of the regression-classi(cid:12)cation problem:
addition to the training data { values of f at locations x i { we have
information about the hypothesis space that is the class of functions to
which f belongs. In particular, we know examples of f in the space and
we know or can estimate (in practice often impossible: more later!)
the correlation function R. Formally: f belongs to a set of functions
f(cid:11) with distribution P ((cid:11)). Then

R(x; y) = E [(f(cid:11)(x)f(cid:11)(y)]
where E [(cid:1)] denotes expectation with respect to P ((cid:11)). We assume that
E [f(cid:11)(x)] = 0.

Since R is positive de(cid:12)nite it induces a RKHS with the (cid:21)n de(cid:12)ned by the
eigenvalue problem satis(cid:12)ed by R. It follows that we have synthesized
a \natural" kernel R { among the many possible { for solving the
regression-classi(cid:12)cation problem from discrete data for f .

Example of R

The sinc function is a translation invariant correlation func-
tion associated with the hypothesis space consisting of
one-dimensional band-limited functions with a (cid:13)at Fourier
spectrum up to fc (and zero for higher frequencies). The
sinc function is a positive de(cid:12)nite reproducing kernel with
negative lobes.

Sometime possible Kernel synthesis:
regression example

(cid:15) Assume that the problem is to estimate the image f on a regular
grid from sparse data yi at location xi; x = (x; y) on the plane.

(cid:15) Assume that I have full resolution images of the same type f(cid:11)
drawn from a probability distribution P ((cid:11)).

(cid:15) Remember that in the Bayesian interpretation choosing a kernel
K is equivalent to assuming a Gaussian prior on f with covariance
equal to K .

(cid:15) Thus an empirical estimate of the correlation function associated
with a function f should be used, if it is available, as the kernel.
Thus K (x; y) = E (f(cid:11)(x)f(cid:11)(y)).

(cid:15) The previous assumption is equivalent to assuming that the RKHS
is the span of the f(cid:11) with the dot product induced by K above.

(cid:15) Problem, may be a project: Suppose I know that the prior on f
is NOT Gaussian. What happens? What can I say?

Usually impossible kernel synthesis:
classi(cid:12)cation

In the classi(cid:12)cation case, unlike the special regression case described
earlier, it is usually impossible to obtain an empirical estimate of the
correlation function

R(x; y) = E [f(cid:11)(x)f(cid:11)(y)]

because a) the dimensionality is usually too high and b) R cannot be
estimated at \all" x; y (unlike the previous grid case).

Classi(cid:12)cation: same scenario, another point
of view: RKHS of experts.

Assume I have a set of examples of functions from the hy-
pothesis space i.e. real-valued classi(cid:12)ers of the same type,
say a set of face detection experts or algorithms. Then I
consider the RKHS induced by the span of such experts,
that is functions f (x) = P b(cid:11)t(cid:11)(x). The RKHS norm is
de(cid:12)ned as jf j2
K = bT (cid:6)(cid:0)1b, with (cid:6) = P P(cid:11)t(cid:11)(x)t(cid:11)(y) being
the correlation function. The (cid:30)i(x) are linear combinations
of the experts t(cid:11); they are orthogonal; they are the solu-
tions of the eigenvalue problem associated with the integral
operator induced by (cid:6) that is
Z (cid:6)(x; y)(cid:30)i(x)dx = (cid:21)i(cid:30)i(y):

Classi(cid:12)cation: same scenario, another point
of view

Of course

K (x; y) = X (cid:21)j (cid:30)j (x)(cid:30)j (y) = (cid:6)(x; y)
Thus regularization (cid:12)nds in this case the optimal combina-
tion of experts with a L2 stabilizer. There are connections
here with Adaboost, but this is another story.

Classi(cid:12)cation: a di(cid:11)erent scenario and why
alignment may be heretical in the Bayesian
church

Assume now that we have a examples of q hypothesis
spaces, in the form of a set of experts for each of the
q hypothesis spaces. Equivalently we have estimates of
the q associated kernels Km.

What we could do is select the "optimal" kernel Km by
looking at the following score

=

;

am =

(Km ; Y )F
(Km ; Y )F
‘jjKmjjF
jjKm jjF jjY jjF
where the norms and inner products are Frobenious norms
( jjX jjF = qPi;j X 2
i;j ) and the matrix Y has elements Yi;j =
yiyj . So we are selecting a kernel by checking which kernel
best "aligns" with the labels.

Classi(cid:12)cation: a di(cid:11)erent scenario and why
alignment may be heretical in the Bayesian
church

From a Bayesian point of view each of the Km corresponds
to a di(cid:11)erent prior.
If we want to do something rather
heretical in a strict Bayesian world we could choose the
prior that (cid:12)ts our data best. This is exactly what align-
ment does! From a learning theory point of view such an
approach may be OK i(cid:11) done in the spirit of SRM { with
kernels de(cid:12)ning a structure of hypothesis spaces. This
would require a change in the alignment process: a new
project?

Appendix A: Gaussian Integrals

We state here some basic results (without proofs) on Gaussian inte-
grals. Let w 2 IRN , A a N (cid:2) N real symmetric matrix which we assume
to be strictly positive de(cid:12)nite.

I (a; A) = Z dw expf(cid:0)w 0Aw + w 0ag = (2(cid:25))
where the integration is over IRN . Similarly

2 det(A)(cid:0) 1
N
2 expf

1
2

a0A(cid:0)1ag (7)

Iu(a; A) = Z dw(w 0u) expf(cid:0)w 0Aw + w 0ag = I (a; A)u0A(cid:0)1u
Iu;v (a; A) = Z dw(w 0u)(w 0v) expf(cid:0)w 0Aw + w 0ag
= I (a; A) = (cid:2)u0A(cid:0)1v + (u0A(cid:0)1a)(v 0A(cid:0)1a)(cid:3)

(8)

(9)

