Regularization Networks
9.520 Class 17, 2003

Tomaso Poggio

Plan

(cid:15) Radial Basis Functions and their extensions

(cid:15) Additive Models

(cid:15) Regularization Networks

(cid:15) Dual Kernels

(cid:15) Conclusions

About this class

We describe a family of regularization techniques based
on radial kernels K and called RBFs. We introduce
RBF extensions such as Hyper Basis Functions and
characterize their relation with other techniques includ-
ing MLPs and splines.

Radial Basis Functions

Radial Basis Functions, as MLPs, have the universal ap-
proximation property.

Theorem: Let K be a Radial Basis Function function and
Ii the n-dimensional cube [0; 1]n. Then (cid:12)nite sums of the
form

f (x) =

N
X
i=1
are dense in C [Ii]. In other words, given a function h 2 C [Ii]
and (cid:15) > 0, there is a sum, f (x), of the above form, for
which:

ciK (x (cid:0) xi)

jf (x) (cid:0) h(x)j < (cid:15) for all x 2 In :

Notice that RBF correspond to RKHS de(cid:12)ned on an in(cid:12)-
nite domain. Notice also that RKHS do not in general have
the same approximation property: RKHS generated by a
K with an in(cid:12)nite countable number of strictly positive
eigenvalues are dense in L2 but not necessarily in C (X ),
though they can be embedded in C (X ).

Density of a RKHS on a bounded domain (the
non-RBF case)

We (cid:12)rst ask under which condition is a RKHS dense in L2(X; (cid:23) ).

1. when LK is strictly positive the RKHS is in(cid:12)nite dimensional and
dense in L2(X; (cid:23) ).

2. in the degenerate case the RKHS is (cid:12)nite dimensional and not
dense in L2(X; (cid:23) ).

3. in the conditionally strictly positive case the RKHS is not dense in
L2(X; (cid:23) ) but when completed with a (cid:12)nite number of polynomials
of appropriate degree can be made to be dense in L2(X; (cid:23) ).

Density of a RKHS on a bounded domain (cont)

Density of RKHS { de(cid:12)ned on a compact domain X { in C (X ) (in the
sup norm) is a trickier issue that has been answered very recently by
Zhou (in preparation).
It is however guaranteed for radial kernels K
for K continuous and integrable, if density in L2(X; (cid:23) ) holds (with X
the in(cid:12)nite domain). These are facts for radial kernels and unrelated
to RKHS properties

(cid:15) span K (x (cid:0) y) : y 2 Rn is dense in L2(Rn) i(cid:11) the Fourier transform
of K goes not vanish on set of positive Lebesque measure (N.
Wiener).

(cid:15) span K (x (cid:0) y) : y 2 Rn is dense in C (Rn) (topology of uniform
convergence) if K 2 C (Rn), K 2 L1(Rn).

Some good properties of RBF

(cid:15) Well motivated in the framework of regularization theory;

(cid:15) The solution is unique and equivalent to solving a linear system;

(cid:15) Degree of smoothness is tunable (with (cid:21));

(cid:15) Universal approximation property;

(cid:15) Large body of applied math literature on the subject;

(cid:15) Interpretation in terms of neural networks(?!);

(cid:15) Biologically plausible;

(cid:15) Simple interpretation in terms of smooth look-up table;

(cid:15) Similar to other non-parametric techniques, such as nearest neigh-
bor and kernel regression (see end of this class).

Some not-so-good properties of RBF

(cid:15) Computationally expensive (O(‘3));

(cid:15) Linear system to be solved for (cid:12)nding the coe(cid:14)cients
often badly ill-conditioned;

(cid:15) The same degree of smoothness is imposed on di(cid:11)erent
regions of the domain (we will see how to deal with this
problem in the class on wavelets);

This function has di(cid:11)erent smoothness properties in dif-
ferent regions of its domain.

A (cid:12)rst extension: less centers than data
points

We look for an approximation to the regularization solu-
tion:

f (x) =

‘
X
i=1

ciK (x (cid:0) xi)

+

m
X
(cid:11)=1
where m << ‘ and the vectors t(cid:11) are called centers.

c(cid:11)K (x (cid:0) t(cid:11))

f (cid:3)(x) =

Homework: show that the interpolation problem is still well-posed
when m < ‘.

(Broomhead and Lowe, 1988; Moody and Darken, 1989; Poggio and Girosi, 1989)

Least Squares Regularization Networks

m
X
(cid:11)=1
Suppose the centers t(cid:11) have been (cid:12)xed.

f (cid:3)(x) =

c(cid:11)K (x (cid:0) t(cid:11))

How do we (cid:12)nd the coe(cid:14)cients c(cid:11)?

+

Least Squares

Finding the coe(cid:14)cients

De(cid:12)ne

E (c1; : : : ; cm) =

‘
(yi (cid:0) f (cid:3)(xi))2
X
i=1

The least squares criterion is

min
c(cid:11)
The problem is convex and quadratic in the c(cid:11), and the
solution satis(cid:12)es:

E (c1; : : : ; cm)

@E

@ c(cid:11)

= 0

Finding the centers

Given the centers t(cid:11) we know how to (cid:12)nd the c(cid:11).

How do we choose the t(cid:11)?

1. a subset of the examples (random);

2. by a clustering algorithm (k-means, for example);

3. by least squares (moving centers);

4. a subset of the examples: Support Vector Machines;

Centers as a subset of the examples

Fair technique. The subset is a random subset, which
should re(cid:13)ect the distribution of the data.

Not many theoretical results available (but we proved that
solution exists since matrix is till pd).

Main problem: how many centers?

Main answer: we don’t know. Cross validation techniques
seem a reasonable choice.

Finding the centers by clustering

Very common. However it makes sense only if the input
data points are clustered.

No theoretical results.

Not clear that it is a good idea, especially for pattern clas-
si(cid:12)cation cases.

Moving centers

De(cid:12)ne

E (c1; : : : ; cm; t1; : : : ; tm) =

‘
(yi (cid:0) f (cid:3)(xi))2
X
i=1

The least squares criterion is

min
c(cid:11) ;t(cid:11)

E (c1; : : : ; cm; t1; : : : ; tm):

The problem is not convex and quadratic anymore: expect
multiple local minima.

Moving centers

:-) Very (cid:13)exible,
SVMs);

in principle very powerful (more than

:-) Some theoretical understanding;

:-( Very expensive computationally due to the local minima
problem;

:-( Centers sometimes move in \weird" ways;

Connection with MLP

Radial Basis Functions with moving centers is a particular
case of a function approximation technique of the form:

N
X
i=1
where the parameters pi can be estimated by least squares
techniques.

ciH (x; pi)

f (x) =

Radial Basis Functions corresponds to the choice N = m
and pi = ti, and

H (x; pi) = K (kx (cid:0) tik)

Extensions of Radial Basis Functions (much beyond what SVMs can do)

(cid:15) Di(cid:11)erent variables can have di(cid:11)erent scales: f (x; y) =
y2 sin(100x);

(cid:15) Di(cid:11)erent variables could have di(cid:11)erent units of measure
(cid:1)
(cid:1)(cid:1)
f = f (x;
x);
x;

(cid:15) Not all the variables are independent or relevant: f (x; y ; z ; t) =
g(x; y ; z(x; y));

(cid:15) Only some linear combinations of the variables are rel-
evant: f (x; y ; z) = sin(x + y + z);

Extensions of regularization theory

A priori knowledge:

(cid:15) the relevant variables are linear combination of the orig-
inal ones:

for some (possibly rectangular) matrix W ;

z = W x

(cid:15) f (x) = g(W x) = g(z) and the function g is smooth;

The regularization functional is now

‘
(yi (cid:0) g(zi))2 + (cid:21)(cid:8)[g ]
X
i=1

where zi = W xi.

Extensions of regularization theory
(continue)

The solution is

g(z) =

‘
X
i=1

ciK (z (cid:0) zi) :

Therefore the solution for f is:

f (x) = g(W x) =

‘
X
i=1

ciK (W x (cid:0) W xi)

Extensions of regularization theory
(continue)

If the matrix W were known, the coe(cid:14)cients could be
computed as in the radial case:

(K + (cid:21)I )c = y

where

(y)i = yi ;

(c)i = ci ; (K )ij = K (W xi (cid:0) W xj )

and the same argument of the Regularization Networks
technique apply, leading to Generalized Regularization Net-
works:

f (cid:3)(x) =

m
X
(cid:11)=1

c(cid:11)K (W x (cid:0) W t(cid:11))

Extensions of regularization theory
(continue)

Since W is usually not known, it could be found by least
squares. De(cid:12)ne

E (c1; :::; cm; W ) =

‘
(yi (cid:0) f (cid:3)(xi))2
X
i=1

Then we can solve:

min
c(cid:11) ;W

E (c1; :::; cm; W )

The problem is not convex and quadratic anymore: expect
multiple local minima.

From RBF to HyperBF

When the basis function K is radial the Generalized Reg-
ularization Networks becomes

m
X
(cid:11)=1
that is a non radial basis function technique.

c(cid:11)K (kx (cid:0) t(cid:11)kw )

f (x) =

Least Squares

1. minc(cid:11) E (c1 ; : : : ; cm)

2. minc(cid:11) ;t(cid:11) E (c1; : : : ; cm; t1 : : : ; tm)

3. minc(cid:11) ;W E (c1 ; :::; cm; W )

4. minc(cid:11) ;t(cid:11) ;W E (c1 ; : : : ; cm; t1; : : : ; tm ; W )

A nonradial Gaussian function

A nonradial multiquadric function

Additive models

In statistics an additive model has the form

where

In other words

f (x) =

d
X
(cid:22)=1

f(cid:22)(x(cid:22))

f(cid:22)(x(cid:22)) =

‘
X
i=1

i G(x(cid:22) (cid:0) x(cid:22)
c(cid:22)
i )

f (x) =

d
X
(cid:22)=1

‘
X
i=1

i G(x(cid:22) (cid:0) x(cid:22)
c(cid:22)
i )

Additive stabilizers

To obtain an approximation of the form

d
X
(cid:22)=1
we choose a stabilizer corresponding to an additive basis
function

f(cid:22)(x(cid:22))

f (x) =

K (x) =

d
X
(cid:22)=1

(cid:18)(cid:22)K (x(cid:22))

This scheme leads to an approximation scheme of the ad-
ditive form with

f(cid:22)(x(cid:22)) = (cid:18)(cid:22)

‘
X
i=1
Notice that the additive components are not independent
since there is only one set of ci { which makes sense since
I have only l data points to determine the ci.

ciK (x(cid:22) (cid:0) x(cid:22)
i )

Extensions of Additive Models

We start from the non-independent additive component
formulation obtained from additive stabilizers

f (x) =

‘
X
i=1

ci

d
X
(cid:22)=1

(cid:18)(cid:22)K (x(cid:22) (cid:0) x(cid:22)
i )

We assume now that the parameters (cid:18)(cid:22) are free. We now
have to (cid:12)t

f (x) =

d
X
(cid:22)=1

‘
X
i=1
with ‘ (cid:2) d independent c(cid:22)
i . In order to avoid over(cid:12)tting we
reduce the number of centers (m << l):

i K (x(cid:22) (cid:0) x(cid:22)
c(cid:22)
i )

f (x) =

d
X
(cid:22)=1

m
X
(cid:11)=1

(cid:11)K (x(cid:22) (cid:0) t(cid:22)
c(cid:22)
(cid:11))

Extensions of Additive Models

If we now allow for an arbitrary linear transformation of
the inputs:

where W is a d0 (cid:2) d matrix, we obtain:

x ! W x

f (x) =

d0
X
(cid:22)=1

m
X
(cid:11)=1

(cid:11)K (x>w(cid:22) (cid:0) t(cid:22)
c(cid:22)
(cid:11))

where w(cid:22) is the (cid:22)-th row of the matrix W .

Extensions of Additive Models

The expression

f (x) =

d0
X
(cid:22)=1

m
X
(cid:11)=1

(cid:11)K (x>w(cid:22) (cid:0) t(cid:22)
c(cid:22)
(cid:11))

can be written as

where

f (x) =

d0
X
(cid:22)=1

h(cid:22)(x>w(cid:22))

m
X
(cid:11)=1
This form of approximation is called ridge approximation

(cid:11)K (y (cid:0) t(cid:22)
c(cid:22)
(cid:11))

h(cid:22)(y) =

Gaussian MLP network

From the extension of additive models we can therefore
justify an approximation technique of the form

f (x) =

d0
X
(cid:22)=1

m
X
(cid:11)=1

(cid:11)G(x>w(cid:22) (cid:0) t(cid:22)
c(cid:22)
(cid:11))

Particular case: m = 1 (one center per dimension). Then
we derive the following technique:

f (x) =

c(cid:22)G(x>w(cid:22) (cid:0) t(cid:22))

d0
X
(cid:22)=1
which is a Multilayer Perceptron with a Radial Basis Func-
tions G instead of the sigmoid function. One can argue
rather formally that for normalized inputs the weight vec-
tors of MLPs are equivalent to the centers of RBFs.

Notice that the sigmoid function cannot be derived {
directly and formally { from regularization but...

Sigmoids and Regularization

Suppose to have learned the representation

f (x) =

d0
X
(cid:22)=1

c(cid:22)K 0(x>w(cid:22) (cid:0) t(cid:22))

where K 0(x) = jxj. Notice that a (cid:12)nite linear combination
of translates of a sigmoidal, piece-wise linear basis function
can be written as a linear combination of translates of jxj.
There is a very close relationship between 1-D radial and
sigmoidal functions.

Regularization Networks

Regularization

Radial
Stabilizer

Additive
Stabilizer

Product
Stabilizer

RBF

Additive
 Splines

 Tensor
Product
 Splines

Movable metric
Movable centers

Movable metric
Movable centers

Movable metric
Movable centers

HyperBF

if ||x|| = 1

Ridge
Approximation

)
N
R
(
 
s
k
r
o
w
t
e
n
 
n
o
i
t
a
z
i
r
a
l
u
g
e
R

n
o
i
t
a
z
)
i
N
r
a
R
l
u
G
g
(
 
e
 
s
R
k
 
d
r
o
e
w
z
i
t
l
e
a
N
r
e
 
 
n
 
 
e
 
 
G
 
 

Regularization networks and Kernel
regression

(cid:15) Kernel regression: no complex global model of the
world is assumed. Many simple local models instead
(a case of kernel methods)

f (x) = P‘
i=1 wi(x)yi
P‘
i=1 wi(x)

(cid:15) Regularization networks:
fairly complex global model
of the world (a case of dictionary methods)

f (x) =

‘
X
i=1

ciK (x (cid:0) xi)

Are these two techniques related? Can you say something
about the apparent dichotomy of \local" vs. \global"?

Least square Regularization networks

A model of the form

m
X
(cid:11)=1
is assumed and the parameters c(cid:11) and t(cid:11) are found by

c(cid:11)K (x (cid:0) t(cid:11))

f (x) =

where

min
c(cid:11) ;t(cid:11)

E [fc(cid:11)g; ft(cid:11)g]

E [fc(cid:11)g; ft(cid:11)g] =

‘
(yi (cid:0) f (xi))2
X
i=1

Least square Regularization networks

The coe(cid:14)cients c(cid:11) and the centers t(cid:11) have to satisfy the
conditions:

@E

@E

= 0 ;

= 0 (cid:11) = 1; : : : ; m

@ t(cid:11)
@ c(cid:11)
The equation for the coe(cid:14)cients gives:

c(cid:11) =

‘
X
i=1

H(cid:11)iyi

where

H = (K T K )(cid:0)1K T ; Ki(cid:11) = K (xi (cid:0) t(cid:11))

Dual representation

Substituting the expression for the coe(cid:14)cients in the reg-
ularization network we obtain

f (x) =

‘
X
i=1

yi

m
X
(cid:11)=1

H T
i(cid:11)K (x (cid:0) t(cid:11))

f (x) = P‘
i=1 yibi(x)

where we have de(cid:12)ned

bi(x) =

m
X
(cid:11)=1

H T
i(cid:11)K (x (cid:0) t(cid:11))

The basis functions bi(x) are called \dual kernels".

Equivalent kernels for multiquadric basis functions

)
N
R
(
 
s
k
r
o
w
t
e
n
 
n
o
i
t
a
z
i
r
a
l
u
g
e
R

n
o
i
t
a
z
)
i
N
r
a
R
l
u
G
g
(
 
e
 
s
R
k
 
d
r
o
e
w
z
i
t
l
e
a
N
r
e
 
 
n
 
 
e
 
 
G
 
 

Regularization

Radial
Stabilizer

Additive
Stabilizer

Product
Stabilizer

RBF

Additive
 Splines

 Tensor
Product
 Splines

Movable metric
Movable centers

Movable metric
Movable centers

Movable metric
Movable centers

HyperBF

if ||x|| = 1

Ridge
Approximation

1
1

0
0

0.5
0.5

1
1

0.5
0.5

0
0

1
1

0.8
0.8
0.6
0.6

0.4
0.4

0.2
0.2

1
1

0.8
0.8

0.6
0.6

0.4
0.4

0.2
0.2

0
0
1
1

0.75
0.75

0.5
0.5

0.25
0.25

0
0

0
0

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1

0.8

0.6

0.4

0.2

-0.2

1.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Dual formulation of Regularization networks
and Kernel regression

f (x) = P‘
i=1 yibi(x) Regularization networks

m

f (x) = P‘
i=1 wi(x)yi
P‘
i=1 wi(x)

Kernel regression

In both cases the value of f at point x is a

weighted average of the values at the data points.

Project: is this true for SVMs? Can it be gener-

alized?

Conclusions

(cid:15) We have extended { with some hand waving { classical,
quadratic Regularization Networks including RBF into
a number of schemes that are inspired by regularization
though do not strictly follow from it.

(cid:15) The extensions described seem to work well in practice.
Main problem { for schemes involving moving centers
and or learning the metric { is e(cid:14)cient optimization.

