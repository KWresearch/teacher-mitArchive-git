Approximation Error and Approximation Theory

Federico Girosi

Center for Basic Research in the Social Sciences
Harvard University
and
Center for Biological and Computational Learning
MIT

fgirosi@latte.harvard.edu

1

Plan of the class
• Learning and generalization error
• Approximation problem and rates of convergence
• N-widths
• “Dimension independent” convergence rates

Note

2

These slides cover more extensive material than what will
be presented in class.

References

3

The background material on generalization error (ﬁrst 8 slides) is explained at length in:

1. P. Niyogi and F. Girosi. On the relationship between generalization error, hypothesis complexity,
and sample complexity for Radial Basis Functions. Neural Computation, 8:819–842, 1996.
2. P. Niyogi and F. Girosi. Generalization bounds for function approximation from scattered noisy
data. Advances in Computational Mathematics, 10:51–80, 1999.
[1] has a longer explanation and introduction, while [2] is more mathematical and also contains
a very simple probabilistic proof of a class of “dimension independent” bounds, like the ones
discussed at the end of this class.
As far as I know it is A. Barron who ﬁrst clearly spelled out the decomposition of the
generalization error in two parts. Barron uses a diﬀerent framework from what we use, and he
summarizes it nicely in:
3. A.R. Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine
Learning, 14:115–133, 1994.
The paper is quite technical, and uses a framework which is diﬀerent from what we use here,
but it is important to read it if you plan to do research in this ﬁeld.
The material on n-widths comes from:
4. A. Pinkus. N-widths in Approximation Theory, Springer-Verlag, New York, 1980.
Although the book is very technical, the ﬁrst 8 pages contain an excellent introduction to the
subject. The other great thing about this book is that you do not need to understand every
single proof to appreciate the beauty and signiﬁcance of the results, and it is a mine of useful
information.
5. H.N. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions.
Neural Computation, 8:164–177, 1996.

6. A.R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transaction on Information Theory, 39:3, 930–945, 1993.
7. F. Girosi and G. Anzellotti. Rates of convergence of approximation by translates A.I. Memo
1288, Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, 1992.
For a curious way to prove dimension independent bounds using VC theory see:
8. F. Girosi. Approximation error bounds that use VC-bounds.
International Conference
In Proc.
on Artiﬁcial Neural Networks, F. Fogelman-Souli`e and P. Gallinari, editors, Vol. 1, 295–302.
Paris, France, October 1995.

4

5

Notations review

R
P
• I[f] =
X×Y V (f(x), y)p(x, y)dxdy
• Iemp[f] = 1
i=1 V (f(xi), yi)
l
l
f0 ∈ T
• f0 = arg minf I[f]
,
• fH = arg minf∈H I[f]
• ^fH,l = arg minf∈H Iemp[f]

6

More notation review
• I[f0] = how well we could possibly do
• I[fH] = how well we can do in space H
• I[ ^fH,l] = how well we do in space H with l data
• |I[f] − Iemp[f]| ≤ Ω(H, l, δ) ∀f ∈ H (from VC theory)
• I[ ^fH,l] is called generalization error
• I[ ^fH,l] − I[f0] is also called generalization error ...

A General Decomposition

7

I[ ^fH,l] − I[f0] = I[ ^fH,l] − I[fH]
generalization error = estimation error + approximation error

+ I[fH] − I[f0]

When the cost function V is quadratic:

I[f] = kf0 − fk2 + I[f0]

and therefore

+ kf0 − fHk2
kf0 − fH,lk2 = I[ ^fH,l] − I[fH]
generalization error = estimation error + approximation error

8

then

A useful inequality
(cid:12)(cid:12)I[f] − Iemp[f]
(cid:12)(cid:12) ≤ Ω(H, l, δ) ∀f ∈ H
If, with probability 1 − δ
(cid:12)(cid:12)I[ ^fH,l] − I[fH]
(cid:12)(cid:12) ≤ 2Ω(H, l, δ)
You can prove it using the following observations:
• I[fH] ≤ I[ ^fH,l]
(from the deﬁnition of fH)
• Iemp[ ^fH,l] ≤ Iemp[fH]

(from the deﬁnition of ^fH,l)

Bounding the Generalization Error

9

kf0 − fH,lk2 ≤ 2Ω(H, l, δ) + kf0 − fHk2

Notice that:
• Ω has nothing to do with the target space T ,
studied mostly in statistics;
• kf0 − fHk has everything to do with the target space T ,
it is studied mostly in approximation theory;

it is

Approximation Error
We consider a nested family of hypothesis spaces Hn:
H0 ⊂ H1 ⊂ . . . Hn ⊂ . . .

10

and deﬁne the approximation error as:
kf − hk
T (f, Hn) ≡ inf
h∈Hn

T (f, Hn) is the smallest error that we can make if we
approximate f ∈ T with an element of Hn (here k · k is
the norm in T ).

Approximation error

11

For reasonable choices of hypothesis spaces Hn:
n→∞ T (f, Hn) = 0
lim
arbitrarily well with elements of {Hn}∞
This means that we can approximate functions of T
n=1
Example: T = continuous functions on compact sets,
and Hn = polynomials of degree at most n.

The rate of convergence

12

The interesting question is:

How fast does T (f, Hn) go to zero?
• The rate of convergence is a measure of the relative
complexity of T with respect to the approximation
scheme H.
• The rate of convergence determines how many samples
we need in order to obtain a given generalization error.

13

An Example
• In the next slides we compute explicitly the rate of
convergence of approximation of a smooth function by
trigonometric polynomials.
• We are interested in studying how fast the approximation
error goes to zero when the number of parameters of
our approximation scheme goes to inﬁnity.
• The reason for this exercise is that the results are
representative: more complex and interesting cases all
share the basic features of this example.

Approximation by Trigonometric Polynomials
\

Consider the set of functions
C2[−π, π] ≡ C[−π, π]

L2[−π, π]

14

Functions in this set can be represented as a Fourier
Z
series:
∞X
k=0

−ikx
dxf(x)e

ck ∝

ckeikx ,

f(x) =

π

−π

The L2 norm satisﬁes the equation:
∞X
k=0

kfk2
L2

ck

=

2

15

Ws,2 ≡

We consider as target space the following Sobolev space
f ∈ C2[−π, π] |

(cid:13)(cid:13)(cid:13)(cid:13) dsf
(cid:13)(cid:13)(cid:13)(cid:13)2
of smooth functions:
< +∞
dxs
L2
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13) dsf
The (semi)-norm in this Sobolev space is deﬁned as:
∞X
dxs
L2
k=1
If f belongs to Ws,2 then Fourier coeﬃcients ck must go
to zero at a rate which increases with s.

kfk2
Ws,2

≡

=

k2sck

2

16

We choose as hypothesis space Hn the set of
nX
trigonometric polynomials of degree n:
k=1
∞X
Given a function of the form
k=0

akeikx

ckeikx

p(x) =

f(x) =

the optimal hypothesis fn(x) is given by the ﬁrst n terms
of its Fourier series:
nX
k=1

ckeikx

fn(x) =

Key Question
For a given f ∈ Ws,2 we want to study the approximation
error:

17

n[f] ≡ kf − fnk2
L2
• Notice that n, the degree of the polynomial, is also the
• Obviously n goes to zero as n → +∞, but the key
number of parameters that we use in the approximation.
question is: how fast?

18

=

< 1
n2s

An easy estimate of n
P∞
P∞
kk2s 1
k=n+1 c2
k=n+1 c2
k =
k2s <
P∞
k=1 c2
kk2s =

n[f] ≡ kf − fnk2
P∞
L2
kk2s < 1
k=n+1 c2
⇓
n2s
kfk2
Ws,2
More smoothness ⇒ faster rate of convergence
n[f] <
n2s
But what happens in more than one dimension?

kfk2
Ws,2
n2s

Rates of convergence in d dimensions

19

It is enough to study d = 2. We proceed in full analogy
with the 1-d case:
∞X
ckmei(kx+my)
f(x, y) =
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13) dsf
(cid:13)(cid:13)(cid:13)(cid:13) dsf
k,m=1
∞X
kfk2
≡
(k2s + m2s)c2
km
Ws,2
dys
dxs
L2
L2
k,m=1
< +∞
Here Ws,2 is deﬁned as the set of functions such that
kfk2
Ws,2

+

=

We choose as hypothesis space Hn the set of
lX
trigonometric polynomials of degree l:
k,m=1

(ikx+imy)
akme

p(x) =

20

A trigonometric polynomial of degree l in d variables has
a number of coeﬃcients n = ld.

We are interested in the behavior of the approximation
error as a function of n. The approximating function is:
lX
k,m=1

(ikx+imy)
ckme

fn(x, y) =

=

An easy estimate of n
∞X
∞X
c2
∞X
km
k,m=l+1
k,m=l+1
k,m=1

c2
km(k2s + m2s) <

c2
km =

1
2l2s

n[f] ≡ kf − fnk2
∞X
L2
1
2l2s
k,m=l+1
kfk2
Ws,2
2l2s

=

<

21

(k2s + m2s)
k2s + m2s

<

c2
km(k2s + m2s) =

Since n = ld, then l = n

1
d (with d = 2), and we obtain:
kfk2
Ws,2
2s
d

n <

2n

(Partial) Summary

22

The previous calculations generalizes easily to the
d-dimensional case. Therefore we conclude that:

if we approximate functions of d variables with s
square integrable derivatives with a trigonometric
polynomial with n coeﬃcients, the approximation error
satisﬁes:

C
n <
More smoothness s ⇒ faster rate of convergence
2s
n
d
Higher dimension d ⇒ slower rate of convergence

23

Another Example: Generalized Translation Networks
Consider networks of the form:
nX
k=1
where x ∈ Rd, bk ∈ Rm, 1 ≤ m ≤ d, Ak are m × d
matrices, ak ∈ R and φ is some given function.
For m = 1 this is a Multilayer Perceptron.

akφ(Akx + bk)

f(x) =

For m = d, Ak diagonal and φ radial this is a Radial Basis
Functions network.

24

Theorem (Mhaskar, 1994)
s (Rd) be the space of functions whose derivatives
p
Let W
up to order s are p-integrable in Rd. Under very general
assumptions on φ one can prove that there exists d × m
k=1 such that, for any f ∈ W
s (Rd), one can
p
matrices {Ak}n
ﬁnd bk and ak such that:
nX
k=1

akφ(Akx + bk)kp ≤ cn

kf −

d kfkW p
− s
s

Moreover, the coeﬃcients ak are linear functionals of f.
This rate is optimal

If the approximation error is

The curse of dimensionality
(cid:18) 1
(cid:19) s
d
n

n ∝

25

then the number of parameters needed to achieve an
(cid:19) d
(cid:18) 1
error smaller than  is:
s


n ∝

the curse of dimensionality is the d factor;

the blessing of smoothness is the s factor;

Jackson type rates of convergence

26

It happens “very often” that rates of convergence for
functions in d dimensions with “smoothness” of order s
!
 (cid:18) 1
(cid:19) s
are of the Jackson type:
d
n

O

Example: polynomial and spline approximation
techniques, many non-linear techniques.

Can we do better than this? Can we defeat the
curse of dimensionality? Have we tried hard enough
to ﬁnd “good” approximation techniques?

N-widths: deﬁnition (from Pinkus, 1980)

27

Let X be a normed space of functions, A a subset of X.
We want to approximate elements of X with linear
superposition of n basis functions {φi}n
i=1.

Some sets of basis functions are better than others: which
are the best basis function? what error do they achieve?
To answer these questions we deﬁne the Kolmogorov
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)X
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)f −
n-width of A in X:
nX
i=1

dn(A, X) = inf
φ1,...φn

ciφi

sup
f∈A

inf
c1,...cn

Example (Kolmogorov, 1936)

28

X = L2[0, 2π]
2 ≡ {f | f ∈ Cs−1[0, 2π], f(j) periodic,
˜W s
2 ≡ {f | f ∈ ˜W s
2 , kf(s)k2 ≤ 1} ⊂ X
A = ˜Bs

j = 0, . . . , s − 1}

Then

2, L2) = d2n(˜Bs
d2n−1(˜Bs
2, L2) =

1
ns

and the following Xn is optimal (in the sense that it
achieves the rate above):

X2n−1 = span{1, sin(x), cos(x), . . . , sin(n − 1)x, cos(n − 1)x}

Example: multivariate case

29

Id ≡ [0, 1]d

X = L2[Id]
2 [Id] ≡ {f | f ∈ Cs−1[Id] , f(s) ∈ L2[Id]}
W s
2 ≡ {f | f ∈ W s
2 [Id] , kf(s)k2 ≤ 1}
Bs
(cid:19) s
(cid:18) 1
Theorem (from Pinkus, 1980)
d
n

2, L2) ≈
dn(Bs

Optimal basis functions are usually splines (or their
relatives)

Dimensionality and smoothness

30

Classes of functions in d dimensions with smoothness of
order s have an intrinsic complexity characterized by the
ratio s
d :
• the curse of dimensionality is the d factor;
• the blessing of smoothness is the s factor;

We cannot expect to ﬁnd an approximation technique
that “beats the curse of dimensionality”, unless we let
the smoothness s increase with the dimension d.

31

Theorem (Barron, 1991)
Let f be a function such that its Fourier transform
Z
satisﬁes
dω kωk|˜f(ω)| < +∞
Rd
Let Ω be a bounded domain in Rd. Then we can ﬁnd a
neural network with n coeﬃcients ci, n weights wi and n
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)f −
biases θi such that
nX
ciσ(x · wi + θi)
i=1
L2(Ω)
The rate of convergence is independent of the
dimension d.

<

c

n

Here is the trick...

32

Z
The space of functions such that
dω kωk|˜f(ω)| < +∞ .
is the space of functions that can be written as

Rd

f =

1
kxkd−1
where λ is any function whose Fourier transform is
integrable.

∗ λ

Notice how the space becomes more constrained as the
dimension increases.

Theorem (Girosi and Anzellotti, 1992)
Let f ∈ Hs,1(Rd), where Hs,1(Rd) is the space of functions
whose partial derivatives up to order s are integrable, and
let Ks(x) be the Bessel-Macdonald kernel, that is the
Fourier transform of

33

˜Ks(ω) =

1
(1 + kωk2)
If s > d and s is even, we can ﬁnd a Radial Basis
Functions network with n coeﬃcients cα and n centers tα
such that
nX
α=1

L∞ <
cαKs(x − tα)k2

kf −

s > 0 .

s
2

c

n

34

Theorem (Girosi, 1992)
Let f ∈ Hs,1(Rd), where Hs,1(Rd) is the space of functions
whose partial derivatives up to order s are integrable. If
s > d and s is even, we can ﬁnd a Gaussian basis function
network with n coeﬃcients cα, n centers tα and n
nX
variances σα such that
α=1

L∞ <
− (x−tα)2
α k2
2σ2
cαe

kf −

c

n

Same rate of convergence: O( 1√
n

)

35

Norm

L2 (Ω)

Function space
R
Rd dω |˜f(ω)| < +∞
(Jones)
R
Rd dω kωk|˜f(ω)| < +∞
(Barron)
R
Rd dω kωk2 |˜f(ω)| < +∞ L2 (Ω)
(Breiman)

L2 (Ω)

˜f(ω) ∈ Cs
0 , 2s > d

(Girosi and Anzellotti)

H2s,1 (Rd ), 2s > d

(Girosi)

L∞ (Rd )

L∞ (Rd )

f(x) =

f(x) =

Approximation scheme
P
i=1 ci sin(x · wi + θi )
n
P
i=1 ciσ(x · wi + θi )
n
P
|x · wi + θi
n
i=1 ci
+ a · x + b
P
α=1 cαe−kx−tαk2
n

f(x) =

f(x) =

|++

P
−
n
α=1 cαe

kx−tαk2
σ2
α

f(x) =

36

Summary
• There is a trade oﬀ between the size of the sample (l)
and the size of the hypothesis space n;
• For a given pair of hypothesis and target space the
approximation error depends on the trade oﬀ between
dimensionality and smoothness;
• The trade oﬀ has a “generic” form and sets bounds
on what can and cannot be done, both in linear and
non-linear approximation;
• Suitable spaces, which trade dimensionality versus
smoothness, can be deﬁned in such a way that the rate
of convergence of the approximation error is independent
of the dimensionality.

