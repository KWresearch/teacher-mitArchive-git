Stability of Tikhonov Regularization
9.520 Class 07, March 2003

Alex Rakhlin

Plan
(cid:15) Review of Stability Bounds
(cid:15) Stability of Tikhonov Regularization Algorithms

Uniform Stability

Review notation: S = fz1; :::; z‘g; S i;z = fz1 ; :::; zi(cid:0)1 ; z ; zi+1 ; :::; z‘g
c(f ; z) = V (f (x); y), where z = (x; y).

An algorithm A has uniform stability (cid:12) if
8(S; z) 2 Z ‘+1 ; 8i; sup
u2Z jc(fS ; u) (cid:0) c(fS i;z ; u)j (cid:20) (cid:12) :
Last class: Uniform stability of (cid:12) = O (cid:16) 1
‘ (cid:17) implies good
generalization bounds.

This class: Tikhonov Regularization has uniform stability
of (cid:12) = O (cid:16) 1
‘ (cid:17).
Reminder: The Tikhonov Regularization algorithm:

fS = arg min
f 2H

1
‘

‘
Xi=1

V (f (xi); yi) + (cid:21)kf k2
K

Generalization Bounds Via Uniform Stability

If (cid:12) = k
‘ for some k, we have the following bounds from
the last lecture:
+ (cid:15)(cid:19) (cid:20) 2 exp  (cid:0)
k
P (cid:18)jI [fS ] (cid:0) IS [fS ]j (cid:21)
‘
Equivalently, with probability 1 (cid:0) (cid:14) ,
+ (2k + M )s2 ln(2=(cid:14))
k
‘
‘

‘(cid:15)2
2(k + M )2 ! :

I [fS ] (cid:20) IS [fS ] +

:

Lipschitz Loss Functions, I

We say that a loss function (over a possibly bounded do-
main X ) is Lipschitz with Lipschitz constant L if
8y1; y2; y 0 2 Y ; jV (y1; y 0) (cid:0) V (y2 ; y 0)j (cid:20) Ljy1 (cid:0) y2j:

Put di(cid:11)erently, if we have two functions f1 and f2, under
an L-Lipschitz loss function,

(x;y) jV (f1(x); y) (cid:0) V (f2(x); y)j (cid:20) Ljf1 (cid:0) f2j1 :
sup
Yet another way to write it:

jc(f1 ; (cid:1)) (cid:0) c(f2 ; (cid:1))j1 (cid:20) Ljf1((cid:1)) (cid:0) f2((cid:1))j1

Lipschitz Loss Functions, I I

If a loss function is L-Lipschitz, then closeness of two func-
tions (in L1 norm) implies that they are close in loss.

The converse is false | it is possible for the di(cid:11)erence in
loss of two functions to be small, yet the functions to be
far apart (in L1). Example: constant loss.

The hinge loss and the (cid:15)-insensitive loss are both L-Lipschitz
with L = 1. The square loss function is L Lipschitz if we
can bound the y values and the f (x) values generated. The
0 (cid:0) 1 loss function is not L-Lipschitz at all | an arbitrarily
small change in the function can change the loss by 1:

f1 = 0; f2 = (cid:15); V (f1(x); 0) = 0; V (f2(x); 0) = 1:

Lipschitz Loss Functions for Stability

Assuming L-Lipschitz loss, we transformed a problem of
bounding

sup
u2Z jc(fS ; u) (cid:0) c(fS i;z ; u)j
into a problem of bounding jfS (cid:0) fS i;z j1.

As the next step, we bound the above L1 norm by the
norm in the RKHS assosiated with a kernel K .

For our derivations, we need to make another assumption:
there exists a (cid:20) satisfying
8x 2 X ; qK (x; x) (cid:20) (cid:20):

Relationship Between L1 and LK
Using the reproducing property and the Cauchy-Schwartz
inequality, we can derive the following:

8x jf (x)j = jhK (x; (cid:1)); f ((cid:1))iK j
(cid:20) jjK (x; (cid:1))jjK jjf jjK
= qhK (x; (cid:1)); K (x; (cid:1))ijjf jjK
= qK (x; x)jjf jjK
(cid:20) (cid:20)jjf jjK

Since above inequality holds for all x, we have jf j1 (cid:20) jjf jjK .
Hence, if we can bound the RKHS norm, we can bound
the L1 norm. Note that the converse is not true.
Note that we now transformed the problem to bounding
jjfS (cid:0) fS i;z jjK .

A Key Lemma

We will prove the following lemma about Tikhonov reg-
ularization:

jjfS (cid:0) fS i;z jj2
K (cid:20)

LjfS (cid:0) fS i;z j1
(cid:21)‘

This theorem says that when we replace a point in the
training set, the change in the RKHS norm (squared) of
the di(cid:11)erence between the two functions cannot be too
large compared to the change in L1.

We will (cid:12)rst explore the implications of this lemma, and
defer its proof until later.

Bounding (cid:12) , I

Using our lemma and the relation between LK and L1,
LjfS (cid:0) fS i;z j1
jjfS (cid:0) fS i;z jj2
K (cid:20)
(cid:21)‘
L(cid:20)jjfS (cid:0) fS i;z jjK
(cid:20)
(cid:21)‘

Dividing through by jjfS (cid:0) fS i;z jjK , we derive
(cid:20)L
(cid:21)‘

jjfS (cid:0) fS i;z jjK (cid:20)

:

Bounding (cid:12) , I I

Using again the relationship between LK and L1, and the
L Lipschitz condition,

sup jV (fS ((cid:1)); (cid:1)) (cid:0) V (fS z ;i((cid:1)); (cid:1))j (cid:20) LjfS (cid:0) fS z ;i j1
(cid:20) L(cid:20)jjfS (cid:0) fS z ;i jjK
L2(cid:20)2
(cid:20)
(cid:21)‘
= (cid:12)

Divergences

Suppose we have a convex, di(cid:11)erentiable function F , and
we know F (f1) for some f1. We can \guess" F (f2) by
considering a linear approximation to F at f1:
^F (f2) = F (f1) + hf2 (cid:0) f1; rF (f1)i:

The Bregman divergence is the error in this linearized ap-
proximation:

dF (f2 ; f1) = F (f2) (cid:0) F (f1) (cid:0) hf2 (cid:0) f1 ; rF (f1)i:

Divergences Illustrated

(f2; F (f2))

dF (f2; f1)

(f1 ; F (f1))

Divergences Cont’d

We will need the following key facts about divergences:

(cid:15) dF (f2 ; f1) (cid:21) 0
(cid:15) If f1 minimizes F , then the gradient is zero, and dF (f2 ; f1) =
F (f2) (cid:0) F (f1).
(cid:15) If F = A + B , where A and B are also convex and
di(cid:11)erentiable, then dF (f2; f1) = dA(f2 ; f1) + dB (f2 ; f1)
(the derivatives add).

The Tikhonov Functionals

We shall consider the Tikhonov functional

‘
V (f (xi); yi) + (cid:21)jjf jj2
Xi=1
K ;
as well as the component functionals

TS (f ) =

1
‘

VS (f ) =

1
‘

‘
Xi=1

V (f (xi); yi)

and

N (f ) = jjf jj2
K :
Hence, TS (f ) = VS (f ) + (cid:21)N (f ).
If the loss function is
convex (in the (cid:12)rst variable), then all three functionals are
convex.

A Picture of Tikhonov Regularization

R

fs

fs’

Ts(f)

Ts’(f)

Vs(f)

Vs’(f)

N(f)

F

Proving the Lemma, I

Let fS be the minimizer of TS , and let fS i;z be the minimizer
of TS i;z , the perturbed data set with (xi; yi) replaced by a
new point z = (x; y). Then

(cid:21)(dN (fS i;z ; fS ) + dN (fS ; fS i;z )) (cid:20)
(fS i;z ; fS ) + dTS i;z
(fS ; fS i;z ) =
dTS
(c(fS i;z ; zi) (cid:0) c(fS ; zi) + c(fS ; z) (cid:0) c(fS i;z ; z)) (cid:20)
2LjfS (cid:0) fS i;z j1
:
‘

1
‘

We conclude that

dN (fS i;z ; fS ) + dN (fS ; fS i;z ) (cid:20)

2LjfS (cid:0) fS i;z j1
(cid:21)‘

Proving the Lemma, I I

But what is dN (fS i;z ; fS )?

We will express our functions as the sum of orthogonal
eigenfunctions in the RKHS:

fS (x) =

fS i;z (x) =

1
Xn=1
1
Xn=1

cn(cid:30)n(x)

c0n(cid:30)n(x)

Once we express a function in this form, we recall that
1
Xn=1

jjf jj2
K =

c2
n
(cid:21)n

Proving the Lemma, I I I

Using this notation, we reexpress the divergence in terms
of the ci and c0i:
K (cid:0) hfS i;z (cid:0) fS ; rjjfS jj2
dN (fS i;z ; fS ) = jjfS i;z jj2
K (cid:0) jjfS jj2
K i
c02
1
1
1
c2
2cn
n
n
(c0n (cid:0) cn)(
Xn=1
Xn=1
Xi=1
=
)
(cid:21)n (cid:0)
(cid:21)n (cid:0)
(cid:21)n
c02
1
n + c2
n (cid:0) 2c0ncn
Xn=1
(cid:21)n
1
(c0n (cid:0) cn)2
Xn=1
(cid:21)n
= jjfS i;z (cid:0) fS jj2
K
We conclude that
dN (fS i;z ; fS ) + dN (fS ; fS i;z ) = 2jjfS i;z (cid:0) fS jj2
K

=

=

Proving the Lemma, IV

Combining these results proves our Lemma:

jjfS i;z (cid:0) fS jj2
K =

(cid:20)

dN (fS i;z ; fS ) + dN (fS ; fS i;z )
2
2LjfS (cid:0) fS i;z j1
(cid:21)‘

Bounding the Loss, I

We have shown that Tikhonov regularization with an L-
Lipschitz loss is (cid:12) -stable with (cid:12) = L2(cid:20)2
(cid:21)‘ . If we want to actu-
ally apply the theorems and get the generalization bound,
we need to bound the loss.

Let C0 be the maximum value of the loss when we predict
a value of zero.
If we have bounds on X and Y , we can
(cid:12)nd C0.

Bounding the Loss, I I

Noting that the \all 0" function ~0 is always in the RKHS,
we see that

(cid:21)jjfS jj2
K (cid:20) T (fS )
(cid:20) T (~0)
‘
1
Xi=1
=
‘
(cid:20) C0:

V (~0(xi); yi)

Therefore,

C0
jjfS jj2
K (cid:20)
(cid:21)
=) jfS j1 (cid:20) (cid:20)jjfS jjK (cid:20) (cid:20)sC0
(cid:21)
Since the loss is L-Lipschitz, a bound on jfS j1 implies
boundedness of the loss function.

A Note on (cid:21)

We have shown that Tikhonov regularization is uniformly
stable with

(cid:12) =

L2(cid:20)2
(cid:21)‘

:

If we keep (cid:21) (cid:12)xed as we increase ‘, the generalization bound
will tighten as O (cid:18) 1p‘(cid:19). However, keeping (cid:21) (cid:12)xed is equiva-
lent to keeping our hypothesis space (cid:12)xed. As we get more
data, we want (cid:21) to get smaller. If (cid:21) gets smaller too fast,
the bounds become trivial.

Tikhonov vs. Ivanov

It is worth noting that Ivanov regularization

1
^fH;S = arg min
‘
f 2H
kf k2
K (cid:20) (cid:28)

s.t.

‘
Xi=1

V (f (xi); yi)

is not uniformly stable with (cid:12) = O (cid:16) 1
n (cid:17), essentially because
the constraint bounding the RKHS norm may not be tight.
This is an important distinction between Tikhonov and
Ivanov regularization.

