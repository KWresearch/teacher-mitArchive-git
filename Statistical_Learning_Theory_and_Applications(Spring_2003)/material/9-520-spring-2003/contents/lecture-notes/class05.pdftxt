Support Vector Machines For Classi(cid:12)cation
9.520 Class 06, 24 February 2003

Ryan Rifkin

Plan

(cid:15) Regularization derivation of SVMs
(cid:15) Geometric derivation of SVMs
(cid:15) Optimality, Duality and Large Scale SVMs
(cid:15) SVMs and RLSC: Compare and Contrast

The Regularization Setting (Again)

We are given ‘ examples (x1; y1); : : : ; (x‘; y‘), with xi 2 IRn
and yi 2 f(cid:0)1; 1g for all i. As mentioned last class, we
can (cid:12)nd a classi(cid:12)cation function by solving a regularized
learning problem:

min
f 2H

1

‘

‘
Xi=1

V (yi; f (xi)) + (cid:21)jjf jj2
K :

Note that in this class we are speci(cid:12)cally consider binary
classi(cid:12)cation.

The Hinge Loss

The classical SVM arises by considering the speci(cid:12)c loss
function

where

V (f (x); y) (cid:17) (1 (cid:0) yf (x))+ ;

(k)+ (cid:17) max(k; 0):

The Hinge Loss

−2

−1

0
y * f(x)

1

2

3

s
s
o
L
 
e
g
n
i
H

4

3.5

3

2.5

2

1.5

1

0.5

0

−3

Substituting In The Hinge Loss

With the hinge loss, our regularization problem becomes

min
f 2H

1

‘

‘
Xi=1

(1 (cid:0) yif (xi))+ + (cid:21)jjf jj2
K :

Slack Variables

This problem is non-di(cid:11)erentiable (because of the \kink" in
V ), so we introduce slack variables (cid:24)i, to make the problem
easier to work with:

min
f 2H
subject to :

1
i=1 (cid:24)i + (cid:21)jjf jj2
‘ P‘
K
yif (xi) (cid:21) 1 (cid:0) (cid:24)i
(cid:24)i (cid:21) 0

i = 1; : : : ; ‘

i = 1; : : : ; ‘

Applying The Representer Theorem

Substituting in:

f (cid:3)(x) =

‘
Xi=1

ciK (x; xi);

we arrive at a constrained quadratic programming problem:

1
‘ P‘
i=1 (cid:24)i + (cid:21)cT K c
min
c2IR‘
subject to : yi P‘
j=1 cjK (xi; xj ) (cid:21) 1 (cid:0) (cid:24)i
(cid:24)i (cid:21) 0

i = 1; : : : ; ‘

i = 1; : : : ; ‘

Adding A Bias Term

If we add an unregularized bias term b, which presents some
theoretical di(cid:14)culties to be discussed later, we arrive at the
\primal" SVM:

1
‘ P‘
i=1 (cid:24)i + (cid:21)cT K c
min
c2IR‘;(cid:24)2IR‘
subject to : yi(P‘
j=1 cjK (xi; xj ) + b) (cid:21) 1 (cid:0) (cid:24)i
(cid:24)i (cid:21) 0

i = 1; : : : ; ‘

i = 1; : : : ; ‘

Forming the Lagrangian

For reasons that will be clear in a few slides, we derive the
Wolfe dual quadratic program using Lagrange multiplier
techniques:

1

‘

(cid:0)

(cid:0)

‘
Xj=1

L(c; (cid:24) ; b; (cid:11); (cid:16) ) =

(cid:24)i + (cid:21)cT K c
yi 8<
(cid:11)i 0
@
:
(cid:16)i(cid:24)i

‘
Xi=1
‘
Xi=1
‘
Xi=1
We want to minimize L with respect to c, b, and (cid:24) , and
maximize L with respect to (cid:11) and (cid:16) , subject to the con-
straints of the primal problem and nonnegativity constraints
on (cid:11) and (cid:16) .

cjK (xi; xj ) + b9=
;

(cid:0) 1 + (cid:24)i1
A

Eliminating b and (cid:24)

@L

@ b

@L
@ (cid:24)i

= 0 =)

= 0 =)

(cid:11)iyi = 0

‘
Xi=1
1
(cid:0) (cid:11)i (cid:0) (cid:16)i = 0
‘

=) 0 (cid:20) (cid:11)i (cid:20)

1

‘

We write a reduced Lagrangian in terms of the remaining
variables:

LR(c; (cid:11)) = (cid:21)cT K c (cid:0)

‘
Xi=1

(cid:11)i(yi

‘
Xj=1

cjK (xi; xj ) (cid:0) 1)

Eliminating c

Assuming the K matrix is invertible,

@LR
@ c

= 0 =) 2(cid:21)K c (cid:0) K Y (cid:11) = 0
(cid:11)iyi
2(cid:21)

=) ci =

Where Y is a diagonal matrix whose i’th diagonal element
is yi; Y (cid:11) is a vector whose i’th element is (cid:11)iyi.

The Dual Program

Substituting in our expression for c, we are left with the
following \dual" program:
i=1 (cid:11)i (cid:0) 1
(cid:11)2IR‘ P‘
4(cid:21)(cid:11)T Q(cid:11)
max
P‘
subject to :
i=1 yi(cid:11)i = 0
0 (cid:20) (cid:11)i (cid:20) 1
‘
Here, Q is the matrix de(cid:12)ned by

i = 1; : : : ; ‘

Q = yK yT () Qij = yiyjK (xi; xj ):

Standard Notation

In most of the SVM literature, instead of the regularization
parameter (cid:21), regularization is controlled via a parameter C ,
de(cid:12)ned using the relationship

1
2(cid:21)‘
Using this de(cid:12)nition (after multiplying our objective func-
tion by the constant 1
2(cid:21) , the basic regularization problem
becomes

C =

:

min
f 2H

C

‘
Xi=1

V (yi; f (xi)) +

1
2

jjf jj2
K :

Like (cid:21); the parameter C also controls the tradeo(cid:11) between
classi(cid:12)cation accuracy and the norm of the function. The
primal and dual problems become. . .

The Reparametrized Problems

i=1 (cid:24)i + 1
C P‘
2 cT K c
min
c2IR‘;(cid:24)2IR‘
subject to : yi(P‘
j=1 cjK (xi; xj ) + b) (cid:21) 1 (cid:0) (cid:24)i
(cid:24)i (cid:21) 0

i = 1; : : : ; ‘

i = 1; : : : ; ‘

i=1 (cid:11)i (cid:0) 1
(cid:11)2IR‘ P‘
2(cid:11)T Q(cid:11)
max
subject to : P‘
i=1 yi(cid:11)i = 0
0 (cid:20) (cid:11)i (cid:20) C

i = 1; : : : ; ‘

The Geometric Approach

The \traditional" approach to developing the mathematics
of SVM is to start with the concepts of separating hyper-
planes and margin. The theory is usually developed in a
linear space, beginning with the idea of a perceptron, a
linear hyperplane that separates the positive and the neg-
ative examples. De(cid:12)ning the margin as the distance from
the hyperplane to the nearest example, the basic observa-
tion is that intuitively, we expect a hyperplane with larger
margin to generalize better than one with smaller margin.

Large and Small Margin Hyperplanes

(a)

(b)

Classi(cid:12)cation With Hyperplanes

We denote our hyperplane by w, and we will classify a new
point x via the function

f (x) = sign (w (cid:1) x):

(1)

Given a separating hyperplane w we let x be a datapoint
closest to w, and we let xw be the unique point on w that
is closest to x. Obviously, (cid:12)nding a maximum margin w is
equivalent to maximizing jjx (cid:0) xw jj. . .

Deriving the Maximal Margin, I

For some k (assume k > 0 for convenience),

w (cid:1) x = k
w (cid:1) xw = 0
=) w (cid:1) (x (cid:0) xw) = k

Deriving the Maximal Margin, I I

Noting that the vector x (cid:0) xw is parallel to the normal
vector w,

w (cid:1) (x (cid:0) xw) = w (cid:1)   jjx (cid:0) xw jj
w!
jjwjj
= jjwjj2 jjx (cid:0) xw jj
jjwjj
= jjwjj jjx (cid:0) xw jj
=) jjwjj jj(x (cid:0) xw)jj = k
k
=) jjx (cid:0) xw jj =
jjwjj

Deriving the Maximal Margin, I I I

k is a \nuisance paramter". WLOG, we (cid:12)x k to 1, and see
that maximizing jjx (cid:0) xw jj is equivalent to maximizing 1
,
jjwjj
which in turn is equivalent to minimizing jjwjj or jjwjj2. We
can now de(cid:12)ne the margin as the distance between the
hyperplanes w (cid:1) x = 0 and w (cid:1) x = 1.

The Linear, Homogeneous, Separable SVM

min
w2IRn
subject to : yi(w (cid:1) x) (cid:21) 1 i = 1; : : : ; ‘

jjwjj2

Bias and Slack

The SVM introduced by Vapnik includes an unregularized
bias term b, leading to classi(cid:12)cation via a function of the
form:

f (x) = sign (w (cid:1) x + b):

In practice, we want to work with datasets that are not
linearly separable, so we introduce slacks (cid:24)i, just as before.
We can still de(cid:12)ne the margin as the distance between the
hyperplanes w (cid:1) x = 0 and w (cid:1) x = 1, but this is no longer
particularly geometrically satisfying.

The New Primal

s

With slack variables, the primal SVM problem becomes
i=1 (cid:24)i + 1
2 jjwjj2
C P‘
min
w2IRn ;b2IR
subject to : yi(w (cid:1) x + b) (cid:21) 1 (cid:0) (cid:24)i
(cid:24)i (cid:21) 0

i = 1; : : : ; ‘

i = 1; : : : ; ‘

Historical Perspective

Historically, most developments begin with the geometric
form, derived a dual program which was identical to the
dual we derived above, and only then observed that the
dual program required only dot products and that these
dot products could be replaced with a kernel function.

More Historical Perspective

In the linearly separable case, we can also derive the sep-
arating hyperplane as a vector parallel to the vector con-
necting the closest two points in the positive and negative
classes, passing through the perpendicular bisector of this
vector. This was the \Method of Portraits", derived by
Vapnik in the 1970’s, and recently rediscovered (with non-
separable extensions) by Keerthi.

The Primal and Dual Problems Again

i=1 (cid:24)i + 1
C P‘
2 cT K c
min
c2IR‘;(cid:24)2IR‘
subject to : yi(P‘
j=1 cjK (xi; xj ) + b) (cid:21) 1 (cid:0) (cid:24)i
(cid:24)i (cid:21) 0

i = 1; : : : ; ‘

i = 1; : : : ; ‘

i=1 (cid:11)i (cid:0) 1
(cid:11)2IR‘ P‘
2(cid:11)T Q(cid:11)
max
subject to : P‘
i=1 yi(cid:11)i = 0
0 (cid:20) (cid:11)i (cid:20) C

i = 1; : : : ; ‘

Optimal Solutions

The primal and the dual are both feasible convex quadratic
programs. Therefore, they both have optimal solutions,
and optimal solutons to the primal and the dual have the
same objective value.

The Reparametrized Lagrangian

We derived the dual from the primal using the (now repa-
rameterized) Lagrangian:

L(c; (cid:24) ; b; (cid:11); (cid:16) ) = C

(cid:0)

(cid:0)

(cid:24)i + cT K c
yi 8<
(cid:11)i 0
@
:
(cid:16)i(cid:24)i

‘
Xj=1

‘
Xi=1
‘
Xi=1
‘
Xi=1

cjK (xi; xj ) + b9=
;

(cid:0) 1 + (cid:24)i1
A

Complementary Slackness

Consider the dual variables are associated with the primal
constraints as follows:
(cid:11)i =) yi 8<
‘
Xj=1
:
(cid:16)i =) (cid:24)i (cid:21) 0

cjK (xi; xj ) + b9=
;

(cid:0) 1 + (cid:24)i

Complementary slackness tells us that at optimality, either
the primal inequality is satis(cid:12)ed with equality or the dual
variable is zero.
In other words, if c, (cid:24) , b, (cid:11) and (cid:16) are
optimal solutions to the primal and dual, then
cjK (xi; xj ) + b9=
@yi 8<
(cid:0) 1 + (cid:24)i1
(cid:11)i 0
A
;
:
(cid:16)i(cid:24)i = 0

‘
Xj=1

= 0

Optimality Conditions

All optimal solutions must satisfy:

‘
Xj=1

cjK (xi; xj ) (cid:0)

‘
Xj=1

yi(cid:11)jK (xi; xj ) = 0

i = 1; : : : ; ‘

(cid:11)iyi = 0

‘
Xi=1
C (cid:0) (cid:11)i (cid:0) (cid:16)i = 0
yi 0
yj (cid:11)jK (xi; xj ) + b1
‘
Xj=1
A (cid:0) 1 + (cid:24)i (cid:21) 0
@
A (cid:0) 1 + (cid:24)i3
4yi 0
yj (cid:11)jK (xi; xj ) + b1
(cid:11)i 2
‘
Xj=1
= 0
@
5
(cid:16)i(cid:24)i = 0
(cid:24)i; (cid:11)i; (cid:16)i (cid:21) 0

i = 1; : : : ; ‘

i = 1; : : : ; ‘

i = 1; : : : ; ‘

i = 1; : : : ; ‘

i = 1; : : : ; ‘

Optimality Conditions, I I

The optimality conditions are both necessary and su(cid:14)-
cient.
If we have c, (cid:24) , b, (cid:11) and (cid:16) satisfying the above
conditions, we know that they represent optimal solutions
to the primal and dual problems. These optimality condi-
tions are also known as the Karush-Kuhn-Tucker (KKT)
conditons.

Toward Simpler Optimality Conditions |
Determining b

Suppose we have the optimal (cid:11)i’s. Also suppose (this \al-
ways" happens in practice") that there exists an i satisfying
0 < (cid:11)i < C . Then

(cid:11)i < C =) (cid:16)i > 0
=) (cid:24)i = 0
=) yi 0
‘
Xj=1
@
=) b = yi (cid:0)

yj (cid:11)jK (xi; xj ) + b1
A (cid:0) 1 = 0
‘
Xj=1
yj (cid:11)jK (xi; xj )
So if we know the optimal (cid:11)’s, we can determine b.

Towards Simpler Optimality Conditions, I

De(cid:12)ning our classi(cid:12)cation function f (x) as

f (x) =

‘
Xi=1
we can derive \reduced" optimality conditions. For exam-
ple, consider an i such that yif (xi) < 1:

yi(cid:11)iK (x; xi) + b;

yif (xi) < 1 =) (cid:24)i > 0
=) (cid:16)i = 0
=) (cid:11)i = C

Towards Simpler Optimality Conditions, I I

Conversely, suppose (cid:11)i = C :

(cid:11)i = C =) yif (xi) (cid:0) 1 + (cid:24)i = 0
=) yif (xi) (cid:20) 1

Reduced Optimality Conditions

Proceeding similarly, we can write the following \reduced"
optimality conditions (full proof: homework):

(cid:11)i = 0 () yif (xi) (cid:21) 1
0 < (cid:11)i < C () yif (xi) = 1
(cid:11)i = C () yif (xi) (cid:20) 1

Geometric Interpretation of Reduced
Optimality Conditions

SVM Training

Our plan will be to solve the dual problem to (cid:12)nd the (cid:11)’s,
and use that to (cid:12)nd b and our function f . The dual problem
It has simple box
is easier to solve the primal problem.
constraints and a single inequality constraint, even better,
we will see that the problem can be decomposed into a
sequence of smaller problems.

O(cid:11)-the-shelf QP software

We can solve QPs using standard software. Many codes
are available. Main problem | the Q matrix is dense,
and is ‘-by-‘, so we cannot write it down. Standard QP
software requires the Q matrix, so is not suitable for large
problems.

Decomposition, I

max
(cid:11)W 2IRjW j ; (cid:11)R2IRjRj

Partition the dataset into a working set W and the remainig
points R. We can rewrite the dual problem as:
P‘
(cid:11)i + P i=1
(cid:11)i
i=1
i2R
i2W
2 [(cid:11)W (cid:11)R] " QW W QW R
QRW QRR # " (cid:11)W
(cid:11)R #
(cid:0) 1
Pi2W yi(cid:11)i + Pi2R yi(cid:11)i = 0
0 (cid:20) (cid:11)i (cid:20) C; 8i

subject to :

Decomposition, I I

Suppose we have a feasible solution (cid:11). We can get a
better solution by treating the (cid:11)W as variable and the (cid:11)R
as constant. We can solve the reduced dual problem:

max
(cid:11)W 2IRjW j
subject to :

(1 (cid:0) QW R(cid:11)R)(cid:11)W (cid:0) 1
2(cid:11)WQW W (cid:11)W
Pi2W yi(cid:11)i = (cid:0) Pi2R yi(cid:11)i
0 (cid:20) (cid:11)i (cid:20) C; 8i 2 W

Decomposition, I I I

The reduced problems are (cid:12)xed size, and can be solved us-
ing a standard QP code. Convergence proofs are di(cid:14)cult,
but this approach seems to always converge to an optimal
solution in practice.

Selecting the Working Set

There are many di(cid:11)erent approaches. The basic idea is to
examine points not in the working set, (cid:12)nd points which
violate the reduced optimality conditions, and add them to
the working set. Remove points which are in the working
set but are far from violating the optimality conditions.

Good Large-Scale Solvers

(cid:15) SVM Light: http://svmlight.joachims.org
(cid:15) SVM Torch: http://www.idiap.ch/learning/SVMTorch.html.
Does regression.
(cid:15) SVM Fu: http://fpn.mit.edu/SvmFu

