6.042/18.062J Mathematics  for  Computer  Science 
Tom  Leighton  and Marten  van  Dijk 

December  3,  2010 

Notes  for  Recitation  22 

1	 Expected Value Rule  for  Functions  of Random Vari­
ables 

In  lecture,  we  have  computed  the  expectation  of  a  function  of  a  random  variable  without 
explicitly  discussing  the  general  rule.  For  example,  yesterday  we  saw  that  the  expectation 
of  the  square  of  the  roll  of  a  die  is  not  equal  to  the  square  of  the  expectation  of  the  roll. 
R)2 .  We  will  now 
That  is,  if  R  is  the  outcome  of  a  single  roll  of  a  die.  Then  Ex (R2 ) = Ex (
explicitly present  the  rule  for computing  the expection of a  function of a  random variable R. 
Rule  (Expected Value for the Function of a Random Variable).  Let R  be  a  random  variable, 
� 
and  let  f (R)  be  a  function  of  R.  Then,  the  expected  value  of  the  random  variable  f (R)  is 
given  by 
f (x)  Pr {R = x}
· 
x∈Range(R) 

E [f (R)] = 

2	 Properties  of  Variance 

In  this  problem we will  study  some  properties  of  the  variance  and  the  standard  deviation  of 
random  variables. 
a.  Show  that  for  any  random  variable  R,  Var [R] = E [R2 ] − E2  [R]. 
� 
� 
Solution.  Let  µ = E [R].  Then 
� 
� 
Var [R] = E  (R − E [R])2	
� 
�	
= E	 (R − µ)2 
� 
� 
= E	 R2  − 2µR + µ 2 
� 
� 
= E	 R2  − 2µ E [R] + µ 2 
� 
� 
= E	 R2  − 2µ 2  + µ 2 
� 
� 
= E	 R2  − µ 2 
= E	 R2  − E2  [R] . 

(linearity  of  expectation) 
(def.  of  µ) 

(Deﬁnition  of  variance) 
(def.  of  µ) 

(def.  of  µ) 

� 

�
Recitation  22 

2 

b.  Show that for any random variable R and constants a and b, Var [aR + b] = a2 Var [R] . 
Conclude that the standard deviation of aR + b  is a times the standard deviation of R. 
Solution.  We  will  transform  the  left  side  into  the  right  side.  The  ﬁrst  step  is  to 
� 
� 
expand  Var [aR + b]  using  the  alternate  deﬁnition  of  variance. 
Var [aR + b] = E  (aR + b)2  − E2  [aR + b] . 
We  will  work  on  the  ﬁrst  term  and  then  the  second  term.  For  the  ﬁrst  term,  note 
� 
� 
� 
� 
� 
� 
that  by  linearity  of  expectation, 
E (aR + b)2  = E  a 2R2  + 2abR + b2  = a 2 E  R2  + 2ab E [R] + b2 . 
Similarly  for  the  second  term: 

(1) 

(2) 

E2  [aR + b] = (a E [R] + b)2  = a 2E2  [R] + 2ab E [R] + b2 . 
� 
� 
Finally,  we  subtract  the  expanded  second  term  from  the  ﬁrst. 
� 
� 
Var [aR + b] = E  (aR + b)2  − E2  [aR + b] 
= a 2 E  R2  + 2ab E [R] + b2− 
� 
� 
(a 2E2  [R] + 2ab E [R] + b2 ) 
� 
� 
= a 2 E  R2  − a 2E2  [R] 
= a 2 (E  R2  − E2  [R]) 
= a 2 Var [R] 
� 
Since  the  standard  deviation  of  a  random  variable  is  the  square  root  of  the  variance, 
a2 Var [R]  which  is  just  a  times  the  standard 
the  standard  deviation  of  aR + b  is 
� 
deviation  of  R. 

(by  (??)  and  (??)) 

(previous  part) 

(previous  part) 

c.  Show  that  if  R1  and  R2  are  independent  random  variables,  then 

Var [R1  + R2 ] = Var [R1 ] + Var [R2 ] . 
Solution.  We  will  transform  the  left  side  into  the  right  side.  We  begin  by  applying 
� 
� 
the  alternate  deﬁnition  of  variance. 
Var [R1  + R2 ] = E  (R1  + R2 )2  − E2  [R1  + R2 ] . 
We  will  work  on  the  ﬁrst  term  and  then  the  second  term  separately.  For  the  ﬁrst 
� 
� 
� 
� 
term,  note 
� 
� 
� 
� 
� 
� 
� 
� 
E (R1  + R2 )2  = E  R1
2  + 2R1R2  + R2
2 
2  + E [2R1R2 ] + E  R2
2 
= E  R1
2  + 2 E [R1 ] E [R2 ] + E  R2
2  . 
= E  R1

Recitation  22 

3 

First,  we  multiply  out  the  squared  expression.  The  second  step  uses  linearity  of 
expectation.  In  the  last  step,  we  break  the  expectation  of  the  product  R1R2  into  a 
product of expectations; this is where we use the fact that R1  and R2  are independent. 
Now  we  work  on  the  second  term. 

E2  [R1  + R2 ]	 =  (E [R1 ] + E [R2 ])2 
= E2  [R1 ] + 2 E [R1 ] E [R2 ] + E2  [R2 ] . 

The  ﬁrst  step  uses  linearity  of  expectation,  and  in  the  second  step  we  multiply  out 
the  squared  expression.  Now we  subtract  the  (expanded)  second  term  from  the  ﬁrst. 
� 
� 
�	
� 
Cancelling  and  rearranging  terms,  we  ﬁnd  that 
Var [R1  + R2 ]  =  (E  R2  − E2  [R1 ]) + (E  R2  ) − E2  [R2 ])
2 
1	
=  Var [R1 ] + Var [R2 ] . 

� 

d.  Give  an  example  of  random  variables  R1  and  R2  for  which 

Var [R1  + R2 ] = Var [R1 ] + Var [R2 ] . 

Solution.  Suppose  R  =  R1  =  R2 .  If  linearity  of  variance  held,  then  Var [R + R] = 
Var [R] + Var [R].  However,  by  part  b,  Var [R + R]  =  Var [2R]  =  4 Var [R].  This  is 
only possible  if Var [R] = 0.  If,  say, we  choose R  to be  the  outcome  of  a  fair  coin ﬂip, 
Var [R] =  0.  In  fact,  any  R  which  holds  at  least  2  distinct  values  each  with  positive 
� 
probability  will  do.	

e.	 Compute  the  variance  and  standard  deviation  of  the  Binomial  distribution  Hn,p  with 
� 
parameters  n  and  p. 
n
Solution.  We  know  that  Hn,p  = 
k=1 Ik  where  the  Ik  are  mutually  independent 
0-1-valued  variables  with  Pr {Ik  = 1}  =  p.  The  variance  of  Ik  is  E [I 2 ] − E [Ik ]2  = 
k 
E [Ik ] − E [Ik ] = E [Ik ] (1 − E [Ik ])  =  p(1 − p).  Thus,  by  linearity  of  variance,  we 
� 
2
have  Var [Hn,p ] =  n Var [Ik ] =  np(1 −  p).  Thus,  the  standard  deviation  of  Hn,p  is 
np(1 − p).	
� 
� 
n
f.  Let’s say we have a random variable T  such that T  = 
j=1 Tj , where all of the Tj ’s are 
mutually  independent  and  take  values  in  the  range  [0, 1].  Prove  that  Var(T)≤Ex(T). 
We’ll  use  this  result  in  lecture  tomorrow.  Hint:  Upper  bound  Var [Tj ]  with  E [Tj ]  using 
the  deﬁnition  of  variance  in  part  (a)  and  the  rule  for  computing  the  expectation  of  a 
function  of  a  random  variable. 

Solution.  We  know  by  linearity  of  variance  for  mutually-independent  random  vari­
ables  that 

�
�
Recitation  22 

4 

Var [T ] = Var [T1  + . . . + Tn ] 
= Var [T1 ] + . . . + Var [Tn ] 

Now  we  evaluate  the  variance  of  Tj  for  any  j .  Using  the  deﬁnition  of  variance  from 
� 
� 
part  (a)  above,  we  have 
Var [Tj ] = E  T 2  − E [Tj ]2 
j 
By the rule  for computing the expectation of a  function of a random variable, we also 
�  � 
� 
know  that 
2  = 
E  Tj 
x∈Range(Tj ) 
Now  we  can  use  the  fact  that  Tj  is  in  the  range  [0, 1]  to  say  that  x2  ≤  x,  and  that 
� 
� 
therefore 
� 
� 
x∈Range(Tj ) 
x∈Range(Tj ) 
2  ≤ E [Tj ]  and 
Thus,  E  Tj 

x 2  · Pr {Tj  = x} ≤ 

x 2 Pr {Tj  = x} 

x · Pr {Tj  = x} 

Var [Tj ] ≤ E [Tj ] − E [Tj ]2 
≤ E [Tj ] 

Since  this  holds  for  any  j ,  we  can  now  conclude  that 

Var [T ] = Var [T1 ] + . . . + Var [Tn ] 
≤ E [T1 ] + . . . + E [Tn ] 
= E [T ] 

� 

MIT OpenCourseWare
http://ocw.mit.edu 

6.042J / 18.062J Mathematics for Computer Science 
Fall 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

