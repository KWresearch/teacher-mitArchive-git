“mcs-ftl” — 2010/9/8 — 0:40 — page 431 — #437

16

Independence

16.1 Deﬁnitions

Suppose that we ﬂip two fair coins simultaneously on opposite sides of a room.
Intuitively, the way one coin lands does not affect the way the other coin lands.
The mathematical concept that captures this intuition is called independence:
Pr (cid:2)A j B (cid:3) D PrŒA:
Deﬁnition 16.1.1. Events A and B are independent if PrŒB  D 0 or if
In other words, A and B are independent if knowing that B happens does not al-
ter the probability that A happens, as is the case with ﬂipping two coins on opposite
sides of a room.

(16.1)

16.1.1 Potential Pitfall
Students sometimes get the idea that disjoint events are independent. The opposite
if A \ B D ;, then knowing that A happens means you know that B
is true:
does not happen. So disjoint events are never independent—unless one of them has
probability zero.

16.1.2 Alternative Formulation
Sometimes it is useful to express independence in an alternate form:

Theorem 16.1.2. A and B are independent if and only if
PrŒA \ B  D PrŒA (cid:1) PrŒB :
(16.2)
Proof. There are two cases to consider depending on whether or not PrŒB  D 0.
Case 1 .PrŒB  D 0/: If PrŒB  D 0, A and B are independent by Deﬁnition 16.1.1.
In addition, Equation 16.2 holds since both sides are 0. Hence, the theorem
is true in this case.
PrŒA \ B  D Pr (cid:2)A j B (cid:3) PrŒB :
Case 2 .PrŒB  > 0/: By Deﬁnition 15.1.1,

1“mcs-ftl” — 2010/9/8 — 0:40 — page 432 — #438

Chapter 16 Independence

So Equation 16.2 holds if

Pr (cid:2)A j B (cid:3) D PrŒA;
which, by Deﬁnition 16.1.1, is true iff A and B are independent. Hence, the
(cid:4)
theorem is true in this case as well.

16.2

Independence Is an Assumption

:

Generally, independence is something that you assume in modeling a phenomenon.
For example, consider the experiment of ﬂipping two fair coins. Let A be the event
that the ﬁrst coin comes up heads, and let B be the event that the second coin is
heads. If we assume that A and B are independent, then the probability that both
coins come up heads is:
PrŒA \ B  D PrŒA (cid:1) PrŒB  D 1
(cid:1) 1
D 1
2
2
4
In this example, the assumption of independence is reasonable. The result of one
coin toss should have negligible impact on the outcome of the other coin toss. And
if we were to repeat the experiment many times, we would be likely to have A \ B
about 1/4 of the time.
There are, of course, many examples of events where assuming independence is
not justiﬁed, For example, let C be the event that tomorrow is cloudy and R be the
event that tomorrow is rainy. Perhaps PrŒC  D 1=5 and PrŒR D 1=10 in Boston.
If these events were independent, then we could conclude that the probability of a
rainy, cloudy day was quite small:
PrŒR \ C  D PrŒR (cid:1) PrŒC  D 1
(cid:1) 1
D 1
5
10
50
Unfortunately, these events are deﬁnitely not independent; in particular, every rainy
day is cloudy. Thus, the probability of a rainy, cloudy day is actually 1=10.
Deciding when to assume that events are independent is a tricky business. In
practice, there are strong motivations to assume independence since many useful
formulas (such as Equation 16.2) only hold if the events are independent. But you
need to be careful lest you end up deriving false conclusions. We’ll see several
famous examples where (false) assumptions of independence led to trouble over
the next several chapters. This problem gets even trickier when there are more than
two events in play.

:

2“mcs-ftl” — 2010/9/8 — 0:40 — page 433 — #439

16.3. Mutual Independence

16.3 Mutual Independence

16.3.1 Deﬁnition
We have deﬁned what it means for two events to be independent. What if there are
more than two events? For example, how can we say that the ﬂips of n coins are all
independent of one another?
Events E1 ; : : : ; En are said to be mutually independent if and only if the prob-
ability of any event Ei is unaffected by knowledge of the other events. More for-
mally:
Deﬁnition 16.3.1. A set of events E1 ; E2 ; : : : ; En , is mutually independent if 8i 2
24 \
35 D 0
24Ei
35 :
Œ1; n and 8S  Œ1; n (cid:0) fi g, either
ˇˇˇ \
j 2S
j 2S
In other words, no matter which other events are known to occur, the probability
that Ei occurs is unchanged for any i .
For example, if we toss 100 fair coins at different times, we might reasonably
assume that the tosses are mutually independent since the probability that the i th
coin is heads should be 1=2, no matter which other coin tosses came out heads.

PrŒEi  D Pr

Ej

Ej

Pr

or

16.3.2 Alternative Formulation
Just as Theorem 16.1.2 provided an alternative deﬁnition of independence for two
events, there is an alternative deﬁnition for mutual independence.
35 D Y
24 \
Theorem 16.3.2. A set of events E1 ; E2 ; : : : ; En is mutually independent iff 8S 
Œ1; n,
j 2S
j 2S
The proof of Theorem 16.3.2 uses induction and reasoning similar to the proof
of Theorem 16.1.2. We will not include the details here.
Theorem 16.3.2 says that E1 ; E2 ; : : : ; En are mutually independent if and only

PrŒEj :

Ej

Pr

3“mcs-ftl” — 2010/9/8 — 0:40 — page 434 — #440

Chapter 16 Independence

if all of the following equations hold for all distinct i , j , k , and l :
PrŒEi \ Ej  D PrŒEi  (cid:1) PrŒEj 
PrŒEi \ Ej \ Ek  D PrŒEi  (cid:1) PrŒEj  (cid:1) PrŒEk 
PrŒEi \ Ej \ Ek \ El  D PrŒEi  (cid:1) PrŒEj  (cid:1) PrŒEk  (cid:1) PrŒEl 
:::
PrŒE1 \ (cid:1) (cid:1) (cid:1) \ En  D PrŒE1  (cid:1) (cid:1) (cid:1) PrŒEn :
For example, if we toss n fair coins, the tosses are mutually independent iff for
all m 2 Œ1; n and every subset of m coins, the probability that every coin in the
(cid:0)m .
subset comes up heads is 2

16.3.3 DNA Testing
Assumptions about independence are routinely made in practice. Frequently, such
assumptions are quite reasonable. Sometimes, however, the reasonableness of an
independence assumption is not so clear, and the consequences of a faulty assump-
tion can be severe.
For example, consider the following testimony from the O. J. Simpson murder
trial on May 15, 1995:

Mr. Clarke: When you make these estimations of frequency—and I believe you
touched a little bit on a concept called independence?

Dr. Cotton: Yes, I did.

Mr. Clarke: And what is that again?

Dr. Cotton: It means whether or not you inherit one allele that you have is not—
does not affect the second allele that you might get. That is, if you inherit
a band at 5,000 base pairs, that doesn’t mean you’ll automatically or with
some probability inherit one at 6,000. What you inherit from one parent is
what you inherit from the other.

Mr. Clarke: Why is that important?

Dr. Cotton: Mathematically that’s important because if that were not the case, it
would be improper to multiply the frequencies between the different genetic
locations.

Mr. Clarke: How do you—well, ﬁrst of all, are these markers independent that
you’ve described in your testing in this case?

4“mcs-ftl” — 2010/9/8 — 0:40 — page 435 — #441

16.4. Pairwise Independence

Presumably, this dialogue was as confusing to you as it was for the jury. Es-
sentially, the jury was told that genetic markers in blood found at the crime scene
matched Simpson’s. Furthermore, they were told that the probability that the mark-
ers would be found in a randomly-selected person was at most 1 in 170 million.
This astronomical ﬁgure was derived from statistics such as:
(cid:15) 1 person in 100 has marker A.
(cid:15) 1 person in 50 marker B .
(cid:15) 1 person in 40 has marker C .
(cid:15) 1 person in 5 has marker D .
(cid:15) 1 person in 170 has marker E .
Then these numbers were multiplied to give the probability that a randomly-selected
person would have all ﬁve markers:
PrŒA \ B \ C \ D \ E  D PrŒA (cid:1) PrŒB  (cid:1) PrŒC  (cid:1) PrŒD  (cid:1) PrŒE 
D 1
(cid:1) 1
(cid:1) 1
(cid:1) 1
(cid:1) 1
50
170
5
40
100
D
1

170;000;000

:

The defense pointed out that this assumes that the markers appear mutually inde-
pendently. Furthermore, all the statistics were based on just a few hundred blood
samples.
After the trial, the jury was widely mocked for failing to “understand” the DNA
evidence. If you were a juror, would you accept the 1 in 170 million calculation?

16.4 Pairwise Independence

The deﬁnition of mutual independence seems awfully complicated—there are so
many subsets of events to consider! Here’s an example that illustrates the subtlety
of independence when more than two events are involved. Suppose that we ﬂip
three fair, mutually-independent coins. Deﬁne the following events:
(cid:15) A1 is the event that coin 1 matches coin 2.
(cid:15) A2 is the event that coin 2 matches coin 3.

5“mcs-ftl” — 2010/9/8 — 0:40 — page 436 — #442

Chapter 16 Independence
(cid:15) A3 is the event that coin 3 matches coin 1.
Are A1 , A2 , A3 mutually independent?
The sample space for this experiment is:
fHHH ; HH T ; H TH ; H T T ; THH ; TH T ; T TH ; T T T g:
Every outcome has probability .1=2/3 D 1=8 by our assumption that the coins are
mutually independent.
To see if events A1 , A2 , and A3 are mutually independent, we must check a
sequence of equalities. It will be helpful ﬁrst to compute the probability of each
event Ai :

PrŒA1  D PrŒHHH  C PrŒHH T  C PrŒT TH  C PrŒT T T 
D 1
C 1
C 1
C 1
8
8
8
8
D 1
:
2
By symmetry, PrŒA2  D PrŒA3  D 1=2 as well. Now we can begin checking all the
equalities required for mutual independence in Theorem 16.3.2:
PrŒA1 \ A2  D PrŒHHH  C PrŒT T T 
D 1
C 1
8
8
D 1
4
D 1
(cid:1) 1
2
2
D PrŒA1  PrŒA2 :
By symmetry, PrŒA1 \ A3  D PrŒA1  (cid:1) PrŒA3  and PrŒA2 \ A3  D PrŒA2  (cid:1) PrŒA3 
must hold also. Finally, we must check one last condition:
PrŒA1 \ A2 \ A3  D PrŒHHH  C PrŒT T T 
D 1
C 1
8
8
D 1
4
¤ PrŒA1  PrŒA2  PrŒA3  D 1
8

:

6“mcs-ftl” — 2010/9/8 — 0:40 — page 437 — #443

16.4. Pairwise Independence

The three events A1 , A2 , and A3 are not mutually independent even though any
two of them are independent! This not-quite mutual independence seems weird at
ﬁrst, but it happens. It even generalizes:

Deﬁnition 16.4.1. A set A1 , A2 , . . . , of events is k -way independent iff every set
of k of these events is mutually independent. The set is pairwise independent iff it
is 2-way independent.

So the sets A1 , A2 , A3 above are pairwise independent, but not mutually inde-
pendent. Pairwise independence is a much weaker property than mutual indepen-
dence.
For example, suppose that the prosecutors in the O. J. Simpson trial were wrong
and markers A, B , C , D , and E appear only pairwise independently. Then the
probability that a randomly-selected person has all ﬁve markers is no more than:
PrŒA \ B \ C \ D \ E   PrŒA \ E 
D PrŒA (cid:1) PrŒE 
D 1
(cid:1) 1
100
170
D 1
:
17;000
The ﬁrst line uses the fact that A \ B \ C \ D \ E is a subset of A \ E . (We picked
out the A and E markers because they’re the rarest.) We use pairwise independence
on the second line. Now the probability of a random match is 1 in 17,000—a far cry
from 1 in 170 million! And this is the strongest conclusion we can reach assuming
only pairwise independence.
On the other hand, the 1 in 17,000 bound that we get by assuming pairwise
independence is a lot better than the bound that we would have if there were no
independence at all. For example, if the markers are dependent, then it is possible
that

everyone with marker E has marker A,
everyone with marker A has marker B ,
everyone with marker B has marker C , and
everyone with marker C has marker D .

In such a scenario, the probability of a match is
PrŒE  D 1=170:

7“mcs-ftl” — 2010/9/8 — 0:40 — page 438 — #444

Chapter 16 Independence

So a stronger independence assumption leads to a smaller bound on the prob-
ability of a match. The trick is to ﬁgure out what independence assumption is
reasonable. Assuming that the markers are mutually independent may well not be
reasonable unless you have examined hundreds of millions of blood samples. Oth-
erwise, how would you know that marker D does not show up more frequently
whenever the other four markers are simultaneously present?
We will conclude our discussion of independence with a useful, and somewhat
famous, example known as the Birthday Paradox.

16.5 The Birthday Paradox

Suppose that there are 100 students in a class. What is the probability that some
birthday is shared by two people? Comparing 100 students to the 365 possible
birthdays, you might guess the probability lies somewhere around 1=3—but you’d
be wrong: the probability that there will be two people in the class with matching
birthdays is actually 0:999999692 : : : . In other words, the probability that all 100
birthdays are different is less than 1 in 3,000,000.
Why is this probability so small? The answer involves a phenomenon known as
the Birthday Paradox (or the Birthday Principle), which is surprisingly important
in computer science, as we’ll see later.
Before delving into the analysis, we’ll need to make some modeling assump-
tions:
(cid:15) For each student, all possible birthdays are equally likely. The idea under-
lying this assumption is that each student’s birthday is determined by a ran-
dom process involving parents, fate, and, um, some issues that we discussed
earlier in the context of graph theory. The assumption is not completely ac-
curate, however; a disproportionate number of babies are born in August and
September, for example.
(cid:15) Birthdays are mutually independent. This isn’t perfectly accurate either. For
example, if there are twins in the class, then their birthdays are surely not
independent.

We’ll stick with these assumptions, despite their limitations. Part of the reason is
to simplify the analysis. But the bigger reason is that our conclusions will apply to
many situations in computer science where twins, leap days, and romantic holidays
are not considerations. After all, whether or not two items collide in a hash table
really has nothing to do with human reproductive preferences. Also, in pursuit of

8“mcs-ftl” — 2010/9/8 — 0:40 — page 439 — #445

16.5. The Birthday Paradox

generality, let’s switch from speciﬁc numbers to variables. Let m be the number of
people in the room, and let N be the number of days in a year.
We can solve this problem using the standard four-step method. However, a tree
diagram will be of little value because the sample space is so enormous. This time
we’ll have to proceed without the visual aid!

Step 1: Find the Sample Space
Let’s number the people in the room from 1 to m. An outcome of the experiment
is a sequence .b1 ; : : : ; bm / where bi is the birthday of the i th person. The sample
space is the set of all such sequences:
S D f .b1 ; : : : ; bm / j bi 2 f1; : : : N g g:
Step 2: Deﬁne Events of Interest
Our goal is to determine the probability of the event A in which some pair of people
have the same birthday. This event is a little awkward to study directly, however.
So we’ll use a common trick, which is to analyze the complementary event A, in
which all m people have different birthdays:
A D f .b1 ; : : : ; bm / 2 S j all bi are distinct g:
If we can compute PrŒA, then we can compute what really want, PrŒA, using the
identity
PrŒA C PrŒA D 1:
Step 3: Assign Outcome Probabilities
We need to compute the probability that m people have a particular combination of
birthdays .b1 ; : : : ; bm /. There are N possible birthdays and all of them are equally
likely for each student. Therefore, the probability that the i th person was born on
day bi is 1=N . Since we’re assuming that birthdays are mutually independent, we
can multiply probabilities. Therefore, the probability that the ﬁrst person was born
on day b1 , the second on b2 , and so forth is .1=N /m . This is the probability of
every outcome in the sample space, which means that the sample space is uniform.
That’s good news, because, as we have seen, it means that the analysis will be
simpler.

Step 4: Compute Event Probabilities
We’re interested in the probability of the event A in which everyone has a different
birthday:
A D f .b1 ; : : : ; bn / j all bi are distinct g:

9“mcs-ftl” — 2010/9/8 — 0:40 — page 440 — #446

Chapter 16 Independence
This is a gigantic set. In fact, there are N choices for bi , N (cid:0) 1 choices for b2 , and
so forth. Therefore, by the Generalized Product Rule,
jAj D
D N .N (cid:0) 1/.N (cid:0) 2/ (cid:1) (cid:1) (cid:1) .N (cid:0) m C 1/:

N Š
.N (cid:0) m/Š
Since the sample space is uniform, we can conclude that
PrŒA D jAj
D
N m

N Š
N m .N (cid:0) m/Š

:

(16.3)

2(cid:25)N

We’re done!
Or are we? While correct, it would certainly be nicer to have a closed-form ex-
pression for Equation 16.3. That means ﬁnding an approximation for N Š and .N (cid:0)
m/Š. But this is what we learned how to do in Section 9.6.
In fact, since N
N (cid:0)m
 N (cid:0) m
N
 N
and N (cid:0) m are each at least 100, we know from Corollary 9.6.2 that
and p
p
2(cid:25) .N (cid:0) m/
e
e
are excellent approximations (accurate to within .09%) of N Š and .N (cid:0) m/Š, re-
N
(cid:16) N
spectively. Plugging these values into Equation 16.3 means that (to within .2%)1
p
N (cid:0)m
(cid:16) N (cid:0)m
N mp
2(cid:25)N
e
2(cid:25) .N (cid:0) m/
r
e
eN ln.N /(cid:0)N
r
N
N (cid:0) m
em ln.N / e .N (cid:0)m/ ln.N (cid:0)m/(cid:0).N (cid:0)m/
e .N (cid:0)m/ ln.N /(cid:0).N (cid:0)m/ ln.N (cid:0)m/(cid:0)m
r
N
N (cid:0) m
e .N (cid:0)m/ ln. N
N (cid:0)m /(cid:0)m
D
N
N (cid:0) m
D e .N (cid:0)mC 1
N (cid:0)m /(cid:0)m :
2 / ln. N
1 If there are two terms that can be off by .09%, then the ratio can be off by at most a factor
of .1:0009/2 < 1:002.

PrŒA D

(16.4)

D

D

10“mcs-ftl” — 2010/9/8 — 0:40 — page 441 — #447

16.5. The Birthday Paradox
We can now evaluate Equation 16.4 for m D 100 and N D 365 to ﬁnd that the
probability that all 100 birthdays are different is2
(cid:0)7 :
3:07 : : : (cid:1) 10

We can also plug in other values of m to ﬁnd the number of people so that the
probability of a matching birthday will be about 1=2. In particular, for m D 23 and
N D 365, Equation 16.4 reveals that the probability that all the birthdays differ is
0.49. . . . So if you are in a room with 23 other people, the probability that some pair
of people share a birthday will be a little better than 1=2. It is because 23 seems
like such a small number of people for a match that the phenomenon is called the
Birthday Paradox.

16.5.1 Applications to Hashing
Hashing is frequently used in computer science to map large strings of data into
short strings of data. In a typical scenario, you have a set of m items and you would
like to assign each item to a number from 1 to N where no pair of items is assigned
to the same number and N is as small as possible. For example, the items might be
messages, addresses, or variables. The numbers might represent storage locations,
devices, indices, or digital signatures.
If two items are assigned to the same number, then a collision is said to occur.
Collisions are generally bad. For example, collisions can correspond to two vari-
ables being stored in the same place or two messages being assigned the same dig-
ital signature. Just imagine if you were doing electronic banking and your digital
signature for a $10 check were the same as your signature for a $10 million dollar
check. In fact, ﬁnding collisions is a common technique in breaking cryptographic
codes.3
In practice, the assignment of a number to an item is done using a hash function
h W S ! Œ1; N ;
where S is the set of items and m D jS j. Typically, the values of h.S / are assigned
randomly and are assumed to be equally likely in Œ1; N  and mutually independent.
For efﬁciency purposes, it is generally desirable to make N as small as necessary
to accommodate the hashing of m items without collisions. Ideally, N would be
only a little larger than m. Unfortunately, this is not possible for random hash
functions. To see why, let’s take a closer look at Equation 16.4.

2The possible .2% error is so small that it is lost in the . . . after 3.07.
3Such techniques are often referred to as birthday attacks because of the association of such
attacks with the Birthday Paradox.

11“mcs-ftl” — 2010/9/8 — 0:40 — page 442 — #448

Chapter 16 Independence

(16.5)

(16.6)

By Theorem 9.6.1 and the derivation of Equation 16.4, we know that the proba-
bility that there are no collisions for a random hash function is
N (cid:0)m /(cid:0)m :
(cid:24) e .N (cid:0)mC 1
2 / ln. N
For any m, we now need to ﬁnd a value of N for which this expression is at least 1/2.
That will tell us how big the hash table needs to be in order to have at least a


 1
 (cid:0) m (cid:24) ln
 N

50% chance of avoiding collisions. This means that we need to ﬁnd a value of N
for which
N (cid:0) m C 1

(cid:16) N
N (cid:0) m
:
2
2
N (cid:0)m
To simplify Equation 16.6, we need to get rid of the ln
this by using the Taylor Series expansion for
ln.1 (cid:0) x / D (cid:0)x (cid:0) x 2
(cid:0) x 3
3
2
 D (cid:0) ln
 N (cid:0) m

 N

(cid:16)
N (cid:0) m
N
D (cid:0) (cid:0) m
1 (cid:0) m
D (cid:0) ln
N
(cid:0) m2
2N 2
N
D m
C m3
C m2
2N 2
3N 3
N
4This may not look like a simpliﬁcation, but stick with us here.

(cid:0) (cid:1) (cid:1) (cid:1) 
(cid:0) m3
3N 3
C (cid:1) (cid:1) (cid:1) :

term. We can do

to ﬁnd that4

ln

ln

(cid:0) (cid:1) (cid:1) (cid:1)

12“mcs-ftl” — 2010/9/8 — 0:40 — page 443 — #449

16.5. The Birthday Paradox
 N


Hence,
N (cid:0) m C 1
N (cid:0) m
2

ln

 (cid:0) m D 
D 

C (cid:1) (cid:1) (cid:1)  (cid:0) m

  m
C (cid:1) (cid:1) (cid:1) 
C m2
N (cid:0) m C 1
C m3
2N 2
3N 3
N
2
(cid:0)  m2
C (cid:1) (cid:1) (cid:1) 
C m3
m C m2
3N 2
2N
 m
C (cid:1) (cid:1) (cid:1)  (cid:0) m
C m4
C m3
2N 2
3N 3
N
C (cid:1) (cid:1) (cid:1) 
D (cid:0)  m2
C m3
C m2
C 1
2N 2
3N 3
N
2
 m
C (cid:1) (cid:1) (cid:1) 
C m3
C m4
12N 3
6N 2
2N
C m3
C m2
C 1
2N 2
3N 3
N
2

:

If N grows faster than m2 , then the value in Equation 16.7 tends to 0 and Equa-
tion 16.6 cannot be satisﬁed. If N grows more slowly than m2 , then the value in
Equation 16.7 diverges to negative inﬁnity, and, once again, Equation 16.6 cannot
be satisﬁed. This suggests that we should focus on the case where N D ‚.m2 /,
when Equation 16.7 simpliﬁes to

(16.7)

and Equation 16.6 becomes

(cid:24) (cid:0)m2
2N
 1
2

(cid:24) ln

(cid:0)m2
2N



:

Equation 16.8 is satisﬁed when

N (cid:24) m2
2 ln.2/

:

(16.8)

(16.9)

In other words, N needs to grow quadratically with m in order to avoid collisions.
This unfortunate fact is known as the Birthday Principle and it limits the efﬁciency
of hashing in practice—either N is quadratic in the number of items being hashed
or you need to be able to deal with collisions.

13“mcs-ftl” — 2010/9/8 — 0:40 — page 444 — #450

14MIT OpenCourseWare
http://ocw.mit.edu 

6.042J / 18.062J Mathematics for Computer Science 
Fall 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

