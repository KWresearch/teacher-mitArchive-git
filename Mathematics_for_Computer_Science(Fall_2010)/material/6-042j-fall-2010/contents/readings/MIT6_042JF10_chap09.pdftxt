9

“mcs-ftl” — 2010/9/8 — 0:40 — page 243 — #249

Sums and Asymptotics
Sums and products arise regularly in the analysis of algorithms, ﬁnancial applica-
tions, physical problems, and probabilistic systems. For example, we have already
encountered the sum 1 C 2 C 4 C (cid:1) (cid:1) (cid:1) C N when counting the number of nodes
in a complete binary tree with N inputs. Although such a sum can be represented
log NX
compactly using the sigma notation
i D0
it is a lot easier and more helpful to express the sum by its closed form value
2N (cid:0) 1:

(9.1)

2i ;

By closed form, we mean an expression that does not make use of summation
or product symbols or otherwise need those handy (but sometimes troublesome)
dots. . . . Expressions in closed form are usually easier to evaluate (it doesn’t get
much simpler than 2N (cid:0) 1, for example) and it is usually easier to get a feel for
their magnitude than expressions involving large sums and products.
But how do you ﬁnd a closed form for a sum or product? Well, it’s part math and
part art. And it is the subject of this chapter.
We will start the chapter with a motivating example involving annuities. Figuring
out the value of the annuity will involve a large and nasty-looking sum. We will then
describe several methods for ﬁnding closed forms for all sorts of sums, including
the annuity sums. In some cases, a closed form for a sum may not exist and so we
will provide a general method for ﬁnding good upper and lower bounds on the sum
(which are closed form, of course).
The methods we develop for sums will also work for products since you can
convert any product into a sum by taking a logarithm of the product. As an example,
we will use this approach to ﬁnd a good closed-form approximation to
nŠ WWD 1 (cid:1) 2 (cid:1) 3 (cid:1) (cid:1) (cid:1) n:

We conclude the chapter with a discussion of asymptotic notation. Asymptotic
notation is often used to bound the error terms when there is no exact closed form
expression for a sum or product. It also provides a convenient way to express the
growth rate or order of magnitude of a sum or product.

1“mcs-ftl” — 2010/9/8 — 0:40 — page 244 — #250

Chapter 9

Sums and Asymptotics

9.1 The Value of an Annuity

Would you prefer a million dollars today or $50,000 a year for the rest of your life?
On the one hand, instant gratiﬁcation is nice. On the other hand, the total dollars
received at $50K per year is much larger if you live long enough.
Formally, this is a question about the value of an annuity. An annuity is a ﬁnan-
cial instrument that pays out a ﬁxed amount of money at the beginning of every year
for some speciﬁed number of years. In particular, an n-year, m-payment annuity
pays m dollars at the start of each year for n years. In some cases, n is ﬁnite, but
not always. Examples include lottery payouts, student loans, and home mortgages.
There are even Wall Street people who specialize in trading annuities.1
A key question is, “What is an annuity worth?” For example, lotteries often pay
out jackpots over many years. Intuitively, $50,000 a year for 20 years ought to be
worth less than a million dollars right now. If you had all the cash right away, you
could invest it and begin collecting interest. But what if the choice were between
$50,000 a year for 20 years and a half million dollars today? Now it is not clear
which option is better.

9.1.1 The Future Value of Money
In order to answer such questions, we need to know what a dollar paid out in the
future is worth today. To model this, let’s assume that money can be invested at a
ﬁxed annual interest rate p . We’ll assume an 8% rate2 for the rest of the discussion.
Here is why the interest rate p matters. Ten dollars invested today at interest rate
p will become .1 C p / (cid:1) 10 D 10:80 dollars in a year, .1 C p /2 (cid:1) 10 (cid:25) 11:66 dollars
in two years, and so forth. Looked at another way, ten dollars paid out a year from
now is only really worth 1=.1 C p / (cid:1) 10 (cid:25) 9:26 dollars today. The reason is that if
we had the $9.26 today, we could invest it and would have $10.00 in a year anyway.
Therefore, p determines the value of money paid out in the future.
So for an n-year, m-payment annuity, the ﬁrst payment of m dollars is truly worth
m dollars. But the second payment a year later is worth only m=.1 C p / dollars.
Similarly, the third payment is worth m=.1 C p /2 , and the n-th payment is worth
only m=.1 C p /n(cid:0)1 . The total value, V , of the annuity is equal to the sum of the
1Such trading ultimately led to the subprime mortgage disaster in 2008–2009. We’ll talk more
about that in Section 19.5.3.
2U.S. interest rates have dropped steadily for several years, and ordinary bank deposits now earn
around 1.5%. But just a few years ago the rate was 8%; this rate makes some of our examples a little
more dramatic. The rate has been as high as 17% in the past thirty years.

2“mcs-ftl” — 2010/9/8 — 0:40 — page 245 — #251

j

x j

(substitute x D 1=.1 C p /):

(9.2)

(substitute j D i (cid:0) 1)

9.1. The Value of an Annuity
V D nX
payment values. This gives:
m
 1
.1 C p /i (cid:0)1
D m (cid:1) n(cid:0)1X
i D1
1 C p
D m (cid:1) n(cid:0)1X
j D0
j D0
The goal of the preceding substitutions was to get the summation into a simple
n(cid:0)1X
special form so that we can solve it with a general formula. In particular, the terms
of the sum
j D0
form a geometric series, which means that the ratio of consecutive terms is always
the same and it is a positive value less than one. In this case, the ratio is always x ,
and 0 < x < 1 since we assumed that p > 0. It turns out that there is a nice
n(cid:0)1X
closed-form expression for any geometric series; namely
x i D 1 (cid:0) x n
1 (cid:0) x
i D0
Equation 9.3 can be veriﬁed by induction, but, as is often the case, the proof by
induction gives no hint about how the formula was found in the ﬁrst place. So we’ll
take this opportunity to describe a method that you could use to ﬁgure it out for
yourself. It is called the Perturbation Method.

x j D 1 C x C x 2 C x 3 C (cid:1) (cid:1) (cid:1) C x n(cid:0)1

:

(9.3)

9.1.2 The Perturbation Method
Given a sum that has a nice structure, it is often useful to “perturb” the sum so that
we can somehow combine the sum with the perturbation to get something much
simpler. For example, suppose
S D 1 C x C x 2 C (cid:1) (cid:1) (cid:1) C x n(cid:0)1 :

An example of a perturbation would be
xS D x C x 2 C (cid:1) (cid:1) (cid:1) C x n :

3“mcs-ftl” — 2010/9/8 — 0:40 — page 246 — #252

Chapter 9

Sums and Asymptotics

The difference between S and xS is not so great, and so if we were to subtract xS
from S , there would be massive cancellation:
S D 1 C x C x 2 C x 3 C (cid:1) (cid:1) (cid:1) C x n(cid:0)1
(cid:0)xS D (cid:0) x (cid:0) x 2 (cid:0) x 3 (cid:0) (cid:1) (cid:1) (cid:1) (cid:0) x n(cid:0)1 (cid:0) x n :
The result of the subtraction is

S (cid:0) xS D 1 (cid:0) x n :

Solving for S gives the desired closed-form expression in Equation 9.3:
S D 1 (cid:0) x n
1 (cid:0) x
We’ll see more examples of this method when we introduce generating functions
in Chapter 12.

:

9.1.3 A Closed Form for the Annuity Value
Using Equation 9.3, we can derive a simple formula for V , the value of an annuity
 1 (cid:0) x n

that pays m dollars at the start of each year for n years.
!
 
V D m
1 (cid:0) x
1 C p (cid:0) .1=.1 C p //n(cid:0)1
p

(substituting x D 1=.1 C p /):

D m

(by Equations 9.2 and 9.3)

(9.4)

(9.5)

Equation 9.5 is much easier to use than a summation with dozens of terms. For
example, what is the real value of a winning lottery ticket that pays $50,000 per
year for 20 years? Plugging in m D $50,000, n D 20, and p D 0:08 gives
V (cid:25) $530,180. So because payments are deferred, the million dollar lottery is
really only worth about a half million dollars! This is a good trick for the lottery
advertisers.

Inﬁnite Geometric Series
9.1.4
The question we began with was whether you would prefer a million dollars today
or $50,000 a year for the rest of your life. Of course, this depends on how long
you live, so optimistically assume that the second option is to receive $50,000 a
year forever. This sounds like inﬁnite money! But we can compute the value of an
annuity with an inﬁnite number of payments by taking the limit of our geometric
sum in Equation 9.3 as n tends to inﬁnity.

4“mcs-ftl” — 2010/9/8 — 0:40 — page 247 — #253

9.1. The Value of an Annuity
Theorem 9.1.1. If jx j < 1, then

Proof.

:

1X
i D0

(by Equation 9.3)

x i D 1
1 (cid:0) x

1X
i D0
n(cid:0)1X
x i WWD lim
x i
n!1
i D0
1 (cid:0) x n
D lim
1 (cid:0) x
n!1
D 1
1 (cid:0) x
:
The ﬁnal line follows from that fact that limn!1 x n D 0 when jx j < 1.
(cid:4)
In our annuity problem, x D 1=.1 C p / < 1, so Theorem 9.1.1 applies, and we
1X
get
j D0
D m (cid:1)
1
1 (cid:0) x
(by Theorem 9.1.1)
D m (cid:1) 1 C p
.x D 1=.1 C p //:
p
Plugging in m D $50,000 and p D 0:08, we see that the value V is only $675,000.
Amazingly, a million dollars today is worth much more than $50,000 paid every
year forever! Then again, if we had a million dollars today in the bank earning 8%
interest, we could take out and spend $80,000 a year forever. So on second thought,
this answer really isn’t so amazing.

V D m (cid:1)

x j

(by Equation 9.2)

9.1.5 Examples
Equation 9.3 and Theorem 9.1.1 are incredibly useful in computer science. In fact,
we already used Equation 9.3 implicitly when we claimed in Chapter 6 than an
N -input complete binary tree has
1 C 2 C 4 C (cid:1) (cid:1) (cid:1) C N D 2N (cid:0) 1

5“mcs-ftl” — 2010/9/8 — 0:40 — page 248 — #254

Chapter 9

Sums and Asymptotics

D 1

D 2
3

D 2
!

10

9

0:99999 (cid:1) (cid:1) (cid:1) D 0:9

1
!
 
1 (cid:0) .1=2/
D 0:9

nodes. Here are some other common sums that can be put into closed form using
i D
 1
1X
Equation 9.3 and Theorem 9.1.1:
1 C 1=2 C 1=4 C (cid:1) (cid:1) (cid:1) D
 
 1
i D 0:9
1X
i D0
2
1
 (cid:0)1
i D
1 (cid:0) 1=10
1X
i D0
10
1 (cid:0) 1=2 C 1=4 (cid:0) (cid:1) (cid:1) (cid:1) D
1
1 (cid:0) .(cid:0)1=2/
1 C 2 C 4 C (cid:1) (cid:1) (cid:1) C 2n(cid:0)1 D n(cid:0)1X
i D0
2
2i D 1 (cid:0) 2n
1 (cid:0) 2
1 C 3 C 9 C (cid:1) (cid:1) (cid:1) C 3n(cid:0)1 D n(cid:0)1X
i D0
3i D 1 (cid:0) 3n
1 (cid:0) 3
i D0
If the terms in a geometric sum grow smaller, as in Equation 9.6, then the sum is
said to be geometrically decreasing. If the terms in a geometric sum grow progres-
sively larger, as in Equations 9.9 and 9.10, then the sum is said to be geometrically
increasing. In either case, the sum is usually approximately equal to the term in the
sum with the greatest absolute value. For example, in Equations 9.6 and 9.8, the
largest term is equal to 1 and the sums are 2 and 2/3, both relatively close to 1. In
Equation 9.9, the sum is about twice the largest term. In Equation 9.10, the largest
term is 3n(cid:0)1 and the sum is .3n (cid:0) 1/=2, which is only about a factor of 1:5 greater.
You can see why this rule of thumb works by looking carefully at Equation 9.3 and
Theorem 9.1.1.

D 2n (cid:0) 1
D 3n (cid:0) 1
2

(9.6)

(9.7)

(9.8)

(9.9)

(9.10)

9.1.6 Variations of Geometric Sums
substitutions to the form P x i .
We now know all about geometric sums—if you have one, life is easy. But in
practice one often encounters sums that cannot be transformed by simple variable
A non-obvious, but useful way to obtain new summation formulas from old is
by differentiating or integrating with respect to x . As an example, consider the
n(cid:0)1X
following sum:
i D1

i x i D x C 2x 2 C 3x 3 C (cid:1) (cid:1) (cid:1) C .n (cid:0) 1/x n(cid:0)1

6“mcs-ftl” — 2010/9/8 — 0:40 — page 249 — #255

9.1. The Value of an Annuity

This is not a geometric sum, since the ratio between successive terms is not ﬁxed,
and so our formula for the sum of a geometric sum cannot be directly applied. But
 n(cid:0)1X
!
 1 (cid:0) x n

suppose that we differentiate Equation 9.3:
D d
1 (cid:0) x
x i
i D0
dx
.x i / D n(cid:0)1X
n(cid:0)1X
The left-hand side of Equation 9.11 is simply
i D0
i D0
The right-hand side of Equation 9.11 is
(cid:0)nx n(cid:0)1 .1 (cid:0) x / (cid:0) .(cid:0)1/.1 (cid:0) x n /
.1 (cid:0) x /2

i x i (cid:0)1 :

(9.11)

dx

:

d

dx

d

D (cid:0)nx n(cid:0)1 C nx n C 1 (cid:0) x n
.1 (cid:0) x /2
D 1 (cid:0) nx n(cid:0)1 C .n (cid:0) 1/x n
.1 (cid:0) x /2

:

n(cid:0)1X
Hence, Equation 9.11 means that
i x i (cid:0)1 D 1 (cid:0) nx n(cid:0)1 C .n (cid:0) 1/x n
.1 (cid:0) x /2
:
i D0
In this case, we now have a formula for a sum of the form P i x i (cid:0)1 , but we want a
formula for the series P i x i . The solution is simple: multiply by x . This gives:
Often, differentiating or integrating messes up the exponent of x in every term.
n(cid:0)1X
i x i D x (cid:0) nx n C .n (cid:0) 1/x nC1
.1 (cid:0) x /2
(9.12)
i D1

and we have the desired closed-form expression for our sum3 . It’s a little compli-
cated looking, but it’s easier to work with than the sum.
Notice that if jx j < 1, then this series converges to a ﬁnite value even if there are
inﬁnitely many terms. Taking the limit of Equation 9.12 as n tends inﬁnity gives
the following theorem:

3Since we could easily have made a mistake in the calculation, it is always a good idea to go back
and validate a formula obtained this way with a proof by induction.

7“mcs-ftl” — 2010/9/8 — 0:40 — page 250 — #256

i x i D

Chapter 9
Sums and Asymptotics
Theorem 9.1.2. If jx j < 1, then
1X
x
.1 (cid:0) x /2 :
i D1
As a consequence, suppose that there is an annuity that pays i m dollars at the
end of each year i forever. For example, if m D $50,000, then the payouts are
$50,000 and then $100,000 and then $150,000 and so on. It is hard to believe that
the value of this annuity is ﬁnite! But we can use Theorem 9.1.2 to compute the
1X
value:
V D
i m
.1 C p /i
i D1
D m (cid:1) 1=.1 C p /
.1 (cid:0) 1
1Cp /2
D m (cid:1) 1 C p
p2 :
The second line follows by an application of Theorem 9.1.2. The third line is
obtained by multiplying the numerator and denominator by .1 C p /2 .
For example, if m D $50,000, and p D 0:08 as usual, then the value of the
annuity is V D $8,437,500. Even though the payments increase every year, the in-
crease is only additive with time; by contrast, dollars paid out in the future decrease
in value exponentially with time. The geometric decrease swamps out the additive
increase. Payments in the distant future are almost worthless, so the value of the
annuity is ﬁnite.
The important thing to remember is the trick of taking the derivative (or integral)
of a summation formula. Of course, this technique requires one to compute nasty
derivatives correctly, but this is at least theoretically possible!

9.2 Power Sums

nX
In Chapter 3, we veriﬁed the formula
i D n.n C 1/
i D1
2
But the source of this formula is still a mystery. Sure, we can prove it is true using
well ordering or induction, but where did the expression on the right come from in

(9.13)

:

8“mcs-ftl” — 2010/9/8 — 0:40 — page 251 — #257

9.2. Power Sums

:

(9.14)

i 2 D .2n C 1/.n C 1/n
6

the ﬁrst place? Even more inexplicable is the closed form expression for the sum
nX
of consecutive squares:
i D1
It turns out that there is a way to derive these expressions, but before we explain
it, we thought it would be fun4 to show you how Gauss proved Equation 9.13 when
he was a young boy.5
S D nX
Gauss’s idea is related to the perturbation method we used in Section 9.1.2. Let
i D1
Then we can write the sum in two orders:
S D 1 C 2 C : : : C .n (cid:0) 1/ C n;
S D n C .n (cid:0) 1/ C : : : C 2 C 1:
Adding these two equations gives
2S D .n C 1/ C .n C 1/ C (cid:1) (cid:1) (cid:1) C .n C 1/ C .n C 1/
D n.n C 1/:

i :

Hence,

:

S D n.n C 1/
2
Not bad for a young child. Looks like Gauss had some potential.. . .
Unfortunately, the same trick does not work for summing consecutive squares.
However, we can observe that the result might be a third-degree polynomial in n,
since the sum contains n terms that average out to a value that grows quadratically
nX
in n. So we might guess that
i D1

i 2 D a n3 C b n2 C cn C d :

If the guess is correct, then we can determine the parameters a, b , c , and d by
plugging in a few values for n. Each such value gives a linear equation in a, b ,

4Remember that we are mathematicians, so our deﬁnition of “fun” may be different than yours.
5We suspect that Gauss was probably not an ordinary boy.

9“mcs-ftl” — 2010/9/8 — 0:40 — page 252 — #258

Chapter 9

Sums and Asymptotics

c , and d . If we plug in enough values, we may get a linear system with a unique
solution. Applying this method to our example gives:
n D 0
implies 0 D d
implies 1 D a C b C c C d
n D 1
n D 2
implies 5 D 8a C 4b C 2c C d
n D 3
implies 14 D 27a C 9b C 3c C d :
Solving this system gives the solution a D 1=3, b D 1=2, c D 1=6, d D 0.
Therefore, if our initial guess at the form of the solution was correct, then the
summation is equal to n3=3 C n2=2 C n=6, which matches Equation 9.14.
The point is that if the desired formula turns out to be a polynomial, then once
you get an estimate of the degree of the polynomial, all the coefﬁcients of the
polynomial can be found automatically.
Be careful! This method let’s you discover formulas, but it doesn’t guarantee
they are right! After obtaining a formula by this method, it’s important to go back
and prove it using induction or some other method, because if the initial guess at
the solution was not of the right form, then the resulting formula will be completely
wrong!6

9.3 Approximating Sums

p

Unfortunately, it is not always possible to ﬁnd a closed-form expression for a sum.
S D nX
For example, consider the sum
i D1
No closed form expression is known for S .
In such cases, we need to resort to approximations for S if we want to have a
closed form. The good news is that there is a general method to ﬁnd closed-form
upper and lower bounds that work for most any sum. Even better, the method is
simple and easy to remember. It works by replacing the sum by an integral and
then adding either the ﬁrst or last term in the sum.

i :

6Alternatively, you can use the method based on generating functions described in Chapter 12,
which does not require any guessing at all.

10“mcs-ftl” — 2010/9/8 — 0:40 — page 253 — #259

f .i /

f .i /

and

Then

f .x / dx :

9.3. Approximating Sums
Theorem 9.3.1. Let f W RC ! RC be a nondecreasing7 continuous function and
S D nX
let
I D Z n
i D1
1
I C f .1/  S  I C f .n/:
Similarly, if f is nonincreasing, then
I C f .n/  S  I C f .1/:
Proof. Let f W RC ! RC be a nondecreasing function. For example, f .x / D p
is such a function.
S D nX
Consider the graph shown in Figure 9.1. The value of
i D1
is represented by the shaded area in this ﬁgure. This is because the i th rectangle in
I D Z n
the ﬁgure (counting from left to right) has width 1 and height f .i /.
The value of
1
is the shaded area under the curve of f .x / from 1 to n shown in Figure 9.2.
Comparing the shaded regions in Figures 9.1 and 9.2, we see that S is at least
I plus the area of the leftmost rectangle. Hence,
S (cid:21) I C f .1/
This is the lower bound for S . We next derive the upper bound.
Figure 9.3 shows the curve of f .x / from 1 to n shifted left by 1. This is the same
as the curve f .x C 1/ from 0 to n (cid:0) 1 and it has the same area I .
Comparing the shaded regions in Figures 9.1 and 9.3, we see that S is at most
I plus the area of the rightmost rectangle. Hence,
S  I C f .n/:
(9.16)
7A function f is nondecreasing if f .x / (cid:21) f .y / whenever x (cid:21) y . It is nonincreasing if f .x / 
f .y / whenever x (cid:21) y .

f .x / dx

x

(9.15)

11“mcs-ftl” — 2010/9/8 — 0:40 — page 254 — #260

Chapter 9

Sums and Asymptotics

Pn
Figure 9.1 The area of the i th rectangle is f .i /. The shaded region has area
i D1 f .i /.

is I D R n
Figure 9.2 The shaded area under the curve of f .x / from 1 to n (shown in bold)
1 f .x / dx .

0123    n�2n�1nf.n/f.n�1/f.3/f.2/f.1/0123  n�2n�1n  f.n/f.x/f.n�1/f.3/f.2/f.1/12“mcs-ftl” — 2010/9/8 — 0:40 — page 255 — #261

9.3. Approximating Sums

Figure 9.3 The shaded area under the curve of f .x C 1/ from 0 to n (cid:0) 1 is I , the
same as the area under the curve of f .x / from 1 to n. This curve is the same as the
curve in Figure 9.2 except that has been shifted left by 1.

Combining Equations 9.15 and 9.16, we ﬁnd that
I C f .1/  S  I C f .n/;
for any nondecreasing function f , as claimed
The argument for the case when f is nonincreasing is very similar. The analo-
gous graphs to those shown in Figures 9.1–9.3 are provided in Figure 9.4. As you
can see by comparing the shaded regions in Figures 9.4(a) and 9.4(b),
S  I C f .1/:
Similarly, comparing the shaded regions in Figures 9.4(a) and 9.4(c) reveals that
S (cid:21) I C f .n/:

Hence, if f is nonincreasing,
I C f .n/  S  I C f .1/:

as claimed.

(cid:4)

Theorem 9.3.1 provides good bounds for most sums. At worst, the bounds will
S D nX
be off by the largest term in the sum. For example, we can use Theorem 9.3.1 to
bound the sum
p
i D1

i

0123  n�2n�1n  f.n/f.n�1/f.xC1/f.3/f.2/f.1/13“mcs-ftl” — 2010/9/8 — 0:40 — page 256 — #262

Chapter 9

Sums and Asymptotics

(a)

(b)

Figure 9.4 The area of the shaded region in (a) is S D Pn
the shaded regions in (b) and (c) is I D R n
(c)
i D1 f .i /. The area in
1 f .x / dx .

0123  n�2n�1n  f.n/f.n�1/f.3/f.2/f.1/0123  n�2n�1n  f.n/f.n�1/f.3/f.2/f.1/f.x/0123  n�2n�1n  f.n/f.n�1/f.xC1/f.3/f.2/f.1/14“mcs-ftl” — 2010/9/8 — 0:40 — page 257 — #263

9.4. Hanging Out Over the Edge

as follows.
We begin by computing

I D Z n
p
ˇˇˇˇˇn
x dx
1
D x 3=2
3=2
1
.n3=2 (cid:0) 1/:
D 2
3
We then apply Theorem 9.3.1 to conclude that
.n3=2 (cid:0) 1/ C p
.n3=2 (cid:0) 1/ C 1  S  2
n
3
n3=2 C p
n (cid:0) 2
n3=2 C 1
 S  2
3
3
3
3
3 n3=2 .
In other words, the sum is very close to 2
We’ll be using Theorem 9.3.1 extensively going forward. At the end of this
chapter, we will also introduce some notation that expresses phrases like “the sum
is very close to” in a more precise mathematical manner. But ﬁrst, we’ll see how
Theorem 9.3.1 can be used to resolve a classic paradox in structural engineering.

and thus that

:

2

3

2

9.4 Hanging Out Over the Edge

Suppose that you have n identical blocks8 and that you stack them one on top of
the next on a table as shown in Figure 9.5. Is there some value of n for which it is
possible to arrange the stack so that one of the blocks hangs out completely over
the edge of the table without having the stack fall over? (You are not allowed to use
glue or otherwise hold the stack in position.)
Most people’s ﬁrst response to this question—sometimes also their second and
third responses—is “No. No block will ever get completely past the edge of the
table.” But in fact, if n is large enough, you can get the top block to stick out as far
as you want: one block-length, two block-lengths, any number of block-lengths!

8We will assume that the blocks are rectangular, uniformly weighted and of length 1.

15“mcs-ftl” — 2010/9/8 — 0:40 — page 258 — #264

Chapter 9

Sums and Asymptotics

Figure 9.5 A stack of 5 identical blocks on a table. The top block is hanging out
over the edge of the table, but if you try stacking the blocks this way, the stack will
fall over.

9.4.1 Stability
A stack of blocks is said to be stable if it will not fall over of its own accord. For
example, the stack illustrated in Figure 9.5 is not stable because the top block is
sure to fall over. This is because the center or mass of the top block is hanging out
over air.
In general, a stack of n blocks will be stable if and only if the center of mass of
the top i blocks sits over the .i C 1/st block for i D 1, 2, . . . , n (cid:0) 1, and over the
table for i D n.
We deﬁne the overhang of a stable stack to be the distance between the edge of
the table and the rightmost end of the rightmost block in the stack. Our goal is thus
to maximize the overhang of a stable stack.
For example, the maximum possible overhang for a single block is 1=2. That is
because the center of mass of a single block is in the middle of the block (which is
distance 1=2 from the right edge of the block). If we were to place the block so that
its right edge is more than 1=2 from the edge of the table, the center of mass would
be over air and the block would tip over. But we can place the block so the center
of mass is at the edge of the table, thereby achieving overhang 1=2. This position
is illustrated in Figure 9.6.

table16“mcs-ftl” — 2010/9/8 — 0:40 — page 259 — #265

9.4. Hanging Out Over the Edge

Figure 9.6 One block can overhang half a block length.

In general, the overhang of a stack of blocks is maximized by sliding the entire
stack rightward until its center of mass is at the edge of the table. The overhang
will then be equal to the distance between the center of mass of the stack and the
rightmost edge of the rightmost block. We call this distance the spread of the stack.
Note that the spread does not depend on the location of the stack on the table—it
is purely a property of the blocks in the stack. Of course, as we just observed,
the maximum possible overhang is equal to the maximum possible spread. This
relationship is illustrated in Figure 9.7.

9.4.2 A Recursive Solution
Our goal is to ﬁnd a formula for the maximum possible spread Sn that is achievable
with a stable stack of n blocks.
We already know that S1 D 1=2 since the right edge of a single block with
length 1 is always distance 1=2 from its center of mass. Let’s see if we can use a
recursive approach to determine Sn for all n. This means that we need to ﬁnd a
formula for Sn in terms of Si where i < n.
Suppose we have a stable stack S of n blocks with maximum possible spread Sn .
There are two cases to consider depending on where the rightmost block is in the
stack.

1=2tablecenter of mass of block 17“mcs-ftl” — 2010/9/8 — 0:40 — page 260 — #266

Chapter 9

Sums and Asymptotics

Figure 9.7 The overhang is maximized by maximizing the spread and then plac-
ing the stack so that the center of mass is at the edge of the table.

overhangtablecenter of mass of whole stackspread18“mcs-ftl” — 2010/9/8 — 0:40 — page 261 — #267

9.4. Hanging Out Over the Edge

Case 1: The rightmost block in S is the bottom block. Since the center of mass
of the top n (cid:0) 1 blocks must be over the bottom block for stability, the spread is
maximized by having the center of mass of the top n (cid:0) 1 blocks be directly over the
left edge of the bottom block. In this case the center of mass of S is9
.n (cid:0) 1/ (cid:1) 1 C .1/ (cid:1) 1
D 1 (cid:0) 1
2
2n
n
to the left of the right edge of the bottom block and so the spread for S is
1 (cid:0) 1
2n

(9.17)

:

For example, see Figure 9.8.
In fact, the scenario just described is easily achieved by arranging the blocks as
shown in Figure 9.9, in which case we have the spread given by Equation 9.17. For
example, the spread is 3=4 for 2 blocks, 5=6 for 3 blocks, 7=8 for 4 blocks, etc.
Can we do any better? The best spread in Case 1 is always less than 1, which
means that we cannot get a block fully out over the edge of the table in this scenario.
Maybe our intuition was right that we can’t do better. Before we jump to any false
conclusions, however, let’s see what happens in the other case.
Case 2: The rightmost block in S is among the top n (cid:0) 1 blocks. In this case, the
spread is maximized by placing the top n (cid:0) 1 blocks so that their center of mass is
.n (cid:0) 1/ (cid:1) C C 1 (cid:1) (cid:0)C (cid:0) 1
(cid:1)
directly over the right end of the bottom block. This means that the center of mass
for S is at location
D C (cid:0) 1
2
n
2n
where C is the location of the center of mass of the top n (cid:0) 1 blocks. In other
words, the center of mass of S is 1=2n to the left of the center of mass of the top
n (cid:0) 1 blocks. (The difference is due to the effect of the bottom block, whose center
of mass is 1=2 unit to the left of C .) This means that the spread of S is 1=2n
greater than the spread of the top n (cid:0) 1 blocks (because we are in the case where
the rightmost block is among the top n (cid:0) 1 blocks.)
Since the rightmost block is among the top n (cid:0) 1 blocks, the spread for S is
maximized by maximizing the spread for the top n (cid:0) 1 blocks. Hence the maximum
spread for S in this case is
Sn(cid:0)1 C 1
2n
9The center of mass of a stack of blocks is the average of the centers of mass of the individual
blocks.

(9.18)

19“mcs-ftl” — 2010/9/8 — 0:40 — page 262 — #268

Chapter 9

Sums and Asymptotics

Figure 9.8 The scenario where the bottom block is the rightmost block. In this
case, the spread is maximized by having the center of mass of the top n (cid:0) 1 blocks
be directly over the left edge of the bottom block.

top n�1blocksbottom blockcenter of mass of topn�1 blockscenter of mass of S20“mcs-ftl” — 2010/9/8 — 0:40 — page 263 — #269

9.4. Hanging Out Over the Edge

Figure 9.9 A method for achieving spread (and hence overhang) 1 (cid:0) 1=2n with
n blocks, where the bottom block is the rightmost block.

 n�1blocks1�1=2n1=2table21“mcs-ftl” — 2010/9/8 — 0:40 — page 264 — #270

Chapter 9
Sums and Asymptotics
where Sn(cid:0)1 is the maximum possible spread for n (cid:0) 1 blocks (using any strategy).
We are now almost done. There are only two cases to consider when designing
a stack with maximum spread and we have analyzed both of them. This means
(cid:27)
(cid:26)
that we can combine Equation 9.17 from Case 1 with Equation 9.18 from Case 2 to
conclude that
; Sn(cid:0)1 C 1
1 (cid:0) 1
Sn D max
2n
2n

(9.19)

for any n > 1.
Uh-oh. This looks complicated. Maybe we are not almost done after all!
Equation 9.19 is an example of a recurrence. We will describe numerous tech-
niques for solving recurrences in Chapter 10, but, fortunately, Equation 9.19 is
simple enough that we can solve it without waiting for all the hardware in Chap-
ter 10.
One of the ﬁrst things to do when you have a recurrence is to get a feel for it
by computing the ﬁrst few terms. This often gives clues about a way to solve the
recurrence, as it will in this case.
We already know that S1 D 1=2. What about S2 ? From Equation 9.19, we ﬁnd
(cid:26)
(cid:27)
that

S2 D max
D 3=4 :

1 (cid:0) 1
4

;

C 1
4

1

2

3

4

(cid:27)
;

C 1
6

Both cases give the same spread, albeit by different approaches. For example, see
Figure 9.10.
(cid:26)
(cid:27)
That was easy enough. What about S3 ?
(cid:26) 5
S3 D max
1 (cid:0) 1
6
D max
11
6
D 11
:
12
(cid:26)
(cid:27)
As we can see, the method provided by Case 2 is the best. Let’s check n D 4.
1 (cid:0) 1
S4 D max
C 1
8
8
D 25
24

(9.20)

11

12

12

;

;

:

22“mcs-ftl” — 2010/9/8 — 0:40 — page 265 — #271

9.4. Hanging Out Over the Edge

(a)

(b)
Figure 9.10 Two ways to achieve spread (and hence overhang) 3=4 with n D 2
blocks. The ﬁrst way (a) is from Case 1 and the second (b) is from Case 2.

1=23=4table1=21=4table23“mcs-ftl” — 2010/9/8 — 0:40 — page 266 — #272

Chapter 9

Sums and Asymptotics

(9.21)

Wow! This is a breakthrough—for two reasons. First, Equation 9.20 tells us that
by using only 4 blocks, we can make a stack so that one of the blocks is hanging
out completely over the edge of the table. The two ways to do this are shown in
Figure 9.11.
The second reason that Equation 9.20 is important is that we now know that
S4 > 1, which means that we no longer have to worry about Case 1 for n > 4 since
Case 1 never achieves spread greater than 1. Moreover, even for n  4, we have
now seen that the spread achieved by Case 1 never exceeds the spread achieved by
Case 2, and they can be equal only for n D 1 and n D 2. This means that
Sn D Sn(cid:0)1 C 1
2n
for all n > 1 since we have shown that the best spread can always be achieved
using Case 2.
The recurrence in Equation 9.21 is much easier to solve than the one we started
with in Equation 9.19. We can solve it by expanding the equation as follows:
Sn D Sn(cid:0)1 C 1
2n
D Sn(cid:0)2 C
1
2.n (cid:0) 1/
D Sn(cid:0)3 C
1
2.n (cid:0) 2/
Sn D nX
and so on. This suggests that
i D1
2 i
which is, indeed, the case.
Equation 9.22 can be veriﬁed by induction. The base case when n D 1 is true
since we know that S1 D 1=2. The inductive step follows from Equation 9.21.
know that S4 > 1, we still don’t know how big the sum Pn
So we now know the maximum possible spread and hence the maximum possible
overhang for any stable stack of books. Are we done? Not quite. Although we
i D1
2 i can get.
1
It turns out that Sn is very close to a famous sum known as the nth Harmonic
number Hn .

C 1
2n
C
1
2.n (cid:0) 1/

C 1
2n

1

;

(9.22)

9.4.3 Harmonic Numbers
Hn WWD nX
Deﬁnition 9.4.1. The nth Harmonic number is
i D1

i

1

:

24“mcs-ftl” — 2010/9/8 — 0:40 — page 267 — #273

9.4. Hanging Out Over the Edge

(a)

(b)

Figure 9.11 The two ways to achieve spread (and overhang) 25=24. The method
in (a) uses Case 1 for the top 2 blocks and Case 2 for the others. The method in (b)
uses Case 2 for every block that is added to the stack.

3=41=81=21=6table1=41=21=81=6table25“mcs-ftl” — 2010/9/8 — 0:40 — page 268 — #274

Chapter 9

Sums and Asymptotics

So Equation 9.22 means that

:

1

x

:

(9.23)

D ln.n/;

dx D ln.x /

Theorem 9.3.1 means that

Sn D Hn
2
The ﬁrst few Harmonic numbers are easy to compute. For example,
D 25
C 1
C 1
H4 D 1 C 1
2
3
4
12
There is good news and bad news about Harmonic numbers. The bad news is that
there is no closed-form expression known for the Harmonic numbers. The good
Z n
news is that we can use Theorem 9.3.1 to get close upper and lower bounds on Hn .
ˇˇˇn
In particular, since
1
1
 Hn  ln.n/ C 1:
ln.n/ C 1
n
In other words, the nth Harmonic number is very close to ln.n/.
Because the Harmonic numbers frequently arise in practice, mathematicians
have worked hard to get even better approximations for them. In fact, it is now
known that
Hn D ln.n/ C (cid:13) C 1
C (cid:15) .n/
C 1
120n4
12n2
2n
Here (cid:13) is a value 0:577215664 : : : called Euler’s constant, and (cid:15) .n/ is between 0
and 1 for all n. We will not prove this formula.
We are now ﬁnally done with our analysis of the block stacking problem. Plug-
ging the value of Hn into Equation 9.23, we ﬁnd that the maximum overhang for
n blocks is very close to 1
2 ln.n/. Since ln.n/ grows to inﬁnity as n increases, this
means that if we are given enough blocks (in theory anyway), we can get a block to
hang out arbitrarily far over the edge of the table. Of course, the number of blocks
we need will grow as an exponential function of the overhang, so it will probably
take you a long time to achieve an overhang of 2 or 3, never mind an overhang
of 100.

(9.24)

(9.25)

9.4.4 Asymptotic Equality
For cases like Equation 9.25 where we understand the growth of a function like Hn
up to some (unimportant) error terms, we use a special notation, (cid:24), to denote the
leading term of the function. For example, we say that Hn (cid:24) ln.n/ to indicate that
the leading term of Hn is ln.n/. More precisely:

26“mcs-ftl” — 2010/9/8 — 0:40 — page 269 — #275

iff

9.5. Double Trouble
Deﬁnition 9.4.2. For functions f; g W R ! R, we say f is asymptotically equal to
g , in symbols,
f .x / (cid:24) g.x /
x!1 f .x /=g.x / D 1:
lim
Although it is tempting to write Hn (cid:24) ln.n/ C (cid:13) to indicate the two leading
terms, this is not really right. According to Deﬁnition 9.4.2, Hn (cid:24) ln.n/ C c where
c is any constant. The correct way to indicate that (cid:13) is the second-largest term is
Hn (cid:0) ln.n/ (cid:24) (cid:13) .
The reason that the (cid:24) notation is useful is that often we do not care about lower
order terms. For example, if n D 100, then we can compute H .n/ to great precision
jHn (cid:0) ln.n/ (cid:0) (cid:13) j  ˇˇˇˇ 1
ˇˇˇˇ <
using only the two leading terms:
1
120 (cid:1) 1004
200
We will spend a lot more time talking about asymptotic notation at the end of the
chapter. But for now, let’s get back to sums.

120000

C

(cid:0)

1

1

:

200

9.5 Double Trouble

Sometimes we have to evaluate sums of sums, otherwise known as double summa-
tions. This sounds hairy, and sometimes it is. But usually, it is straightforward—
you just evaluate the inner sum, replace it with a closed form, and then evaluate the

27“mcs-ftl” — 2010/9/8 — 0:40 — page 270 — #276

y n

x i

y nx nC1

.xy /n

Equation 9.3

Theorem 9.1.1

Theorem 9.1.1

Chapter 9
Sums and Asymptotics
!
 


nX
1X
1X
outer sum (which no longer has a summation inside it). For example,10
y n 1 (cid:0) x nC1
D
D  1
 1X
y n (cid:0)  1
 1X
1 (cid:0) x
nD0
nD0
i D0
1 (cid:0) x
1 (cid:0) x
 1X
(cid:0) (cid:16) x
nD0
nD0
D
1
.1 (cid:0) x /.1 (cid:0) y /
1 (cid:0) x
nD0
(cid:0)
D
x
1
.1 (cid:0) x /.1 (cid:0) xy /
.1 (cid:0) x /.1 (cid:0) y /
D .1 (cid:0) xy / (cid:0) x .1 (cid:0) y /
.1 (cid:0) x /.1 (cid:0) y /.1 (cid:0) xy /
1 (cid:0) x
D
.1 (cid:0) x /.1 (cid:0) y /.1 (cid:0) xy /
1
.1 (cid:0) y /.1 (cid:0) xy /
When there’s no obvious closed form for the inner sum, a special trick that is
often useful is to try exchanging the order of summation. For example, suppose we
nX
Hk D nX
kX
want to compute the sum of the ﬁrst n Harmonic numbers
j D1
kD1
kD1
Z n
For intuition about this sum, we can apply Theorem 9.3.1 to Equation 9.24 to con-
ˇˇˇn
clude that the sum is close to
ln.x / dx D x ln.x / (cid:0) x
1
1
Now let’s look for an exact answer. If we think about the pairs .k ; j / over which

D n ln.n/ (cid:0) n C 1:

D

:

1

j

(9.26)

10Ok, so maybe this one is a little hairy, but it is also fairly straightforward. Wait till you see the
next one!

28“mcs-ftl” — 2010/9/8 — 0:40 — page 271 — #277

9.5. Double Trouble

we are summing, they form a triangle:

j
1
1
1
1
1
: : :
1

k 1
2
3
4

n

2

3

4

5 : : :

n

1=2
1=2 1=3
1=2 1=3 1=4

1=2

: : :

1=n

1

j

1

1

The summation in Equation 9.26 is summing each row and then adding the row
sums. Instead, we can sum the columns and then add the column sums. Inspecting
kX
Hk D nX
nX
the table we see that this double sum can be written as
D nX
nX
j D1
kD1
kD1
1
nX
D nX
j D1
kDj
j
D nX
j D1
kDj
.n (cid:0) j C 1/
(cid:0) nX
D nX
j D1
j
n C 1
j
(cid:0) nX
nX
j D1
j D1
j
j
D .n C 1/
j D1
j D1
j
D .n C 1/Hn (cid:0) n:

(9.27)

1

1

j

1

29“mcs-ftl” — 2010/9/8 — 0:40 — page 272 — #278

Chapter 9

Sums and Asymptotics

9.6 Products

i :

f .i /;

then

(9.28)

ln.f .i //:

We’ve covered several techniques for ﬁnding closed forms for sums but no methods
for dealing with products. Fortunately, we do not need to develop an entirely new
nŠ WWD nY
set of tools when we encounter a product such as
i D1
P D nY
That’s because we can convert any product into a sum by taking a logarithm. For
example, if
i D1
ln.P / D nX
i D1
We can then apply our summing tools to ﬁnd a closed form (or approximate closed
form) for ln.P / and then exponentiate at the end to undo the logarithm.
For example, let’s see how this works for the factorial function nŠ We start by
taking the logarithm:
ln.nŠ/ D ln.1 (cid:1) 2 (cid:1) 3 (cid:1) (cid:1) (cid:1) .n (cid:0) 1/ (cid:1) n/
D nX
D ln.1/ C ln.2/ C ln.3/ C (cid:1) (cid:1) (cid:1) C ln.n (cid:0) 1/ C ln.n/
i D1
Unfortunately, no closed form for this sum is known. However, we can apply
Z n
Theorem 9.3.1 to ﬁnd good closed-form bounds on the sum. To do this, we ﬁrst
ˇˇˇn
compute
ln.x / dx D x ln.x / (cid:0) x
1
D n ln.n/ (cid:0) n C 1:
1
n ln.n/ (cid:0) n C 1  nX
Plugging into Theorem 9.3.1, this means that
ln.i /  n ln.n/ (cid:0) n C 1 C ln.n/:
i D1

ln.i /:

30“mcs-ftl” — 2010/9/8 — 0:40 — page 273 — #279

9.6. Products

Exponentiating then gives

(9.29)

 nŠ  nnC1
nn
en(cid:0)1
en(cid:0)1 :
This means that nŠ is within a factor of n of nn=en(cid:0)1 .
9.6.1 Stirling’s Formula
nŠ is probably the most commonly used product in discrete mathematics, and so
mathematicians have put in the effort to ﬁnd much better closed-form bounds on its
value. The most useful bounds are given in Theorem 9.6.1.
(cid:16) n
n
Theorem 9.6.1 (Stirling’s Formula). For all n (cid:21) 1,
nŠ D p
e

e (cid:15).n/

2(cid:25) n

where

2(cid:25) n
11The (cid:24) notation was deﬁned in Section 9.4.4.

:

nŠ >

 (cid:15) .n/  1
1
12n C 1
12n
Theorem 9.6.1 can be proved by induction on n, but the details are a bit painful
(even for us) and so we will not go through them here.
There are several important things to notice about Stirling’s Formula. First, (cid:15) .n/
n
(cid:16) n
is always positive. This means that
p
2(cid:25) n
e
for all n 2 NC .
(cid:16) n
n
Second, (cid:15) .n/ tends to zero as n gets large. This means that11
nŠ (cid:24) p
e
which is rather surprising. After all, who would expect both (cid:25) and e to show up in
a closed-form expression that is asymptotically equal to nŠ?
Third, (cid:15) .n/ is small even for small values of n. This means that Stirling’s For-
(cid:16) n
n
mula provides good approximations for nŠ for most all values of n. For example, if
p
we use
e

(9.30)

(9.31)

2(cid:25) n

;

31“mcs-ftl” — 2010/9/8 — 0:40 — page 274 — #280

e1=12n < 1%

:

or

e1=12n

Chapter 9
Sums and Asymptotics
2(cid:25) n (cid:0) n
(cid:1)n
n (cid:21) 10
n (cid:21) 1
n (cid:21) 100
n (cid:21) 1000
Approximation
p
2(cid:25) n (cid:0) n
(cid:1)n
< 10% < 1%
< 0.1%
< 0.01%
p
e
< 0.01% < 0.0001% < 0.000001%
2(cid:25) n (cid:0) n
(cid:1)n approximates nŠ to within 0.1%.
e
p
Table 9.1 Error bounds on common approximations for nŠ from Theorem 9.6.1.
For example, if n (cid:21) 100, then
e
as the approximation for nŠ, as many people do, we are guaranteed to be within a
factor of
e (cid:15).n/  e
1
12n
of the correct value. For n (cid:21) 10, this means we will be within 1% of the correct
value. For n (cid:21) 100, the error will be less than 0.1%.
n
(cid:16) n
If we need an even closer approximation for nŠ, then we could use either
p
(cid:16) n
n
2(cid:25) n
e
e1=.12nC1/
e
depending on whether we want an upper bound or a lower bound, respectively. By
Theorem 9.6.1, we know that both bounds will be within a factor of
(cid:0) 1
12nC1 D e
144n2C12n
1
1
e
12n
of the correct value. For n (cid:21) 10, this means that either bound will be within 0.01%
of the correct value. For n (cid:21) 100, the error will be less than 0.0001%.
For quick future reference, these facts are summarized in Corollary 9.6.2 and
Table 9.1.
n
(cid:16) n
Corollary 9.6.2. For n (cid:21) 1,
:
n
(cid:16) n
e
n
(cid:16) n
:
e
e

2(cid:25) n
p
nŠ < 1:0009
2(cid:25) n

p
nŠ < 1:009

p
nŠ < 1:09

2(cid:25) n

p

2(cid:25) n

For n (cid:21) 10,

For n (cid:21) 100,

32“mcs-ftl” — 2010/9/8 — 0:40 — page 275 — #281

9.7. Asymptotic Notation

9.7 Asymptotic Notation

Asymptotic notation is a shorthand used to give a quick measure of the behavior of
a function f .n/ as n grows large. For example, the asymptotic notation (cid:24) of Deﬁ-
nition 9.4.2 is a binary relation indicating that two functions grow at the same rate.
There is also a binary relation indicating that one function grows at a signiﬁcantly
slower rate than another.

9.7.1 Little Oh
Deﬁnition 9.7.1. For functions f; g W R ! R, with g nonnegative, we say f is
asymptotically smaller than g , in symbols,
f .x / D o.g.x //;

iff

x!1 f .x /=g.x / D 0:
lim
For example, 1000x 1:9 D o.x 2 /, because 1000x 1:9=x 2 D 1000=x 0:1 and since
x 0:1 goes to inﬁnity with x and 1000 is constant, we have limx!1 1000x 1:9=x 2 D
0. This argument generalizes directly to yield
Lemma 9.7.2. x a D o.x b / for all nonnegative constants a < b .
Using the familiar fact that log x < x for all x > 1, we can prove
Lemma 9.7.3. log x D o.x (cid:15) / for all (cid:15) > 0.
Proof. Choose (cid:15) > ı > 0 and let x D z ı in the inequality log x < x . This implies
log z < z ı =ı D o.z (cid:15) /

by Lemma 9.7.2:

(9.32)
(cid:4)

Corollary 9.7.4. x b D o.ax / for any a; b 2 R with a > 1.
Lemma 9.7.3 and Corollary 9.7.4 can also be proved using l’H ˆopital’s Rule or
the McLaurin Series for log x and ex . Proofs can be found in most calculus texts.

33“mcs-ftl” — 2010/9/8 — 0:40 — page 276 — #282

Chapter 9

Sums and Asymptotics

9.7.2 Big Oh
Big Oh is the most frequently used asymptotic notation. It is used to give an upper
bound on the growth of a function, such as the running time of an algorithm.
Deﬁnition 9.7.5. Given nonnegative functions f; g W R ! R, we say that
f D O .g/

iff

(cid:4)

x!1 f .x /=g.x / < 1:
lim sup
This deﬁnition12 makes it clear that
Lemma 9.7.6. If f D o.g/ or f (cid:24) g , then f D O .g/.
Proof. lim f =g D 0 or lim f =g D 1 implies lim f =g < 1.
It is easy to see that the converse of Lemma 9.7.6 is not true. For example,
2x D O .x /, but 2x 6(cid:24) x and 2x ¤ o.x /.
The usual formulation of Big Oh spells out the deﬁnition of lim sup without
mentioning it. Namely, here is an equivalent deﬁnition:
Deﬁnition 9.7.7. Given functions f; g W R ! R, we say that
f D O .g/
iff there exists a constant c (cid:21) 0 and an x0 such that for all x (cid:21) x0 , jf .x /j  cg.x /.
This deﬁnition is rather complicated, but the idea is simple: f .x / D O .g.x //
means f .x / is less than or equal to g.x /, except that we’re willing to ignore a
constant factor, namely, c , and to allow exceptions for small x , namely, x < x0 .
We observe,
Lemma 9.7.8. If f D o.g/, then it is not true that g D O .f /.
12We can’t simply use the limit as x ! 1 in the deﬁnition of O ./, because if f .x /=g.x / oscillates
between, say, 3 and 5 as x grows, then f D O .g/ because f  5g , but limx!1 f .x /=g.x /
does not exist. So instead of limit, we use the technical notion of lim sup. In this oscillating case,
lim supx!1 f .x /=g.x / D 5.
The precise deﬁnition of lim sup is
x!1 h.x / WWD lim
x!1 luby(cid:21)x h.y /;
lim sup
where “lub” abbreviates “least upper bound.”

34“mcs-ftl” — 2010/9/8 — 0:40 — page 277 — #283

9.7. Asymptotic Notation

Proof.

lim
x!1

g.x /

f .x /

D

1
limx!1 f .x /=g.x /

D 1;

D 1
0

(cid:4)

so g ¤ O .f /.
Proposition 9.7.9. 100x 2 D O .x 2 /.
x (cid:21) 1, ˇˇ100x 2 ˇˇ  100x 2 .
Proof. Choose c D 100 and x0 D 1. Then the proposition holds, since for all
(cid:4)
Proposition 9.7.10. x 2 C 100x C 10 D O .x 2 /.
Proof. .x 2 C 100x C 10/=x 2 D 1C 100=x C 10=x 2 and so its limit as x approaches
inﬁnity is 1C0C0 D 1. So in fact, x 2C100xC10 (cid:24) x 2 , and therefore x 2C100xC
10 D O .x 2 /. Indeed, it’s conversely true that x 2 D O .x 2 C 100x C 10/.
(cid:4)
Proposition 9.7.10 generalizes to an arbitrary polynomial:
Proposition 9.7.11. ak x k C ak(cid:0)1x k(cid:0)1 C (cid:1) (cid:1) (cid:1) C a1x C a0 D O .x k /.
We’ll omit the routine proof.
Big Oh notation is especially useful when describing the running time of an
algorithm. For example, the usual algorithm for multiplying n (cid:2) n matrices uses
a number of operations proportional to n3 in the worst case. This fact can be
expressed concisely by saying that the running time is O .n3 /. So this asymptotic
notation allows the speed of the algorithm to be discussed without reference to
constant factors or lower-order terms that might be machine speciﬁc. It turns out
that there is another, ingenious matrix multiplication procedure that uses O .n2 :55 /
operations. This procedure will therefore be much more efﬁcient on large enough
matrices. Unfortunately, the O .n2 :55 /-operation multiplication procedure is almost
never used in practice because it happens to be less efﬁcient than the usual O .n3 /
procedure on matrices of practical size.13

9.7.3 Omega
Suppose you want to make a statement of the form “the running time of the algo-
rithm is a least. . . ”. Can you say it is “at least O .n2 /”? No! This statement is
meaningless since big-oh can only be used for upper bounds. For lower bounds,
we use a different symbol, called “big-Omega.”

13 It is even conceivable that there is an O .n2 / matrix multiplication procedure, but none is known.

35“mcs-ftl” — 2010/9/8 — 0:40 — page 278 — #284

Chapter 9
Sums and Asymptotics
Deﬁnition 9.7.12. Given functions f; g W R ! R, we say that
f D (cid:127).g/
iff there exists a constant c > 0 and an x0 such that for all x (cid:21) x0 , we have
f .x / (cid:21) c jg.x /j.
In other words, f .x / D (cid:127).g.x // means that f .x / is greater than or equal
to g.x /, except that we are willing to ignore a constant factor and to allow ex-
ceptions for small x .
If all this sounds a lot like big-Oh, only in reverse, that’s because big-Omega is
the opposite of big-Oh. More precisely,
Theorem 9.7.13. f .x / D O .g.x // if and only if g.x / D (cid:127).f .x //.
Proof.
f .x / D O .g.x //
iff 9c > 0; x0 : 8x (cid:21) x0 : jf .x /j  cg.x /
iff 9c > 0; x0 : 8x (cid:21) x0 : g.x / (cid:21) 1
jf .x /j
c
0
0 jf .x /j
0 D 1=c )
iff 9c
> 0; x0 : 8x (cid:21) x0 : g.x / (cid:21) c
(set c
iff g.x / D (cid:127).f .x //
(Deﬁnition 9.7.12) (cid:4)
For example, x 2 D (cid:127).x /, 2x D (cid:127).x 2 /, and x=100 D (cid:127).100x C p
x /.
So if the running time of your algorithm on inputs of size n is T .n/, and you
want to say it is at least quadratic, say
T .n/ D (cid:127).n2 /:

(Deﬁnition 9.7.7)

Little Omega
There is also a symbol called little-omega, analogous to little-oh, to denote that one
function grows strictly faster than another function.
Deﬁnition 9.7.14. For functions f; g W R ! R with f nonnegative, we say that
f .x / D ! .g.x //

iff

In other words,

iff

D 0:

g.x /

lim
x!1
f .x /
f .x / D ! .g.x //
g.x / D o.f .x //:

36“mcs-ftl” — 2010/9/8 — 0:40 — page 279 — #285

9.7. Asymptotic Notation
For example, x 1:5 D ! .x / and
x D ! .ln2 .x //.
The little-omega symbol is not as widely used as the other asymptotic symbols
we have been discussing.

p

9.7.4 Theta
Sometimes we want to specify that a running time T .n/ is precisely quadratic up to
constant factors (both upper bound and lower bound). We could do this by saying
that T .n/ D O .n2 / and T .n/ D (cid:127).n2 /, but rather than say both, mathematicians
have devised yet another symbol, ‚, to do the job.

Deﬁnition 9.7.15.

iff f D O .g/ and g D O .f /:
f D ‚.g/
The statement f D ‚.g/ can be paraphrased intuitively as “f and g are equal
to within a constant factor.” Indeed, by Theorem 9.7.13, we know that
iff f D O .g/ and f D (cid:127).g/:
f D ‚.g/

The Theta notation allows us to highlight growth rates and allow suppression
of distracting factors and low-order terms. For example, if the running time of an
algorithm is
T .n/ D 10n3 (cid:0) 20n2 C 1;
then we can more simply write

T .n/ D ‚.n3 /:
In this case, we would say that T is of order n3 or that T .n/ grows cubically, which
is probably what we really want to know. Another such example is
(cid:25) 23x(cid:0)7 C .2 :7x 113 C x 9 (cid:0) 86/4
(cid:0) 1:083x D ‚.3x /:
p
x

Just knowing that the running time of an algorithm is ‚.n3 /, for example, is use-
ful, because if n doubles we can predict that the running time will by and large14
increase by a factor of at most 8 for large n. In this way, Theta notation preserves in-
formation about the scalability of an algorithm or system. Scalability is, of course,
a big issue in the design of algorithms and systems.

14Since ‚.n3 / only implies that the running time, T .n/, is between cn3 and d n3 for constants
0 < c < d , the time T .2n/ could regularly exceed T .n/ by a factor as large as 8d =c . The factor is
sure to be close to 8 for all large n only if T .n/ (cid:24) n3 .

37“mcs-ftl” — 2010/9/8 — 0:40 — page 280 — #286

Chapter 9

Sums and Asymptotics

9.7.5 Pitfalls with Asymptotic Notation
There is a long list of ways to make mistakes with asymptotic notation. This section
presents some of the ways that Big Oh notation can lead to ruin and despair. With
minimal effort, you can cause just as much chaos with the other symbols.

The Exponential Fiasco
Sometimes relationships involving Big Oh are not so obvious. For example, one
might guess that 4x D O .2x / since 4 is only a constant factor larger than 2. This
reasoning is incorrect, however; 4x actually grows as the square of 2x .

Constant Confusion
Every constant is O .1/. For example, 17 D O .1/. This is true because if we let
f .x / D 17 and g.x / D 1, then there exists a c > 0 and an x0 such that jf .x /j 
cg.x /. In particular, we could choose c = 17 and x0 D 1, since j17j  17 (cid:1) 1 for all
x (cid:21) 1. We can construct a false theorem that exploits this fact.
nX
False Theorem 9.7.16.
i D O .n/
Bogus proof. Deﬁne f .n/ D Pn
i D1
i D1 i D 1 C 2 C 3 C (cid:1) (cid:1) (cid:1) C n. Since we have shown
Of course in reality Pn
that every constant i is O .1/, f .n/ D O .1/ C O .1/ C (cid:1) (cid:1) (cid:1) C O .1/ D O .n/.
(cid:4)
i D1 i D n.n C 1/=2 ¤ O .n/.
The error stems from confusion over what is meant in the statement i D O .1/.
For any constant i 2 N it is true that i D O .1/. More precisely, if f is any constant
function, then f D O .1/. But in this False Theorem, i is not constant—it ranges
over a set of values 0, 1,. . . , n that depends on n.
And anyway, we should not be adding O .1/’s as though they were numbers. We
never even deﬁned what O .g/ means by itself; it should only be used in the context
“f D O .g/” to describe a relation between functions f and g .
Lower Bound Blunder
Sometimes people incorrectly use Big Oh in the context of a lower bound. For
example, they might say, “The running time, T .n/, is at least O .n2 /,” when they
probably mean15 “T .n/ D (cid:127).n2 /.”
15This can also be correctly expressed as n2 D O .T .n//, but such notation is rare.

38“mcs-ftl” — 2010/9/8 — 0:40 — page 281 — #287

9.7. Asymptotic Notation

Equality Blunder
The notation f D O .g/ is too ﬁrmly entrenched to avoid, but the use of “=”
is really regrettable. For example, if f D O .g/, it seems quite reasonable to
write O .g/ D f . But doing so might tempt us to the following blunder: because
2n D O .n/, we can say O .n/ D 2n. But n D O .n/, so we conclude that n D
O .n/ D 2n, and therefore n D 2n. To avoid such nonsense, we will never write
“O .f / D g .”

 1
Similarly, you will often see statements like
Hn D ln.n/ C (cid:13) C O
n
(cid:16) n
n
p
nŠ D .1 C o.1//
e
In such cases, the true meaning is
Hn D ln.n/ C (cid:13) C f .n/
n
(cid:16) n
for some f .n/ where f .n/ D O .1=n/, and
p
nŠ D .1 C g.n//
2(cid:25) n
e
where g.n/ D o.1/. These transgressions are OK as long as you (and you reader)
know what you mean.

2(cid:25) n

or

:

39“mcs-ftl” — 2010/9/8 — 0:40 — page 282 — #288

40MIT OpenCourseWare
http://ocw.mit.edu 

6.042J / 18.062J Mathematics for Computer Science 
Fall 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

