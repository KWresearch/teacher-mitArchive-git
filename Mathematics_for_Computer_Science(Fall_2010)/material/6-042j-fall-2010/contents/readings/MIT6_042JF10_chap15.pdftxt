“mcs-ftl” — 2010/9/8 — 0:40 — page 417 — #423

15

Conditional Probability

15.1 Deﬁnition

Suppose that we pick a random person in the world. Everyone has an equal chance
of being selected. Let A be the event that the person is an MIT student, and let
B be the event that the person lives in Cambridge. What are the probabilities of
these events? Intuitively, we’re picking a random point in the big ellipse shown in
Figure 15.1 and asking how likely that point is to fall into region A or B .

Figure 15.1 Selecting a random person. A is the event that the person is an MIT
student. B is the even that the person lives in Cambridge.

The vast majority of people in the world neither live in Cambridge nor are MIT
students, so events A and B both have low probability. But what about the prob-
ability that a person is an MIT student, given that the person lives in Cambridge?
This should be much greater—but what is it exactly?
What we’re asking for is called a conditional probability; that is, the probability
that one event happens, given that some other event deﬁnitely happens. Questions
about conditional probabilities come up all the time:
(cid:15) What is the probability that it will rain this afternoon, given that it is cloudy
this morning?

set of all people in the worldset of people who live in Cambridgeset of MITstudentsBA1“mcs-ftl” — 2010/9/8 — 0:40 — page 418 — #424

Chapter 15 Conditional Probability
(cid:15) What is the probability that two rolled dice sum to 10, given that both are
odd?
(cid:15) What is the probability that I’ll get four-of-a-kind in Texas No Limit Hold
There is a special notation for conditional probabilities. In general, Pr (cid:2)A j B (cid:3)
’Em Poker, given that I’m initially dealt two queens?
Pr (cid:2)A j B (cid:3) is the probability that a random person is an MIT student, given that he
denotes the probability of event A, given that event B happens. So, in our example,
How do we compute Pr (cid:2)A j B (cid:3)? Since we are given that the person lives in
or she is a Cambridge resident.
outcomes outside event B are irrelevant. So, intuitively, Pr (cid:2)A j B (cid:3) should be the
Cambridge, we can forget about everyone in the world who does not. Thus, all
fraction of Cambridge residents that are also MIT students; that is, the answer
should be the probability that the person is in set A \ B (the darkly shaded region
in Figure 15.1) divided by the probability that the person is in set B (the lightly
shaded region). This motivates the deﬁnition of conditional probability:
Pr (cid:2)A j B (cid:3) WWD PrŒA \ B 
Deﬁnition 15.1.1.
If PrŒB  D 0, then the conditional probability Pr (cid:2)A j B (cid:3) is undeﬁned.
PrŒB 
Pure probability is often counterintuitive, but conditional probability is even
worse! Conditioning can subtly alter probabilities and produce unexpected results
in randomized algorithms and computer systems as well as in betting games. Yet,
the mathematical deﬁnition of conditional probability given above is very simple
and should give you no trouble—provided that you rely on formal reasoning and
not intuition. The four-step method will also be very helpful as we will see in the
next examples.

15.2 Using the Four-Step Method to Determine Conditional
Probability

15.2.1 The “Halting Problem”
The Halting Problem was the ﬁrst example of a property that could not be tested
by any program. It was introduced by Alan Turing in his seminal 1936 paper. The
problem is to determine whether a Turing machine halts on a given . . . yadda yadda

2“mcs-ftl” — 2010/9/8 — 0:40 — page 419 — #425

15.2. Using the Four-Step Method to Determine Conditional Probability

yadda . . . more importantly, it was the name of the MIT EECS department’s famed
C-league hockey team.
In a best-of-three tournament, the Halting Problem wins the ﬁrst game with prob-
ability 1=2. In subsequent games, their probability of winning is determined by the
outcome of the previous game. If the Halting Problem won the previous game,
then they are invigorated by victory and win the current game with probability 2=3.
If they lost the previous game, then they are demoralized by defeat and win the
current game with probability only 1=3. What is the probability that the Halting
Problem wins the tournament, given that they win the ﬁrst game?
game. Our goal is then to determine the conditional probability Pr (cid:2)A j B (cid:3).
This is a question about a conditional probability. Let A be the event that the
Halting Problem wins the tournament, and let B be the event that they win the ﬁrst
We can tackle conditional probability questions just like ordinary probability
problems: using a tree diagram and the four step method. A complete tree diagram
is shown in Figure 15.2.

Figure 15.2 The tree diagram for computing the probability that the “Halting
Problem” wins two out of three games given that they won the ﬁrst game.

Step 1: Find the Sample Space
Each internal vertex in the tree diagram has two children, one corresponding to
a win for the Halting Problem (labeled W ) and one corresponding to a loss (la-

WWWLLLWLWL1=21=22=31=32=31=31=32=31=32=3WWWLWWLLLWWLWLLL1=31=181=91=91=181=3game 1game 2game 3outcomeevent A:win theseriesevent B:wingame 1outcomeprobability3“mcs-ftl” — 2010/9/8 — 0:40 — page 420 — #426

Chapter 15 Conditional Probability

beled L). The complete sample space is:
S D fW W ; W LW ; W LL; LW W ; LW L; LLg:
Step 2: Deﬁne Events of Interest
The event that the Halting Problem wins the whole tournament is:
T D fW W ; W LW ; LW W g:
And the event that the Halting Problem wins the ﬁrst game is:
F D fW W ; W LW ; W LLg:
The outcomes in these events are indicated with check marks in the tree diagram in
Figure 15.2.

Step 3: Determine Outcome Probabilities
Next, we must assign a probability to each outcome. We begin by labeling edges
as speciﬁed in the problem statement. Speciﬁcally, The Halting Problem has a 1=2
chance of winning the ﬁrst game, so the two edges leaving the root are each as-
signed probability 1=2. Other edges are labeled 1=3 or 2=3 based on the outcome
of the preceding game. We then ﬁnd the probability of each outcome by multi-
plying all probabilities along the corresponding root-to-leaf path. For example, the
probability of outcome W LL is:

1

(cid:1) 2
3

:

D 1
9

(cid:1) 1
3
2
Step 4: Compute Event Probabilities
We can now compute the probability that The Halting Problem wins the tourna-
Pr (cid:2)A j B (cid:3) D PrŒA \ B 
ment, given that they win the ﬁrst game:
PrŒB 
PrŒfW W ; W LW g
D
PrŒfW W ; W LW ; W LLg
1=3 C 1=18
1=3 C 1=18 C 1=9
D 7
9
We’re done! If the Halting Problem wins the ﬁrst game, then they win the whole
tournament with probability 7=9.

D

:

4“mcs-ftl” — 2010/9/8 — 0:40 — page 421 — #427

15.2. Using the Four-Step Method to Determine Conditional Probability

15.2.2 Why Tree Diagrams Work
We’ve now settled into a routine of solving probability problems using tree dia-
grams. But we’ve left a big question unaddressed: what is the mathematical justiﬁ-
cation behind those funny little pictures? Why do they work?
The answer involves conditional probabilities.
In fact, the probabilities that
we’ve been recording on the edges of tree diagrams are conditional probabilities.
For example, consider the uppermost path in the tree diagram for the Halting Prob-
lem, which corresponds to the outcome W W . The ﬁrst edge is labeled 1=2, which
is the probability that the Halting Problem wins the ﬁrst game. The second edge
is labeled 2=3, which is the probability that the Halting Problem wins the second
game, given that they won the ﬁrst—that’s a conditional probability! More gener-
ally, on each edge of a tree diagram, we record the probability that the experiment
proceeds along that path, given that it reaches the parent vertex.
So we’ve been using conditional probabilities all along. But why can we multiply
edge probabilities to get outcome probabilities? For example, we concluded that:
PrŒW W  D 1
D 1
(cid:1) 2
2
3
3

:

Why is this correct?
The answer goes back to Deﬁnition 15.1.1 of conditional probability which could
be written in a form called the Product Rule for probabilities:
PrŒE1 \ E2  D PrŒE1  (cid:1) Pr (cid:2)E2 j E1
(cid:3) :
Rule (Product Rule for 2 Events). If PrŒE1  ¤ 0, then:
Multiplying edge probabilities in a tree diagram amounts to evaluating the right
side of this equation. For example:
D PrŒwin ﬁrst game (cid:1) Pr (cid:2)win second game j win ﬁrst game(cid:3)
PrŒwin ﬁrst game \ win second game
(cid:1) 2
D 1
:
2
3
So the Product Rule is the formal justiﬁcation for multiplying edge probabilities to
get outcome probabilities! Of course to justify multiplying edge probabilities along
longer paths, we need a Product Rule for n events.
(cid:3) (cid:1) Pr (cid:2)E3 j E1 \ E2
PrŒE1 \ E2 \ : : : \ En  D PrŒE1  (cid:1) Pr (cid:2)E2 j E1
(cid:3) (cid:1) (cid:1) (cid:1)
Rule (Product Rule for n Events).
(cid:1) Pr (cid:2)En j E1 \ E2 \ : : : \ En(cid:0)1
(cid:3)

5“mcs-ftl” — 2010/9/8 — 0:40 — page 422 — #428

Chapter 15 Conditional Probability

provided that

PrŒE1 \ E2 \ (cid:1) (cid:1) (cid:1) \ En(cid:0)1  ¤ 0:
This rule follows from the deﬁnition of conditional probability and induction
on n.

15.2.3 Medical Testing
There is an unpleasant condition called BO suffered by 10% of the population.
There are no prior symptoms; victims just suddenly start to stink. Fortunately,
there is a test for latent BO before things start to smell. The test is not perfect,
however:
(cid:15) If you have the condition, there is a 10% chance that the test will say you do
not. These are called “false negatives”.
(cid:15) If you do not have the condition, there is a 30% chance that the test will say
you do. These are “false positives”.
Suppose a random person is tested for latent BO. If the test is positive, then what
is the probability that the person has the condition?

Step 1: Find the Sample Space
The sample space is found with the tree diagram in Figure 15.3.

Figure 15.3 The tree diagram for the BO problem.

yesposnonegposneg0:90:10:90:10:30:70:090:010:270:63personhas BOtest resultevent A:has BOevent B:tests positiveoutcomeprobabilityevent A\B6“mcs-ftl” — 2010/9/8 — 0:40 — page 423 — #429

15.2. Using the Four-Step Method to Determine Conditional Probability

Step 2: Deﬁne Events of Interest
to ﬁnd Pr (cid:2)A j B (cid:3), the probability that a person has BO, given that the test was
Let A be the event that the person has BO. Let B be the event that the test was
positive. The outcomes in each event are marked in the tree diagram. We want
positive.
Step 3: Find Outcome Probabilities
First, we assign probabilities to edges. These probabilities are drawn directly from
the problem statement. By the Product Rule, the probability of an outcome is the
product of the probabilities on the corresponding root-to-leaf path. All probabilities
are shown in Figure 15.3.
Step 4: Compute Event Probabilities
Pr (cid:2)A j B (cid:3) D PrŒA \ B 
From Deﬁnition 15.1.1, we have
0:09
0:09 C 0:27
PrŒB 
So, if you test positive, then there is only a 25% chance that you have the condition!
This answer is initially surprising, but makes sense on reﬂection. There are two
ways you could test positive. First, it could be that you have the condition and the
test is correct. Second, it could be that you are healthy and the test is incorrect. The
problem is that almost everyone is healthy; therefore, most of the positive results
arise from incorrect tests of healthy people!
We can also compute the probability that the test is correct for a random person.
This event consists of two outcomes. The person could have the condition and
test positive (probability 0:09), or the person could be healthy and test negative
(probability 0:63). Therefore, the test is correct with probability 0:09 C 0:63 D
0:72. This is a relief; the test is correct almost three-quarters of the time.
But wait! There is a simple way to make the test correct 90% of the time: always
return a negative result! This “test” gives the right answer for all healthy people
and the wrong answer only for the 10% that actually have the condition. So a better
strategy by this measure is to completely ignore the test result!
There is a similar paradox in weather forecasting. During winter, almost all days
in Boston are wet and overcast. Predicting miserable weather every day may be
more accurate than really trying to get it right!

D 1
4

D

:

7“mcs-ftl” — 2010/9/8 — 0:40 — page 424 — #430

Chapter 15 Conditional Probability

15.3 A Posteriori Probabilities

If you think about it too much, the medical testing problem we just considered
could start to trouble you. The concern would be that by the time you take the test,
you either have the BO condition or you don’t—you just don’t know which it is.
So you may wonder if a statement like “If you tested positive, then you have the
condition with probability 25%” makes sense.
In fact, such a statement does make sense. It means that 25% of the people who
test positive actually have the condition. It is true that any particular person has it
or they don’t, but a randomly selected person among those who test positive will
have the condition with probability 25%.
Anyway, if the medical testing example bothers you, you will deﬁnitely be wor-
ried by the following examples, which go even further down this path.

15.3.1 The “Halting Problem,” in Reverse
Suppose that we turn the hockey question around: what is the probability that the
Halting Problem won their ﬁrst game, given that they won the series?
This seems like an absurd question! After all, if the Halting Problem won the
series, then the winner of the ﬁrst game has already been determined. Therefore,
who won the ﬁrst game is a question of fact, not a question of probability. However,
our mathematical theory of probability contains no notion of one event preceding
another—there is no notion of time at all. Therefore, from a mathematical perspec-
tive, this is a perfectly valid question. And this is also a meaningful question from
a practical perspective. Suppose that you’re told that the Halting Problem won the
series, but not told the results of individual games. Then, from your perspective, it
A conditional probability Pr (cid:2)B j A(cid:3) is called a posteriori if event B precedes
makes perfect sense to wonder how likely it is that The Halting Problem won the
ﬁrst game.
event A in time. Here are some other examples of a posteriori probabilities:
(cid:15) The probability it was cloudy this morning, given that it rained in the after-
noon.
(cid:15) The probability that I was initially dealt two queens in Texas No Limit Hold
’Em poker, given that I eventually got four-of-a-kind.

Mathematically, a posteriori probabilities are no different from ordinary probabil-
ities; the distinction is only at a higher, philosophical level. Our only reason for
drawing attention to them is to say, “Don’t let them rattle you.”

8“mcs-ftl” — 2010/9/8 — 0:40 — page 425 — #431

15.3. A Posteriori Probabilities
won their ﬁrst game, given that they won the series is Pr (cid:2)B j A(cid:3). We can com-
Let’s return to the original problem. The probability that the Halting Problem
pute this using the deﬁnition of conditional probability and the tree diagram in
Pr (cid:2)B j A(cid:3) D PrŒB \ A
Figure 15.2:
1=3 C 1=18
D 7
D
This answer is suspicious! In the preceding section, we showed that Pr (cid:2)A j B (cid:3)
1=3 C 1=18 C 1=9
:
PrŒA
was also 7=9. Could it be true that Pr (cid:2)A j B (cid:3) D Pr (cid:2)B j A(cid:3) in general? Some
9
reﬂection suggests this is unlikely. For example, the probability that I feel uneasy,
Let’s work out the general conditions under which Pr (cid:2)A j B (cid:3) D Pr (cid:2)B j A(cid:3).
given that I was abducted by aliens, is pretty large. But the probability that I was
abducted by aliens, given that I feel uneasy, is rather small.
By the deﬁnition of conditional probability, this equation holds if an only if:
PrŒA \ B 
D PrŒA \ B 
PrŒB 
PrŒA

or

This equation, in turn, holds only if the denominators are equal or the numerator
is 0; namely if
PrŒA \ B  D 0:
PrŒB  D PrŒA
The former condition holds in the hockey example; the probability that the Halting
Problem wins the series (event A) is equal to the probability that it wins the ﬁrst
game (event B ) since both probabilities are 1=2.
In general, such pairs of probabilities are related by Bayes’ Rule:
Pr (cid:2)B j A(cid:3) D Pr (cid:2)A j B (cid:3) (cid:1) PrŒB 
Theorem 15.3.1 (Bayes’ Rule). If PrŒA and PrŒB  are nonzero, then:
PrŒA
Pr (cid:2)A j B (cid:3) (cid:1) PrŒB  D PrŒA \ B  D Pr (cid:2)B j A(cid:3) (cid:1) PrŒA
Proof. When PrŒA and PrŒB  are nonzero, we have
by deﬁnition of conditional probability. Dividing by PrŒA gives (15.1).

(15.1)

Next, let’s look at a problem that even bothers us.

(cid:4)

9“mcs-ftl” — 2010/9/8 — 0:40 — page 426 — #432

Chapter 15 Conditional Probability

15.3.2 A Coin Problem
Suppose that someone hands you either a fair coin or a trick coin with heads on
both sides. You ﬂip the coin 100 times and see heads every time. What can you say
about the probability that you ﬂipped the fair coin? Remarkably, nothing!
In order to make sense out of this outrageous claim, let’s formalize the problem.
The sample space is worked out in the tree diagram shown in Figure 15.4. We
do not know the probability p that you were handed the fair coin initially—you
were just given one coin or the other. Let A be the event that you were handed the

Figure 15.4 The tree diagram for the coin-ﬂipping problem.
ing for Pr (cid:2)A j B (cid:3), the probability that you were handed the fair coin, given that
fair coin, and let B be the event that you ﬂipped 100 straight heads. We’re look-
you ﬂipped 100 heads. The outcome probabilities are worked out in Figure 15.4.
Pr (cid:2)A j B (cid:3) D PrŒA \ B 
Plugging the results into the deﬁnition of conditional probability gives:
PrŒB 
D
p=2100
1 (cid:0) p C p=2100
D
p
2100 .1 (cid:0) p / C p
This expression is very small for moderate values of p because of the 2100 term
in the denominator. For example, if p D 1=2, then the probability that you were
given the fair coin is essentially zero.

:

fair coinall headsall headstrick coinsome tailsp1�p1=210011001�1=2100p=2100p�p=21001�pcoin givento youresult of100 flipsevent A:given faircoinevent B:flipped allheadsprobability10“mcs-ftl” — 2010/9/8 — 0:40 — page 427 — #433

15.4. Conditional Identities

But we do not know the probability p that you were given the fair coin. And
(cid:0)100 . Then there
perhaps the value of p is not moderate; in fact, maybe p D 1 (cid:0) 2
is nearly an even chance that you have the fair coin, given that you ﬂipped 100
heads. In fact, maybe you were handed the fair coin with probability p D 1. Then
the probability that you were given the fair coin is, well, 1!
Of course, it is extremely unlikely that you would ﬂip 100 straight heads, but in
this case, that is a given from the assumption of the conditional probability. And so
if you really did see 100 straight heads, it would be very tempting to also assume
that p is not close to 1 and hence that you are very likely to have ﬂipped the trick
coin.
We will encounter a very similar issue when we look at methods for estimation
by sampling in Section 17.5.5.

15.4 Conditional Identities

15.4.1 The Law of Total Probability
Breaking a probability calculation into cases simpliﬁes many problems. The idea
is to calculate the probability of an event A by splitting into two cases based on
whether or not another event E occurs. That is, calculate the probability of A \ E
and A \ E . By the Sum Rule, the sum of these probabilities equals PrŒA. Express-
ing the intersection probabilities as conditional probabilities yields:
ˇˇ E (cid:3) (cid:1) PrŒE :
PrŒA D Pr (cid:2)A j E (cid:3) (cid:1) PrŒE  C Pr (cid:2)A
Rule 15.4.1 (Law of Total Probability, single event). If PrŒE  and PrŒE  are nonzero,
then
For example, suppose we conduct the following experiment. First, we ﬂip a fair
coin. If heads comes up, then we roll one die and take the result. If tails comes up,
then we roll two dice and take the sum of the two results. What is the probability
that this process yields a 2? Let E be the event that the coin comes up heads,
a 2 on a single die with probability Pr (cid:2)A j E (cid:3) D 1=6. On the other hand, if we
and let A be the event that we get a 2 overall. Assuming that the coin is fair,
PrŒE  D PrŒE  D 1=2. There are now two cases. If we ﬂip heads, then we roll
ˇˇ E (cid:3) D 1=36.
ﬂip tails, then we get a sum of 2 on two dice with probability Pr (cid:2)A
Therefore, the probability that the whole process yields a 2 is
D 7
(cid:1) 1
C 1
(cid:1) 1
PrŒA D 1
2
6
2
36
72
There is also a form of the rule to handle more than two cases.

:

11“mcs-ftl” — 2010/9/8 — 0:40 — page 428 — #434

Chapter 15 Conditional Probability

Rule 15.4.2 (Law of Total Probability). If E1 ; : : : ; En are disjoint events whose
PrŒA D nX
(cid:3) (cid:1) PrŒEi :
Pr (cid:2)A j Ei
union is the whole sample space, then:
i D1
15.4.2 Conditioning on a Single Event
The probability rules that we derived in Chapter 14 extend to probabilities condi-
tioned on the same event. For example, the Inclusion-Exclusion formula for two
Pr (cid:2)A [ B j C (cid:3) D Pr (cid:2)A j C (cid:3) C Pr (cid:2)B j C (cid:3) (cid:0) Pr (cid:2)A \ B j C (cid:3) :
sets holds when all probabilities are conditioned on an event C :
This follows from the fact that if PrŒC  ¤ 0, then
Pr (cid:2)A [ B j C (cid:3) D PrŒ.A [ B / \ C 
PrŒC 
D PrŒ.A \ C / [ .B \ C /
PrŒC 
D PrŒA \ C  C PrŒB \ C  (cid:0) PrŒA \ B \ C 
D Pr (cid:2)A j C (cid:3) C Pr (cid:2)B j C (cid:3) (cid:0) Pr (cid:2)A \ B j C (cid:3) :
PrŒC 
It is important not to mix up events before and after the conditioning bar. For
example, the following is not a valid identity:
Pr (cid:2)A j B [ C (cid:3) D Pr (cid:2)A j B (cid:3) C Pr (cid:2)A j C (cid:3) (cid:0) Pr (cid:2)A j B \ C (cid:3) :
False Claim.
A counterexample is shown in Figure 15.5. In this case, Pr (cid:2)A j B (cid:3) D 1=2,
(15.2)
Pr (cid:2)A j C (cid:3) D 1=2, Pr (cid:2)A j B \ C (cid:3) D 1, and Pr (cid:2)A j B [ C (cid:3) D 1=3. However,
since 1=3 ¤ 1=2 C 1=2 (cid:0) 1, Equation 15.2 does not hold.
So you’re convinced that this equation is false in general, right? Let’s see if you
really believe that.

15.4.3 Discrimination Lawsuit
Several years ago there was a sex discrimination lawsuit against a famous uni-
versity. A female math professor was denied tenure, allegedly because she was

12“mcs-ftl” — 2010/9/8 — 0:40 — page 429 — #435

15.4. Conditional Identities

Figure 15.5 A counterexample to Equation 15.2. Event A is the gray rectangle,
event B is the rectangle with vertical stripes, and event C is the rectangle with
horizontal stripes. B \ C lies entirely within A while B (cid:0) C and C (cid:0) B are entirely
outside of A.

a woman. She argued that in every one of the university’s 22 departments, the
percentage of male applicants accepted was greater than the percentage of female
applicants accepted. This sounds very suspicious!
However, the university’s lawyers argued that across the university as a whole,
the percentage of male applicants accepted was actually lower than the percentage
of female applicants accepted. This suggests that if there was any sex discrimi-
nation, then it was against men! Surely, at least one party in the dispute must be
lying.
Let’s simplify the problem and express both arguments in terms of conditional
probabilities. To simplify matters, suppose that there are only two departments, EE
and CS, and consider the experiment where we pick a random applicant. Deﬁne
the following events:
(cid:15) Let A be the event that the applicant is accepted.
(cid:15) Let FEE the event that the applicant is a female applying to EE.
(cid:15) Let FCS the event that the applicant is a female applying to CS.
(cid:15) Let MEE the event that the applicant is a male applying to EE.
(cid:15) Let MCS the event that the applicant is a male applying to CS.
Assume that all applicants are either male or female, and that no applicant applied
to both departments. That is, the events FEE , FCS , MEE , and MCS are all dis-
joint.

sample spaceBACŠŠ13“mcs-ftl” — 2010/9/8 — 0:40 — page 430 — #436

Chapter 15 Conditional Probability

CS

EE

and

Overall

0%
0 females accepted, 1 applied
50%
50 males accepted, 100 applied
70%
70 females accepted, 100 applied
100%
1 male accepted, 1 applied
70 females accepted, 101 applied (cid:25) 70%
51 males accepted, 101 applied (cid:25) 51%
Table 15.1 A scenario where females are less likely to be admitted than males in
each department, but more likely to be admitted overall.
Pr (cid:2)A j FEE
(cid:3)
(cid:3) < Pr (cid:2)A j MEE
In these terms, the plaintiff is making the following argument:
Pr (cid:2)A j FCS
(cid:3) < Pr (cid:2)A j MCS
(cid:3) :
That is, in both departments, the probability that a woman is accepted for tenure is
less than the probability that a man is accepted. The university retorts that overall,
Pr (cid:2)A j FEE [ FCS
(cid:3) :
(cid:3) > Pr (cid:2)A j MEE [ MCS
a woman applicant is more likely to be accepted than a man; namely that
It is easy to believe that these two positions are contradictory. In fact, we might
even try to prove this by adding the plaintiff ’s two inequalities and then arguing as
Pr (cid:2)A j FEE
(cid:3)
(cid:3) C Pr (cid:2)A j MCS
(cid:3) < Pr (cid:2)A j MEE
(cid:3) C Pr (cid:2)A j FCS
follows:
Pr (cid:2)A j FEE [ FCS
(cid:3) < Pr (cid:2)A j MEE [ MCS
(cid:3) :
The second line exactly contradicts the university’s position! But there is a big
problem with this argument; the second inequality follows from the ﬁrst only if we
accept the false identity (15.2). This argument is bogus! Maybe the two parties do
not hold contradictory positions after all!
In fact, Table 15.1 shows a set of application statistics for which the assertions of
both the plaintiff and the university hold. In this case, a higher percentage of males
were accepted in both departments, but overall a higher percentage of females were
accepted! Bizarre!

)

14MIT OpenCourseWare
http://ocw.mit.edu 

6.042J / 18.062J Mathematics for Computer Science 
Fall 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

