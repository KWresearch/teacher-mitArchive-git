“mcs-ftl” — 2010/9/8 — 0:40 — page 445 — #451

17

Random Variables and Distributions
Thus far, we have focused on probabilities of events. For example, we computed
the probability that you win the Monty Hall game, or that you have a rare medical
condition given that you tested positive. But, in many cases we would like to more
more. For example, how many contestants must play the Monty Hall game until
one of them ﬁnally wins? How long will this condition last? How much will I lose
gambling with strange dice all night? To answer such questions, we need to work
with random variables.

17.1 Deﬁnitions and Examples

Deﬁnition 17.1.1. A random variable R on a probability space is a total function
whose domain is the sample space.

The codomain of R can be anything, but will usually be a subset of the real
numbers. Notice that the name “random variable” is a misnomer; random variables
are actually functions!
For example, suppose we toss three independent1 , unbiased coins. Let C be the
number of heads that appear. Let M D 1 if the three coins come up all heads or
all tails, and let M D 0 otherwise. Every outcome of the three coin ﬂips uniquely
determines the values of C and M . For example, if we ﬂip heads, tails, heads, then
C D 2 and M D 0. If we ﬂip tails, tails, tails, then C D 0 and M D 1. In effect,
C counts the number of heads, and M indicates whether all the coins match.
Since each outcome uniquely determines C and M , we can regard them as func-
tions mapping outcomes to numbers. For this experiment, the sample space is
S D fHHH ; HH T ; H TH ; H T T ; THH ; TH T ; T TH ; T T T g

and C is a function that maps each outcome in the sample space to a number as

1Going forward, when we talk about ﬂipping independent coins, we will assume that they are
mutually independent.

1“mcs-ftl” — 2010/9/8 — 0:40 — page 446 — #452

Chapter 17 Random Variables and Distributions

follows:

C .THH / D 2
C .HHH / D 3
C .TH T / D 1
C .HH T / D 2
C .T TH / D 1
C .H TH / D 2
C .T T T / D 0:
C .H T T / D 1
Similarly, M is a function mapping each outcome another way:
M .THH / D 0
M .HHH / D 1
M .HH T / D 0
M .TH T / D 0
M .H TH / D 0
M .T TH / D 0
M .T T T / D 1:
M .H T T / D 0
So C and M are random variables.

17.1.1
Indicator Random Variables
An indicator random variable is a random variable that maps every outcome to
either 0 or 1. Indicator random variables are also called Bernoulli variables. The
random variable M is an example. If all three coins match, then M D 1; otherwise,
M D 0.
Indicator random variables are closely related to events.
In particular, an in-
dicator random variable partitions the sample space into those outcomes mapped
to 1 and those outcomes mapped to 0. For example, the indicator M partitions the
„
ƒ‚
…
„
ƒ‚
…
sample space into two blocks as follows:
HHH T T T
HH T H TH H T T
THH TH T
T TH
M D 0
M D 1
In the same way, an event E partitions the sample space into those outcomes in E
and those not in E . So E is naturally associated with an indicator random variable,
IE , where IE .w / D 1 for outcomes w 2 E and IE .w / D 0 for outcomes w … E .
Thus, M D IE where E is the event that all three coins match.
17.1.2 Random Variables and Events
There is a strong relationship between events and more general random variables
as well. A random variable that takes on several values partitions the sample space
„
HHH„ ƒ‚ …
„
T T T„ƒ‚…
ƒ‚
…
ƒ‚
…
into several blocks. For example, C partitions the sample space as follows:
T TH TH T H T T
THH H TH HH T
C D 0
C D 1
C D 2
C D 3

:

:

2“mcs-ftl” — 2010/9/8 — 0:40 — page 447 — #453

17.1. Deﬁnitions and Examples

Each block is a subset of the sample space and is therefore an event. Thus, we
can regard an equation or inequality involving a random variable as an event. For
example, the event that C D 2 consists of the outcomes THH , H TH , and HH T .
The event C  1 consists of the outcomes T T T , T TH , TH T , and H T T .
Naturally enough, we can talk about the probability of events deﬁned by proper-
ties of random variables. For example,
PrŒC D 2 D PrŒTHH  C PrŒH TH  C PrŒHH T 
D 1
C 1
C 1
8
8
8
D 3
8

:

:

As another example:
PrŒM D 1 D PrŒT T T  C PrŒHHH 
D 1
C 1
8
8
D 1
4
17.1.3 Functions of Random Variables
Random variables can be combined to form other random variables. For exam-
ple, suppose that you roll two unbiased, independent 6-sided dice. Let Di be the
random variable denoting the outcome of the i th die for i D 1, 2. For example,
PrŒD1 D 3 D 1=6:
Then let T D D1 C D2 . T is also a random variable and it denotes the sum of the
two dice. For example,
PrŒT D 2 D 1=36
PrŒT D 7 D 1=6:
Random variables can be combined in complicated ways, as we will see in Chap-
ter 19. For example,
Y D eT
is also a random variable. In this case,
PrŒY D e2  D 1=36

and

and

3“mcs-ftl” — 2010/9/8 — 0:40 — page 448 — #454

Chapter 17 Random Variables and Distributions
PrŒY D e7  D 1=6:
17.1.4 Conditional Probability
new difﬁculties. For example, Pr (cid:2)C (cid:21) 2 j M D 0(cid:3) is the probability that at least
Mixing conditional probabilities and events involving random variables creates no
two coins are heads (C (cid:21) 2) given that not all three coins are the same (M D 0).
Pr (cid:2)C (cid:21) 2 j M D 0(cid:3) D PrŒC (cid:21) 2 \ M D 0
We can compute this probability using the deﬁnition of conditional probability:
PrŒM D 0
PrŒfTHH ; H TH ; HH T g
D
PrŒfTHH ; H TH ; HH T ; H T T ; TH T ; T TH g
D 3=8
6=8
D 1
:
2
The expression C (cid:21) 2 \ M D 0 on the ﬁrst line may look odd; what is the set
operation \ doing between an inequality and an equality? But recall that, in this
context, C (cid:21) 2 and M D 0 are events, and so they are sets of outcomes.
17.1.5
Independence
The notion of independence carries over from events to random variables as well.
Random variables R1 and R2 are independent iff for all x1 in the codomain of R1 ,
(cid:3) D PrŒR1 D x1 :
Pr (cid:2)R1 D x1 j R2 D x2
and x2 in the codomain of R2 for which PrŒR2 D X2  > 0, we have:
As with events, we can formulate independence for random variables in an equiva-
lent and perhaps more useful way: random variables R1 and R2 are independent if
for all x1 and x2
PrŒR1 D x1 \ R2 D x2  D PrŒR1 D x1  (cid:1) PrŒR2 D x2 :
For example, are C and M independent? Intuitively, the answer should be “no”.
The number of heads, C , completely determines whether all three coins match; that
is, whether M D 1. But, to verify this intuition, we must ﬁnd some x1 ; x2 2 R
such that:

PrŒC D x1 \ M D x2  ¤ PrŒC D x1  (cid:1) PrŒM D x2 :

4“mcs-ftl” — 2010/9/8 — 0:40 — page 449 — #455

17.1. Deﬁnitions and Examples
One appropriate choice of values is x1 D 2 and x2 D 1. In this case, we have:
PrŒC D 2 \ M D 1 D 0

and

¤ 0:
(cid:1) 3
PrŒM D 1 (cid:1) PrŒC D 2 D 1
8
4
The ﬁrst probability is zero because we never have exactly two heads (C D 2)
when all three coins match (M D 1). The other two probabilities were computed
earlier.
On the other hand, let F be the indicator variable for the event that the ﬁrst ﬂip
is a Head, so
“F D 1” D fHHH ; H TH ; HH T ; H T T g:
PrŒM D 1 D 1=4 D Pr (cid:2)M D 1 j F D 1(cid:3) D Pr (cid:2)M D 1 j F D 0(cid:3)
Then F is independent of M , since
PrŒM D 0 D 3=4 D Pr (cid:2)M D 0 j F D 1(cid:3) D Pr (cid:2)M D 0 j F D 0(cid:3) :
This example is an instance of a simple lemma:
Lemma 17.1.2. Two events are independent iff their indicator variables are inde-
pendent.

and

As with events, the notion of independence generalizes to more than two random
variables.
Deﬁnition 17.1.3. Random variables R1 ; R2 ; : : : ; Rn are mutually independent iff
PrŒR1 D x1 \ R2 D x2 \ (cid:1) (cid:1) (cid:1) \ Rn D xn 
D PrŒR1 D x1  (cid:1) PrŒR2 D x2  (cid:1) (cid:1) (cid:1) PrŒRn D xn :
for all x1 ; x2 ; : : : ; xn .
A consequence of Deﬁnition 17.1.3 is that the probability that any subset of the
variables takes a particular set of values is equal to the product of the probabilities
that the individual variables take their values. Thus, for example, if R1 ; R2 ; : : : ; R100
are mutually independent random variables, then it follows that:
PrŒR1 D 7 \ R7 D 9:1 \ R23 D (cid:25) 
D PrŒR1 D 7 (cid:1) PrŒR7 D 9:1 (cid:1) PrŒR23 D (cid:25) :
The proof is based on summing over all possible values for all of the other random
variables.

5“mcs-ftl” — 2010/9/8 — 0:40 — page 450 — #456

Chapter 17 Random Variables and Distributions

17.2 Distribution Functions

A random variable maps outcomes to values. Often, random variables that show up
for different spaces of outcomes wind up behaving in much the same way because
they have the same probability of having any given value. Hence, random variables
on different probability spaces may wind up having the same probability density
function.

Deﬁnition 17.2.1. Let R be a random variable with codomain V . The probability
(
density function (pdf) of R is a function PDFR W V ! Œ0; 1 deﬁned by:
if x 2 range.R/
PrŒR D x 
PDFR .x / WWD
if x … range.R/:
0
A consequence of this deﬁnition is thatX
PDFR .x / D 1:
x2range.R/

This is because R has a value for each outcome, so summing the probabilities over
all outcomes is the same as summing over the probabilities of each value in the
range of R.
As an example, suppose that you roll two unbiased, independent, 6-sided dice.
Let T be the random variable that equals the sum of the two rolls. This random
variable takes on values in the set V D f2 ; 3; : : : ; 12g. A plot of the probability
density function for T is shown in Figure 17.1: The lump in the middle indicates
that sums close to 7 are the most likely. The total area of all the rectangles is 1
since the dice must take on exactly one of the sums in V D f2 ; 3; : : : ; 12g.
A closely-related concept to a PDF is the cumulative distribution function (cdf)
for a random variable whose codomain is the real numbers. This is a function
CDFR W R ! Œ0; 1 deﬁned by:
CDFR .x / D PrŒR  x :
As an example, the cumulative distribution function for the random variable T
is shown in Figure 17.2: The height of the i th bar in the cumulative distribution
function is equal to the sum of the heights of the leftmost i bars in the probability

6“mcs-ftl” — 2010/9/8 — 0:40 — page 451 — #457

17.2. Distribution Functions

Figure 17.1 The probability density function for the sum of two 6-sided dice.

Figure 17.2 The cumulative distribution function for the sum of two 6-sided dice.

3=366=36x2V23456789101112PDFT.x/01=21x2V0123456789101112:::CDFT.x/7“mcs-ftl” — 2010/9/8 — 0:40 — page 452 — #458

Chapter 17 Random Variables and Distributions
density function. This follows from the deﬁnitions of pdf and cdf:
D X
CDFR .x / D PrŒR  x 
PrŒR D y 
D X
yx
PDFR .y /:
yx
In summary, PDFR .x / measures the probability that R D x and CDFR .x /
measures the probability that R  x . Both PDFR and CDFR capture the same
information about the random variable R—you can derive one from the other—but
sometimes one is more convenient.
One of the really interesting things about density functions and distribution func-
tions is that many random variables turn out to have the same pdf and cdf. In other
words, even though R and S are different random variables on different probability
spaces, it is often the case that

PDFR D PDFs :
In fact, some pdfs are so common that they are given special names. For exam-
ple, the three most important distributions in computer science are the Bernoulli
distribution, the uniform distribution, and the binomial distribution. We look more
closely at these common distributions in the next several sections.

17.3 Bernoulli Distributions

The Bernoulli distribution is the simplest and most common distribution func-
tion. That’s because it is the distribution function for an indicator random vari-
able. Speciﬁcally, the Bernoulli distribution has a probability density function of
the form fp W f0; 1g ! Œ0; 1 where
fp .0/ D p;
and
fp .1/ D 1 (cid:0) p;
for some p 2 Œ0; 1. The corresponding cumulative distribution function is Fp W
8ˆ<ˆ:0
R ! Œ0; 1 where:
if x < 0
p if 0  x < 1
if 1  x :
1

Fp .x / D

8“mcs-ftl” — 2010/9/8 — 0:40 — page 453 — #459

17.4. Uniform Distributions

17.4 Uniform Distributions

17.4.1 Deﬁnition
A random variable that takes on each possible value with the same probability is
said to be uniform. If the sample space is f1; 2 ; : : : ; ng, then the uniform distribu-
tion has a pdf of the form

where

Fn .x / D

fn W f1; 2 ; : : : ; ng ! Œ0; 1
fn .k / D 1
n
8ˆ<ˆ:0
for some n 2 NC . The cumulative distribution function is then Fn W R ! Œ0; 1
where
if x < 1
k=n if k  x < k C 1 for 1  k < n
if n  x :
1
Uniform distributions arise frequently in practice. For example, the number rolled
on a fair die is uniform on the set f1; 2 ; : : : ; 6g. If p D 1=2, then an indicator
random variable is uniform on the set f0; 1g.
17.4.2 The Numbers Game
Enough deﬁnitions—let’s play a game! I have two envelopes. Each contains an in-
teger in the range 0; 1; : : : ; 100, and the numbers are distinct. To win the game, you
must determine which envelope contains the larger number. To give you a ﬁghting
chance, we’ll let you peek at the number in one envelope selected at random. Can
you devise a strategy that gives you a better than 50% chance of winning?
For example, you could just pick an envelope at random and guess that it contains
the larger number. But this strategy wins only 50% of the time. Your challenge is
to do better.
So you might try to be more clever. Suppose you peek in one envelope and see
the number 12. Since 12 is a small number, you might guess that that the number in
the other envelope is larger. But perhaps we’ve been tricky and put small numbers
in both envelopes. Then your guess might not be so good!
An important point here is that the numbers in the envelopes may not be random.
We’re picking the numbers and we’re choosing them in a way that we think will
defeat your guessing strategy. We’ll only use randomization to choose the numbers
if that serves our purpose, which is to make you lose!

9“mcs-ftl” — 2010/9/8 — 0:40 — page 454 — #460

Chapter 17 Random Variables and Distributions

Intuition Behind the Winning Strategy
Amazingly, there is a strategy that wins more than 50% of the time, regardless of
what numbers we put in the envelopes!
Suppose that you somehow knew a number x that was in between the numbers
in the envelopes. Now you peek in one envelope and see a number. If it is bigger
than x , then you know you’re peeking at the higher number. If it is smaller than x ,
then you’re peeking at the lower number. In other words, if you know a number x
between the numbers in the envelopes, then you are certain to win the game.
The only ﬂaw with this brilliant strategy is that you do not know such an x . Oh
well.
But what if you try to guess x ? There is some probability that you guess cor-
rectly. In this case, you win 100% of the time. On the other hand, if you guess
incorrectly, then you’re no worse off than before; your chance of winning is still
50%. Combining these two cases, your overall chance of winning is better than
50%!
Informal arguments about probability, like this one, often sound plausible, but
do not hold up under close scrutiny. In contrast, this argument sounds completely
implausible—but is actually correct!

Analysis of the Winning Strategy
For generality, suppose that we can choose numbers from the set f0; 1; : : : ; ng. Call
the lower number L and the higher number H .
Your goal is to guess a number x between L and H . To avoid confusing equality
(cid:26) 1
(cid:27)
cases, you select x at random from among the half-integers:
; : : : ; n (cid:0) 1
2
2

; 1

; 2

1

2

1

2

But what probability distribution should you use?
The uniform distribution turns out to be your best bet. An informal justiﬁcation
is that if we ﬁgured out that you were unlikely to pick some number—say 50 1
2 —
then we’d always put 50 and 51 in the envelopes. Then you’d be unlikely to pick
an x between L and H and would have less chance of winning.
After you’ve selected the number x , you peek into an envelope and see some
number T . If T > x , then you guess that you’re looking at the larger number.
If T < x , then you guess that the other number is larger.
All that remains is to determine the probability that this strategy succeeds. We
can do this with the usual four step method and a tree diagram.

10“mcs-ftl” — 2010/9/8 — 0:40 — page 455 — #461

17.4. Uniform Distributions

Step 1: Find the sample space.
You either choose x too low (< L), too high (> H ), or just right (L < x < H ).
Then you either peek at the lower number (T D L) or the higher number (T D H ).
This gives a total of six possible outcomes, as show in Figure 17.3.

Figure 17.3 The tree diagram for the numbers game.

Step 2: Deﬁne events of interest.
The four outcomes in the event that you win are marked in the tree diagram.

Step 3: Assign outcome probabilities.
First, we assign edge probabilities. Your guess x is too low with probability L=n,
too high with probability .n (cid:0) H /=n, and just right with probability .H (cid:0) L/=n.
Next, you peek at either the lower or higher number with equal probability. Multi-
plying along root-to-leaf paths gives the outcome probabilities.

choices of xnumberpeeked atTDHTDLTDHTDLTDHTDL1=21=21=21=21=21=2L=n.H�L/=n.n�H/=nresultlosewinwinwinwinloseprobabilityL=2nL=2n.H�L/=2n.H�L/=2n.n�H/=2n.n�H/=2nx too lowx too highx just right11“mcs-ftl” — 2010/9/8 — 0:40 — page 456 — #462

Chapter 17 Random Variables and Distributions

Step 4: Compute event probabilities.
The probability of the event that you win is the sum of the probabilities of the four
outcomes in that event:
C n (cid:0) H
C H (cid:0) L
C H (cid:0) L
PrŒwin D L
C H (cid:0) L
2n
2n
2n
2n
D 1
2
2n
C 1
(cid:21) 1
2
2n
The ﬁnal inequality relies on the fact that the higher number H is at least 1 greater
than the lower number L since they are required to be distinct.
Sure enough, you win with this strategy more than half the time, regardless of
the numbers in the envelopes! For example, if I choose numbers in the range
C 1
D 50:5%. Even
0; 1; : : : ; 100, then you win with probability at least 1
2
200
better, if I’m allowed only numbers in the range 0; : : : ; 10, then your probability of
winning rises to 55%! By Las Vegas standards, those are great odds!

17.4.3 Randomized Algorithms
The best strategy to win the numbers game is an example of a randomized algo-
rithm—it uses random numbers to inﬂuence decisions. Protocols and algorithms
that make use of random numbers are very important in computer science. There
are many problems for which the best known solutions are based on a random num-
ber generator.
For example, the most commonly-used protocol for deciding when to send a
broadcast on a shared bus or Ethernet is a randomized algorithm known as expo-
nential backoff. One of the most commonly-used sorting algorithms used in prac-
tice, called quicksort, uses random numbers. You’ll see many more examples if
you take an algorithms course. In each case, randomness is used to improve the
probability that the algorithm runs quickly or otherwise performs well.

17.5 Binomial Distributions

17.5.1 Deﬁnitions
The third commonly-used distribution in computer science is the binomial distri-
bution. The standard example of a random variable with a binomial distribution is
the number of heads that come up in n independent ﬂips of a coin. If the coin is

12“mcs-ftl” — 2010/9/8 — 0:40 — page 457 — #463

17.5. Binomial Distributions

Figure 17.4 The pdf for the unbiased binomial distribution for n D 20, f20 .k /.

fair, then the number of heads has an unbiased binomial distribution, speciﬁed by
the pdf
fn W f1; 2 ; : : : ; ng ! Œ0; 1
 
!
where
(cid:0)n
fn .k / D
for some n 2 NC . This is because there are (cid:0)n
(cid:1) sequences of n coin tosses with
n
2
k
(cid:0)n .
k
exactly k heads, and each such sequence has probability 2
A plot of f20 .k / is shown in Figure 17.4. The most likely outcome is k D 10
heads, and the probability falls off rapidly for larger and smaller values of k . The
falloff regions to the left and right of the main hump are called the tails of the
distribution. We’ll talk a lot more about these tails shortly.
The cumulative distribution function for the unbiased binomial distribution is
8ˆ<ˆ:0
Fn W R ! Œ0; 1 where
(cid:1)2
(cid:0)n
Pk
i D0
i
1

if x < 1
if k  x < k C 1 for 1  k < n
if n  x :

Fn .x / D

(cid:0)n

f20.k/0:180:160:140:120:100:080:060:040:020k1015205013“mcs-ftl” — 2010/9/8 — 0:40 — page 458 — #464

Chapter 17 Random Variables and Distributions

Figure 17.5 The pdf for the general binomial distribution fn;p .k / for n D 20
and p D :75.

The General Binomial Distribution
If the coins are biased so that each coin is heads with probability p , then the number
of heads has a general binomial density function speciﬁed by the pdf
fn;p W f1; 2 ; : : : ; ng ! Œ0; 1
!
 
where
pk .1 (cid:0) p /n(cid:0)k :
fn;p .k / D
for some n 2 NC and p 2 Œ0; 1. This is because there are (cid:0)n
(cid:1) sequences with
n
k
k heads and n (cid:0) k tails, but now the probability of each such sequence is pk .1 (cid:0)
k
p /n(cid:0)k .
For example, the plot in Figure 17.5 shows the probability density function
fn;p .k / corresponding to ﬂipping n D 20 independent coins that are heads with
probability p D 0:75. The graph shows that we are most likely to get k D 15
heads, as you might expect. Once again, the probability falls off quickly for larger
and smaller values of k .

f20;:75.k/0:250:20:150:10:050k1015205014“mcs-ftl” — 2010/9/8 — 0:40 — page 459 — #465

Fn;p .x / D

17.5. Binomial Distributions
The cumulative distribution function for the general binomial distribution is Fn;p W
8ˆ<ˆ:0
R ! Œ0; 1 where
(cid:1)p i .1 (cid:0) p /n(cid:0)i
(cid:0)n
Pk
if x < 1
if k  x < k C 1 for 1  k < n
i D0
if n  x :
i
1
17.5.2 Approximating the Probability Density Function
Computing the general binomial density function is daunting when k and n are
large. Fortunately, there is an approximate closed-form formula for this function
based on an approximation for the binomial coefﬁcient. In the formula below, k is
 
!
replaced by ˛ n where ˛ is a number between 0 and 1.
p
 
!
˛ n
p
2nH .˛/
2(cid:25) ˛ .1 (cid:0) ˛ /n
<
 1
 C .1 (cid:0) ˛ / log

 1
where H .˛ / is the entropy function2
H .˛ / WWD ˛ log
1 (cid:0) ˛
:
˛
Moreover, if ˛ n > 10 and .1 (cid:0) ˛ /n > 10, then the left and right sides of Equa-
tion 17.2 differ by at most 2%. If ˛ n > 100 and .1 (cid:0) ˛ /n > 100, then the difference
is at most 0:2%.

2nH .˛/
2(cid:25) ˛ .1 (cid:0) ˛ /n

Lemma 17.5.1.

n

(cid:24)

n

˛ n

(17.1)

(17.2)

(17.3)

and

The graph of H is shown in Figure 17.6.
Lemma (17.5.1) provides an excellent approximation for binomial coefﬁcients.
We’ll skip its derivation, which consists of plugging in Theorem 9.6.1 for the fac-
torials in the binomial coefﬁcient and then simplifying.
Now let’s plug Equation 17.2 into the general binomial density function. The
probability of ﬂipping ˛ n heads in n tosses of a coin that comes up heads with

2 log.x / means log2 .x /.

15“mcs-ftl” — 2010/9/8 — 0:40 — page 460 — #466

Chapter 17 Random Variables and Distributions

n

D 2

probability p is:

Figure 17.6 The Entropy Function
p
fn;p .˛ n/ (cid:24) 2nH .˛/p˛ n .1 (cid:0) p /.1(cid:0)˛/n
2(cid:25) ˛ .1 (cid:0) ˛ /n
(cid:16) 1(cid:0)p
(cid:16)
p
˛ /C.1(cid:0)˛/ log
1(cid:0)˛
˛ log. p
2(cid:25) ˛ .1 (cid:0) ˛ /n
where the margin of error in the approximation is the same as in Lemma 17.5.1.
(cid:16)

(cid:16) 1(cid:0)p
From Equation 17.3, we also ﬁnd that
p
˛ /C.1(cid:0)˛/ log
1(cid:0)˛
˛ log. p
2(cid:25) ˛ .1 (cid:0) ˛ /n
The formula in Equations 17.4 and 17.5 is as ugly as a bowling shoe, but it’s
useful because it’s easy to evaluate. For example, suppose we ﬂip a fair coin n
times. What is the probability of getting exactly p n heads? Plugging ˛ D p
into Equation 17.4 gives:
p

fn;p .p n/ (cid:24)

fn;p .˛ n/ <

(17.5)

(17.4)



;

:

n

2

1
2(cid:25)p .1 (cid:0) p /n

:

H.’/10:80:60:40:20’0:40:60:810:2016“mcs-ftl” — 2010/9/8 — 0:40 — page 461 — #467

17.5. Binomial Distributions
Thus, for example, if we ﬂip a fair coin (where p D 1=2) n D 100 times, the
probability of getting exactly 50 heads is within 2% of 0:079, which is about 8%.

17.5.3 Approximating the Cumulative Distribution Function
In many ﬁelds, including computer science, probability analyses come down to get-
ting small bounds on the tails of the binomial distribution. In a typical application,
you want to bound the tails in order to show that there is very small probability that
too many bad things happen. For example, we might like to know that it is very
unlikely that too many bits are corrupted in a message, or that too many servers or
communication links become overloaded, or that a randomized algorithm runs for
too long.
So it is usually good news that the binomial distribution has small tails. To
get a feel for their size, consider the probability of ﬂipping at most 25 heads in
100 independent tosses of a fair coin.
 
!
The probability of getting at most ˛ n heads is given by the binomial cumulative
Fn;p .˛ n/ D ˛ nX
distribution function
n
i D0
We can bound this sum by bounding the ratio of successive terms.
 
!
In particular, for i  ˛ n,
p i (cid:0)1 .1 (cid:0) p /n(cid:0).i (cid:0)1/
 
!
n
i (cid:0) 1
n

p i .1 (cid:0) p /n(cid:0)i :

(17.6)

i

p i .1 (cid:0) p /n(cid:0)i

i

D

nŠp i (cid:0)1 .1 (cid:0) p /n(cid:0)i C1
.i (cid:0) 1/Š.n (cid:0) i C 1/Š
nŠp i .1 (cid:0) p /n(cid:0)i
i Š.n (cid:0) i /Š
D i .1 (cid:0) p /
.n (cid:0) i C 1/p
 ˛ n.1 (cid:0) p /
.n (cid:0) ˛ n C 1/p
 ˛ .1 (cid:0) p /
.1 (cid:0) ˛ /p

:

17“mcs-ftl” — 2010/9/8 — 0:40 — page 462 — #468

Chapter 17 Random Variables and Distributions

(cid:21)i

fn;p .˛ n/:

(17.7)

Fn;p .˛ n/ < fn;p .˛ n/

 ˛ .1 (cid:0) p /
.1 (cid:0) ˛ /p

This means that for ˛ < p ,

1X
i D0
D fn;p .˛ n/
1 (cid:0) ˛ .1 (cid:0) p /

D  1 (cid:0) ˛
.1 (cid:0) ˛ /p
1 (cid:0) ˛=p
In other words, the probability of at most ˛ n heads is at most
1 (cid:0) ˛
1 (cid:0) ˛=p
times the probability of exactly ˛ n heads. For our scenario, where p D 1=2 and
˛ D 1=4,
1 (cid:0) ˛
D 3
D 3=4
1 (cid:0) ˛=p
:
1=2
2
Plugging n D 100, ˛ D 1=4, and p D 1=2 into Equation 17.5, we ﬁnd that the
probability of at most 25 heads in 100 coin ﬂips is
p
4 log.2/C 3
4 log. 2
(cid:1) 2100. 1
3 //
75(cid:25)=2
This says that ﬂipping 25 or fewer heads is extremely unlikely, which is consis-
tent with our earlier claim that the tails of the binomial distribution are very small.
In fact, notice that the probability of ﬂipping 25 or fewer heads is only 50% more
than the probability of ﬂipping exactly 25 heads. Thus, ﬂipping exactly 25 heads is
twice as likely as ﬂipping any number between 0 and 24!

 3 (cid:1) 10

(cid:0)7 :

F100;1=2 .25/ <

3

2

Caveat. The upper bound on Fn;p .˛ n/ in Equation 17.7 holds only if ˛ < p . If
this is not the case in your problem, then try thinking in complementary terms; that
is, look at the number of tails ﬂipped instead of the number of heads. In fact, this
is precisely what we will do in the next example.

17.5.4 Noisy Channels
Suppose you are sending packets of data across a communication channel and that
each packet is lost with probability p D :01. Also suppose that packet losses are
independent. You need to ﬁgure out how much redundancy (or error correction) to

18“mcs-ftl” — 2010/9/8 — 0:40 — page 463 — #469

17.5. Binomial Distributions

build into your communication protocol. Since redundancy is expensive overheard,
you would like to use as little as possible. On the other hand, you never want to be
caught short. Would it be safe for you to assume that in any batch of 10,000 packets,
only 200 (or 2%) are lost? Let’s ﬁnd out.
The noisy channel is analogous to ﬂipping n D 10;000 independent coins, each
with probability p D :01 of coming up heads, and asking for the probability that
there are at least ˛ n heads where ˛ D :02. Since ˛ > p , we cannot use Equa-
tion 17.7. So we need to recast the problem by looking at the numbers of tails. In
this case, the probability of tails is p D :99 and we are asking for the probability
of at most ˛ n tails where ˛ D :98.
Now we can use Equations 17.5 and 17.7 to ﬁnd that the probability of losing 2%
 1 (cid:0) :98
 210000.:98 log. :99
or more of the 10,000 packets is at most
p
:98 /C:02 log. :01
:02 //
1 (cid:0) :98=:99
2(cid:25) .:98/.1 (cid:0) :98/10000
This is good news. It says that planning on at most 2% packet loss in a batch of
10,000 packets should be very safe, at least for the next few millennia.

(cid:0)60 :
< 2

17.5.5 Estimation by Sampling
Sampling is a very common technique for estimating the fraction of elements in
a set that have a certain property. For example, suppose that you would like to
know how many Americans plan to vote for the Republican candidate in the next
presidential election.
It is infeasible to ask every American how they intend to
vote, so pollsters will typically contact n Americans selected at random and then
compute the fraction of those Americans that will vote Republican. This value is
then used as the estimate of the number of all Americans that will vote Republican.
For example, if 45% of the n contacted voters report that they will vote Republican,
the pollster reports that 45% of all Americans will vote Republican. In addition,
the pollster will usually also provide some sort of qualifying statement such as
“There is a 95% probability that the poll is accurate to within ˙4 per-
centage points.”

The qualifying statement is often the source of confusion and misinterpretation.
For example, many people interpret the qualifying statement to mean that there is
a 95% chance that between 41% and 49% of Americans intend to vote Republican.
But this is wrong! The fraction of Americans that intend to vote Republican is a
ﬁxed (and unknown) value p that is not a random variable. Since p is not a random
variable, we cannot say anything about the probability that :41  p  :49.

19“mcs-ftl” — 2010/9/8 — 0:40 — page 464 — #470

Chapter 17 Random Variables and Distributions

(17.8)

To obtain a correct interpretation of the qualifying statement and the results of
the poll, it is helpful to introduce some notation.
Deﬁne Ri to be the indicator random variable for the i th contacted American in
the sample. In particular, set Ri D 1 if the i th contacted American intends to vote
Republican and Ri D 0 otherwise. For the purposes of the analysis, we will assume
that the i th contacted American is selected uniformly at random (with replacement)
from the set of all Americans.3 We will also assume that every contacted person
responds honestly about whether or not they intend to vote Republican and that
there are only two options—each American intends to vote Republican or they
don’t. Thus,
PrŒRi D 1 D p
where p is the (unknown) fraction of Americans that intend to vote Republican.
We next deﬁne
T D R1 C R2 C (cid:1) (cid:1) (cid:1) C Rn
to be the number of contacted Americans who intend to vote Republican. Then
T =n is a random variable that is the estimate of the fraction of Americans that
intend to vote Republican.
We are now ready to provide the correct interpretation of the qualifying state-
Pr (cid:2) jT =n (cid:0) p j  :04(cid:3) (cid:21) :95:
ment. The poll results mean that
In other words, there is a 95% chance that the sample group will produce an esti-
mate that is within ˙4 percentage points of the correct value for the overall popu-
lation. So either we were “unlucky” in selecting the people to poll or the results of
the poll will be correct to within ˙4 points.
How Many People Do We Need to Contact?
There remains an important question: how many people n do we need to contact to
make sure that Equation 17.9 is true? In general, we would like n to be as small as
possible in order to minimize the cost of the poll.
Surprisingly, the answer depends only on the desired accuracy and conﬁdence of
the poll and not on the number of items in the set being sampled. In this case, the
desired accuracy is .04, the desired conﬁdence is .95, and the set being sampled is
the set of Americans. It’s a good thing that n won’t depend on the size of the set
being sampled—there are over 300 million Americans!

(17.9)

3This means that someone could be contacted multiple times.

20“mcs-ftl” — 2010/9/8 — 0:40 — page 465 — #471

17.5. Binomial Distributions

(17.10)

The task of ﬁnding an n that satisﬁes Equation 17.9 is made tractable by observ-
ing that T has a general binomial distribution with parameters n and p and then
applying Equations 17.5 and 17.7. Let’s see how this works.
Since we will be using bounds on the tails of the binomial distribution, we ﬁrst
Pr (cid:2) jT =n (cid:0) p j  :04(cid:3) D 1 (cid:0) Pr (cid:2) jT =n (cid:0) p j > :04(cid:3) :
do the standard conversion
Pr (cid:2) jT =n (cid:0) p j > :04(cid:3) D PrŒT < .p (cid:0) :04/n C PrŒT > .p C :04/n
We then proceed to upper bound
D Fn;p ..p (cid:0) 0:4/n/ C Fn;1(cid:0)p ..1 (cid:0) p (cid:0) :04/n/:
We don’t know the true value of p , but it turns out that the expression on the
Pr (cid:2) jT =n (cid:0) p j > :04(cid:3)  2Fn;1=2 .:46 n/
righthand side of Equation 17.10 is maximized when p D 1=2 and so

 1 (cid:0) :46
1 (cid:0) .:46=:5/
fn;1=2 .:46 n/
< 2
:46 /C:54 log. :5
< 13:5 (cid:1) 2n.:46 log. :5
p
:54 //
2(cid:25) (cid:1) 0:46 (cid:1) 0:54 (cid:1) n
(cid:0):00462n
10:81 (cid:1) 2
p
(17.11)
<
:
n
The second line comes from Equation 17.7 using ˛ D :46. The third line comes
from Equation 17.5.
Equation 17.11 provides bounds on the conﬁdence of the poll for different values
of n. For example, if n D 665, the bound in Equation 17.11 evaluates to :04978 : : : .
Hence, if the pollster contacts 665 Americans, the poll will be accurate to within
˙4 percentage points with at least 95% probability.
Since the bound in Equation 17.11 is exponential in n, the conﬁdence increases
greatly as n increases. For example, if n D 6;650 Americans are contacted, the poll
(cid:0)10 . Of course,
will be accurate to within ˙4 points with probability at least 1 (cid:0) 10
most pollsters are not willing to pay the added cost of polling 10 times as many
people when they already have a conﬁdence level of 95% from polling 665 people.

21“mcs-ftl” — 2010/9/8 — 0:40 — page 466 — #472

22MIT OpenCourseWare
http://ocw.mit.edu 

6.042J / 18.062J Mathematics for Computer Science 
Fall 2010 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 

