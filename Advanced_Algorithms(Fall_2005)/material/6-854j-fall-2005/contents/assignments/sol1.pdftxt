Massachusetts  Institute  of  Technology 
6.854J/18.415J:  Advanced  Algorithms 
David   Karger 

Handout  4  
Wednesday,  September  14,  2005 

Problem  Set  1  Solutions 

Suppose  we  have  a  Fibonacci  heap   that   is  a  single  chain   of  k  − 1  nodes.  The 
Problem  1. 
following  operations  make  a   chain  of  length  k .  Let  min  be  the  current  minimum  of  the 
Fibonacci  heap:  

1.  Insert  items  x1  < x2  < x3  <  min   in  an  arbitrary  order. 

2.  Delete  the  minimum,  which  is  x1 . 
3.  Decrease  the  key   of  x3  to  be  −∞,  i.e.   the   minimum. 
4.  Delete  the  minimum,   which  is  x3  =  −∞. 

The   second  step  is  the   key   one:   it  removes   x1 ,  joins   x2  and  x3  as  a  chain,  and  then  joins  the 
original  chain   with  the  chain  containing  x2  and  x3  (obtaining  a  tree  where  x2  is   the  root, 
with   x3  and  the  original  k − 1­nodes  chain  as  children).   The  third  step  just   removes  x3  from 
the   chain,  and  the  last  step  completely  deletes   it.   The  result  is  that   x2  is   now   the  root  of 
the   original  chain,   so  we  have   constructed   a  chain  of  length  k . For   the  base  case,  just   insert  
a  single  node.  

Thus,  we   obtain  k ­nodes  chain  with  O(k) operations;  therefore, we  can   construct  Ω(n)­nodes 
chain  with  n  operations. 

Note  that  decrease­key  operation  was  essential  for  obtaining  Ω(n)  depth:  without  it,  you  
can  only  obtain  binomial   heaps  (which   have  logarithmic  depth). 

� 
Problem  2. 
For  each  node,  we  store  a  counter   of   how   many  of  its  children  were  removed 
(call  counti  the  counter  of   node  i).   To  analyze  the  running  time  of  the  operations,  we  use 
2 
counti
the  following  potential   function:  φ  = #roots  + 
. 
i
k−1 
The   insert  operation  has  O(1)  amortized  cost.  Note  that  φ  increases   by  1  unit   as  in  the 
case  of  the  original  Fibonacci   heap.   Thus,  the  cost   of  insert  does  not   change. 

The   decrease­key  operation  will  have  a  lower   amortized  cost.  Suppose  there  are  c  cascading  
−2c(k−1)+2  .
cuts.  Then,   the  amortized  cost  of  decrease­key  is   1  +  c  +  Δφ,  with  Δφ  = +c  + 
k−1 
Concluding,   the  cost  of  decrease­key  is   1  +  c  +  c  − 2c  + 
2  = 1  + 
2
.  Note  that   in  the 
k−1 
k−1
original  Fibonacci  heaps,   this  cost  was  1  +  2  =   3. 

Conversely,   the  delete­min  operation   will  have  a  higher  amortized  cost.  The  analysis  is  the 
same  as  in   the  case  of  the  original   Fibonacci  heaps.  Thus,  the  amortized  cost   of  delete­min  
is  bounded  by  the  maximum  degree  of  a  heap  in  our   data  structure. 

2 

Handout  4:  Problem  Set  1  Solutions 

To  analyze  the  maximum  degree,  we  use  arguments   similar   to  those  used  for  the   original 
Fibonacci  heaps.  Let   Sm  be   the  minimum  number  of  nodes  in  a  heap  with  degree  m.  We 
will  try  to  ﬁnd  a  recurrence  formula  for  Sm  and  then  lower  bound   Sm . 
� 
Consider  a  node  with  degree   m.  Then  the  degree  of  its  i’s  child  is  at  least   max{0, i − k}. 
Considering  that   Sm  =  m +  1   for  m  = 0 . . . k − 1,  we  have  that   Sm  =  k +  m  Si−k  for
m ≥ k .  Next,   note  that  Sm  − Sm−1  =  Sm−k .  The  solution  for   this  recurrence   is  Sm  ≥ Ω(λk  ), 
i=k 
m
where   λk  is  the   largest  solution  to   the  characteristic  equation  λk  − λk−1  − 1  = 0  (note  that   in  
the   case  of  k = 2,   the  largest  solution,  λ2 ,  is  the  golden  ratio).   If  M  is   the  highest  possible 
degree  of  a  heap,  then   we  have  that   SM  ≤  n,  meaning  that  M  ≤  O(logλk 
logλ2 
n 
n) =  O( 
).
λk 
logλ2 
log  λ2  (λk
Thus,  our  modiﬁcation   slows  down  the  running  time  of  delete­min  by  a  factor   of 
log  λk 
is  a  decreasing  function  of  k). 

Note:  a  common  mistake  was  to  take  a   potential  function   that  gives  suitable  amortized 
cost  of   one  operation.  Remember  that  if  you   use  a  potential  function,  you   have  to  check   the 
running  time  of  all  operations  using   the  same potential  function. 

Another  common  mistake  was  to   use  for  the  analysis   the  same  potential  function  as   was   used 
for  the  original   Fibonacci   heaps.   That  function   does  not   give  you  a  lower  amortized  cost   for  
decrease­key  (consider  the  case  when  there  are  no  cuts). 

Problem  3. 
(a)  We  can  augment   the  priority  queue  P  with  a  linked   list  l.  We 
modify  the  insert  operation  so  it  just   puts  the  element   in   the  linked  list  l.  Now 
we  deﬁne  a   consolidate   operation  that  adds  the  elements  of  the  linked  list   to  the 
priority  queue. We  do  this  by  creating  a  new   priority  queue  P �  containing  only  the 
items  in  the  linked  list  l using   make­heap.  This   takes  O(m)  time,  where  m is   the 
size  of  the  linked  list.   We  then  merge  the  two  queues  P  and  P �  in   O(log n)  time. 
Therefore,   the  total  consolidation   time  is  O(m +  log n).  We  modify  delete­min 
to  ﬁrst  consolidate,  and  then  call   the  original  delete­min.  We  modify  merge   to 
ﬁrst  consolidate  each  of  the  augmented  priority  queues,  and  then   call  the  original 
merge. 
Consider  a  set  of  initially  empty  augmented   priority  queues  {P �}  (that  may  be 
merged  later)  on  which  all  operations   are  performed.  The  potential  function  φ� 
is  deﬁned  as  the  sum  of  the   size  of  the  lists   of  each  of  the  priority  queues   P � . 
Note  that  inserting   in  a  particular  priority  queue  takes  O(1)   amortized  time. 
Delete­min  on  any   particular  priority  queue  also  takes  only  O(log nh )  amortized, 
time,  where  nh  is  the  size  of  that  priority  queue,   since  the  O(mh  +  log  nh )  real 
work  to  consolidate   is  decreased  to  amortized   O(log nh ) by  the  potential  from 
the   queue  before  delete­min  was  processed.  Now,  consider   the  amortized   time  
to  merge  two  of  the   augmented   priority  queues.  We  spend   amortized  time  of 
� 
O(log nh ) +  O(log nh )  to  consolidate  each  one,   plus  the  real  work  of  merging  the 
two  priority  queues  which  takes  O(log nh )  time,  assuming  nh  > n�
h .  The  total  
amortized  time,   then,  is  O(log nh ) 

Handout  4:   Problem  Set  1   Solutions 

3 

(b)	 The   basic  idea  is  to  use  a   heap  of  heaps  (together  with  a  list   as  in   part  (a)).

The   data   structure   is  composed  of  several  binary  heaps  P1 , . . . Pk  and  a  “master”

binary  heap  M .  The   heaps  P1 , . . . Pk  contain  the  elements  of  the  data  struc­

ture.  The  heap  M  contains  as  its  elements  the  heaps   P1 , . . . Pk ,  which  are  keyed

(compared)  according   to   the  values   of  their  roots.

To  insert  an  element  into   the   data  structure,  we  just   add   it  to  the  linked  list  l. 
Delete­min  ﬁrst  does  a  consolidation  (which takes  O(m + log  n)  time,  where  m  is  
the   length  of  l).  Then,  delete­min  retrieves  the  “smallest”  heap  Pi  from  M .  The 
root  of  Pi  is  the   minimum  element  in  the  data  structure.  Remove  the  minimum 
from  Pi  (usual   heap  operation).  If  Pi  is   not   empty,  insert   the  modiﬁed   Pi  back

into  M .

In  the  consolidation  step,  we  construct  a  binary  heap  Pk+1  from  l  (if  l  is   non­

empty)  and  empty   the   list  l.  This  can  be  done  in  O(m)  using  a  standard   heap 
construction  algorithm.   To   ﬁnish   the  consolidation  step,  we  insert   Pk+1  into  into 
M . 
To  analyze  the  running  time,   let   the  potential  function   be  equal  to  the  length 
of  the  list  l.  Then,  insert  takes   O(1).  Consolidation  takes  O(m +  log  n)  real 
time,  but  O(log n)  amortized  time  (since  the  length  of  the  list   decreases   by  m). 
Delete­min  takes  O(log n) time  (note  that   the  depth  of  all  heaps,  M , P1 , . . . Pk  is  
always  O(log n)). 

Problem  4. 
Consider  the  oﬄine   algorithm:  we   process  nodes  in   postorder   (i.e.,  we  traverse 
the   nodes  using   DFS,   and  process  a  node  only  after   processsing  all  of  its  children).  When 
we  process  a   node  a,  we  answer  queries  (a, b),  such  that   b  was   processed   earlier  than   a  by 
doing  a  ﬁnd  in  our   union­ﬁnd  data   structure  D ;  the  “name”  of   the  result  is   the  answer   to  
the   query.  Then  we  union  a  with  the  parent   of  a,  and  set   the  name  of  the  set­representative 
to  be   the   name  of  the   parent. 

The   relationship   to   persistent  data   structures   is  as  follows.  We  view   the  order  in  which 
we  process  the  nodes  as  time.  Note   that  changes  to  the  union­ﬁnd  data  structure  D  occur 
exactly  at  the  times  the  nodes  are   processed,  so  that  we  can   think  of  the  data  structure  as 
changing  over  time:   D1 , D2 , . . . Dn .  Suppose  we  run  the  above  algorithm,  but   at   each  time 
t  we  process  a   node,  we   save  the  state  of  Dt  .  Now,  suppose  we  wish  to  answer   a  query  of 
the   form  (a, b).  Suppose   b was  processed  after  a  at  time  t.  Revert   to  the  data  structure   Dt , 
and   do   a  “ﬁnd”  of  a.  This  would  answer   the  query  (a, b). 

The   goal,  then,  is  to  design  a   persistent  version   of  the  union­ﬁnd  data  structure  to   support 
the   following  two   operations: 
•	 ﬁnd(x,t):   Find   the  name  of  x’s  component  at  time  t. 
•	 union(w,p,t):   Union  the  component  with  name  w  and  the  component  with  name   p at  
time   t.  

4 

Handout  4:  Problem  Set  1  Solutions 

We   use  the  disjoint­forest  implementation  of  the  union­ﬁnd   data  structure  using  the   union 
by  rank  heuristic. For   each  node,   the  parent   pointer   will  also  store  the  timestamp  t  at  which 
the   parent  pointer   became   non­null  (note  that   this  occurs  exactly  once  for   each  node   in  the  
tree). 

Therefore,  to  do   ﬁnd(x,t),   we  walk  up   the  parent  pointers  until  we  ﬁnd   a  node  whose  parent  
pointer  became  non­null  at   a  time  later  than   t.  However,  we  need  to  ﬁnd   the  name  of 
this  component.   To  do  this,   we  create  a  log  of  the  operations  done  on  the  union­ﬁnd  data  
structure. The  log   is  an  array  mapping  time­stamps  to  the  names  of  the  components   unioned. 
To  compute  the  root  node,   we  can  lookup   the  name  of  the  parent   component  corresponding  
to  the  time­stamp  of  the   last  edge  traversed.  Following  parent  pointers  takes   O(log n)  time 
due  to  union   by  rank,   so  the   ﬁnd  operation  takes  O(log n)  time. 

To  do  a   union(w,p,t),   we  ﬁrst  do   a   ﬁnd(w,t)   and  a  ﬁnd(p,t).  Then  we  do  union  by  rank  and  
timestamp  the  edge  added  with  t.   Now   we  need   to  update  the  log:  a  log  entry  (w, p)  is 
added  to  the  tth  element  in  the  log   array.  It  is  clear  that  the  union  operation   takes  O(log n) 
time,  so   the  preprocessing   time   takes  O(n log  n)  time. 

