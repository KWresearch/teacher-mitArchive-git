Massachusetts  Institute  of  Technology 
6.854J/18.415J: Advanced  Algorithms 
David  Karger 

Handout  20 
Wednesday,  November  9,  2005 

Problem   Set   9   Solutions  

Problem  1. 

 
 
 
(a)   Let  S  be  an  independent  set  in  G.  If  in  the  product  graph  we  choose  from  each  Gv , 
where  v ∈ S , a  copy  of  set  S , we will  get  a  set  S (cid:1)  of  size  |S | .  S (cid:1)  is  an  independent  set,  since 
2
there  is  no  edge  between  copies  Gv  for  v  ∈  S ,  and  inside  a  copy  there  is  no  edge  between 
vertices  v for  v ∈ S . 

√ 
(b) Let S be the independent  set of size s in the product graph of G. If elements of S belong 
to  at  least  s copies Gv  of  the  original  graph, we  are  done,  since  these  copies  correspond  to 
√ 
vertices  that  are  independent  in  G.  Otherwise,  by  the  pigeon  hole  principle,  there  must  be 
√ 
s vertices,  and  they  constitute  an  independent  set  of  size  at 
at  least  one  copy  containing 
least  s.

√ 
Note  that  the  proof  that  we  have  just  presented  is  constructive,  and  shows  how  to  get  an

s,  knowing  an  independent  set  of  size  s  in  the  product 
independent  set  of  size  at  least 
graph  in  polynomial  time. 

(c)   Suppose  that  there  is  an  α-approximation  algorithm  (α > 1).  For  a  given   > 0 and  for 
some  large  enough  integer  k ,  it  holds  that 
√ 
1 +  ≥  2k α. 

Let  OPT  be  the  size  of  the  maximum  independent  set  in  our  graph  G. We  take  G  =  G0 , 
and  construct  a  product  graph G1  of  it,  then  the  product  graph G2  of G1 ,  and  so  on,  till we 
vertices.  Gk  has  an  independent  set  of  size OPT2k 
achieve  the  product  graph Gk  of n2k 
. We 
ﬁnd  an α-approximation of an  independent  set  in Gk ,  and using  the algorithm  from part  (b) 
we  ﬁnd  an  independent  set  in G of  size  at  least 
OPT √ 
. 
2k α 

The  algorithm  obviously  runs  in  polynomial  time  for  ﬁxed  . 

Problem  2. 
(a)   We  will  show  that  the  cost  of  the  minimum  spanning  tree  in  G(cid:1)  is  at  most  two  times 
greater  than  the  cost  of  the  optimum  steiner  tree  S  in  G. 

2 

Handout  20:  Problem  Set  9  Solutions 

We  can  run  the  DFS  algorithm  from  an  arbitrary  vertex  v  in  S ,  and  order  terminals  T 
according to a time of  the  ﬁrst  visit.  Let  t1 ,  t2 ,  . . . ,  tk  be  these  vertices  in  the  just  speciﬁed 
order.  Let  us  consider  a  traversal  starting  in  t1 ,  and  ending  in  tk ,  visiting  consecutively  t2 , 
t3 , and  so  on.  From  ti  to  ti+1   we  go  along  some  shortest  path  from  ti  to  ti+1 .  It  is  clear  that 
this  traversal  corresponds  to  some  spanning  tree  in  G(cid:1)
,  namely,  we  take  edges  (ti , ti+1) for 
1 ≤  i < k ,  so  the  cost  of  the  optimal  spanning  tree  in G(cid:1)  is  not  greater  than  the  cost  of  our 
traversal. 
Our  claim  is  that  the  cost  of  the  traversal  is  at  most  two  times  greater  than  the  cost  of  S . 
This  is  true,  since  if  we  went  from  a  vertex  to  the  next  along  edges  that  belong  to  S , as we 
did  in  the  DFS, we  would  go  along  each  edge  at most  twice. 

(b)   We  will  use  the  knowledge  from  part  (a)  to  construct  a  2-approximation  algorithm.  It 
will  create  a  tree  connecting  all  the  terminals  in  the  fashion  of  Prim’s  algorithm.  We  start 
from  an  arbitrary  terminal,  ﬁnd  the  shortest  path  from  it  to  another  terminal,  and  connect 
them  by  this  path.  Then,  until  all  terminals  are  connected,  we  in  turn  ﬁnd  a  terminal  that 
is  closest  to  the  already  created  tree,  and  connect  it  by  some  shortest  path.  Obviously, 
we  will  eventually  get  a  tree,  and  since  its  cost  will  not  be  greater  than  the  cost  of  the 
optimal  spanning  tree  in  G(cid:1)
,  we  get  in  polynomial  time  a  2-approximation  for  the  Steiner 
tree  problem. 

Problem  3. 

(a) Note ﬁrst that the number of all diﬀerent  subsets of the set of items is polynomial.  Let n 
be the number of all items.  The number of diﬀerent subsets can be bounded by C = (n + 1)k  . 
This  also means  that  there  are  at most  C  diﬀerent  conﬁguration  of  bins. 
We  will  compute  sets  Si .  Si  is  the  set  of  all  possible  sets  of  items  left  when  i bins  has  been 
already  packed.  Obviously,  S0  contains  only  one  subset,  the  set  of  all  items.  Si+1   can  be 
computed  from  Si  in  polynomial  time,  for  each  item  in  Si  we  try  all  possible  packings,  and 
each  time  we  add  the  set  of  items  that  are  left  to  Si+1 .  We  stop  when  in  a  new  computed 
j  there  is  an  empty  subset.  This  implies  that  the  optimum  packing  of  items  uses  j  bins. 
S
The  optimal  number  of  bins  can  be  upper  bounded  by  n,  since  each  item  can  be  put  into  a 
separate  bin.  We  can  compute  sets  Si  and  reconstruct  the  optimum  packing  in  polynomial 
time. 

(b)   Starting  over  the  ﬁrst  bin,  we  will  consider  items we  want  to  add  one  by  one.  We  check 
whether it is possible to add an item to the current bin.  If it is, then simply do it.  Otherwise, 
we move  to the next bin  (possibly adding  a new bin),  try to add an  item  there,  and  so  forth, 
until  we  eventually  put  it  into  some  bin.  We  do  the  same  with  any  other  item,  starting 
always  over  the  bin  to  which  the  last  item  was  inserted.  The  algorithm  runs  in  time  linear 
in  the  number  of  items  and  bins. 

Handout  20:  Problem  Set  9  Solutions 

3 

Suppose  ﬁrst  that   ∈ (0, 1/2].  For  such    it  can  be  simply  proven  that 
1  ≤ 1 + 2. 
1 −  
When we cannot add an  item to a current bin,  it  follows by the upper bound on size of items 
that  it  must  contain  items  of  the  sum  of  sizes  greater  than  1 − .  Since  the  sum  of  sizes  of 
∗
all  items  is  bounded  by  B , we  are  able  to  ﬁt  all  items  of  size  at  most    into  the  ﬁrst 
� 
� 
B ∗ 
< (cid:5)(1 + 2)B  (cid:6) ≤ 1 + (1 + 2)B 
∗ 
∗
1 −  
bins,  and  therefore  we  can  achieve  a  packing  of  at most  max{B , 1 + (1 + 2)B ∗}  bins. 
∗
When   > 1/2,  it  suﬃces  if  we  can  ﬁt  all  items  into  2B bins,  since 
∗
∗ 
1 + (1 + 2)B > 2B . 

We  shall  show  that  the  average  ﬁll  ratio  of  bins  (possibly  with  exclusion  of  the  last  one)  is 
∗
at  least  1/2, what  will  in  turn  bound  the  number  of  bins  by  required  2B .  The  ﬁrst  B  bins 
contain  elements  greater  than  ,  so  even  if  we  do  not  manage  to  add  something  to  them, 
they  are  ﬁlled  enough  for  our  purpose.  Consider  a  pair  of  consecutive  bins  that we  add  over 
the  initial  B  bins.  At  some  moment  we  cannot  put  an  item  into  the  ﬁrst  bin,  and  therefore 
we  put  it  into  a  new  bin,  that  is  into  our  second  bin.  It  implies  that  in  both  bins  there  are 
items  of  the  sum  of  sizes  greater  than  1,  that  is  at  least  1/2  on  average  in  both  of  them. 
This  proves  our  claim  on  the  average  ﬁll  ratio. 

(c)   In bin packing,  even  small enlargement of sizes of items may double the required number 
of  bins  we  need  (if  almost  all  ai  = 1/2).  It  was  not  the  case  for  P ||Cmax   because  loads  of 
machines  were  not  bounded,  and  we  were  minimizing  a  maximum  load,  not  the  number  of 
machines. 

(d)   We  put  aside  ﬁrst  n/k  items—they  can  be  placed  each  into  its  own  bin.  Notice  that 
after  the  grouping  procedure  the  (n/k) + i-th  item  has  size  not  greater  than  the  i-th  item 
before  grouping,  and  therefore  all  items  that  do  not  belong  to  the  ﬁrst  group  can  be  put 
into  the  same  number  of  bins  as  the  original  items,  if  we  take  the  (n/k) + i-th  item  after 
grouping  instead  of  the  i-th  item  before  the  grouping.  Eventually,  the  optimal  number  of 
bins  increases  by  at most  n/k . 

(e)   For  a  given    we  set  k  = 2/2 . Let  m  be  the  number  of  items  of  size  greater  than  /2. 
We  know  that 
m  ≤ B . 
 
∗ 
2 

4 

Handout  20:  Problem  Set  9  Solutions 

First, we apply the grouping from part (d), and we know by part (a) that using the algorithm 
from part  (a), we  can ﬁnd  the optimum  packing  for modiﬁed  sizes  of  size  at most  the  ceiling 
of 
≤ B  + B  = (1 + )B ,
∗ 
∗ 
∗

m2 
2 
∗ B  (1 + ) + 1. 
By  part  (b)  we  know  that  we  can  add  items  of  size  not  exceeding  /2  in  linear  time,  ﬁtting 
∗
into  B (1 + ) + 1  bins.  The  algorithm works  in  polynomial  time. 

m∗
B  +
k 

∗ 
= B  + 

i.e.  of  size 

Problem  4. 

� 
(a) An integer variable xj t , for j  = 1 . . . n, where n is the number of jobs, and t = 1 . . . 
j  pj , 
is  an  indicator  that  job  j  completed  at  time  t.  Obviously, 
0 ≤ xj t   ≤ 1. 
∀j  ∀t 
� 
xj t   = 1, 
t 

We  enforce  that  every  job  completes: 
∀j

that  a  job  is  not  processed  before  its  predecessor: 
�  � 
t−pk 
t 
xki   ≥ 
xj i , 
i=1  
i=1  

∀j  ∀k ∈ A(j )  ∀t

and  that  the  total  processing  time  of  jobs  completed  before  time  t  is  at most  t: 
� � 
t 
xj i   ≤ t. 
j 
i=1  
� � 
We  only  need  to  specify  a  goal  function.  It  is 
wj txj t . 
min 
t
j 

∀t

(b)   For  a  job  j  and  its  halfway  point  hj 
�
�
txj t   ≥ 
txj t   ≥ 
t≥hj 

C j  = 

t

� 
hj xj t   = hj 
t≥hj 

� 
xj t   ≥  hj . 
1 
2 
t≥hj 

Voil`a! 

Handout  20:  Problem  Set  9  Solutions 

5 

(c)   The  set  of  constraints  that  worked  for  the  integral  linear  program,  and  expressed  that 
a job  j  cannot  be  processed  before  its  predecessor  k ,  now  says  that  a  half  of  j  must  be 
processed  after  a  half  of  k .  This  implies  that  no  job  runs  before  its  predecessor. 

(d)   Note  that  at  the  halfway  point  of  job  j  at  least  half  of  each  of  jobs  proceeding  j  in 
the  order  has  been  completed.  This  implies  that  a  half  of  the  sum  of  their  (including  j ) 
processing  times  is  not  greater  than  hj ,  and  therefore  the  completion  time  of  j  is  at  most 
2hj .  By  part  (b)  we  know  in  turn  that  their  completion  time  is  not  greater  than  4C j . 

(e)   Solving  the  linear  program, we  minimize  function 
�  �  � 
� � 
wj txj t   = 
wj 
txj t   = 
wj C j . 
t
t
j 
j
j 

The  minimum  we  achieve  is  not  worse  than  the  minimum  for  the  integer  linear  program, 
� 
and  because  our  completion  times  are  not  worse  than  four  times  average  completion  times 
in  the  optimal  solution,  we  achieve  a  constant  factor-approximation  for  1 | prec   |  Cj . 

