6.854  Advanced  Algorithms 

Lecture  1:  10/13/2004 

Lecturer:  David  Karger 

Scribes:  Jay  Kumar  Sundarara jan 

Duality 

This  lecture  covers  weak  and  strong duality,  and  also  explains  the  rules  for ﬁnding the dual 
of  a  linear  program, with  an  example.  Before we move  on  to  duality,  we  shall ﬁrst  see  some 
general  facts  about  the  location  of  the  optima  of  a  linear  program. 

1.1  Structure  of  LP  solutions 

1.1.1  Some  intuition  in  two  dimensions 

Consider  a  linear  program  -

Maximize  yT  b 
sub ject  to  yT  A ≤ c 
The feasible region of this LP is in general, a convex polyhedron.  Visualize it as a polygon in 
2 dimensions, for simplicity.  Now, maximizing yT  b is the same as maximizing the pro jection 
of the vector y  in the direction represented by vector b.  For whichever direction b we choose, 
the point y  that maximizes  yT  b  cannot  lie  strictly  in  the  interior  of  the  feasible  region.  The 
reason  is  that,  from  an  interior  point,  we  can  move  further  in  any  direction,  and  still  be 
feasible.  In  particular,  by  moving  along  b,  we  can  get  to  a  point  with  a  larger  pro jection 
along  b.  This  intuition  suggests  that  the  optimal  solution  of  an  LP  will  never  lie  in  the 
interior  of  the  feasible  region,  but  only  on  the  boundaries.  In  fact,  we  can  say  more.  We 
can  show  that  for  any  LP,  the  optimal  solutions  are  always  at  the  “corners”  of  the  feasible 
region  polyhedron.  This  notion  is  formalized  in  the  next  subsection. 

1.1.2  Some  deﬁnitions 

Deﬁnition  1  (Vertex  of  a  Polyhedron)  A point  in  the polyhedron  which  is uniquely op-
timal  for  some  linear  objective,  is  cal led  a  vertex  of  the  polyhedron. 

Deﬁnition  2  (Extreme  Point  of  a  Polyhedron)  A point  in the polyhedron which is not 

1-1 


Lecture  1:  10/13/2004 

1-2 

a  convex  combination  of  two  other  points  in  the  polyhedron  is  cal led  an  extreme  point  of  the 
polyhedron. 

Deﬁnition  3  (Tightness)  A  constraint  of  the  form  aT  x  ≤  b,  aT  x  =  b  or  aT  x  ≥  b  in  a 
linear  program  is  said  to  be  tight  for  a  certain  point  y , if  aT  y = b. 

Deﬁnition  4  (Basic  Solution)  For  an  n-dimensional  linear  program,  a  point  is  cal led  a 
basic  solution,  if  n  linearly  independent  constraints  are  tight  for  that  point. 

Deﬁnition  5  (Basic  Feasible  Solution)  A  point  is  a  basic  feasible  solution,  iﬀ  it  is  a 
basic  solution  that  is  also  feasible. 

Note:  If x  is  a  basic  feasible  solution,  then  it  is  in  fact,  the unique point  that  is  tight  for  all 
its  tight  constraints.  This  is  because,  there  can  be  only  one  solution  for  a  set  of  n  linearly 
independent  equalities,  in  n-dimensional  space. 

Theorem   1   For  a  polyhedron  P  and  a  point  x ∈ P ,  the  fol lowing  are  equivalent: 

1.	 x  is  a  basic  feasible  solution 

2.	 x  is  a  vertex  of  P 

3.	 x  is  an  extreme  point  of  P 

Proof:  Assume  the  LP  is  in  the  canonical  form. 
1.	 Vertex⇒  Extreme   Point  
Let  v  be  a  vertex.  Then  for  some  ob jective  function  c,  cT  x  is  uniquely  minimized  at 
v .  Assume  v  is  not  an  extreme  point.  Then,  v  can  be  written  as  v  =  λy + (1 − λ)z 
for  some  y ,  z  neither  of  which  is  v , and  some  λ  satisfying  0 ≤ λ ≤ 1. 
Now,  cT  v = cT  [λy + (1 − λ)z ] = λcT  y + (1 − λ)c z 
T
This  means  cT  y  ≤  cT  v  ≤  cT  z . But,  since  v  is  a  minimum  point,  cT  v  ≤  cT  y  and 
cT  v  ≤  cT  z . Thus,  cT  y  =  cT  v  =  cT  z .  This  is  a  contradiction,  since  v  is  the  unique 
point at which  cT  x  is  minimized. 
2. 	 Extreme   Point   ⇒  Basic  Feasible  Solution 
Let  x  be  an  extreme  point.  By  deﬁnition,  it  lies  in  the  polyhedron  and  is  therefore 
feasible.  Assume  x  is  not  a  basic  solution.  Let  T  be  the  set  of  rows  of  the  constraint 
matrix  A  for  which  the  constraints  are  tight  at  x. Let  ai  (a  1 × n  vector)  denote  the 

Lecture  1:  10/13/2004 

1-3 

ith	 row  of  A. For  ai  /∈ T ,  ai .x >  bi . Since  x  is  not  a  basic  solution,  T  does  not  span 
Rn  .  So,  there  is  a  vector  d  (cid:5)= 0  such  that  ai .d = 0  ∀ai  ∈ T . 
Consider  y  =  x + d  and  z  =  x − d. If  ai  ∈  T , then  ai .y  =  ai .z  =  bi . If  ai  /∈  T , 
then,  by  choosing  a  suﬃciently  small  : 0  <   ≤ mini /
ai .x−bi  ,  we  can  ensure  that 
∈T 
|ai .d|
ai .y  ≥ bi  and  ai .z  ≥ bi . Thus  y  and  z  are  feasible.  Since  x =  y/2 + z/2,  x  cannot  be 
an  extreme  point  –  a  contradiction. 
3. 	 Basic  Feasible  Solution  ⇒ Vertex 
Let  x  be  a  basic  feasible  solution.  Let  T  = {i  | ai .x =  bi }.  Consider  the  ob jective  as 
� 
� 
�	
i∈T  (ai .x) = 
i∈T  bi . 
i∈T  ai .  Then,  c.x = 
minimizing  c.y  for  c = 
� 
� 
For  any  x(cid:2)  ∈ P ,  c.x = 
i∈T  bi  with  equality  only  if  ai .x =  bi  ∀i ∈ T . 
i∈T  (ai .x(cid:2) ) ≥ 
(cid:2)	
(cid:2) 
(cid:2) 
This  implies  that  x = x  and  that  x  uniquely minimizes  the  ob jective  c.y . 

This  proves  that  vertex,  extreme  point  and  basic  feasible  solution  are  equivalent  terms. 

Theorem   2   Any bounded  LP  in  standard  form  has  an optimum  at  a basic  feasible  solution. 

Proof:  Consider  an  optimal  x  which  is  not  a  basic  feasible  solution.  Being  optimal,  it  is 
feasible,  hence  it  is  not  basic.  As  in  the  previous  proof,  let  T  be  the  set  of  rows  of  the 
constraint matrix A  for which  the constraints  are  tight  at x. Since x  is not a basic  solution, 
T  does  not  span  Rn  .  So,  there  is  a  vector  d  (cid:5)= 0  such  that  ai .d = 0  ∀ai  ∈ T . For  a  scalar   
with  suﬃciently small absolute value, y = x + d is  feasible,  and  represents a  line containing 
x  in  the direction  d.  The ob jective  function  at  y  is  cT  x + cT d. Since x  is  optimal,  cT  d = 0, 
as  otherwise,  an    of  the  opposite  sign  can  reduce  the  ob jective.  This  means,  all  feasible 
points on  this line are optimal.  One of the directions of motion on this  line will reduce some 
xi .  Keep  going  till  some  xi  reduces  to  0.  This  results  in  one  more  tight  constraint  than 
before. 

This  technique  can  be  repeated,  till  the  solution  becomes  basic. 

Thus, we  can  convert  any  feasible  solution  to  a  basic  feasible  solution  of  no worse  value.  In 
fact,  this  proof  gives  an  algorithm  for  solving  a  linear  program:  evaluate  the  ob jective  at 
all  basic  feasible  solutions,  and  take  the  best  one.  Suppose  there  are  m  constraints  and  n 
variables.  Since  a  set  of  n  constraints  deﬁnes  a  basic  feasible  solution,  there  can  be  upto 
�	 � 
m  basic  feasible  solutions.  For  each  set  of  n  constraints,  a  linear  system  of  inequalities 
n 
has  to  be  solved,  which  by  Gaussian  elimination,  takes  O(n3 )  time.  This  is  in  general  an 
exponential  complexity  algorithm  in  n.  Note  that  the  output  size  is  polynomial  in  n, since 
the  optimal  solution  is  just  the  solution  of  a  system  of  linear  equalities. 

Lecture  1:  10/13/2004 

1-4 

1.2   The   dual   of   a   linear   program  

Given  an  LP  in  the  standard  form: 

Minimize  c.x 
sub ject  to:  Ax = b; x ≥ 0 

We call the above LP the primal LP. The decision version of the problem is:  Is the optimum 
c.x  ≤  δ  ?  This  problem  is  in  N P ,  because,  if  we  ﬁnd  a  feasible  solution  with  optimum 
value  ≤  δ ,  we  can  verify  that  it  satisﬁes  these  requirements,  in  polynomial  time.  A  more 
interesting question  is whether this problem is in co-NP. We need to ﬁnd an easily veriﬁable 
proof for the fact that there is no x which satisﬁes c.x < δ .  To do this, we require the concept 
of  duality. 

1.2.1  Weak  Duality 

We  seek a  lower bound on  the optimum.  Consider a vector  y  (treat  is as a  row vector  here). 
For  any  feasible  x,  yAx = yb  holds.  If  we  require  that  yA ≤ c, then  yb = yAx ≤ cx. Thus, 
yb  is  a  lower  bound  on  cx,  and  in  particular  on  the  optimum  cx.  To  get  the  best  lower 
bound, we  need  to  maximize  yb.  This  new  linear  program: 

Maximize  yb 
sub ject  to:  yA ≤ c 

is  called  the  dual  linear  program.  (Note:  The  dual  of  a  dual  program  is  the  primal).  Thus 
primal  optimum  is  lower  bounded  by  the  dual  optimum.  This  is  called  weak  duality. 

Theorem   3   (Weak   Duality)   Consider  the  LP  z  =  M in{c.x  |  Ax  =  b, x  ≥  0}  and  its 
dual  w = max{y .b  |  yA ≤ c}. Then  z ≥ w. 

Corollary  1  If  the  primal  is  feasible  and  unbounded,  then  the  dual  is  infeasible. 

1.3  Strong  Duality 

In  fact,  if  either  the  primal  or  the  dual  is  feasible,  then  the  two  optima  are  equal  to  each 
other.  This  is  known  as  strong  duality.  In  this  section,  we  ﬁrst  present  an  intuitive  expla-
nation  of  the  theorem,  using  a  gravitational  model.  The  formal  proof  follows  that. 

Lecture  1:  10/13/2004 

1-5 

1.3.1  A  gravitational  model 
Consider  the  LP  min{y .b|yA ≥  c}.  We  represent  this  feasible  region  as  a  hollow  polytope, 
with  the  vector  b  pointing  “upwards”.  If  a  ball  is  dropped  into  the  polytope,  it  will  settle 
down  at  the  lowest  point,  which  is  the  optimum  of  the  above  LP.  Note  that  any  minimum 
is  a  global  minimum,  since  the  feasible  region  of  an  LP  is  a  convex  polyhedron.  At  the 
equilibrium  point,  there  is  a  balance  of  forces  –  the  gravitational  force  and  the  normal 
reaction  of  the  ﬂoors  (constraints).  Let  xi  represent  the  amount  of  force  exerted  by  the  ith 
constraint.  The direction  of  this  force  is  given  by  the  ith  column  of A.  Then  the  total  force 
exerted  by  all  the  constraints  Ax  balances  the  gravity  b:  Ax = b. 
The  physical  world  also  gives  the  constraints  that  x  ≥  0,  since  the  ﬂoors’  force  is  always 
outwards.  Only  those  ﬂoors  which  the  ball  touches  exert  a  force.  This  means  that  for  the 
constraints  which  are  not  tight,  the  corresponding  xi ’s  are  zero:  xi  = 0  if  yAi  > ci . This 
can  be  summarized  as 
(ci  − yAi )xi  = 0 

.  This means  x  and  y  satisfy: 

� 
� 
cixi  = c.x 
yAixi  = 
y .b = 
But  weak  duality  says  that  yb ≤ cx,  for  every  x  and  y .  Hence  the  x  and  y  are  the  optimal 
solutions  of  their  respective  LP’s.  This  implies  strong  duality  –  the  optima  of  the  primal 
and  dual  are  equal. 

1.3.2  A  formal  proof 
Theorem   4   (Strong   Duality)   Consider w = min{y .b | yA ≥ c} and z = min{c.x | Ax = 
b, x ≥ 0}. Then  z = w. 

Proof:  Consider  the  LP min{y .b|yA ≥ c}.  Consider  the  optimal  solution  y 
∗ .  Without  loss 
∗
of  generality,  ignore  all  the  constraints  that  are  loose  for  y  .  If  there  are  any  redundant 
constraints,  drop  them.  Clearly,  these  changes  cannot  alter  the  optimal  solution.  Dropping 
these  constraints  leads  to  a  new  A  with  fewer  columns  and  a  new  shorter  c.  We  will  prove 
that the dual of the new LP has an optimum equal in value to the primal.  This dual optimal 
solution  can  be  extended  to  an  optimal  solution  of  the  dual  of  the  original  LP,  by  ﬁlling  in 
zeros  at  places  corresponding  to  the  dropped constraints.  The point  is  that we  do not need 
those  constraints  to  come  up  with  the  dual  optimal  solution. 

After  dropping  those  constraints,  at most  n  tight  constraints  remain  (where n  is  the  length 
of  the  vector  y).  Since  we  have  removed  all  redundancy,  these  constraints  are  linearly 
∗  is  still  the 
independent.  In  terms  of  the  new  A  and  c,  we have new  constraints  yA = c.  y 
optimum. 

Lecture  1:  10/13/2004 

1-6 

Claim:  There  exists  an  x,  such  that  Ax = b.

Proof:  Assume  such  an  x  does  not  exist,  i.e.  Ax  =  b  is  infeasible.  Then  “duality”  for

linear  equalities  implies  that  there  exists  a  z  such  that  zA  =  0,  but  zb  (cid:5)

=  0.  Without 
∗
loss  of  generality,  assume  z .b  <  0  (otherwise,  just  negate  the  z ).  Now  consider  (y  + z ). 
A(y  + z ) =  Ay + Az  =  Ay∗ .  Hence,  it  is  feasible.  (y  + z ).b  =  y  .b + z .b  <  y 
∗
∗ 
∗
∗
∗ .b, which 
is  better  than  the  assumed  optimum  –  a  contradiction.  So,  there  is  an  x  such  that  Ax = b. 
∗
Let  this  be  called  x  . 
∗
∗
Claim:  y  .b = c.x  . 
∗
∗ 
∗ 
∗
∗
∗
∗
∗
Proof:  y  .b = y .(Ax ) = (y A).x  = c.x  (since  Ax = b  and  y A = c) 
∗  ≥ 0 
Claim:  x 
(cid:2)
∗
Proof:  Assume  the  contrary.  Then,  for  some  i,  xi  <  0.  Let  c =  c + ei , where  ei  is  all 
0’s  except at the  ith  position,  where  it  has  a  1.  Since  A  has  full  rank,  yA  ≥  c has  a 
(cid:2) 
solution,  say  y .  Besides,  since  c(cid:2)  ≥ c,  y (cid:2)  is  feasible  for  the  original  constraints  yA ≥ c. But, 
(cid:2) 
y (cid:2) .b = y (cid:2)Ax∗  = c(cid:2)x  < cx  = y b  (since  ci  is  now  higher  and  xi  < 0).  This means  y (cid:2)  gives  a 
∗
∗
∗
(cid:2)
∗  ≥ 0. 
better  ob jective  value  than  the  optimal  solution  –  a  contradiction.  Hence,  x 
∗
Thus, there  is an x  which  is  feasible  in the dual, and whose ob jective  is equal to  the primal 
∗
optimum.  Hence,  x  must  be  the  dual  optimal  solution,  using  weak  duality.  Thus,  the 
optima  of  primal  and  dual  are  equal. 

Corollary  2  Checking  for  feasibility  of  a  linear  system  of  inequalities  and  optimizing  an 
LP  are  equal ly  hard. 

Proof:  Optimizer  →  Feasibility  checker 
Use  the  optimizer  to  optimize  any  arbitrary  function  with  the  linear  system  of  inequalities 
as  the constraints.  This will automatically  check  for  feasibility,  since every optimal  solution 
is  feasible. 
Feasibility  checker  →  Optimizer  
We  construct  a  reduction  from  the  problem  of  ﬁnding  an  optimal  solution  of  LP1  to  the 
problem  of  ﬁnding  a  feasible  solution  of  LP2 .  LP1  is  min{c.x  |  Ax  =  b, x  ≥  0}. Consider 
LP2  =  min{0.x|Ax  =  b, x  ≥  0, yA  ≤  c, c.x  =  b.y}.  Any  feasible  solution  of  LP2  gives  an 
optimal  solution  of  LP1  due  to  the  strong  duality  theorem.  Finding  an  optimal  solution  is 
thus  no  harder  than  ﬁnding  a  feasible  solution. 

Lecture  1:  10/13/2004 

1.4   Rules   for   duals  

1-7 

Usually  the  primal  is  constructed  as  a  minimization  problem  and  hence  the  dual  becomes 
a  maximization  problem.  For  the  standard  form,  the  primal  is  given  by: 

while  the  dual  is  given  by: 

T 
z  = min  (c x) 
Ax  ≥  b 
x  ≥  0 

w  = max  (bT  y) 
AT  y  ≤  c 
y  ≥  0 

For  a  mixed  form  of  the  primal,  the  following  describes  the  dual: 

Primal: 

z  = min  c1x1  + c2x2  + c3x3 
A11x1  + A12x2  + A13x3  =  b1

A21x1  + A22x2  + A23x3  ≥  b2

A31x1  + A32x2  + A33x3  ≤  b3

x1  ≥  0 
x2  ≤  0 
UIS 
x3 

(UIS  =  unrestricted  in  sign) 

Dual: 

w  = max  y1b1  + y2b2  + y3b3 
y1A11  + y2A21  + y3A31  ≤  c1

y1A12  + y2A22  + y3A32  ≥  c2

y1A13  + y2A23  + y3A33  =  c3


Lecture  1:  10/13/2004 

1-8 

UIS 
y1 
y2  ≥  0 
y3  ≤  0 

These  rules  are  summarized  in  the  following  table. 

PRIMAL  Minimize  Maximize 
≥ 0 
≥ bi 
Constraints 
≤ 0 
≤ bi 
= bi 
Free 
≤ cj ≤ cj 
≥ 0 ≥ 0 
= cj 
Free 

Variables 

DUAL 
Variables 

Constraints 

Each  variable  in  the  primal  corresponds  to  a  constraint  in  the  dual,  and  vice  versa.  For  a 
maximization, an upper bound constraint is a “natural” constraint, while for a minimization, 
a  lower  bound  constraint  is  natural.  If  the  constraint  is  in  the  natural  direction,  then  the 
corresponding  dual  variable  is  non-negative. 

An  interesting  observation  is  that,  the  tighter  the  primal  gets,  the  looser  the  dual  gets. 
For  instance,  an  equality  constraint  in  the  primal  leads  to  an  unrestricted  variable  in  the 
dual.  Adding more constraints in the primal  leads to more variables  in the dual, hence more 
ﬂexibility. 

1.5  Shortest  Path  –  an  example 

Consider  the  problem  of  ﬁnding  the  shortest  path  in  a  graph.  Given  a  graph  G, we wish 
to  ﬁnd  the  shortest  path  from  a  speciﬁed  source  node,  to  all  other  nodes.  This  can  be 
formulated  as  a  linear  program: 

w = max  (dt  − ds ) 
s.t.  dj  − di  ≤ cij , 
∀i, j 

In  this  formulation,  di  represents  the  distance  of  node  i  from  the  source  node  s. The 
cij  constraints  are  essentially  the  triangle  inequalities  –  the  distance  from  the  source  to  a 
node  i  should  not  be  more  than  the  distance  to  some  node  j  plus  the  distance  from  j  to 

Lecture  1:  10/13/2004 

1-9 

i.  Intuitively,  one  can  imagine  stretching  the  network  physically,  to  increase  the  source-
destination  distance.  When  we  cannot  pull  any  further  without  breaking  an  edge,  we  have 
found  a  shortest  path. 

The  dual  to  this  program  is  found  thus.  The  constraint  matrix  in  the  primal  has  a  row  for 
every  pair  of  nodes  (i, j ),  and  a  column  for  every  node.  The  row  corresponding  to  (i, j ) has 
a +1  in the  ith  column  and  a  -1  in  the  j th  column,  and  zeros  elsewhere. 

1.  Using  this,  we  conclude  that  the  dual  has  a  variable  for  each  pair  (i, j ),  say  yij . 

2.  It has a constraint for each node i.  The constraint has a coeﬃcient of +1 for each edge 
entering  node  i  and  a  -1  for  each  edge  leaving  i.  The  right  side  for  the  constraints 
are  -1  for  the  node  s  constraint,  1  for  the  node  t  constraint,  and  0  for  others,  based 
on  the  ob jective  function  in  the  primal.  Moreover,  all  the  constraints  are  equality 
constraints,  since  the  di  variables  were unrestricted in sign in the  primal. 

3.  The  dual  variables  will  have  to  have  a  non-negativity  constraint  as  well,  since  the 
constraints  in  the  primal  were  “natural”  (upper  bounds  for  a maximization). 
� 
i,j  cij yij ,  since  the  right  side  of  the  primal  constraints 

4.  The  ob jective  is  to  minimize 
are  cij . 

Thus  the  dual  is: 

z = min 

� 
cij yij 

i,j 

� 
(yj s   − ysj ) =  −1 
j 
� 
(yj t   − ytj ) = 1 
j 
� 
(yj i   − yij ) = 0, ∀i  (cid:5)= s, t 
yij  ≥  0, ∀i, j 

j 

This  is  precisely  the  linear  program  to  solve  the  minimum  cost  unit  ﬂow,  in  a  gross  ﬂow 
formulation.  The constraints  correspond  to  the ﬂow  conservation  at  all nodes  except  at  the 
source  and  sink.  The  value  of  the  ﬂow  is  forced  to  be  1.  Intuitively,  this  says  that  we  can 
use minimum  cost  unit  ﬂow  algorithms  to  ﬁnd  the  shortest  path  in  a  network. 

Duality is a very useful concept, especially because it helps to view the optimization problem 
on  hand  from  a  diﬀerent  perspective,  which  might  be  easier  to  handle. 

