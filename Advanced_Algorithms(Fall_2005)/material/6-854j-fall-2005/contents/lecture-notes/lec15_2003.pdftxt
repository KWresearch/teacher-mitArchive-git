Lecture  15:  October  15,  2003 

6.854   Advanced   Algorithms  
Lecturer:  David Karger  and  Erik Demaine 
Scribes:  Nelson  Lai 

15.1   Addendum   from   last   lecture  

Theorem  1  If   the   primal   P  (primal)  or  D  (dual)  are  feasible,  then  they  have  the  same  value. 

15.2  Rules  for  Taking  Duals  

In  general  we  construct  the  primal  P  as  a  minimization  problem  and,  conversely,  the  dual  D  as  a 
maximization  problem.  If  P  is  a  linear  program  in  standard  form  given  by: 

then  the  dual, D  is  given  by: 

T 
z  = min(c x) 
Ax  ≥  b 
x  ≥  0 

w  = max(bT  y ) 
AT  y  ≤  c 
y  ≥  0 

In general, the  form of the dual will depend on the  form of the primal.  If one  is given a primal linear 
program P  in  mixed  form: 

x  = min(c1x1  + c2x2  + c3x3 ) 
A11x1  + A12x2  + A13 x3  =  b1

A21x1  + A22x2  + A23 x3  ≥  b2

A31x1  + A32x2  + A33 x3  ≤  b3

x1  ≥  0 
x2  ≤  0 
unrestricted  in  sign  (UIS) 
x3 

then  the  dual D  is  given  by: 

15-1 

Lecture   15:   October   15,   2003  

15-2 

w  = max(b1y1  + b2y2  + b3y3 ) 
y1A11  + y2A21  + y3A31  ≤  c1

y1A12  + y2A22  + y3A32  ≥  c2

y1A13  + y2A23  + y3A33  =  c3

unrestricted  in  sign  (UIS) 

y1 
y2  ≥  0

y3  ≤  0


By  simple  transformations,  we  can  conﬁrm  that  this  is  consistent  with  the  dual  for  the  standard 
form  of  the  primal  and  that  in  fact  the  dual  of  the  dual  is  the  primal. 

We  can  summarize  these  results  with  the  following  table  which  states  the  rules  for  taking  duals. 
Note  that  each  variable  in  the  primal  corresponds  to  a  variable  in  the  dual  and  each  constraint  in 
the  primal  corresponds  to  a  variable  in  the  dual. 

PRIMAL 

constraints 

variables 

minimize 
≥ bi 
≤ bi 
= bi 
≥ 0 ≥ 0 
unrestricted 

maximize 
≥ 0 
≤ 0 
unrestricted 
≤ cj ≤ cj 
= cj 

DUAL 

variables 

constraints 

Note  that  this  makes  intuitive  sense.  For  example,  the  primal  minimization  problem  has  lower 
bounds  as  the  natural constraints.  This  corresponds to  a  positive variable  in  the  dual maximization 
problem.  Conversely,  the  primal  maximization  problem  has  upper  bounds  as  natural  constraints. 
The  dual minimization  problem  now  has  a  negative  variable. 

To  develop  an  intuition  for  these  relationships,  we  consider  the  eﬀect  of  the  sign  of  a  variable  in 
the minimization problem on the  type of the corresponding constraint in the maximization problem. 
We  know  from  weak  duality  that  cT  x  ≥ yb   = yAx.  Consider  the  case  where  x1  ≥ 0.  Then  in  order 
to  have  yAx1  ≤  c1x1 , we  must have  yA11  ≤  c1  for  any  y .  Similarly,  if  x2  ≤  0,  then  we  must  have 
yA12  ≥  c2  in  order  for  cT  x  ≥  yAx.  Finally,  for  x3  unrestricted,  we  must  have  yA13  =  c3  since 
multiplying  both  sides  by  x  might  or  might  not  change  the  direction  of  any  inequality.  In  general, 
tighter  constraints  in  the  primal  lead  to  looser  constraints  in  the  dual.  An  equal  constraint  leads  to 
an  unrestricted  variable  and  adding  new  constraints  creates  new  variables  and more  ﬂexibility. 

Lecture   15:   October   15,   2003  
15.3 
Shortest  Paths 

15-3 

We  now  examine  an  example  showing  the  relationship  between  the  primal  and  dual  problems.  We 
consider  formulating  the  shortest  paths  problem  as  a  linear  program.  Given  a  graph G, we  wish  to 
ﬁnd the shortest path from any one point (the source) to any other point.  We formulate the problem 
as  a  dual  (or maximization)  linear  program. 
w  = max(dt  − ds ) 
dj  − di  ≤  cij 
unrestricted 
dj 

Each  variable  di  represents  the  distance  to  vertex  i  and  each  constraint  represents  the  triangle 
inequality —  that  is,  the  the  distance  to  vertex  i  should  always be  less  than or  equal to  the distance 
to  vertex  j  plus  the  distance  from  vertex  j  to  vertex  i.  Any  feasible  solution  to  this  would  ﬁnd  a 
lower bound  to  the  shortest path distances —  the maximization ob jective makes  sure  these  shortest 
path distances  are valid.  You  can  imagine physically holding up  the  source and  the  sink  and pulling 
them  apart  slowly.  The  ﬁrst  time  we  cannot  pull  any  further,  this  indicates  the  shortest  path  has 
been  reached. 
The  constraint  matrix  A  has  n2  rows  and  n  columns  of  ±1 or  0.  Each  row  ij   has  a  1  in  column  i, 
−1 in column  j ,  and  0  in  all  others.  Thus  we  can  write  the  primal  as  follows: 

T 
z  = min(c x) 
� 
=
cij xij 

i,j 
n � 
xjs   − xsj  =  −1 
j=1  
n � 
xj t   − xtj  = 1 
j=1  
n � 
xj i   − xij  = 0 

j=1  

∀i 

(cid:4)= s,   t  

But  this  is  simply  a  linear  program  for  a  minimum  cost  unit-ﬂow!  The  constraints  represent  the 
conservation  of  ﬂow  with  one  unit  of  ﬂow  going  into  the  sink  and  one  unit  coming  out  from  the 
source.  All  other  vertices  are  constrained  to  have  the  same  amount  of  ﬂow  coming  in  as  going  out. 
Thus any feasible solution to the linear program will be a feasible ﬂow.  The ob jective function simply 
tries  to  minimize  the  cost  of  this  ﬂow.  We  see  that  often  the  dual  of  a  LP  allows  us  to  understand 
the  problem  from  a  diﬀerent  (but  equivalent)  perspective. 

Lecture   15:   October   15,   2003  
15.4   The   Gravitational   Model  

15-4 

Consider  a  linear  program  min{cx   |  Ax  ≥  b}.  We  consider  a  hollow  polytope  deﬁned  by  a  set  of 
constraints.  Let c  be  the gravitation vector, pointing straight up.  We can put a ball  in the polytope, 
and  let  it  fall  to  the  bottom. 
∗
At  equilibrium  point  x  ,  the  forces  exerted  by  the  ﬂoors  are  balanced  by  the  gravitational  force. 
� 
The  normal  forces  by  the  ﬂoors  are  aligned  with  the  Ai ’s.  Therefore,  we  have  c  = 
yiAi  for  some 
is a feasible solution for max{yb   | yA   = c,   y   ≥ 0}. 
∗
nonnegative  force coeﬃcients yi .  In particular, y 
Since  the  forces  can  be  only  be  exerted  by  those walls  touching  the  ball, we  have  yi  = 0  if Aix >  bi . 
Therefore, we  have 
yi (aix  − bi ) = 0, 
� 
yi (aixi ) = cx,  

thus, 

yb   = 

which  means  that  y 

∗

is  dual  optimal. 

15.5  Complementary  Slackness 

The  above  example  leads  to  the  idea  of  complementary  slackness.  Given  feasible  solutions  x  and 
y ,  cx   − by   ≥  0  is  called  the  duality  gap.  The  solutions  are  optimal  if  and  only  if  the  gap  is  zero. 
Therefore,  the  gap  is  a  good measure  of  “how  far  oﬀ ”  we  are  from  the  optimum. 
Going  back  to  original  primal  and  dual  forms,  we  can  rewrite  the  dual:  yA   + s  =  c  for  some  s  ≥  0 
(that  is,  s  = cj  − yAj ). 

Theorem  2  The  fol lowings  are  equivalent  for  feasible  x  and  y : 
•  x  and  y  are  optimal 
•  sx   = 0 
•  xj sj  = 0  for  al l  j 
•  sj  >  0  implies  xj  = 0 

Proof:  First,  cx   = by   if  and  only  if 

(yA   + s)x  = (Ax)y ,  
thus  sx   = 0.  If  sx   = 0,  then  since  s,   x   ≥ 0, we have have  sj xj  = 0,  so  of  course sj  >  0  forces xj  = 0. 
The  converse  is  easy. 

The  basic  idea  of  complementary  slackness  is  that  an  optimum  solution  cannot  have  a  variable  xj 
and  corresponding  dual  constraint  sj  slack  at  same  time  —  one must  be  tight. 

This  can  be  stated  in  another  way: 

Lecture   15:   October   15,   2003  
Theorem  3  In  arbitrary  form  LPs,  feasible  points  optimal  if: 
yi (aix  − bi ) = 0  ∀i 
(cj  − yAj )xj  = 0  ∀j 

15-5 

Proof:  Note  that  in  the  deﬁnition  of  primal/dual,  feasibility  means  yi (aix  −  bi )  ≥  0 (since  ≥ 
constraint  corresponds  to  nonnegative  yi ).  Also,  (cj  − yAj )xj  ≥ 0,  thus 
� 
yi (aix  − bi ) + (cj  − yAj )xj  =  yAx   − yb   + cx   − yAx  
=  cx   − yb  
= 0 

at  optimum.  But  since  all  terms  are  nonnegative,  they  must  be  all  0. 

15.6  Examples  Using  Complementary  Slackness 

In  some  linear  optimization  problems,  we  can  gain  new  insight  by  investigating  its  primal  and  dual 
optimal  solutions  using  complementary  slackness.  We  are  going  to  give  two  examples.  In  the  ﬁrst 
example, we will  consider  the  LP  formulation of  the maximum  ﬂow problem.  Using  complementary 
slackness,  we  derive  the  Max-Flow  Min-Cut  Theorem.  In  the  second  example,  we  consider  the 
minimum cost circulation problem.  Using the linear programming framework, we give an alternative 
proof  of  the  complementary  slackness  property  introduced  in  lecture  13  (the  lecture  on  minimum 
cost  ﬂow). 

15.6.1  Max-ﬂow  Min-Cut  Theorem 

In  the  maximum  ﬂow  problem,  we  can  imagine  the  network  has  an  arc  (t,   s)  with  inﬁnite  capacity. 
And  we  are  maximizing  the  ﬂow  on  that  arc.  Therefore,  the  max  ﬂow  problem  can  be  written  as 
follows  (in  the  gross ﬂow  form): 

max  xts 
� 
xvw   − xwv   = 0 
xvw   ≤  uvw  
xvw   ≥  0 

w 

In the dual problem,  for  each node v  there  is a  conservation constraint.  Besides,  for  each edge  (v ,   w) 
there  is  a  capacity  constraint.  Therefore,  in  the  primal  formulation,  we  have  a  variable  zv  for  each 
conservation  constraint  and  a  variable  yvw   for  each  capacity  constraint.  The  primal  formulation  is 
therefore: 

Lecture   15:   October   15,   2003  

� 
min 
uvw yvw  
vw  
zv  − zw  + yvw   ≥  0 
zt  − zs  + yts  ≥  1 
yvw   ≥  0 

15-6 

We rewrite the ﬁrst set of constraints as yvw   ≥ zw  − zv .  Besides, the second constraint can be written 
as  zt  − zs  ≥ 1.  This  is  because  yts  = 0  in  any  optimal  solution.  If  yts  > 0  in  some  optimal  solution, 
the  fact  that  uts  = ∞  implies  that  utsyts  = ∞  and  therefore  the  optimal  value  is  unbounded.  This 
is  impossible  since  the max  ﬂow problem  is  never  infeasible  (in  particular,  the  zero  ﬂow  is  a  feasible 
solution). 

If  we  consider  yvw   as  the  edge  length  of  (v , w) and  zv  as  the  distance  from  s  to  v ,  we  can  interpret 
the  dual  problem as  follows:  Minimize  the  volume  of  the  network  by  tuning  the  edge  lengths,  subject 
to   the   constraint   that   the   distance   from   s  to   t is  at  least  1.  Here  the  volume  of network  is  deﬁned  as 
the  sum  of  edge  volumes,  which  is  the  product  of  edge  capacity  uvw   and  edge  length  yvw . 

Note that the optimal solution in this primal problem is at most the min-cut value of the network, as 
we can assign length 1 to the min-cut edges and 0 otherwise.  This satisﬁes the s-t distance constraint 
(because  any  s-t  path  has  to  traverse  some  edge  of  a  cut.)  The  value  of  this  solution  is  the  sum 
of  min-cut  edge  capacities.  By  strong  duality  this  implies  max-ﬂow  ≤  min-cut.  We  now  prove  the 
other  direction. 
∗
∗ 
∗
Denote zv , y  
as an optimal solution for the primal problem and similarly x 
for the dual problem. 
∈  T  and 
to  0.  Let  T  =  {v |z  ≥  1}. Note  that  s /
vw  
vw  
∗ 
∗
∗ 
Since  z  are  distances,  we  can  always  rescale  z 
t ∈ T .  Therefore  T  is  a  s − t cut. 
v 
s
v 
Now  consider  any  edge  (v , w)  crossing  the  cut: 

1.  if  v  ∈  T  and  w /∈  T , then  zv  ≥  1 and  z <  1.  Therefore,  zv  − z  + y ∗  ≥  z  − z  ∗  >  0.
∗
∗
∗ 
∗
∗
w 
w 
v
vw
w 
Therefore,  the  constraint   for  edge  (v , w)  in  the  primal  problem  is  slack.  By  complementary 
∗
∗
slackness,  the  variable  xvw   in  the  dual  problem  has  to  be  tight,  i.e.,  x  = 0. 
vw  
2.  if  v /∈ T  and  w  ∈  T , then  zw  ≥  1 and  z < 1.  It  follows  that  the  variable  y  ∗  ≥  z  ∗  − z  ∗  > 0.
∗
∗
Again,  by  complementary  slackness,  the  constraint   in  the  dual  problem  xvw   ≤  uvw   is  tight. 
v 
vw  
w
v  
∗
Therefore,  xvw   = uvw . 

In  other  words,  in  a  max  ﬂow,  all  edges  entering  T  is  saturated  and  all  edge  leaving  T  is  empty. 
Therefore,  in  a  max  ﬂow,  the  net  ﬂow  entering  T  equals  the  cut  value  of  T .  Since  the  ﬂow  value 
equals  the  amount  of  net  ﬂow  entering  any  s − t  cut,  the  max-ﬂow  value  equals  the  cut  value  of  T , 
which  is  at  least  the  min-cut  value.  As  a  result,  we  have  shown  that  max-ﬂow  ≥  min-cut,  which 
completes  the  proof  of  the Max-Flow Min-Cut  Theorem. 

