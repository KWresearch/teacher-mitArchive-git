MIT OpenCourseWare 
http://ocw.mit.edu

6.006 Introduction to Algorithms
Spring 2008

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

Lecture  2  Ver  2.0 

More  on  Document  Distance 

6.006  Spring  2008


Lecture  2:  More  on  the  Document  Distance

Problem


Lecture  Overview 

Today we will continue improving the algorithm for solving the document distance problem. 
•	 Asymptotic  Notation:  Deﬁne  notation  precisely  as  we  will  use  it  to  compare  the 
complexity  and  eﬃciency  of  the  various  algorithms  for  approaching  a  given  problem 
(here  Document  Distance). 
•	 Document  Distance  Summary  - place  everything  we  did  last  time  in  perspective. 
•	 Translate  to  speed  up  the  ‘Get Words  from  String’  routine. 
•	 Merge  Sort  instead  of  Insertion  Sort  routine 
–	 Divide  and  Conquer 
–	 Analysis  of  Recurrences 
•	 Get  rid  of  sorting  altogether? 

Readings 

CLRS  Chapter  4 

Asymptotic  Notation 

General  Idea 

For  any  problem  (or  input),  parametrize  problem  (or  input)  size  as  n  Now  consider  many 
diﬀerent  problems  (or  inputs)  of  size  n.  Then, 

T (n) =  worst  case  running  time  for  input  size  n 
= 
max 
running  time  on  X 
X :  Input  of  Size  n 
How  to make  this  more  precise? 
•	 Don’t  care  about  T (n)  for  small  n 
•	 Don’t  care  about  constant  factors  (these  may  come  about  diﬀerently  with  diﬀerent 
computers,  languages,  . . . ) 
For  example,  the  time  (or  the  number  of  steps)  it  takes  to  complete  a  problem  of  size  n 
might  be  found  to  be  T (n) = 4n2  − 2n + 2  µs.  From  an  asymptotic  standpoint,  since  n2 
will  dominate  over  the  other  terms  as  n  grows  large,  we  only  care  about  the  highest  order 
term.  We  ignore  the  constant  coeﬃcient  preceding  this  highest  order  term  as  well  because 
we  are  interested  in  rate  of  growth. 

1 

Lecture  2  Ver  2.0 

More  on  Document  Distance 

6.006  Spring  2008


Formal  Deﬁnitions 
1. 	 Upper  Bound:  We  say  T (n)  is  O(g(n))  if  ∃  n0 , ∃  c  s.t.  0 ≤ T (n) ≤ c.g(n)  ∀n ≥ n0 
Substituting  1  for  n0 ,  we  have  0 ≤ 4n2  − 2n + 2 ≤ 26n2  ∀n ≥ 1 
∴ 4n2  − 2n + 2 = O(n2 ) 
Some  semantics: 
•	 Read  the  ‘equal  to’  sign  as  “is”  or  �  belongs  to  a  set. 
•	 Read  the  O  as  ‘upper  bound’ 
2. 	 Lower  Bound:  We  say  T (n)  is  Ω(g(n))  if  ∃  n0 , ∃  d  s.t.  0 ≤ d.g(n) ≤ T (n)  ∀n ≥ n0 
Substituting  1  for  n0 ,  we  have  0 ≤ 4n2  + 22n − 12 ≤ n2  ∀n ≥ 1 
∴ 4n2  + 22n − 12 = Ω(n2 ) 
Semantics: 
•	 Read  the  ‘equal  to’  sign  as  “is”  or  �  belongs  to  a  set. 
• 
Read  the  Ω  as  ‘lower  bound’ 

3. 	 Order:  We  say  T (n)  is  Θ(g(n))  iﬀ  T (n) = O(g(n))  and  T (n) = Ω(g(n))

Semantics:  Read  the  Θ  as  ‘high  order  term  is  g(n)’


Document  Distance  so  far:  Review 

To  compute  the  ‘distance’  between  2  documents,  perform  the  following  operations: 

For  each  of  the  2  ﬁles: 

Read  ﬁle

Make  word  list 
Count  frequencies 
Sort  in  order 
Once  vectors  D1 ,D2  are  obtained: 
Compute  the  angle 

Θ(n2 )

+  op  on  list 
double  loop 
Θ(n2 )

�
� 
insertion  sort,  double  loop  Θ(n2 )

arccos  D1 ·D2 
�D1 �∗�D2 � 

Θ(n)

2


Lecture  2  Ver  2.0 

More  on  Document  Distance 

6.006  Spring  2008


The  following  table  summarizes  the  eﬃciency  of  our  various  optimizations  for  the  Bobsey 
vs.  Lewis  comparison  problem: 

Version  Optimizations 
V1 
initial 
add  proﬁling 
V2 
wordlist.extend(. . . ) 
V3 
V4 
dictionaries  in  count-frequency 
process  words  rather  than  chars  in  get  words  from  string 
V5 
merge  sort  rather  than  insertion  sort 
V6 
V6B 
eliminate  sorting  altogether 

Time  Asymptotic 
? 
? 
195  s 
→
84  s  Θ(n2 )  Θ(n)
→
41  s  Θ(n2 )  Θ(n)
→
13  s  Θ(n)  Θ(n)
→
6  s  Θ(n2 )  Θ(n lg(n))
1  s 
a  Θ(n)  algorithm 

The  details  for  the  version  5  (V5)  optimization will  not  be  covered  in  detail  in  this  lecture. 
The  code,  results  and  implementation  details  can  be  accessed  at  this  link. The  only  big 
obstacle  that  remains  is  to  replace  Insertion  Sort  with  something  faster  because  it  takes 
time Θ(n2 )  in  the worst  case.  This will  be  accomplished with  the Merge  Sort  improvement 
which  is  discussed  below. 

Merge  Sort 

Merge  Sort  uses  a  divide/conquer/combine  paradigm  to  scale  down  the  complexity  and 
scale  up  the  eﬃciency  of  the  Insertion  Sort  routine. 

Figure  1:  Divide/Conquer/Combine  Paradigm 

3 

input array of size nALRsortsortL’R’mergesorted array A2 arrays of size n/22 sorted arrays    of size n/2sorted array of size nLecture  2  Ver  2.0 

More  on  Document  Distance 

6.006  Spring  2008


Figure  2:  “Two  Finger” Algorithm  for Merge 
���� 
���� 
�  ��  � 
The  above  operations  give  us  T (n) =  C1 + 2.T (n/2) +  C.n
recursion  merge 
div ide 

Keeping  only  the  higher  order  terms, 
T (n) = 2T (n/2) + C n· 
=  C n + 2 × (C  n/2 + 2(C  (n/4) + . . .))
· 
·
· 

Detailed notes on implementation of Merge Sort and results obtained with this improvement 
are available here.  With Merge Sort,  the  running  time  scales “nearly  linearly” with  the  size 
of  the  input(s)  as  n lg(n)  is  “nearly  linear”in  n. 

An  Experiment 

Insertion  Sort  Θ(n2 ) 
Merge  Sort 
Θ(n lg(n))  if  n = 2i 
Built  in  Sort  Θ(n lg(n)) 
•  Test Merge  Routine:  Merge  Sort  (in  Python)  takes  ≈ 2.2n lg(n)  µs 
•  Test  Insert  Routine:  Insertion  Sort  (in  Python)  takes  ≈ 0.2n2  µs 
•  Built  in  Sort  or  sorted  (in  C)  takes  ≈ 0.1n lg(n)  µs 

The  20X  constant  factor  diﬀerence  comes  about  because Built  in  Sort  is written  in C while 
Merge  Sort  is  written  in  Python. 

4 

5473619234571269ij12345679inc jinc jinc iinc iinc iinc jinc iinc j(array L done)(array R done)Lecture  2  Ver  2.0 

More  on  Document  Distance 

6.006  Spring  2008


Figure  3:  Eﬃciency  of  Running  Time  for  Problem  of  size  n  is  of  order Θ(n lg(n)) 

Question:  When  is  Merge  Sort  (in  Python)  2n lg(n)  better  than  Insertion  Sort  (in  C) 
0.01n2? 
Aside:  Note  the  20X  constant  factor  diﬀerence  between  Insertion  Sort  written  in  Python 
and  that  written  in  C 
Answer:  Merge  Sort  wins  for  n ≥ 212  = 4096 
Take  Home  Point:  A  better  algorithm  is much more  valuable  than  hardware  or  compiler 
even  for modest  n 

See  recitation  for more  Python  Cost Model  experiments  of  this  sort  . . . 

5


CnC(n/2)C(n/2)C(n/4)CCCnCnCnCnCn}lg(n)+1   levelsincluding leavesT(n) = Cn(lg(n)+1)         = Θ(nlgn)