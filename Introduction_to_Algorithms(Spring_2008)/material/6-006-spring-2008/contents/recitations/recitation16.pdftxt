MIT OpenCourseWare 
http://ocw.mit.edu 

6.006 Introduction to Algorithms 
Spring 2008 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

Recitation 16  Notes  
These notes are supposed to be complementary to the lecture notes. Everything that 
we covered in section is contained either here or there.
 
D y n am i c  
P r o g r amm i n g  
Ou1.2.tline
  Introduction to Dynamic Programming
 
  Memoization: computing Fibonacci numbers
 
 
Good news: Dynamic Programming (DP) is used heavily in optimization problems. 
IApplications range from financial models and operations research to biology and 
basic algorithm research. So understanding DP is profitable. 
ntroduction to Dynamic Programming 
Bad news: DP is not 
an algorithm or a data structure. You don’t get to walk away 
with algorithms that you can memorize.
 
 of a problem. 
DP takes advantage of the 
A problem has an optimal suboptimal sub­structure
‐structure if the optimum answer to the problem 
‐problems. 
Ocontains optimum answers to smaller sub
ptimal sub‐structure 
 u  v is a minimum‐cost 
For instance, in the minimum‐path problem, suppose s 
path from s to v. This implies that s 
 u is a minimum‐cost path from s to u, and can 
 v, and this would yield a better path between  
be proved by contradiction. If there is a better path between s and u, we can replace
s 
 u with the better path in s 
 u 
s and v
. But we assumed that s  u 
 v is an optimal path between s and v, so we 
have a contradiction.
How do you take advantage of the optimal sub
‐structure? DP comes as as a bunch of 

 
tricks, and I think the only way to master it is to understand DP solutions to 
problems.
 
 
In DP, the answer to the same sub
‐problem is used multiple times. Computing the 
same emanswer multiple times can be very inefficient (as demonstrated in the next 
problem), so DP solutions store and reuse the answers to sub
‐problems. This is 
M
oization 
called memoization. 

Memoization i s  very  simple t o  implement in high-level languages like Python. The 
easiest approach  is to  use  a dictionary tha t  persists across  function calls (no t  a local 
variable O ) to  s to r e  pre-computed results. The dictionary keys a r e  parame ters  t o  
the  function, and  the  values a r e  the  resu l t  of  the  computation. 

At th e  beginning of  th e  function call, check if  th e  parame ters   (n for  th e  Fibonacci 
example) exist in the  dictionary. I f  tha t  is the  case, re turn  th e  associated value, 
bypassing the  computation logic completely. Otherwise, execute the  computation 
logic, and  s to re  the  resu l t  in the  dictionary before re turn ing  from th e  function. 

Fibonacci numbers 
A dramatic example of  the  usefulness of  memoization is computing Fibonacci 
numbers. This is done according t o  th e  following formula: 

A straightforward implementation  of  this computation would take  very  long t o  run .  
The running time for F(n) is exponential in n. We can see  very  quickly this by 
observing tha t  all the  leaves of  the  recursion t r e e  re tu rn  0 o r  1,so  the  t r e e  for F(n) 
will have a t  least F(n) leaves, and  F (n )- 9%~. 
As  Srini said in lecture, exponential = bad, polynomial = good. I f  w e  d raw  th e  
recursion  t ree  for  F(5) w e  see  a lo t  of waste: F(3) is re-computed twice, and  F(2) is 
re-computed 4 times. 

Memoization addresses  th is  was te  by storing F(n) once it is computed, and  reusing 
i ts  value. With memoization in place, computing F(n) takes O(n). The easiest way to  
visualize this resu l t  is re-drawing the  recusion t ree  above, unde r  th e  assumption of 
memoization. 

I  have used * to  deno te  calls to  F(n] th a t  re tu rn  immediately by re-using a 
previously computed results. 

As  a last word on th e  Fibonacci problem  and  memoization, notice th a t  both the  
original and  the  memoized  implementation have a recursion  dep th  of O(n). 

