6.252 NONLINEAR PROGRAMMING 


LECTURE 16:  PENALTY METHODS


LECTURE OUTLINE


•  Quadratic Penalty Methods 
•  Introduction to Multiplier Methods 
******************************************* 
•  Consider the equality constrained problem 
minimize  f (x) 
subject to  x ∈ X, 
h(x) = 0, 
where f  : (cid:2)n(cid:160) → (cid:2)  and h : (cid:2)n(cid:160) → (cid:2)m(cid:160) are continuous, 
and X  is closed. 
•  The quadratic penalty method: 
(cid:2) 
ck(cid:160)(cid:5)h(x)(cid:5)2 
L ck(cid:160)(x, λk ) ≡ f (x) +  λk
x
k(cid:160) = arg min 
h(x) + 

x∈X(cid:160)
2 
where  the  {λk }  is  a  bounded  sequence  and  {ck }
satisﬁes  0 < ck(cid:160) < ck+1  for all k and ck(cid:160) → ∞. 

TWO CONVERGENCE MECHANISMS


•  Taking λk(cid:160) close to a Lagrange multiplier vector 
−	 Assume  X  =  (cid:2)n(cid:160) and  (x 
∗
∗
)  is  a  local min-
, λ
Lagrange  multiplier  pair  satisfying  the  2nd 
order sufﬁciency  conditions 
−  For  c  suff.  large,  x 
∗  is  a  strict  local  min  of 
Lc (·, λ
∗
)

•	 Taking ck(cid:160) very  large 
−  For large c and any λ 
(cid:1) 
if x ∈ X  and h(x) = 0  
f (x) 
∞ 
otherwise 

Lc (·, λ) ≈


•  Example: 
minimize  f (x) =  1  2
2 
2 (x1 + x2 )
subject to  x1 = 1  
2 (x1 + x2 ) +  λ(x1 − 1) +
Lc (x, λ) =  1  2
2 
c − λ 
x1 (λ, c) = 

,
c + 1  

x2 (λ, c) = 0  

(x1 − 1)2 
c 
2

EXAMPLE CONTINUED


2
2 
min x1 + x2 ,
x1=1 

∗
x  = 1,

λ  = −1 
∗ 

x2 

x2 

c = 1 
λ  = 0 

c = 1 
λ  = - 1/2 

0 

1/2 

1 

x1 

0 

3/4 

1 

x1 

x2 

x2 

c = 1 
λ  = 0 

c = 10 
λ  = 0 

0 

1/2 

1 

x1 

0 

10/11 

1 

x1

GLOBAL CONVERGENCE


∗  = 

•  Every limit point of {xk }  is a global min. 
Proof:  The  optimal  value  of  the  problem  is  f
inf h(x)=0, x∈X(cid:160)L ck(cid:160)(x, λk ). We have 
∀  x ∈ X 
L ck(cid:160)(x k , λk ) ≤ L ck(cid:160)(x, λk ), 
so taking the inf of the RHS over x ∈ X , h(x) = 0  
(cid:2) 
ck(cid:160)(cid:5)h(x k )(cid:5)2 ≤ f 
∗ . 
h(x k ) +  
2 
x, λ) be a  limit point of  {xk , λk }.  Without  loss
Let  (¯  ¯ 
of generality, assume that {xk , λk } →  (¯  ¯ 
x, λ). Taking
the limsup above 

L ck(cid:160)(x k , λk ) =  f (x k ) +  λk

(cid:5)h(x k )(cid:5)2 ≤ f . 
∗ 
(cid:2)
ck(cid:160)
x) + ¯  x) + lim sup 
f (¯ 
h(¯
λ
k→∞  2 
Since  (cid:5)h(xk )(cid:5)2  ≥  0  and  ck(cid:160) → ∞, it follows  that 
h(xk ) →  0 and  h(¯ 
x) = 0.  Hence,  ¯
x  is  feasible,  and 
x) ≤  f 
∗ , ¯
since from Eq. (*) we have f (¯ 
x  is optimal. 
Q.E.D. 

(*) 

LAGRANGE MULTIPLIER ESTIMATES


•  Assume  that  X  =  (cid:2)n ,  and  f  and  h  are  cont. 
differentiable.  Let  {λk }  be bounded, and  ck(cid:160) → ∞. 
Assume xk(cid:160) satisﬁes  ∇xL ck(cid:160)(xk , λk ) = 0  for all k, and 
that xk(cid:160) → x 
∗  is such that ∇h(x 
∗ , where x 
∗
) has rank 
) = 0  and ˜λk(cid:160) → λ
∗ , where 
∗
m.  Then h(x 
∇xL(x 
∗ , λ 
) = 0.

∗ 
˜ 
λk(cid:160) = λk(cid:160)+ c k h(x k ), 

Proof:  We have 
(cid:3) 
(cid:2) 
0 =  ∇xLck(cid:160)(x(cid:160)k , λk ) =  ∇f (x(cid:160)k ) +  ∇h(x(cid:160)k )  λk(cid:160)+ c(cid:160)k h(x(cid:160)k )

= ∇f (x(cid:160)k ) +  ∇h(x(cid:160)k )˜λk(cid:160).(cid:160)
(cid:2) 
(cid:3)−1 
Multiply with 
(cid:2)∇h(x k )  ∇h(x k )
∇h(x k )
and take lim to obtain ˜λk(cid:160) → λ
∗  with 
(cid:2) 
(cid:3)−1 ∇h(x
∗  = −

(cid:2)∇h(x  ) 
∇h(x

∗
∗ 
∗
λ

∗
) = 0  and h(x 

) = 0  (since 


∗
, λ

We  also have ∇xL(x 
∗
˜
λk(cid:160) converges).


(cid:2) 

)

(cid:2)∇f (x  ).
∗ 

)

PRACTICAL BEHAVIOR

•  Three possibilities: 
−	 The method breaks down because an xk(cid:160)with 
∇xL ck(cid:160)(xk , λk ) ≈ 0 cannot be found. 
−	 A sequence {xk } with ∇xL ck(cid:160)(xk , λk ) ≈ 0 is ob-
tained, but it either has no limit points, or for 
∗  the  matrix  ∇h(x 
∗
each  of  its  limit  points  x 
) 
has rank < m. 
−	 A  sequence  {xk }  with with ∇xL ck(cid:160)(xk , λk ) ≈  0 
∗  such that 
is found and  it has a  limit point x 
(cid:4) 
(cid:5) 
∇h(x 
∗  together with λ
∗
∗ 
) has rank m.  Then, x 
[the corresp. limit point of
 λk(cid:160)+ ck h(xk )
 ] sat-

isﬁes the ﬁrst-order necessar
y conditions. 
• 
Ill-conditioning:  The  condition  number  of  the 
Hessian ∇2 
xxL ck(cid:160)(xk , λk ) tends to increase with ck . 
•  To  overcome ill-conditioning: 
−	 Use Newton-like method (and double preci-
sion). 
−  Use good star ting points. 
−	 Increase  ck(cid:160) at  a  moderate  rate  (if  ck(cid:160) is  in-
creased at a fast rate, {xk } converges faster, 
but the likelihood of ill-conditioning is greater). 

INEQUALITY CONSTRAINTS


•  Conver t  them  to  equality  constraints  by  using 
squared slack variables that are eliminated later. 
•  Conver t inequality constraint gj(cid:160)(x) ≤ 0 to equality 
2 = 0. 
constraint gj(cid:160)(x) +  zj(cid:160)
•  The penalty method solves problems of the form 

¯minLc (x, z , λ, µ) =  f (x) 
(cid:6) (cid:1) 
x,z(cid:160)
(cid:2) 
(cid:3) 
r(cid:160)
2  +
µj(cid:160) gj(cid:160)(x) +  zj(cid:160)
+	

j=1 

(cid:7) 
2 |2 
|gj(cid:160)(x) +  zj(cid:160)
c 
, 
2 

for various values of µ and c. 
•  First minimize  ¯Lc (x, z , λ, µ) with respect to z , 

¯
Lc (x, λ, µ) =  min Lc (x, z , λ, µ) =  f (x) 
(cid:1) 
(cid:6) 
(cid:2) 
(cid:3) 
z(cid:160)
r(cid:160)
2  +
min  µj(cid:160) gj(cid:160)(x) +  zj(cid:160)
+ 
zj(cid:160)
j=1 

(cid:7) 
2 |2 
|gj(cid:160)(x) +  zj(cid:160)
c 
2 

and then minimize Lc (x, λ, µ) with respect to x.


MULTIPLIER METHODS


•  Recall  that  if  (x 
∗
∗
)  is  a  local  min-Lagrange 
, λ
multiplier pair satisfying the 2nd order suf ﬁciency 
∗  is a strict local 
conditions, then for c suff. large, x 
min of Lc (·, λ
∗
). 
∗ , xk(cid:160) ≈ x 
•  This suggests that for λk(cid:160) ≈ λ
∗ . 
•  Hence it is a good idea to use λk(cid:160) ≈ λ
∗ , such as 

λk+1 = ˜λk(cid:160) = λk(cid:160)+ c k h(x k ) 

This is the (1st order) method of multipliers. 
•  Key advantages to be shown: 
−	 Less ill-conditioning:  It is not necessary that 
ck(cid:160) → ∞  (only  that  ck(cid:160) exceeds  some  thresh-
old). 
−	 Faster convergence when λk(cid:160) is updated than 
when λk(cid:160) is kept constant (whether ck(cid:160) → ∞ or 
not). 

