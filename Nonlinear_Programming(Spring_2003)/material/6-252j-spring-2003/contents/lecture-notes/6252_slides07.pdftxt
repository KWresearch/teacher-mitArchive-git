6.252 NONLINEAR PROGRAMMING 


LECTURE 7:  ADDITIONAL METHODS


LECTURE OUTLINE

•  Least-Squares Problems and Incremental Gra-
dient Methods 
•  Conjugate Direction Methods 
•  The Conjugate Gradient Method 
•  Quasi-Newton Methods 
•  Coordinate Descent Methods 
•  Recall the least-squares problem: 
(cid:1) 
m

(cid:1)gi (x)(cid:1)2

i=1


(cid:1)g(x)(cid:1)2 =  1 
minimize  f (x) =  1
2 
2
sub ject to  x ∈ (cid:3)n , 
where g = (g1 , . . . , gm ), gi  : (cid:3)n → (cid:3)ri(cid:160). 

INCREMENTAL GRADIENT METHODS 
•  Steepest descent method 

xk+1 = xk − αk∇f (xk ) =  xk − αk


(cid:1) 
m

∇gi (xk )gi (xk )

i=1

•  Incremental gradient method: 
ψi  = ψi−1 − αk∇gi (ψi−1 )gi (ψi−1 ),

 = 1, . . . , m  

i

ψ0 = xk ,

xk+1 = ψm 

(a i x - bi )2 

x * 

mini

a i 
bi 
Advantage of incrementalism


R

a i
max i b i 

x

VIEW AS GRADIENT METHOD W/ ERRORS

•  Can write incremental gradient method as 
(cid:1) 
m

xk+1 = xk − αk

∇gi (xk )gi (xk )

i=1

(cid:1) (cid:2) 
(cid:3) 
m

∇gi (xk )gi (xk ) − ∇gi (ψi−1 )gi (ψi−1 )


+ αk 

i=1

•  Error term is propor tional to stepsize αk 
•  Convergence (generically) for a diminishing step-
size (under a Lipschitz condition on ∇gi gi ) 
•  Convergence to a “neighborhood”
for a constant 
stepsize 

CONJUGATE DIRECTION METHODS

•  Aim  to  improve  convergence  rate  of  steepest 
descent,  without  incurring  the  overhead  of  New-
ton’ s method 
•  Analyzed for a quadratic model.  They require n 
iterations to minimize f (x) = (1/2)x(cid:1)Qx − b(cid:1)x with 
Q an n × n positive de ﬁnite matr ix Q >  0. 
•  Analysis also applies to nonquadratic problems 
in the neighborhood of a nonsingular local min 
•  Directions d1 , . . . , dk are Q-conjugate , if di
(cid:1)
Qdj  = 
0 for all i (cid:6)= j 
•  Generic  conjugate  direction  method:  xk+1  = 
xk + αk dk  where the dk s are Q-conjugate and αk 
is obtained by line minimization 

y0 

y1 

y2 

w0 

x0 

x1 

x2 

d0 = Q -1/2w0 

w1 

d1 = Q -1/2w1 
Expanding Subspace Theorem 

GENERATING Q-CONJUGATE DIRECTIONS 
•  Given set of linearly independent vectors ξ 0 , . . . , ξ k , 
we can construct a set of Q-conjugate directions 
d0 , . . . , dk  s.t. S pan(d0 , . . . , di ) =  S pan(ξ 0 , . . . , ξ i ) 
•  Gram-Schmidt  procedure .  Star t  with  d0  =  ξ 0 . 
If  for some  i < k , d0 , . . . , di  are Q-conjugate and 
the above proper ty holds, take 

(cid:1) 
i

di+1 = ξ i+1 +

c(i+1)mdm ;

m=0


choose c(i+1)m so di+1 is Q-conjugate to d0 , . . . , di ,

(cid:5)(cid:1) 
(cid:4) 
(cid:1) 
i

c(i+1)mdm

m=0


Qdj  = ξ i+1

Qdj +


Qdj  = 0.


(cid:1)

di+1

(cid:1)

d1= ξ1 + c10d0 

ξ1 

d2= ξ2 + c20d0 + c21d1 

ξ2 

0


- c10d0 

ξ0 = d0 

0


d1 

d0 

CONJUGATE GRADIENT METHOD 

•  Apply Gram-Schmidt  to  the vectors ξ k  = gk  = 
∇f (xk ), k = 0, 1, . . . , n  − 1 
k−1 
(cid:1)
(cid:1) 
dk  = −gk + 
Qdj 
gk
(cid:1)
dj 
Qdj 
dj 
j=0 
•  Key fact:  Direction formula can be simpli ﬁed. 
Proposition  :  The  directions  of  the  CGM  are 
generated by d0 = −g0 , and 
dk  = −gk + β k dk−1 ,

k = 1, . . . , n  − 1, 

where β k  is given by 
(cid:1) gk 
(cid:1) gk−1 

gk
gk−1

β k  = 

or  β k  =

(gk − gk−1 )(cid:1) gk 
(cid:1) gk−1 
gk−1

Fur thermore, the method terminates with an opti-
mal solution after at most n steps. 
•  Extension to nonquadratic problems. 

QUASI-NEWTON METHODS

•  xk+1  =  xk  −  αkDk∇f (xk ),  where  Dk  is  an 
inverse Hessian approximation 
•  Key idea:  Successive iterates xk , xk+1 and gra-
dients ∇f (xk ), ∇f (xk+1 ), yield curvature info 
qk  ≈ ∇2 f (xk+1 )pk , 
pk  = xk+1 − xk ,
qk  = ∇f (xk+1 ) − ∇f (xk ). 
(cid:7)−1 
(cid:7)(cid:6) 
(cid:6) 
∇2 f (xn ) ≈  q0  · · ·  qn−1  p0  · · ·  pn−1 
•  Most popular Quasi-Newton method is a clever 
way to implement this idea 
(cid:1)  − 
Dk+1 = Dk + pk pk
(cid:1) qk 
pk

+ ξ k τ k vk vk

(cid:1) , 

(cid:1)

Dk qk qk
Dk 
(cid:1)
Dk qk 
qk
Dk qk ,  0 ≤ ξ k  ≤ 1 

(cid:1)

vk  =  pk  − 
Dk qk 
(cid:1) qk 
,
τ k 
pk
and D0  >  0 is  arbitrary,  αk  by  line minimization, 
and Dn  = Q−1  for a  quadratic. 

τ k  = qk

NONDERIVATIVE METHODS

•  Finite difference implementations 
•  Forward and central difference formulas 
1 (cid:2) 
(cid:3) 
∂ f (xk ) ≈ 
f (xk + hei ) − f (xk )
∂xi 
h 
1  (cid:2) 
(cid:3) 
∂ f (xk ) ≈ 
f (xk + hei ) − f (xk − hei )
2h 
∂xi 
•  Use central difference  for more accuracy near 
convergence 

xk+2 

xk+1 

xk 

•  Coordinate descent. 
Applies also to the case 
where there are bound 
constraints on the vari-
ables. 

•  Direct search methods.  Nelder-Mead method. 

PROOF OF CONJUGATE GRADIENT RESULT

•  Use induction to show that all gradients gk  gen-
erated up to termination are linearly independent. 
True  for  k  = 1.  Suppose  no  termination  after  k 
steps,  and  g0 , . . . , gk−1  are  linearly  independent. 
Then,  S pan(d0 , . . . , dk−1 ) =  S pan(g0 , . . . , gk−1 ) 
and there are two possibilities: 
−  gk  = 0, and the method terminates. 
−	 gk  (cid:6)= 0, in  which  case  from  the  expanding 
manifold proper ty 
gk  is orthogonal to d0 , . . . , dk−1 
gk  is orthogonal to g0 , . . . , gk−1 
so gk  is linearly independent of g0 , . . . , gk−1 , 
completing the induction. 
•  Since at most n lin. independent gradients can 
be generated, gk  = 0  for some k ≤ n. 
•  Algebra to verify the direction formula. 

