6.252 NONLINEAR PROGRAMMING 


LECTURE 17:  AUGMENTED LAGRANGIAN METHODS 


LECTURE OUTLINE


•  Multiplier Methods 
******************************************* 
•  Consider the equality constrained problem 
minimize  f (x) 
subject to  h(x) = 0, 
where f  : (cid:1)n  → (cid:1) and h : (cid:1)n  → (cid:1)m are continuously 
differentiable. 
•  The (1st order) multiplier method  ﬁnds 
(cid:3) 
ck (cid:4)h(x)(cid:4)2 
L ck(cid:160)(x, λk ) ≡ f (x) +  λk
k  = arg  min 
h(x) + 

x

x∈(cid:2)n(cid:160)
2 

and updates λk  using 

λk+1 = λk + c k h(x k ) 

CONVEX EXAMPLE


•  Problem:  minx1=1 (1/2)(x2
2 ) with optimal so-
1 + x2 
∗  = −1. 
∗  = (1, 0) and Lagr. multiplier λ
lution x 
•  We have 
(cid:1) 
(cid:2) 
ck − λk 
x k  = arg  min  L ck(cid:160)(x, λk ) =  
, 0 
x∈(cid:2)n(cid:160)
ck + 1  
(cid:1) 
(cid:2) 
ck − λk 
− 1 
ck + 1  
λk − λ
∗ 
ck + 1  

λk+1 − λ 
∗  = 

λk+1 = λk + c k 

•  We see that: 
−	 λk  →  λ
∗  =  −1  and  xk  →  x 
∗  = (1, 0)  for  ev-
ery nondecreasing sequence {ck }. It is NOT 
necessary to increase ck  to ∞. 
(cid:4) 
(cid:3) 
−  The convergence rate becomes faster as ck 
becomes larger ; in fact  |λk − λ
∗ |  converges 
superlinearly if ck  → ∞. 

NONCONVEX EXAMPLE 


•  Problem:  minx1=1 (1/2)(−x2
2 ) with optimal so-
1 + x2 
∗  = (1, 0) and Lagr. multiplier λ
∗  = 1. 
lution x 
•  We have 
(cid:1) 
(cid:2) 
ck − λk 
L ck(cid:160)(x, λk ) = 

ck − 1 
, 0

k  = arg  min 
x

x∈(cid:2)n(cid:160)

provided ck  > 1 (otherwise the min does not exist)

(cid:2) 
(cid:1) 
ck − λk

− 1

ck − 1


λk+1 = λk + c k


λk+1 − λ 
∗  = −


λk − λ
∗

ck − 1


•  We see that: 
−	 No need to increase ck  to ∞ for convergence; 
doing so results in faster convergence rate. 
−	 To  obtain  convergence,  ck  must  eventually 
exceed the threshold 2. 

THE PRIMAL FUNCTIONAL


•  Let (x 
∗
∗
) be a regular local min-Lagr. pair sat-
, λ
isfying the 2nd order suff. conditions are satis ﬁed. 
•	 The primal functional 
p(u) =   min  f (x), 
h(x)=u


deﬁned  for u in an open sphere centered at u = 0, 
and we have 
∇p(0) = −λ

∗ ), 
∗ ,

p(0) = f (x


p(u) = 

1 
2

(u + 1)2

p(u) = -

1 
2

(u + 1)2

p(0) = f(x*) =  1 
2 

-1 

0 

u 

-1 

0 

u 

p(0) = f(x*) = - 1 
2 

(a) 

(b) 

p(u) =   min  1  2
2 
2 (x1 + x2 ),
x1−1=u


p(u) =   min 
x1−1=u 

2 (−x1 + x2 ) 
2 
2
1 

AUGM. LAGRANGIAN MINIMIZATION 

•  Break down the minimization of Lc (·, λ): 
(cid:6) 
(cid:5) 
h(x) +   �h(x)�2 
(cid:3)
c 
(cid:6) 
(cid:5) 
Lc (x, λ) =  min 
min

min

f (x) +  λ
2 
x 
u 
h(x)=u 
�u�2

(cid:3) u +

c

= min

p(u) +  λ
,

2

u 

where  the  minimization  above  is  understood  to 
be local in a neighborhood of u = 0. 
•  Interpretation of this minimization: 

Penalized Primal Function 
c 
||u||2 
p(u) + 
2

Slope = - λ 

Slope = - λ * 

p(0) = f(x*) 

- λ'u(λ ,c) 

Primal Function 
p(u) 

min Lc (x,λ ) 
x 

0 
u(λ ,c) 
u
•  If  c  is  suf.  large,  p(u) +  λ
(cid:4)u(cid:4)2  is  convex  in 
(cid:3)
u +  c 
a neighborhood of 0.  Also, for λ ≈  λ
∗  and large c, 
2 
the value minx Lc (x, λ) ≈ p(0) = f (x 
∗
). 

INTERPRETATION OF THE METHOD 

•  Geometric interpretation of the iteration 

λk+1 = λk + c k h(x k ). 

p(u) +  c  ||u||2 
2 

Slope = - λ k+1 

Slope = - λ k 

p(0) = f(x*) 

Slope = - λ * 
Slope = - λ k+2 

p(u) 

Slope = - λ k+1 = ∇p(uk ) 

min Lc k+1 (x,λ k+1) 
x 

min Lc k(x,λ k ) 
x 

uk+1 
uk 
u
0 
•  If  λk  is  sufﬁciently  close  to  λ
∗  and/or  ck  is  suf. 
∗  than λk . 
large, λk+1  will be closer to λ
•  ck  need  not  be  increased  to ∞  in  order  to  ob-
tain convergence; it is suf ﬁcient  that ck  eventually 
exceeds some threshold level. 
•  If p(u) is linear, convergence to λ
∗ will be achieved 
in one iteration. 

COMPUTATIONAL ASPECTS


•  Key issue is how to select {ck }. 
−	 ck  should eventually become larger than the 
“threshold”  of the given problem. 
−	 c0  should  not  be  so  large  as  to  cause  ill-
conditioning at the 1st minimization. 
−	 ck  should  not  be  increased  so  fast  that  too 
much  ill-conditioning  is  forced upon  the un-
constrained minimization too early. 
−	 ck  should  not  be  increased  so  slowly  that 
the multiplier iteration has poor convergence 
rate. 
•  A  good  practical  scheme  is  to  choose  a mod-
erate  value  c0 ,  and  use  ck+1  =  β ck ,  where  β  is  a 
scalar with  β >  1  (typically  β  ∈  [5, 10]  if  a Newton-
like method is used). 
•  In practice  the minimization of L ck(cid:160)(x, λk )  is  typ-
ically  inexact  (usually  exact  asymptotically). 
In 
some  variants  of  the  method,  only  one  Newton 
step per minimization is used (with safeguards). 

DUALITY FRAMEWORK 

•  Consider the problem 
minimize  f (x) +   (cid:4)h(x)(cid:4)2 
c
2 
∗ (cid:4) < �,  h(x) = 0, 
subject to  (cid:4)x − x 

where  �  is  small  enough  for  a  local  analysis  to 
hold based on the implicit function theorem, and c 
is large enough for the minimum to exist. 
•  Consider the dual function and its gradient 
(cid:8) 
(cid:7) 
qc (λ) =   min  Lc (x, λ) =  Lc 
x(λ, c), λ 

(cid:5)x−x 
∗ (cid:5)<� 
(cid:8) 
(cid:7) 
(cid:7) 
(cid:8) 
(cid:7) 
(cid:8) 
∇qc (λ) =  ∇λ x(λ, c)∇xLc

x(λ, c), λ 
 + h  x(λ, c)

= h

x(λ, c)
 .

We have ∇qc (λ
) = 0  and ∇2 qc (λ
∗
∗
∗
) > 0. 
) =  h(x
•  The multiplier method is a steepest ascent iter-
ation for maximizing qck(cid:160)
λk+1 = λk + c k∇qck(cid:160)(λk ), 

