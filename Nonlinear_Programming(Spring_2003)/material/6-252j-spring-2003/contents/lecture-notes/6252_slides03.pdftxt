6.252 NONLINEAR PROGRAMMING 

LECTURE 3:  GRADIENT METHODS 

LECTURE OUTLINE 
•  Quadratic Unconstrained Problems 
•  Existence of Optimal Solutions 
•  Iterative Computational Methods 
•  Gradient Methods - Motivation 
•  Principal Gradient Methods 
•  Gradient Methods - Choices of Direction 

QUADRATIC UNCONSTRAINED PROBLEMS


2 x(cid:3)Qx − b(cid:3)x, 
min f (x) =  1 
x∈(cid:2)n(cid:160)
where Q is n × n symmetric, and b ∈ (cid:2)n . 

•  Necessary conditions: 
∇f (x ∗ ) =  Qx∗  − b = 0, 
∇2 f (x ∗ ) =  Q ≥ 0 :  positive semide ﬁnite. 
•  Q ≥ 0  ⇒  f  :  convex, nec. conditions are also 
sufﬁcient, and  local minima are also global 
•  Conclusions: 
−  Q :  not  ≥ 0  ⇒  f has no local minima 
−	 If Q >  0 (and hence  inver tible),  x ∗  = Q−1 b 
is the unique global minimum. 
−	 If Q ≥ 0 but not inver tible, either no solution 
or ∞ number of solutions 

y 

y 

α > 0,  β > 0 
(1/α, 0) is the unique 
global minimum 

α = 0 
There is no global minimum 

0 

1 /α 

x 

0

x 

y 

α > 0,  β = 0 
{(1/α ,  ξ) | ξ :  real} is the set of 
global minima 

y 

α > 0,  β < 0 
There is no global minimum 

0 

1 /α 

x 

0 

1 /α 

x 

Illustration  of  the  isocost  surfaces  of  the  quadratic  cost 
function f(cid:160) : (cid:1)2  (cid:2)→ (cid:1)  given by 
(cid:2) 
(cid:1) 
f (x, y) =  1  αx2 + βy2  − x
2 

for various values of α(cid:160)and β .


EXISTENCE OF OPTIMAL SOLUTIONS •

Consider 

min f (x) 
x∈X 

Two possibilities: 
(cid:3) 
(cid:4) 
•  The  set  f (x)  |  x ∈  X 
is  unbounded  below, 
and there is no optimal solution 
(cid:4) 
(cid:3) 
•  The set  f (x) | x ∈ X  is bounded below 
−  A  global  minimum  exists  if  f  is  continuous 
and X  is compact (Weierstrass theorem) 
−	 A global minimum exists if X  is closed, and 
f is coercive, that is, f (x) → ∞ when (cid:8)x(cid:8) → 
∞ 

GRADIENT METHODS - MOTIVATION•

∇f(x) 

x 

xα  = x - α∇f(x) 

f(x) =  1 
c

f(x) =  2 < c1 
c

c
f(x) =  3 < c2 

x - δ∇f(x) 

If ∇f (x)  (cid:5)= 0,  there  is  an 
interval  (0, δ) of  stepsizes 
such that 
(cid:1) 
(cid:2) 
f x(cid:160)− α∇f (x)  < f (cid:160)(x)

for all α(cid:160)∈  (0, δ). 

∇f(x) 

x 

xα  = x + α d 

f(x) =  1 
c

f(x) =  2 < c1 
c

f(x) =  3 < c2 
c

x + δd 

d 

If  d(cid:160) makes  an  angle  with 
∇f (x) that is greater than 
90 degrees, 
∇f(cid:160)(x)

d < (cid:160)0,(cid:160)

�

there  is  an  interval  (0, δ) 
of stepsizes such that f(cid:160)(x+ 
αd)  < f (cid:160)(x)  for  all  α(cid:160) ∈ 
(0, δ). 

PRINCIPAL GRADIENT METHODS•

k = 0, 1, . . .  
xk+1 = xk + αk dk ,
where, if ∇f (xk ) (cid:9)= 0, the direction dk  satisﬁes 
∇f (xk )(cid:3)dk  < 0, 

and αk  is a positive stepsize.  Principal example: 
xk+1 = xk − αkDk∇f (xk ), 

where Dk  is a positive de ﬁnite symmetr ic matrix 
•	 Simplest method:  Steepest descent 
xk+1 = xk − αk∇f (xk ),
k = 0, 1, . . .  
•  Most sophisticated method:  Newton’ s method 
(cid:1) 
(cid:2)−1 ∇f (xk ),
xk+1 = xk−αk
 ∇2 f (xk ) 

k = 0, 1, . . . 


STEEPEST DESCENT AND NEWTON’S METHOD•

x0 

Slow convergence of steep­
est descent 

f(x) =  1 
c

Quadratic Approximation of f at x0 

c
f(x) =  2 < c1 

f(x) =  3 < c2 
c

. 
x0 

x1 
. 

. 
x2 

Quadratic Approximation of f at x1 

Fast  convergence  of New-
ton’s method w/ αk(cid:160) = 1. 

Given xk(cid:160), the method ob­
tains xk+1 as the minimum 
of a quadratic approxima­
tion  of  f(cid:160) based  on  a  sec­
ond order Taylor expansion 
around xk(cid:160). 

OTHER CHOICES OF DIRECTION 
•  Diagonally Scaled Steepest Descent 
(cid:1) 
(cid:2)−1 
∇2 f (xk ) 

Dk  =  Diagonal approximation to

•	 Modi ﬁed Ne wton ’ s Method 
Dk  = (∇2 f (x0 ))
−1 ,
k = 0, 1, . . . ,  
•  Discretized Newton’ s Method 
(cid:1) 
(cid:2)−1 
Dk  =

H (xk ) 
,

k = 0, 1, . . . ,  

where H (xk ) is a  ﬁnite-diff erence based approxi-
mation of ∇2 f (xk ), 
•	 Gauss-Newton method for least squares prob-
(cid:8)g(x)(cid:8)2 .  Here 
lems minx∈(cid:2)n(cid:160) 2 
1
(cid:1) 
(cid:2)−1 
∇g(xk )∇g(xk )(cid:3) 
Dk  =

,

k = 0, 1, . . .

