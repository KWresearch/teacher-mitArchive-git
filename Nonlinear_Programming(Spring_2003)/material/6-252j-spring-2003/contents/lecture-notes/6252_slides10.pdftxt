6.252 NONLINEAR PROGRAMMING 


LECTURE 10 


ALTERNATIVES TO GRADIENT PROJECTION


LECTURE OUTLINE

•  Three Alternatives/Remedies for Gradient Pro-
jection 
−  Two-Metric Projection Methods 
−  Manifold Suboptimization Methods 
−  Afﬁne Scaling  Methods 

Scaled GP method with scaling matrix H k(cid:160) > 0: 
xk+1 = xk(cid:160)+ αk (xk(cid:160)− xk ), 
(cid:2) 
(cid:1) 
x(cid:160)k(cid:160) = arg min  ∇f(cid:160)(x(cid:160)k )
H k (x(cid:160)− x(cid:160)k )  .(cid:160)
(x(cid:160)− x(cid:160)k ) +  
(x(cid:160)− x(cid:160)k )
1 
x∈X(cid:160)
2sk(cid:160)
•  The QP direction subproblem is complicated by: 
−  Difﬁcult
inequality (e.g., nonor thant) constraints 
−  Nondiagonal H k , needed for Newton scaling 

(cid:2)

(cid:2)

THREE WAYS TO DEAL W/ THE DIFFICULTY

•  Two-metric projection methods: 
(cid:3) 
(cid:4)
xk(cid:160)− αkDk∇f (xk ) 
+ 
xk+1 =

−	 Use Newton-like scaling but use a standard 
projection 
−	 Suitable  for  bounds,  simplexes,  Car tesian 
products of simple sets, etc 
•  Manifold suboptimization methods: 
−	 Use (scaled) gradient projection on the man-
ifold of active inequality constraints 
−	 Need strategies to cope with changing active 
manifold (add-drop constraints) 
−  Each QP subproblem is equality-constrained 
•  Af ﬁne Scaling  Methods 
−  Go through the interior of the feasible set 
−	 Each QP subproblem is equality-constrained, 
AND we don’t have to deal with changing ac-
tive manifold 

TWO-METRIC PROJECTION METHODS 

•  In their simplest form, apply to constraint:  x ≥ 0, 
but generalize to bound and other constraints 
•  Like unconstr. gradient methods except for [·]+ 
(cid:4)
(cid:3) 
xk+1 =  xk(cid:160)− αkDk∇f (xk ) 
+ 
Dk(cid:160) > 0 
,
•  Major difﬁculty:  Descent  is not guaranteed  for 
Dk :  arbitrary 

∇f(xk) 

xk 

xk 

xk - αkDk∇f(xk ) 

xk - αkDk∇f(xk ) 

(a) 
(b) 
•  Remedy:  Use Dk(cid:160) that is diagonal w/ respect to 
indices that “are activ e and want to stay active” 
(cid:5) (cid:5) 
(cid:2) 
(cid:1) 
I + (xk ) =   i (cid:5) xk
i(cid:160) = 0, ∂ f (xk )/∂xi(cid:160) > 0


PROPERTIES OF 2-METRIC PROJECTION

•  Suppose Dk(cid:160) is diagonal with respect to I + (xk ), 
ij(cid:160) = 0  for i, j ∈ I + (xk ) with i (cid:4)= j , and let 
i.e., dk(cid:160)
(cid:3) 
(cid:4)
xk(cid:160)− αDk∇f (xk ) 
+ 
xk (a) = 

−  If xk(cid:160) is stationary, xk(cid:160) = xk (α) for all α >  0.

(cid:6) 
(cid:7) 
−  Otherwise f
 x(α)
 < f (xk ) for all sufﬁciently

small α >  0 (can use Armijo rule). 
•  Because  I + (x) is  discontinuous w/  respect  to 
x, to guarantee convergence we need  to  include 
in  I + (x) constraints  that  are  “ �-active”  [those  w/ 
∈ [0, �] and ∂ f (xk )/∂xi(cid:160) > 0]. 
xk
i(cid:160)
•  The  constraints  in  I + (x ∗ )  eventually  become 
active and don’t  matter. 
•  Method  reduces  to unconstrained Newton-like 
method on the manifold of active constraints at x ∗ . 
•  Thus,  superlinear  convergence  is  possible  w/ 
simple projections. 

MANIFOLD SUBOPTIMIZATION METHODS

•  Feasible direction methods for 
(cid:2) x ≤ bj(cid:160), j  = 1, . . . , r  
min f (x) 
subject to aj(cid:160)
•  Gradient is projected on a linear manifold of ac-
tive constraints rather than on the entire constraint 
set (linearly constrained QP). 

x0 

x1 

x2 

x0 

x1 

x3 

x4 

x2 

x3 
•  Searches through sequence of manifolds, each 
(a) 
(b) 
differing by at most one constraint from the next. 
•  Potentially many iterations to identify the active 
manifold; then method reduces to (scaled) steep-
est descent on the active manifold. 
•  Well-suited  for  a  small  number  of  constraints, 
and for quadratic programming. 

OPERATION OF MANIFOLD METHODS 
•  Let A(x) =  {j  |  aj(cid:160)
(cid:2) x = bj(cid:160)}  be  the active  index 
set at x.  Given xk , we ﬁnd 
∇f (xk )(cid:2)d + 1 
2 d(cid:2)H k d 
dk(cid:160) = arg 
min 
j(cid:160)d=0, j∈A(xk(cid:160)) 
� 
a
•  If dk(cid:160) (cid:4)= 0, then dk(cid:160) is a feasible descent direction. 
Perform feasible descent on the current manifold. 
•  If  dk(cid:160) = 0,  either  (1)  xk(cid:160) is  stationary  or  (2)  we 
enlarge the current manifold (drop an active con-
straint).  For this, use the scalars µj(cid:160) such that 
(cid:8) 
∇f (xk ) +  
µj(cid:160)aj(cid:160) = 0  
j∈A(xk(cid:160)) 
If  µj(cid:160) ≥  0  for  all  j ,  xk(cid:160) is 
stationary, since for all fea-
(x − xk ) is 
sible x, ∇f(cid:160)(xk )
(cid:2)
a1 
(cid:8) 
equal to 
(cid:2) (x−x(cid:160)k ) ≥ 0 
∇f(xk )  − 
µj(cid:160)aj(cid:160)
j∈A(xk(cid:160)) 

(µ1 < 0) 
- µ1a1 

- µ2a2 
(µ2 > 0) 
a1'x = b1 

a2 

xk 

a2'x = b2 

X 

Else,  drop  a  constraint  j(cid:160)
with µj(cid:160) <(cid:160)0. 

AFFINE SCALING METHODS FOR LP

•  Focus on the LP minAx=b, x≥0 c(cid:2)x, and the scaled 
gradient projection xk+1 = xk(cid:160)+ αk (xk(cid:160)− xk ), with 
H k (x(cid:160)− x(cid:160)k ) 
(x(cid:160)− x(cid:160)k ) +  
(x(cid:160)− x(cid:160)k )
(cid:2)
(cid:2)
1 
k(cid:160) = arg  min 
x(cid:160)
c(cid:160)
Ax=b, x≥0 
2sk(cid:160)
•  If xk(cid:160) > 0 then xk(cid:160) > 0 for  sk(cid:160) small enough,  so 
xk(cid:160) = xk(cid:160)− sk (H k )−1 (c − A(cid:2)λk ) with 
(cid:6) 
(cid:7)−1 
A(H k )−1A(cid:2)  A(H k )−1 c 
λk(cid:160) =

Lumping sk(cid:160) into αk : 
xk+1 = xk(cid:160)− αk (H k )−1 (c − A(cid:2)λk ), 

where αk(cid:160) is small enough to ensure that xk+1 > 0 

{x | Ax = b, x ≥  0} 

x0 

x1 

x2 

x3 

x * 

Importance of using time-
varying  H k(cid:160) (should  bend 
xk−xk(cid:160) away from the bound­
ary) 

AFFINE SCALING 
•  Par ticularly interesting choice (af ﬁne scaling) 
H k(cid:160) = (X k )−2 , 

where X k(cid:160) is the diagonal matrix having the (pos-
itive) coordinates xk
i(cid:160) along the diagonal: 
(cid:7)−1 
(cid:6) 
x(cid:160)k+1 = x(cid:160)k−αk (X k )2 (c−A
(cid:2) 
λk ), λk(cid:160) =  A(X k )2A
A(X k )2 c(cid:160)

(cid:2)

•  Corresponds to unscaled gradient projection it-
−1 x.  The vector xk(cid:160)
eration in the variables y(cid:160)= (X k )
is mapped onto the unit vector yk(cid:160) = (1, . . . , (cid:160)1). 

x * 

xk+1 

xk 

y *= (Xk )-1 x * 

yk= (Xk )-1 xk 

yk+1 

yk = (1,1,1) 

•  Extensions, convergence, practical issues. 

