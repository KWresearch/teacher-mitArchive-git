6.252 NONLINEAR PROGRAMMING 


LECTURE 6


NEWTON AND GAUSS-NEWTON METHODS


LECTURE OUTLINE

•  Newton’ s Method 
•  Convergence Rate of the Pure Form 
•  Global Convergence 
•  Variants of Newton’ s Method 
•  Least Squares Problems 
•  The Gauss-Newton Method 

NEWTON ’S METHOD


(cid:1) 
(cid:2)−1 ∇f (xk )

xk+1 = xk − αk
 ∇2 f (xk ) 

assuming that the Newton direction is de ﬁned and 
is a direction of descent 
•  Pure form of Newton’ s method (stepsize = 1) 
(cid:2)−1 
(cid:1) 
xk+1 = xk −

∇2 f (xk )  ∇f (xk ) 
−  Very  fast when it converges (how fast?) 
−	 May  not  converge  (or  worse,  it may  not  be 
deﬁned)  when star ted far from a nonsingular 
local min 
−	 Issue:  How  to  modify  the  method  so  that 
it  converges  globally,  while maintaining  the 
fast convergence rate 

CONVERGENCE RATE OF PURE FORM

•  Consider solution of nonlinear system g(x) = 0  
where g(cid:160): (cid:2)n  (cid:3)→ (cid:2)n , with method 
(cid:1) 
(cid:2)−1 
xk+1 = xk − ∇g(xk )(cid:1) 
g(xk ) 
−  If g(x) =  ∇f (x), we get pure form of Newton 
•  Quick derivation: Suppose xk  → x(cid:160)∗ with g(x(cid:160)∗ ) =  
0 and ∇g(x(cid:160)∗ ) is inver tible.  By Taylor 
(cid:2) 
(cid:1) 
0 =  g(x(cid:160)∗ ) =  g(xk )+∇g(xk )(cid:1) (x(cid:160)∗−xk )+o(cid:160) (cid:5)xk−x(cid:160)∗ (cid:5)  .(cid:160)
(cid:1) 
(cid:2)−1 : 
Multiply with  ∇g(xk )(cid:1) 
(cid:1) 
(cid:2) 
(cid:1) 
(cid:2)−1 
xk − x(cid:160)∗  − ∇g(xk )(cid:1) 
g(xk ) =  o(cid:160) (cid:5)xk − x(cid:160)∗ (cid:5)  ,(cid:160)

so 

(cid:1) 
(cid:2) 
xk+1 − x(cid:160)∗  = o(cid:160) (cid:5)xk − x(cid:160)∗ (cid:5)  ,(cid:160)
implying superlinear convergence and capture.


CONVERGENCE BEHAVIOR OF PURE FORM


g(x) = e x - 1 

k 

0 
1 
2 
3 
4 
5 

xk 

g(xk ) 

- 1.00000 
0.71828 
0.20587 
0.01981 
0.00019 
0.00000 

- 0.63212 
1.05091 
0.22859 
0.02000 
0.00019 
0.00000 

x0 = -1 

0 

x2 

x1 

x 

g(x) 

x3 

x1

0 

x0 

x2

x 

MODIFICATIONS FOR GLOBAL CONVERGENCE

•(cid:160) Use a stepsize 
•(cid:160) Modify the Newton direction when: 
−(cid:160) Hessian is not positive de ﬁnite 
−(cid:160) When Hessian is nearly singular (needed to 
improve performance) 
•(cid:160) Use 

(cid:1) 
(cid:2)−1 
∇2 f (xk ) + ∆k  ∇f (xk ), 
dk  = −(cid:127)

whenever  the Newton direction does not exist or 
is not a descent direction.  Here ∆k  is a diagonal 
matrix such that 
∇2 f (xk ) + ∆k  ≥(cid:160) 0 
−(cid:160) Modiﬁed Cholesky  factorization 
−(cid:160) Trust region methods 

LEAST-SQUARES PROBLEMS


(cid:5)g(x)(cid:5)2 =  1 
minimize  f (x) =  1
2 
2
sub ject to  x ∈ (cid:2)n , 
where g = (g1 , . . . , gm ), gi  : (cid:2)n  → (cid:2)ri(cid:160). 

m (cid:3) 
(cid:5)gi (x)(cid:5)2

i=1


••Many applications: 
−(cid:160) Model Construction – Cur ve Fitting 
−(cid:160) Neural Networks 
−(cid:160) Pattern Classiﬁcation 

THE GAUSS-NEWTON METHOD


•(cid:160)

Idea:  Linearize around the current point xk 
g˜(x, xk ) =  g(xk ) +  ∇g(xk )(cid:1) (x −(cid:160) xk ) 

and minimize  the norm of  the  linearized  function 
g˜: 

(cid:5)g˜(x, xk )(cid:5)2 
xk+1 = arg min  2 
1
x∈(cid:4)n(cid:160)
(cid:2)−1 
(cid:1) 
= xk − ∇g(xk )∇g(xk )(cid:1)  ∇g(xk )g(xk ) 
•(cid:160) The direction 
(cid:1) 
(cid:2)−1 
− ∇g(xk )∇g(xk )(cid:1)  ∇g(xk )g(xk ) 

is a descent direction since 
(cid:1) 
(cid:2) 
∇g(xk )g(xk ) =  ∇(cid:160) (1/2)(cid:5)g(x)(cid:5)2 
∇g(xk )∇g(xk )(cid:1)  > 0


MODIFICATIONS OF THE GAUSS-NEWTON 
•(cid:160) Similar to those for Newton’ s method: 
(cid:2)−1 
(cid:1) 
∇g(xk )∇g(xk )(cid:1)+∆k  ∇g(xk )g(xk ) 
xk+1 = xk −αk


where αk  is a stepsize and ∆k  is a diagonal matrix 
such that 

∇g(xk )∇g(xk )(cid:1)  + ∆k  > 0 
•(cid:160) Incremental version of the Gauss-Newton method: 
−(cid:160) Operate in cycles 
−(cid:160) Star t a cycle with ψ0  (an estimate of x) 
−(cid:160) Update ψ using a single component of g 
i (cid:3) 
(cid:5)g˜ j (x, ψj−1 )(cid:5)2 , i = 1, . . . , m,(cid:127)
j=1


ψi  = arg min 
x∈(cid:4)n(cid:160)

where g˜ j  are the linearized functions

g˜ j (x, ψj−1 ) =g j (ψj−1 )+∇gj (ψj−1 )(cid:1) (x−ψj−1 )


MODEL CONSTRUCTION 

•(cid:160) Given set of m input-output data pairs  (yi , zi ), 
i = 1, . . . , m, from the physical system 
•(cid:160) Hypothesize an input/output relation z = h(x, y), 
where x is a vector of unknown parameters, and 
h is known 
•(cid:160) Find x that matches best the data in the sense 
that it minimizes the sum of squared errors 

m (cid:3) 
(cid:5)zi  −(cid:160) h(x, yi )(cid:5)2


1

2


i=1

•(cid:160) Example of a linear model:  Fit the data pairs by 
a cubic polynomial approximation.  Take 

h(x, y) =  x3 y3 + x2 y2 + x1 y + x0 , 

where x = (x0 , x1 , x2 , x3 ) is the vector of unknown 
coefﬁcients of the cubic  polynomial. 

NEURAL NETS

•(cid:160) Nonlinear  model  construction  with  multilayer 
perceptrons 
•(cid:160) x of the vector of weights 
•(cid:160) Universal approximation proper ty 

PATTERN CLASSIFICATION 
•(cid:160) Objects  are  presented  to  us,  and  we  wish  to 
classify them in one of s categories 1, . . . , s, based 
on a vector y of their features. 
•(cid:160) Classical  maximum  posterior  probability  ap-
proach:  Assume we know 
p(j |y) =  P (object w/ feature vector y is of category j ) 

Assign object with feature vector y to category 
p(j |y). 
j ∗ (y) =  arg  max 
j=1,...,s 
If  p(j |y) are  unknown,  we  can  estimate  them 
•(cid:160)
using functions hj (xj , y) parameterized by vectors 
xj .  Obtain xj  by minimizing 
(cid:3)(cid:1) 
(cid:2)
m
i  −(cid:160) hj (xj , yi )
zj

1 
2 

2

, 

where 

i=1 
(cid:4) 
1 
if yi  is of category j , 
z i  = 
j 
0  otherwise. 

