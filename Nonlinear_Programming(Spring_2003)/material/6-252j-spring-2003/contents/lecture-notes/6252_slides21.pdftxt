6.252 NONLINEAR PROGRAMMING 


LECTURE 21:  DUAL COMPUTATIONAL METHODS


LECTURE OUTLINE


•  Dual Methods 
•	 Nondifferentiable Optimization 

******************************** 
•  Consider the primal problem 

minimize  f (x)

gj(cid:160)(x) ≤ 0,
subject to  x ∈ X, 
assuming −∞ < f  
∗  < ∞. 
•	 Dual problem:  Maximize 

j = 1, . . . , r,


q(µ) =  inf  L(x, µ) =  inf 
x∈X(cid:160)
x∈X
subject to µ ≥ 0.


{f (x) +  µ g(x)}
(cid:3) 

PROS AND CONS FOR SOLVING THE DUAL


•  The dual is concave. 
•  The  dual  may  have  smaller  dimension  and/or 
simpler constraints. 
•  If there is no duality gap and the dual is solved 
∗ , all optimal pri-
exactly for a Lagrange multiplier µ 
mal  solutions  can be obtained by minimizing  the 
) over x ∈ X . 
∗
Lagrangian L(x, µ 
•  Even  if  there  is  a  duality  gap,  q(µ)  is a lower 
bound to the optimal primal value for every µ ≥ 0. 
•  Evaluating  q(µ)  requires minimization of  L(x, µ) 
over x ∈ X . 
•  The dual function is often nondifferentiable. 
•  Even if we ﬁnd an optimal dual solution µ 
∗ , it may 
be difﬁcult to obtain  a primal optimal solution. 

STRUCTURE


•  Separability:  Classical  duality  structure  (La-
grangian relaxation). 
•  Par titioning:  The problem 
minimize  F (x) +  G(y)

subject to  Ax + By  = c,  x ∈ X,  y ∈ Y


can be written as 

minimize  F (x) +  
inf 
By=c−Ax, y∈Y(cid:160)
subject to  x ∈ X. 

G(y) 

With no duality gap, this problem is written as 
minimize  F (x) +  Q(Ax) 
subject to  x ∈ X, 

where 

Q(Ax) =  max q(λ, Ax) 
λ(cid:160)
� 
� 
(Ax + By  − c) 
(cid:3)
q(λ, Ax) =  inf  G(y) +  λ
y∈Y(cid:160)

DUAL DERIVATIVES


•  Let 

� 
� 
(cid:3) 
xµ(cid:160) = arg min L(x, µ) =  arg min  f (x) +  µ g(x)  . 
x∈X
x∈X(cid:160)
Then for all µ ∈ (cid:5)r , 
� 
� 
(cid:3) 
µ) =  inf  f (x) + ˜
q( ˜  
µ g(x) 
x∈X(cid:160)
≤ f (xµ ) + ˜µ g(xµ ) 
(cid:3) 
µ − µ)
(cid:3) 
= f (xµ ) +  µ g(xµ ) + ( ˜
= q(µ) + ( ˜µ − µ)
(cid:3) g(xµ ). 

(cid:3) g(xµ ) 

•  Thus g(xµ ) is a subgradient of q  at µ. 
•  Proposition:  Let X  be compact, and let f  and g 
be continuous over X .  Assume also that for every 
µ, L(x, µ) is minimized over x ∈ X at a unique point 
xµ .  Then,  q  is  everywhere  continuously  differen-
tiable and 

∇q(µ) =  g(xµ ), 

∀  µ ∈ (cid:5)r(cid:160). 

NONDIFFERENTIABLE DUAL


•  If there exists a duality gap, the dual function is 
nondifferentiable at every dual optimal solution. 
• 
Impor tant  nondifferentiable  case:  When  q  is 
polyhedral, that is, 

� 
� 
(cid:3) 
q(µ) =  min  aiµ + bi(cid:160) , 
i∈I(cid:160)
index  set,  and  ai(cid:160) ∈ (cid:5)r(cid:160) and  bi(cid:160)
where  I  is  a  ﬁnite 
are  given  (arises when X  is  a  discrete  set,  as  in 
integer programming). 
•  Proposition:  Let q be polyhedral as above, and 
let Iµ(cid:160) be the set of indices attaining the minimum 
� 
� 
Iµ(cid:160) =  i ∈ I  | aiµ + bi(cid:160) = q(µ)  . 
(cid:3) 

The set of all subgradients of q at µ is 

 � 
�  � 
ξi ai , ξi(cid:160) ≥ 0,
∂ q(µ) =   g  � g = 

i∈Iµ 

i∈Iµ


 
 
� 
ξi(cid:160) = 1   . 



NONDIFFERENTIABLE OPTIMIZATION


•  Consider maximization  of  q(µ) over M  = {|  µ ≥ 
0, q(µ) > −∞} 
•  Subgradient method: 
� 
�+k(cid:160)
k+1 =  µ k(cid:160)+ s k(cid:160)g

µ

,
where  gk(cid:160) is  the  subgradient  g(x µk ),  [·]+  denotes 
projection on the closed convex set M , and sk(cid:160) is a 
positive scalar stepsize. 

Contours of q 

M 

µ* 

µk 

[µk  + skgk ]+ 

gk 

µk  + skgk


KEY SUBGRADIENT METHOD PROPERTY 


•  For a  small  stepsize  it  reduces  the  Euclidean 
distance to the optimum. 

Contours of q 

M 

µk 

< 90 o 

µ* 

gk 
µk+1 = [µk  + skgk ]+ 

µk  + skgk 

•  Proposition:  For  any  dual  optimal  solution  µ 
∗ , 
we have 
(cid:8)µ k+1 − µ 
∗ (cid:8) < (cid:8)µ k(cid:160)− µ  (cid:8), 
∗ 
for all stepsizes sk(cid:160) such that 
� 
� 
) − q(µk )

∗
2
 q(µ 
(cid:8)gk (cid:8)2


0 < sk(cid:160) <


.


STEPSIZE RULES


•  Diminishing stepsize is one possibility. 
•  More common method: 
� 
αk(cid:160) qk(cid:160)− q(µk )
(cid:8)gk (cid:8)2 

k(cid:160)s  = 

� 

,

where qk(cid:160) ≈ q 
∗  and 

0 < αk(cid:160) < 2. 

•  Some possibilities: 
−	 qk(cid:160) is the best known upper bound to q 
∗ ; α0 = 1  
and  αk(cid:160) decreased by a  cer tain  factor every 
few iterations. 
−  αk(cid:160) = 1  for all k and 
� 
� 
q k(cid:160) =  1 +  β (k) ˆk(cid:160),q 

where  ˆqk(cid:160) =  max0≤i≤k(cid:160)q(µi ),  and  β (k)  >  0  is 
adjusted depending on algorithmic progress 
of the algorithm. 

