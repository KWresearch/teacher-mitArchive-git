6.252 NONLINEAR PROGRAMMING 


LECTURE 4


CONVERGENCE ANALYSIS OF GRADIENT METHODS


LECTURE OUTLINE

•  Gradient Methods - Choice of Stepsize 
•  Gradient Methods - Convergence Issues 

CHOICES OF STEPSIZE I 

•	 Minimization Rule:  αk(cid:160) is such that 

f (xk(cid:160)+ αk dk ) =  min f (xk(cid:160)+ αdk ).(cid:160)
α≥0 
•  Limited Minimization Rule:  Min over α(cid:160)∈ [0, s] 
•  Armijo rule: 

Set of Acceptable 
Stepsizes 

Unsuccessful 
Trials 

Stepsize 

0 

× 
β2s
Stepsize α k = 

× 
βs 

× 
s 

α 

f(xk + α dk ) - f(xk ) 

α∇f(xk )'dk 

σα∇f(xk )'dk 

Start with s(cid:160)and continue with β s, β 2 s, ..., until βm s(cid:160) falls 
within the set of α(cid:160)with 
f (x(cid:160)k ) − f(cid:160)(x(cid:160)k(cid:160)+ αdk ) ≥ −σα∇f(cid:160)(x(cid:160)k )

dk(cid:160).(cid:160)

(cid:1)

CHOICES OF STEPSIZE II 
•	 Constant stepsize:  αk(cid:160) is such that 

αk(cid:160) = s(cid:160):  a constant 
•  Diminishing stepsize: 
αk(cid:160) → 0 

but satisﬁes the in ﬁnite tr avel condition 
∞ 
(cid:1) 
αk(cid:160) = ∞

k=0 

GRADIENT METHODS WITH ERRORS

xk+1 = xk(cid:160)− αk (∇f (xk ) +  ek ) 

where ek(cid:160) is an uncontrollable error vector 
•  Several special cases: 
−	 ek(cid:160) small  relative  to  the  gradient;  i.e.,  for  all 
k , (cid:5)ek (cid:5) <(cid:160)(cid:5)∇f (xk )(cid:5) 

∇f(xk ) 

ek 

gk 

Illustration of the descent 
property  of  the  direction 
gk(cid:160) = ∇f(cid:160)(xk ) +  ek(cid:160). 

− {ek }  is  bounded,  i.e.,  for  all  k ,  (cid:5)ek (cid:5) ≤  δ,(cid:160)
where δ(cid:160)is some scalar. 
− {ek }  is propor tional  to  the  stepsize,  i.e.,  for 
all k , (cid:5)ek (cid:5) ≤ qαk(cid:160),(cid:160)where q(cid:160)is some scalar. 
− {ek } are independent zero mean random vec-
tors 

CONVERGENCE ISSUES

•  Only  convergence  to  stationary  points  can  be 
guaranteed 
•  Even convergence to a single limit may be hard 
to guarantee (capture theorem) 
•  Danger of nonconvergence if directions dk(cid:160) tend 
to be or thogonal to ∇f (xk ) 
•  Gradient related condition: 
For any subsequence {xk }k∈K  that converges to 
a  nonstationary  point,  the  corresponding  subse-
quence {dk }k∈K  is bounded and satis ﬁes 
lim sup  ∇f (xk )(cid:5)dk(cid:160) <(cid:160)0.(cid:160)
k→∞, k∈K


•  Satisﬁed if  dk(cid:160) = −Dk∇f (xk ) and the eigenval-
ues of Dk(cid:160) are bounded above and bounded away 
from zero 

CONVERGENCE RESULTS


CONSTANT AND DIMINISHING STEPSIZES 

Let {xk }  be a sequence generated by a gradient 
method xk+1 = xk(cid:160)+ αk dk , where {dk } is gradient 
related.  Assume  that  for  some  constant  L > (cid:160)0, 
we have 
(cid:5)∇f (x) − ∇f (y)(cid:5) ≤ L(cid:5)x(cid:160)− y(cid:5),(cid:160)

∀ x, y(cid:160)∈ (cid:8)n(cid:160),(cid:160)

Assume that either 
(1)  there exists a scalar (cid:160)such that for all k(cid:160)
(2 − )|∇f (xk )(cid:5)dk |
0 <  (cid:160)≤ αk(cid:160) ≤ 
L(cid:5)dk (cid:5)2 

or 
(cid:2)∞ 
k=0 αk(cid:160) = ∞.(cid:160)
(2)  αk(cid:160) → 0 and 
Then  either  f (xk )  → −∞  or  else  {f (xk )}  con-
verges to a ﬁnite v alue and ∇f (xk ) → 0. 

MAIN PROOF IDEA


α∇f(xk )'dk  + (1/2)α 2L||dk ||2 

α  = |∇f(x k)'d k| 
L||d k|||2  
× 

0 

α 

α∇f(xk )'dk 

f(xk + α dk ) - f(xk ) 

The idea of the convergence proof for a constant stepsize. 
Given  xk(cid:160) and  the  descent  direction  dk(cid:160),  the  cost  diﬀer-
ence  f(cid:160)(xk(cid:160) + αdk ) − f(cid:160)(xk ) is ma jorized by α∇f(cid:160)(xk )
(cid:1)
dk(cid:160) + 
2 α2L(cid:3)dk (cid:3)2  (based  on  the Lipschitz  assumption;  see next 
1 
slide). Minimization of this function over α(cid:160)yields the step- 
size 
|∇f(cid:160)(xk )
dk |
(cid:1)
L(cid:3)dk (cid:3)2 
This stepsize reduces the cost function f(cid:160) as well. 

α(cid:160)= 

DESCENT LEMMA


Let α(cid:160)be a scalar and let g(α) = f (x(cid:160)+ αy). Have 
(cid:3) 

f (x(cid:160)+ y) − f (x) = g(1) − g(0) = 
1(cid:160) dg
(α) dα(cid:160)
dα(cid:160)
(cid:3) 
0(cid:160)
y (cid:5)∇f (x(cid:160)+ αy) dα(cid:160)
1(cid:160)
= 
0(cid:160)(cid:3) 
≤ 
y (cid:5)∇f (x) dα(cid:160)
1(cid:160)
0(cid:4)(cid:3) 
(cid:4)(cid:4) 
(cid:4) 
(cid:6)
1(cid:160) (cid:5) 
y (cid:5)  ∇f (x(cid:160)+ αy) − ∇f (x)  dα(cid:160)(cid:4)(cid:4) 
+ (cid:4)(cid:4) 
0(cid:160)(cid:3) 
≤ 
y (cid:5)∇f (x) dα(cid:160)
1(cid:160)
0(cid:160)(cid:3) 
(cid:5)y(cid:5) · (cid:5)∇f (x(cid:160)+ αy) − ∇f (x)(cid:5)dα(cid:160)
1(cid:160)
+ 
(cid:3) 
0(cid:160)
≤ y (cid:5)∇f (x) + (cid:5)y(cid:5) 
Lα(cid:5)y(cid:5) dα(cid:160)
1(cid:160)
0(cid:160)
(cid:5)y(cid:5)2 .(cid:160)
= y (cid:5)∇f (x) + 
L(cid:160)
2 

CONVERGENCE RESULT – ARMIJO RULE
Let {xk } be generated by xk+1 = xk +αk dk , where
{dk } is gradient related and αk is chosen by the
Armijo rule. Then every limit point of {xk } is sta-
tionary.
Proof Outline: Assume x is a nonstationary limit
point. Then f (xk ) → f (x), so αk∇f (xk )(cid:5)dk → 0.
• If {xk }K → x, lim supk→∞, k∈K ∇f (xk )(cid:5)dk < 0,
by gradient relatedness, so that {αk }K → 0.
• By the Armijo rule, for large k ∈ K
(cid:6)
(cid:5)
< −σ(αk /β )∇f (xk )(cid:5)dk .
f (xk ) − f
xk + (αk /β )dk
Deﬁ ning pk = dk(cid:6)dk (cid:6) and αk = αk (cid:6)dk (cid:6)
β
f (xk ) − f (xk + αk pk )
< −σ∇f (xk )(cid:5)pk .
αk
Use the Mean Value Theorem and let k → ∞.
We get −∇f (x)(cid:5)p ≤ −σ∇f (x)(cid:5)p, where p is a limit
point of pk – a contradiction since ∇f (x)(cid:5)p < 0.

, we have

