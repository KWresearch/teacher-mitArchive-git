6.252 NONLINEAR PROGRAMMING 


LECTURE 5:  RATE OF CONVERGENCE


LECTURE OUTLINE

•  Approaches for Rate of Convergence Analysis 
•  The Local Analysis Method 
•  Quadratic Model Analysis 
•  The Role of the Condition Number 
•  Scaling 
•  Diagonal Scaling 
•  Extension to Nonquadratic Problems 
•  Singular and Dif ﬁcult Prob lems 

APPROACHES FOR RATE OF


CONVERGENCE ANALYSIS


•  Computational complexity approach 
•  Informational complexity approach 
•  Local analysis 
•  Why we will focus on the local analysis method 

THE LOCAL ANALYSIS APPROACH

•  Restrict attention  to sequences xk(cid:160) converging 
to a local min x(cid:160)∗ 
•  Measure progress in terms of an error function 
e(x) with e(x(cid:160)∗ ) = 0, such as 
e(x) =  (cid:1)x(cid:160)− x(cid:160)∗ (cid:1),
e(x) =  f (x) − f (x(cid:160)∗ ) 
•  Compare the tail of the sequence e(xk ) with the 
tail of standard sequences 
•  Geometric or linear convergence [if e(xk ) ≤ qβ k(cid:160)
for some q > (cid:160)0 and β(cid:160)∈ [0,(cid:160)1), and for all k ].  Holds 
if 
lim sup e(xk+1 ) 
k→∞ 
e(xk ) 
•  Superlinear convergence  [if e(xk ) ≤  q(cid:160)· β p(cid:160)
for 
k(cid:160)
some q > (cid:160)0, p > (cid:160)1 and β(cid:160)∈ [0,(cid:160)1), and for all k ]. 
•  Sublinear convergence 

< β (cid:160)

QUADRATIC MODEL ANALYSIS 
•  Focus on the quadratic function f (x) = (1/2)x(cid:4)Qx, 
with Q > (cid:160)0. 
•  Analysis also applies to nonquadratic problems 
in the neighborhood of a nonsingular local min 
•  Consider steepest descent 
xk+1 = xk(cid:160)− αk∇f (xk ) = (I(cid:160)− αkQ)xk(cid:160)
(cid:1)xk+1 (cid:1)2 = xk
(I(cid:160)− αkQ)2xk(cid:160)
(cid:4)
(cid:1) 
(cid:2) 
≤  max eig. (I(cid:160)− αkQ)2  (cid:1)xk (cid:1)2 
The eigenvalues of (I(cid:160)− αkQ)2  are equal  to (1 − 
αk λi )2 , where λi(cid:160) are the eigenvalues of Q, so 
(cid:4) 
(cid:3) 
max eig of (I −αkQ)2 = max  (1−αkm)2 ,(cid:160)(1−αkM )2 

where m, M(cid:160) are  the  smallest  and  largest  eigen-
values of Q.  Thus 
(cid:1)xk+1 (cid:1) 
(cid:4) 
(cid:3)
≤ max  |1 − αkm|,(cid:160)|1 − αkM |
(cid:1)xk (cid:1) 

OPTIMAL CONVERGENCE RATE

•  The  value  of  αk(cid:160) that  minimizes  the  bound  is 
α∗  = 2/(M(cid:160)+ m),(cid:160)in which case 
(cid:1)xk+1 (cid:1)  M(cid:160)− m(cid:160)
≤ 
(cid:1)xk (cid:1)  M(cid:160)+ m(cid:160)
max {|1 - αm|, |1 - αM|} 

1 

M  - m 
M  +  m 

|1 - αM | 

|1 - αm | 

α 

2

2 
1 
0 
2 
1 
M 
m 
M 
M  +  m 
Stepsizes that 
•  Conv.  rate for minimization stepsize (see text) 
Guarantee Convergence 
(cid:6)
(cid:5)
M(cid:160)− m(cid:160)
f (xk+1 ) 
f (xk ) 
M(cid:160)+ m(cid:160)
•  The  ratio M /m(cid:160)is  called  the  condition number 
of  Q,  and  problems  with M /m:  large  are  called 
il l-conditioned . 

≤ 

SCALING AND STEEPEST DESCENT

•	 View the more general method 
xk+1 = xk(cid:160)− αkDk∇f (xk ) 

as a scaled version of steepest descent. 
•  Consider  a  change  of  variables  x(cid:160) =  S y (cid:160)with 
S(cid:160)= (Dk )1/2 .(cid:160)In the space of y , the problem is 
minimize  h(y) ≡ f (S y) 
subject to  y(cid:160)∈ (cid:6)n(cid:160)
•  Apply steepest descent to this problem, multiply 
with  S ,  and  pass  back  to  the  space  of  x,  using 
∇h(yk ) =  S∇f (xk ), 
yk+1 = yk(cid:160)− αk∇h(yk ) 
S yk+1 = S yk(cid:160)− αk S∇h(yk ) 
xk+1 = xk(cid:160)− αkDk∇f (xk ) 

DIAGONAL SCALING
• Apply the results for steepest descent to the
scaled iteration yk+1 = yk − αk∇h(yk ):
(cid:1)yk+1 (cid:1)
(cid:3)|1 − αkmk |, |1 − αkM k |(cid:4)
(cid:1)yk (cid:1) ≤ max
(cid:6)
(cid:5)
f (xk+1 )
= h(yk+1 )
f (xk )
h(yk )

M k − mk
M k + mk

≤

2

where mk and M k are the smallest and largest
eigenvalues of the Hessian of h, which is
∇2h(y) = S∇2 f (x)S = (Dk )1/2Q(Dk )1/2
• It is desirable to choose Dk as close as possible
to Q−1 . Also if Dk is so chosen, the stepsize α = 1
is near the optimal 2/(M k + mk ).
• Using as Dk a diagonal approximation to Q−1
is common and often very effective. Corrects for
poor choice of units expressing the variables.

NONQUADRATIC PROBLEMS
• Rate of convergence to a nonsingular local min-
imum of a nonquadratic function is very similar to
the quadratic case (linear convergence is typical).
• If Dk → (cid:1)∇2 f (x∗ )
(cid:2)−1 , we asymptotically obtain
optimal scaling and superlinear convergence
• More generally, if the direction dk = −Dk∇f (xk )
approaches asymptotically the Newton direction,
i.e.,

(cid:1)dk +

(cid:1)∇2 f (x∗ )
(cid:2)−1∇f (xk )(cid:1)
(cid:1)∇f (xk )(cid:1)

lim
k→∞

= 0

and the Armijo rule is used with initial stepsize
equal to one, the rate of convergence is superlin-
ear.
• Convergence rate to a singular local min is typ-
ically sublinear (in effect, condition number = ∞)

