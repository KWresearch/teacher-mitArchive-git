6.252 NONLINEAR PROGRAMMING 


LECTURE 9:  FEASIBLE DIRECTION METHODS 


LECTURE OUTLINE

•  Conditional Gradient Method 
•  Gradient Projection Methods 
A  feasible direction at an x(cid:160)∈ X(cid:160) is a vector d(cid:160)(cid:2)= 0  
such that x(cid:160)+ αd(cid:160)is feasible for all suff. small α > (cid:160)0 
x2 

x 

Feasible 
directions at x 

Constraint set X 

d 

x1 
•  Note:  the  set  of  feasible  directions  at  x(cid:160)is  the 
set of all α(z(cid:160)− x) where z(cid:160)∈ X , z(cid:160)(cid:2)= x, and α > (cid:160)0 

FEASIBLE DIRECTION METHODS

•  A feasible direction method: 

xk+1 = xk(cid:160)+ αk dk ,(cid:160)
where dk : feasible descent direction [∇f (xk )(cid:1)dk(cid:160) <(cid:160)
0], and αk(cid:160) >(cid:160)0 and such that xk+1 ∈ X.(cid:160)
•	 Alternative de ﬁnition: 
xk+1 = xk(cid:160)+ αk (xk(cid:160)− xk ),(cid:160)
where αk(cid:160) ∈ (0,(cid:160)1] and if xk(cid:160) is nonstationary, 
xk(cid:160) ∈ X,(cid:160)
∇f (xk )(cid:1) (xk(cid:160)− xk ) <(cid:160)0.(cid:160)
•  Stepsize rules:  Limited minimization, Constant 
αk(cid:160) = 1, Armijo:  αk(cid:160) = βmk(cid:160)s, where mk(cid:160) is the  ﬁrst 
nonnegative m(cid:160)for which 
(cid:1) 
(cid:2) 
f (xk )−f  xk+βm (xk−xk )  ≥ −σβm∇f (xk )(cid:1) (xk−xk ) 

CONVERGENCE ANALYSIS

•  Similar  to  the one  for (unconstrained) gradient 
methods. 
•  The direction sequence {dk } is gradient related 
to {xk }  if the following proper ty can be shown: 
For  any  subsequence  {xk }k∈K  that  converges  to 
a  nonstationary  point,  the  corresponding  subse-
quence {dk }k∈K  is bounded and satisﬁes 
lim sup  ∇f (xk )(cid:1)dk(cid:160) <(cid:160)0.(cid:160)
k→∞, k∈K


Proposition  (Stationarity  of  Limit  Points) 
Let {xk } be a sequence generated by the feasible 
direction method xk+1 = xk(cid:160)+ αk dk .  Assume that: 
− {dk }  is gradient related 
−  αk(cid:160) is chosen by the limited minimization rule 
or the Armijo rule. 
Then every limit point of {xk } is a stationary point. 
•  Proof:  Nearly  identical  to  the  unconstrained 
case. 

CONDITIONAL GRADIENT METHOD

•	 xk+1 = xk(cid:160)+ αk (xk(cid:160)− xk ), where 
xk(cid:160) = arg min ∇f (xk )(cid:1) (x(cid:160)− xk ).(cid:160)
x∈X(cid:160)
•  Assume that X(cid:160)is compact, so xk(cid:160) is guaranteed 
to exist by Weierstrass. 
∇f(x) 

Illustration of the direction 
of the conditional gradient 
method. 

Constraint set X 

x 

_ 
x 

Surfaces of 
equal cost 

Constraint set X 

_ 
x0

Operation of the method. 
Slow (sublinear) convergence.

x0 

x2 

x1 

x* 

_ 
x1 

Surfaces of 
equal cost 

CONVERGENCE OF CONDITIONAL GRADIENT 
•  Show that the direction sequence of the condi-
tional gradient method  is gradient related, so  the 
generic convergence result applies. 
•  Suppose that {xk }k∈K(cid:160) converges to a nonsta-
tionary point  ˜x. We must prove that 
(cid:3) 
(cid:4) 
(cid:1)x(cid:160)k−x(cid:160)k (cid:1) 
lim sup  ∇f(cid:160)(x(cid:160)k )
(x(cid:160)k−x(cid:160)k ) <(cid:160)0.(cid:160)
(cid:4)
: bounded, 
k∈K(cid:160)
k→∞, k∈K(cid:160)
•  1st  relation:  Holds  because  xk(cid:160) ∈  X ,  xk(cid:160) ∈  X , 
and X(cid:160) is assumed compact. 
•  2nd relation:  Note that by de ﬁnition  of xk(cid:160), 
∇f (xk )(cid:1) (xk(cid:160)− xk ) ≤ ∇f (xk )(cid:1) (x − xk ),(cid:160)
∀ x(cid:160)∈ X(cid:160)
Taking limit as k(cid:160)→ ∞, k (cid:160)∈ K , and min of the RHS 
over x(cid:160)∈ X , and using the nonstationarity of  ˜x, 
lim sup  ∇f (xk )(cid:1) (xk−xk ) ≤ min ∇f ( ˜  
x)(cid:1) (x− ˜ 
x) <(cid:160)0,
x∈X(cid:160)
k→∞, k∈K

thereby proving the 2nd relation.


GRADIENT PROJECTION METHODS
• Gradient projection methods determine the fea-
sible direction by using a quadratic cost subprob-
lem. Simplest variant:
xk+1 = xk + αk (xk − xk )
(cid:6)
(cid:5)
xk − sk∇f (xk )
+
xk =
where, [·]+ denotes projection on the set X , αk ∈
(0, 1] is a stepsize, and sk is a positive scalar.

xk+2 - sk+2∇f(xk+2)

xk+1

xk

xk+3
xk+2

Constraint set X

xk+1 = xk - sk∇f(xk )

Gradient pro jection itera-
tions for the case
αk ≡ 1,
xk+1 ≡ xk
If αk < 1, xk+1 is in the
line segment connecting xk
and xk .
xk+1 - sk+1∇f(xk+1)
• Stepsize rules for αk (assuming sk ≡ s): Limited
minimization, Armijo along the feasible direction,
constant stepsize. Also, Armijo along the projec-
tion arc (αk ≡ 1, sk : variable).

k∈K

(cid:4)

(xk−xk ) < 0.

: bounded,

CONVERGENCE
• If αk is chosen by the limited minimization rule
or by the Armijo rule along the feasible direction,
every limit point of {xk } is stationary.
• Proof: Show that the direction sequence {xk −
xk } is gradient related. Assume{xk }k∈K converges
to a nonstationary ˜x. Must prove
(cid:3)(cid:1)xk−xk (cid:1)(cid:4)

∇f (xk )
lim sup
k→∞, k∈K
(cid:3)(cid:10)xk − xk (cid:10)(cid:4)
con-
1st relation holds because
k∈K
verges to (cid:10)[ ˜x−s∇f ( ˜x)]+ − ˜x(cid:10). By optimality condi-
(cid:2)(cid:1)
(cid:1)
(x−xk ) ≤
xk − s∇f (xk )−xk
tion for projections,
0 for all x ∈ X. Applying this relation with x = xk ,
and taking limit,
(cid:7)
(cid:7)
(cid:7)˜x−(cid:5)
(cid:7)2
(xk−xk ) ≤ − 1
∇f (xk )
˜x−s∇f (˜x)
(cid:4)
< 0
lim sup
k→∞, k∈K
s
• Similar conclusion for constant stepsize αk = 1,
sk = s (under a Lipschitz condition on ∇f ).
• Similar conclusion for Armijo rule along the pro-
jection arc.

(cid:6)+

CONVERGENCE RATE – VARIANTS
• Assume f (x) = 1
2 x(cid:1)Qx − b(cid:1)x, with Q > 0, and
a constant stepsize (ak ≡ 1, sk ≡ s). Using the
nonexpansiveness of projection
(cid:7)
(cid:7)
∗(cid:7)
(cid:6)+ − (cid:5)
(cid:7)(cid:5)
(cid:7)xk+1 − x
(cid:7) =
∗ − s∇f (x
xk − s∇f (xk )
x
(cid:7)
(cid:7)(cid:1)
(cid:2) − (cid:1)
∗ − s∇f (x
xk − s∇f (xk )
≤
∗
(cid:7)
(cid:7)
(cid:7)(I − sQ)(xk − x
(cid:7)
∗
=
)
(cid:3)|1 − sm|, |1 − sM |(cid:4)(cid:7)
(cid:7)xk − x
≤ max

(cid:6)+
∗
)
(cid:2)(cid:7)
(cid:7)

∗(cid:7)
(cid:7)

x

)

(cid:7)
(cid:7)

where m, M : min and max eigenvalues of Q.
• Scaled version: xk+1 = xk + αk (xk − xk ), where
(cid:8)
∇f (xk )

(x − xk ) +

(x − xk )

H k (x − xk )

(cid:4)

(cid:9)

,

xk = arg min
x∈X

(cid:4)

1
2sk

and H k > 0. Since the minimum value above is
negative when xk is nonstationary, ∇f (xk )(cid:1) (xk −
xk ) < 0. Newton ’s method for H k = ∇2 f (xk ).
• Variants: Projecting on an expanded constraint
set, projecting on a restricted constraint set, com-
binations with unconstrained methods, etc.

