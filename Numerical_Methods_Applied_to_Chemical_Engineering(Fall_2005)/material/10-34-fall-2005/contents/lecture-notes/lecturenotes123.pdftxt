1.2.3 Pivoting Techniques in Gaussian Elimination 

 
Let us consider again the 1st row operation in Gaussian Elimination, where we start with 
the original augmented matrix of the system 
 

(A, b) = 

     (1.2.3-1) 

1N

11

13

21

22

a
       
...
       
a
       
a
       
a

12
     

a
       
a
         
a
...
       
a
       


a
a
a
...
a
       
       
         
       
31
32
33
3N

          
:   
          
    
          
          
:
:
: 
:


...
a
a
       
        
a
       
       
a


N3

N2

N1

23

2N

b
       
1
       
b
       
b

2

3

      
b

N

NN










 
and perform the following row operation, 
 
 
(A(2,1), b(2,1)) = 
b 
          
         
          
a
          
a
       
          
...
   
          
a  
          
          
a 
          

13
     
12
11
1
1N

)aλ-
         
)aλ-
       
...
       
aλ-
       
)aλ-
)bλ
(b
)
(a
(a
(a
      
(a
−

2
1N
2N
21
13
23
21
12
22
21
11
21
21
1
21

         
          
          
a 
          
a    
...
          
       
a
          
          
b 
   
          
          
a
31
32
33
3N
3

          
: 
          
      
          
        
          
        
          
     
          
          
:
          
:
          
:
          
:


         
          
a 
          
...
       
          
a
          
          
          
          
a
          
b
   
a   

N3
(1.2.3-2) 
 
To place a zero at the (2,1) position as desired, we want to define  λ
a
a

     (1.2.3-3) 

  as 21

λ =
21

NN

N2

N1

21

N

      










11

 
 λ 21
 toup 
blows

but what happens if a11 = 0? => 
 
The technique of partial pivoting is designed to avoid such problems and make Gaussian 
Elimination a more robust method. 
 
 

!
∞±

 

 

 
 
 
 
 
 
 
 
 

A(: , 1) = 

     (1.2.3-4) 

11

21

31

a


a

a


:


:

a














Let us first examine the elements of the 1st column of A, 
 

N1
 
Let us search all the elements of this column to find the row #j that contains the value 
with the largest magnitude, i.e. 
 

a

ji

≥

a

k1

for 
k all
 

=

 2, 1,
N 
...,

     (1.2.3-5) 

 
or
a

j1

=

max

}a{
k1

k
[1,
∈

N]

     (1.2.3-6) 

 
Since the order of the equations does not matter, we are perfectly free to exchange rows  
# 1 and j to form the system            
 

j3

j1

N1

N2

N3

(

j

jN

b
       
       
b
2
2N
       
1b

                row # j                   row # 1     

)b,A =

a
       
...
       
a
       
a
       
a

     
j2

a
       
a
         
a
...
a
       
       

23
22
21

a
       
a
a
...
a
       
       
         
11
12
13
1N

          
:   
          
    
          
: 
          
:
:
:


...
a
a
       
        
a
a
       
       









      
b

NN
N
(1.2.3-7) 
 
Now, as long as any of the elements of the 1st column of A are non-zero, aj1 is non-zero 
and we are safe to begin eliminating the values below the diagonal in the 1st column. 
 
If all the elements of the 1st column are zero, we immediately see that no equation in the 
system makes reference to the unknown x, and so there is no unique solution.  We 
therefore stop the elimination process at this point and “give up”. 
 
 
 
 
 
 
 
 

The row-swapping procedure outlined in (1.2.3-1), (1.2.3-6), (1.2.3-7) is known as a 
partial pivoting operation. 
 
For every new column in a Gaussian Elimination process, we 1st perform a partial pivot 
to ensure a non-zero value in the diagonal element before zeroing the values below. 
 
The Gaussian Elimination algorithm, modified to include partial pivoting, is 
 
For i= 1, 2, …, N-1      % iterate over columns 
a,a{
a
max
(cid:190) select row j > i such that 
=
j
ji
i
ii
≥
(cid:190) if aji = 0, no unique solution exists, STOP 
(cid:190) if j
i, interchange rows i and j 
≠
 
For j = i+1, i+2, …, N    % rows in column i below diagonal 
a ji
λ ←  
iia
For k = i, i+1, …, N     % elements in row j from left (cid:198) right 
 
>ajk (cid:197) ajk - 
 ikλa
end 
>bj (cid:197) bj - 

,...,

>

i

i1,
+

}

 

a

iN,

 
 
 
 
 
 
 

 

 iλb

end 
 

 
end 
 
 
 
 
 
 
Backward substitution then proceeds, in the same manner as before. 
 
 
 
 
 
 
Even if rows must be swapped at each column, computational overhead of partial 
pivoting is low, and gain in robustness is large! 
 

To demonstrate how Gaussian Elimination with partial pivoting is performed, let us 
consider the system of equations with the augmented matrix 
 

                                        (A, b) = 

4     1     1     1


7     3     1     2

2     6     1     3









              pivot     (1.2.3-8) 

 
First, we examine the elements in the 1st column to see that the element of largest 
magnitude is found in row #3. 
We therefore perform a partial pivot to interchange rows 1 and 3. 
 
2     6     1     3


7     3     1     2

4     1     1     1



     (1.2.3-9) 

)b,A(







=

λ

21

=

=

21

11

a
a

     (1.2.3-10) 

 
We now perform a row operation to zero the (2,1) element 
 
 
2
3
 
      
          
1
          
3     


2
2
−

−


3     1
1     3




3
3






          
          
      
1
1
      

2     6     1     3




25     1     
1


3
3




      
4     1
1     1



     0

(A

b,

= 

)

=

(2,1)

(2,1)

2

        
2
−



7     6

6     
          
2

−


3


          
      
1

       
4

2
3





2








      

       (1.2.3-11) 

 
We now perform another row operation to zero the (3,1) element 
 
 
 

     (1.2.3-12) 

λ

31

=

a
a

(2,1)
31
(2,1)
11

=

1
3
 
 
 
 

(3,1)

(A

b,

(3,1)

)

=

          

          
3     





          
0     


1-1




3



1 
          
1 
          
          
3
1-1


          
3 


3



         
          
6

          

2   
25
         
3
1-4


          
6 


3



        
          
1-
1-1


          
1 


3



 












  2 



 

= 

        
0

          
6 


1
          
        
3


1


3

2

3


2 
25
          
         
1-
3
13
        
1- 
3

          

        
0










      

 

(1.2.3-13) 

 
We now move to the 2nd column, and note that the element of largest magnitude appears 
in the 3rd row.  We therefore perform a partial pivot to swap rows 2 and 3. 
 

-(3,1)

(A

b,

-(3,1)

)

=

     (1.2.3-14) 

        
0

          
6 


1
2 
          
        
3


2
13

        
1- 

3
3

25
1

          
1-
         
3
3


          

        
0










 
We now perform a row operation to zero the (3,2) element. 
1
3
2
3

     (1.2.3-15) 

λ 32

1
2

=

=

(3,2)

(A

b,

(3,2)

)

=

          
0



          
3








        
0

          

          
1   
2   
3
1-
1
2
3

2
3





          

          
6   

          
          
1- 
1-1-

          

2


(
−

          

          
2
13
          
      
3
1-  
25 
3
2

)
          
1

 











13
3






  



=

     (1.2.3-16) 

        
0


1
          
6
        
        
3


2

3


      
        
0
0

−


2   
13 
3
4
          

1-
          
      

1
2










 
After the elimination method, we have an upper triangular form that is easy to solve by 
backward substitution. 
 
 
 
We write out the system of equations, 

 

First, we find  

Then, from the 2nd equation, 

−

x 2 =

And finally from the 1st equation 
x1 =

The solution is therefore 
 

 

3x1 + x2 + 6x3 = 2
2
1
3
3

x2 − x3 = 3
1
2

x3 = 4

(1.2.3-17) 

x 3 = −8
(1.2.3-18) 

= −7

1
3

(3

+ x3 )
2
3
 
(1.2.3-19) 

(2 − 6x 3 − x2 )
3
(1.2.3-20) 

= 19

x =

19
 
 
−7
 
 
 
 
−8
 
 

(1.2.3-21) 

Note that in our partial pivoting algorith
 rows to make sure that the largest 
m, we swap
magnitude element in each column at and below the diagonal is found in the diagonal 
position.  We do this even if the diagonal element is non-zero. 
 
his may seem like wasted effort, but there is a very good reason to do so.  It reduces the 
T
"round-off error" in the final answer.  To see why, we must consider briefly how numbers 
are stored in a computer's memory. 
 
 
 
 
 
 
 
 
 we were to look at the memory in a computer, we would find data represented digitally 
If
as a sequence of 0'a and 1's 
 
0
0100101   00101011 , ….. 
-----------    ------------- 
byte # i        byte # i+1 
 
o store a real number in memory, we need to represent it in such format.  This is done 
T
using floating point notation. 
 
Let f be a real number that we want to store in memory.  We do so by representing it as 
some value  

~
f ≈ f

hat we write as 

 
T
 

~
f

∗ e −1 +
∗ e −2 +
∗ e −t
...d t 2
[d 1 2
d2 2
= ±
t= machine precision 
(1.2.3-22) 

]

Each 

d i = 0 or 1
And so is represented by one bit in mem y.  e is an integer exponent in the range 
or
L ≤ e ≤ U
L ≡ underflow limit
U = overflow limit
(1.2.3-23) 
example if w

e is also stored as a binary number, for 
then 

e allocate a byte (8 bits) to storing e, 

±
e 7

0
e 6

0
e 5

0
e 4

0
e 3

0
e 2

1
e1

1
e 0

= 3

 
 

(1.2.3-24)&(1.2.3-25) 

The largest e is when 
e 0 = e1 = e2 = ... = e6 = 1
e = e 0 ∗ 20 + e 1 ∗ 21
+ e 2 ∗ 2 2 + e 3 ∗ 2 3 + e 4 ∗ 24 + e 5 ∗ 25 + e 6 ∗ 2 6
 

For which 
6∑ = 2 7 − 1
e max = 2 0 + 21 + 22 + 2 3 + 24 + 25 + 2 6 =
2 k
k = 0
(1.2.3-26)
 

So, say in general 

e max ≈ 2 k
Where k depends on number of bits allocated to store e.     (1.2.3-27) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

For the largest magnitude variable that can be stored in memory, M 
 

M = 

M

0
1 =±
1d
[
2
+=

1
d
2
1e
−

1
d
3
2
+

1
d
2e
−

4

1
d
5
2
+

1
d
3e
−

1
d
7
6
2
+

1
d
4e
−

8
+

10
e
±
6
2
6e
−
+

1
1
e
e
4
5
2
7e
−
+

1
e
3
+

1
1
e
e
2
]8e
2
−

    (1.2.3-34) 

1
    (1.2.3-35) 

          

2

5e
−
 

e

+=

2

6

64
+=

    (1.2.3-36) 

8
= ∑
1k
=
k
ral, for machine precision t and  = 2 , 

M

ke
−

=

2

1.8375x10

19

     (1.2.3-37) 

U

−

i

2

2

=

=

iU
−

M

i
1
t
t
t



∑
∑
∑
=


2



1i
1i
1i
=
=
=
, 
We now use the identity for a geometric progression
x,1
x
N
−
∑ −
N
     (1.2.3-39) 
x
1i
1x
−
1i
=

t
∑
1i
=

1
2

1
2













=

=

≠

2

2

2

1

U

U

1i
−

     (1.2.3-38) 

where 

 
so 

In gene
 

 
to write 
 

k 
4 
6 
8 

 
 
 

U

1

2

=





M

1
2

t
1


−

2



1
 −




2


at, for a given t and k (i.e. how much memory we wish to allocate to storing 
We see th
ach number), there is a maximum and minimum magnitude to the real numbers that can 
e
be represented. 
 

]t−2
     (1.2.3-
40) 

[U
12

=

−

1

~
Mfm
≤
≤

 

 
For t = 8, for various k, U = 2k, we have the following m and M, 
 

U=2k=-L 
16 
64 
256 

m = 2L-1 
~ 7.36x10-6 
~ 2.71x10-20 
~ 4.32x10-78 

M = 2U(1-2-t) 
~ 6.53x104 
~1.84x1019 
~ 1.15x1077 

The typical representation on a 32-bit machine is 
 

__     __ __ __ __ __ __ __ __      __       
  +/-          8 bit exponent                  +/- 

 
__ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ 
23 bit mantissa 
 
total = 32 bits for each real number 
 
 we wish to store a real number f in memory, in 
The important point to note is that when
general f cannot be exactly represented by a finite set of bits in floating point notation; 
,1
e,
e for 
certainly this is tru
π .  Instead, we represent it with the closest possible value 
3
]te
[
td
d
22
2
...
−
∗
1
~
so that the difference between the “true” value of f and the represented value f  is called 
the round-off error, rd(f) 

     (1.2.3-22, repeated) 

±=

e

1
−

+

e

−

2

∗

2

+

d

~
f

∗

 
~
f

rd

(

f

)

=

f

−

     (1.2.3-41) 

For binary representation of a number f with 

fm ≤

M≤

, from (1.2.3-22), w

e see that 

the magnitude of the round-off error is 

rd

(

f

2~)

te
−

2
= −

t

∗

e

2

where we define the machine precision
 as 
 

 
=
 

(

eps

2)
∗

e

     (1.2.3-42) 

(eps) = 2-t, see MATLAB command “eps”     (1.2.3-43) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Let us write rd(f) = rf(eps)x2e
f     (1.2.3-44) 
 
Where  

rf is some number of O(1 (i.e. is on the order of 1) 
) 
ef = exponent of f 
 
o  f, also O(1)     (1.2.3-45) 
, where mf = mantissa 
f

e

f

~
f x2mf =

We write 
 
Then,  
 

rd(f)
~
f

=

r
f
m

f

(ep

s)x

e f

e

f

2
2

=

r
f
m

f

(eps)

     (1.2.3-46) 

~
f

1,

<<

<<

eps

rd(f)

     (1.2.3-47) 

 
F  
or
 
S
o, when we initially assign a value in memory, the round-off error may be small.  We 
want to make sure that this initial small error, as it propagates through our algorithms, 
does not “blow up” to become large. 
 
For example, let us take the difference of two close, large numbers 
 

f = 3.000 0001x106 
g = 3.000 00009x106 
(1.2.3-48) 
 
<<
 

g,f

−

g

f

f-g = 0.01 so 

     (1.2.3-49) 

If 

f

~
f
+=

rd(f),
g   

~
g
+=

so 

f

rd(g)

2.3-50) 
     (1.
~
~
[gf
g
+−=−

rd(f) −

rd(g)

]

     (1.2.3-51) 

rd(f-g) = rd

(1.2.3-52) 
(f) – rd(g)     
 
 
 
 
 
 
 
 
 
 
 

Let us write 
 

then 
 

rd(f) = rf(eps)x2e
f,       rd(g) = rg(eps)x2e
g     (1.2.3-53) 
~
~
e
ge
     (1.2.3-54) 
x2mf
x2mg     
,
=
=
f
g
 

~
~
rd(
)gf
−
~
~
gf
−

r
f

=

(eps)2
2m
f

e

f

e

f

−
−

(eps)2
r
g
e
2m
g
g
 

e

g

     (1.2.3-55) 

Let us now take the case of numbers like 
f = 3.000 0001x106 
g = 3.000 00009x106     (1.2.3-48, repeated) 
 
for which, in binary or decimal notation, ef = eg and mf 
– mg << 1 
 
Then  

~
~
)gf
rd(
−
~
~
gf
−

 
)− g
(
(eps)
r
r
f
m
m
−
f
 
as (r -r ) = O(1) mf – mg << 1, we see that compared to 
f
g
 
     (1.2.3-46, repeated) 

     (1.2.3-56) 

(eps)

=

g

=

r
rd(f)
f
m
~
f
~
~
)gf
rd(
−
~
~
gf −

f

>>

rd(f)
~
f

,

rd(g)
~
g

     (1.2.3-57) 

 
 similar numbers therefore
Taking the difference between two large,
 is bad, from the view 
of propagation of error, since the accumu
e result is much larger 
lated round-off error in th
than it should be from a direct assignment. 
 
 
 
 
 
 
 
 
 
 

We wish to design, and operate, our algorithms so that the accumulated round-off errors 
o not grow larger, and ideally decay to zero.  If error “blows up”, the errors become 
d
rger in magnitude than the values that we are trying to represent, and we get instability 
la
th
at crashes the program. 
 
F
 

or example, let us say that we wish to perform the operation 

     (1.2.3-58) 
b
a
a
λ−←
We really perform the operation on their floating point representations 
~
~~
~
     (1.2.3-59) 
a
b
a
λ−←
, we subtract these equations 
~~
neweb
b
λλ
+
+
 
 

     (1.2.3-60) 

~
aa
−=

ard
)(

ard
)(

ard
)(

←

−

S

ince

 

If 

~
, we can write 
λλ=
ard
)(

←

ard
)(

−

brd
)(
λ

+

newe

     (1.2.3-61) 

 
If  λ> 1, any round-off error in b is magnified during this operation, but if  λ
< 1, then 
lue of a. 
error accumulated to date by b is decreased a
s it is “passed” to the new va
 
In Gaussian elimination
 

, we perform a number of operations 

a
ji
iia
a >
a
By performing partial pivoting, we ensure 
ii
fa
vorable error propagation characteristics. 

a
λλ ,
=
ik

a
−←
jk

a

jk

     (1.2.3-62) 

, so  λ< 1 and the algorithm h

as 

ji

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

We can further enhance this favorable property of error propagation by performing 
complete, or full, pivoting. 
 
 
 
 
 
In complete pivoting, one searches for the maximum magnitude element not only in 
the 
current column, but in other
s as well.  The pivoting involves not merely interchange of 
ro
ws, but also of columns.  This makes the book keeping more complex as column 
in
terchange implies an interchange of the values of the unknowns in their position in the 
so
lution vector x
.  While full pivoting improves the accuracy of calculation, by more 
ra
pidly decaying the round-off error, it is not strictly necessary for systems that are well-
b
alanced, i.e. all elements along any given row ai1, ai2…,aiN are all of the same order of 
magnitude.  We therefore do not discuss this technique further. 
 
 
 
 
 
We now note that with the addition of partial pivoting, Gaussian elimination provides a 
robust method of solving linear equations that is easily impleme
nted by a computer.  It 
e
ither returns a solution to the linear system, or, if no non-zero pivot element if found, it 
re
cognizes that there is no unique solution and STOP’s. 
 
 
 
 
 
We therefore have a dependable method that can be used in higher-level algorithms to 
solve non-linear algebraic equations, partial differential e
quations, etc.  First, we must 
e
xamine in closer detail the existence and uniqueness of solutions. 
 
 

 

