1.4.1 LU Decomposition 
 
In section 1.3.6, we have seen that calculating the inverse of a matrix A involves solving 
N linear problems: 
 

[k]~
a = e[k]     (1.4.1-1) 

A
 

[N]

[1]~
~
 of A-1.  Each of these N problems involve the 
For the N column vectors 
, … , 
a
a
same matrix A, so the Gaussian elimination procedure for each is the same. 
 
Often in the practice of numerical calculations for more advanced problems, we have to 
repeatedly solve a linear system with the same matrix A, but with different right hand 
side (RHS) vectors b[1], b[2],… 
 
Ax[k] = b[k]     (1.4.1-2) 
 
LU decomposition is a technique that allows us to “remember” all of the row eliminations 
that we must perform to solve a linear problem with the matrix A.  In the future, we 
therefore need only perform the N2 operations required for substitution to solve the 
system with a new RHS vector b[k]. 
 
Let us consider LU decomposition 1st for Gaussian elimination without partial pivoting. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Consider the system Ax = b with the augmented matrix 
 

(A, b) = 

     (1.4.1-3) 

11

12

21

22

2N

b  
a  
...
  
a  
a
1N
1
a
a  
  
...
a  
b  
a
a  
...
a  
b  
  
31
32
3N
3
:   
       
          
 :
:
      
:
a
a  
...
a  
b  
  

2

NN

N2

N1










N










 
We now perform the row operation 
 
a

     
,aλ
1j
21

a
−←
2j

(2,1)
2j

b,

=

λ

21

21

2

a
a

11

 
to obtain the equivalent system 
 

 
 
 
 
 
 
 
 
 

b
−←
2

bλ
21

1

     (1.4.1-4) 

(A(2,1), b(2, 1)) = 

     (1.4.1-5) 

1) 
(2,
2

b  
a  
a  
...
  
a
1N
12
1
11
a  0
...
a  
b  
  
(2,
1) 
(2,
1) 
22
2N
a  
  
...
a  
b  
a
31
32
3N
3
:   
          
      
       
:
:
 :
a
a  
...
a  
b  
  

NN

N2

N1










N










 
(2, 1) must contain a zero.  
We know from our choice of  λ  that the (2, 1) position of A
21
Therefore, we are free to use the location in memory “wasted” in storing the known value 
of zero to store something else – for example the valyue of 
. 
21λ
 
In this case, the contents stored in memory will actually be 
...
b  
a  
a
a  
  


12
1N
1


b  
a  
a  
...
  
λ
(2,
1) 
1) 
(2,


2N
22


a
a  
b  
...
  
a  
31
32
3N
3


:   
       
          
 :
:
      
:




b  
a
a  
...
a  
  



(A(2,1), b(2, 1))= 

     (1.4.1-6) 

(2,
1) 
2

NN

N2

N1

21

11

N

Next, we perform a row operation to “zero” the (3, 1) position. 
 
The operations required are  
a
a
     
,
a
     
λ
(3,1)
31
−←
3j
3j
a

 2, 1, 
 (j  aλ
 
...,
=
31
1j

b     
N),

b
−←
3

=

31

3

11

 
Since we know that  a
=0 by the coice of 
31λ    
(3,1)
31
store 
31λ    
.  The contents of memory are then 
 

bλ
31

1

     (1.4.1-7) 

, we can use this position in memory to 

(A(3,1), b(3, 1)) = 

      (1.4.1-8)    

11

b  
...
  
a  
a  
a
1
12
1N
λ
a  
a  
b  
...
  
1) 
1) 
(2,
(2,
2N
21
22
a  λ
...
a  
b  
  
1) 
(3,
(3,
1) 
31
32
3N
          
      
 :
:
:   
       
:
a
a  
a  
b  
...
  

NN

N2

N1

N











1) 
(2,
2
(3,
1) 
3











 
We then continue this process for the remainder of the Gaussian elimination algorithm.  
At the end, our augmented matrix and all of the 
 factors are stored in memory as 
ijλ    
 

(A(N,N-1), b(N, N-1)) = 

     (1.4.1-9) 

11

b  
...
  
     
a  
a
a  
a
1
13
1N
12
λ
a  
a  
a  
b  
...
1) 
1) 
1) 
(2,
(2,
(2,
21
23
2N
22
λ  λ
a    
a  
b  
...
(3,
2) 
1) 
(3,
31
32
33
3N
          
      
 :
:
:   
       
:
λ
λ  
λ
...
a  
  
     

N1

N2

N3











1) 
(2,
2
(3,
1) 
3

1)-N 
(N,
NN

b  

1)-N 
(N,
NN











 
Because we now store all of the 
 factors, we have perfect memory of the row 
ijλ    
x[k] = b[k] with new RHS vectors b[k].  This saves us 
operations that are required to solve A
the work of performing Gaussian elimination each time. 
 
 
 
 
 
 
 
 
 
 
 
 
 

We 1st state how this knowledge of the 
’s can be used to save the work of Gaussian 
ijλ    
elimination for repeated problems with the same matrix A.  Then, we demonstrate that 
this approach is valid. 
 
From the final augmented matrix obtained by Gaussian elimination, let us extract the 
lower and upper triangular parts into separate matrices. 
 

     (1.4.1-10) 











  

L = 

1


  1  
λ

21
λ  λ
    1  

32
31

       
          
:
:   


λ
λ  
λ
     


N2

N1

 

N3










...1
 

   U = 

  
a    
...
a
     
a  
a
  
12
11
1N
13
       
a
...
 
a  
a  
(2,
1) 
1) 
1) 
(2,
(2,
22
2N
23
       
a
a  
          
...
2) 
(3,
2) 
(3,
3N
33
          
 :
          
:
:
      
       
          
          
         
a
(N,
1)-N 
NN

  











 
Where the Gaussian elimination has been performed without pivoting.  Then, LU 
Decomposition of A yields 
 
A = LU     (1.4.1-11) 
 
This will be demonstrated shortly, but 1st let us see how we may use LU decomposition 
to avoid repeated Gaussian eliminations when solving Ax[k] = b[k]. 
 
We substitute A = LU in this problem to obtain 
 
Ax[k] = LU x[k] = b[k]     (1.4.1-12) 
 
If we define Ux[k] = c[k], we can solve Ax[k] = b[k] by solving successively the 2 triangular 
problems 
 
Lc[k] = b[k] 
Ux[k] = c[k]     (1.4.1-13) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

= 

N

11

22

1

2

3

N

N1

N2

...λ

 

NN

N3

1

2

3




























c


c

c


::


c


b


b

b


::


b


     (1.4.1-14) 

λ
λ
  
λ  
21
λ  λ
λ  
    
33
32
31
       
          
:   
 
:
λ
λ  
     
λ

The 1st problem involves a lower triangular matrix L, 
 








 
This problem can be solved in N2 FLOP’s using forward substitution, 
 
L11c1 = b1 => c1 = b1/L11 
L21c1 + L22c2 = b2 => c2 = (b2-L21c1)/L22 
L31c1 + L32c2 + L33c3 = b3 => c3 = (b3 – L32c2 – L31c1)/L33 
 
: 
: 
 
Note that L11=L22=L33 = 1, so there is no problem here with division by zero. 
 
The 2nd problem Ux[k] = c[k] involves an upper-triangular matrix and so may be solves in 
N2 FLOPs’ using backward substitution. 
 
Therefore, after we have performed LU decomposition on A in N3 FLOP’s once, we can 
solve successive problems Ax[k] = b[k] on only 2N2 <<N3 FLOP’s. 
 
LU decomposition (factorization) is a very useful and efficient technique that is used very 
often in practice. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Now, let us show that the LU decomposition with L and U defined by (1.4.1-10) does in 
fact satisfy A = LU. 
 
To do so, let us define the matrix M as M = LU. 
 

a21λ

12] = a22     (1.4.1-20) 

a21λ

1j] = a2j     (1.4.1-21) 

     (1.4.1-16)  











  

M = 

1


  1  
λ

21
λ  λ
    1  

32
31

       
          
:
:   


λ
λ  
λ
     


N2

N1

 

N3










...1
 

     
  
...
a
  
      
a
a  
a
12
11
1N
13
       
a
 
...
a  
a    
(2,
1) 
(2,
(2,
1) 
1) 
22
23
2N
       
a
  
          
...
a   
2) 
2) 
(3,
(3,
33
3N
          
 :
          
:
      
:
       
          
          
          
a
1)-N 
(N,
NN











a

a21λ

a21λ

12 + 

22a
(2,
1) 

 
 = 21λ

12 + [a22 - 

11     (1.4.1-18) 

21/a11, M21 = a21     (1.4.1-19) 

 
By the rules of matrix multiplication, we have  
M11 = a11     M12 = a12     …..     M1N = a1N     (1.4.1-17) 
 
So the 1st row of M equals the 1st row of A. 
 
For the 2nd row, 
 
M21 = 
 
But since  λ  = a
21
 
Next, for the (2, 2) position, using (1.4.1-4) 
 
M22 = 
 
And similarly for j = 3, …, N 
 
M2j = 
 
Therefore, the 2nd row of M equals the 2nd row of A. 
 
 
 
 
 
 
 
 
 
 
 
 

a

1j + [a2j - 

a21λ

1j + 

2ja
(2,
1) 

= 

21λ

a31λ

 =  

32λ

a

12+

32λ

 = 

 - 

 -  λ

31

 = 
]

31λ

1) 
(3,
32

 =  a

1) 
(2,
32

12a
1) 
(2,

22a
(2,
1) 

1) 

32a
(3,

/ a

(2,
1) 
22

32a
(3,
1) 

/ a

(3,
1) 
22

32a
1) 
(3,

 

/

22a
1) 
(2,

]

a31λ

12] 

a31λ

12 + [

32a
(3,
1) 

22a
(2,
1) 

 = 31λ
 

a31λ

12 + [

32a
1) 
(2,

31λ

 

12a
(2,
1) 

a31λ

12 + 

a

12 +[a32 - 

     (1.4.1-24) 

     (1.4.1-23) 

     (1.4.1-25) 

11 = (a31/a11)a11 = a31     (1.4.1-22) 

Continue with 3rd row, 
 
M31 = 
 
M32  = 
 
With 
 
32λ  = 
 
and   a
 
we have 
 
M32 = 
 
= 
 
M32 = a32     (1.4.1-26) 
 
And for j = 3, …, N 
 
M3j = 
 
Using  
 
3ja
(3,
2) 
 
3ja
(3,
1) 
 
we have 
 
therefore,  
 
M3j = 
 
=
 
M3j = a3j 
The 3rd rows of M and A are equal. 
We can continue this process to demonstrate that M = LU satisfies M = A. 
 

3j -  λ a31 1j     (1.4.1-29) 

     (1.4.1-30) 

     (1.4.1-31) 

     (1.4.1-28) 

    (1.4.1-27) 

a31λ

1j + 

a31λ

1j - 

32λ

a31λ

1j - 

32λ

a31λ

1j + 

32λ

 + [a

3j - 

a31λ

1j +  λ

 = a
3j - 

=

3ja
(3,
1) 

3ja
(3,
2) 

3ja
(3,
2) 

3ja
(2,
1) 

 - 

31λ

 - 

32λ

3ja
(3,
2) 

2ja
(3,
1) 

 = 

3ja
(3,
1) 

 - 

32λ

2ja
(2,
1) 

 + 

2ja
(2,
1) 

 + 

1ja
(2,
1) 

= a

2ja
1) 
(2,

     ] 

2ja
(2,
1) 

2ja
1) 
(2,

32

2ja
(2,
1) 

    1




 1    2


1    2    3





1     1    1




1    1-
    


          
1 





 
Consult pages 1.2.1-6 to 1.2.1-11 to see the calculation performed during Gaussian 
elimination for this system. 
 
Then if M = LU, we have 
 
M11 = 1 
 
M21 = (2)(1) = 2 
M31 = (3)(1) = 3 

 
   M12 = (1)(1) = 1 
M13 = (1)(1) = 1  
   M22 = (2)(1) + (1)(-1) = 1  M23 = (2)(1) + (1)(1) = 3 
   M32 = (3)(1) + (2)(-1) = 1    M33 = (3)(1) + (2)(1) +(1)(1) = 6 
(1.4.1-34) 
 

1     1    1




3    1    2


6    1    3





.  So we see that for this example A = LU.     (1.4.1-35) 

32λ

1) 

2ja
(2,

     (1.2.1-15),  

As an example, let us consider the system of a3j -  λ a31 1j - 
page (1.2.1-6), 
 
1     1    1




3    1    2


6    1    3




 
We performed Gaussian elimination without pivoting on this system, and write the results 
as the L and U matrices 
 

     (1.4.1-32) 

x
x
x

4
7
2

























1

2

3

=

L = 

    U = 

     (1.4.1-33) 

Then, 
 

M = LU = 

 
 
 
 
 
 
 
 
 
 
 
 
 
 

What form does an LU decomposition take when pivoting is performed?  The book 
keeping is more complex, but the routine for performing LU decomposition will return a 
lower-triangular matrix L, an upper-triangular matrix U, and a permutation matrix P such 
that 
 
PA = LU     (1.4.1-36) 
 
Then to solve Ax[k] = b[k], we pre-multiply by P, 
 
P Ax[k] = Pb[k]     (1.4.1-37) 
 
Substitute LU = PA, 
 
Lux[k] = Pb[k]     (1.4.1-38) 
 
We now solve the successive triangular problems 
 
Lc[k] = Pb[k] 
Ux[k] = c[k]     (1.4.1-39) 
 
By permutation matrix, we mean only matrix that is obtained from the identity matrix by 
a sequence of row or column interchanges. 
 

For example, the permutation matrix P = 

0   0   1




1   0   0


0   1   0





   

has the effect (1.4.1-40) 

 

Pv = 

0   0   1




1   0   0


0   1   0





v
v
v

1

2

3













=

v
v
v

1

3

2













     (1.4.1-41) 

 
P, obtained from I by interchanging row #2 and 3, therefore acts on a vector v by 
interchanging the 2nd and 3rd components. 
 
Since P performs a permutation of v, it is called a permutation matrix. 
 
Note:  As det(I) = 1, for any permutation matrix P, det(P) =  ± 1     (1.4.1-42) 

