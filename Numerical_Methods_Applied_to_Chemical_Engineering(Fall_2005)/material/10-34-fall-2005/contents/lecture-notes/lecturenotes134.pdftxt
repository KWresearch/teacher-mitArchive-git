1.3.4 Null Space (kernel) and Existence/Uniqueness of Solutions 
 
We now have the tools necessary to consider the existence and uniqueness of solutions to 
the linear system of equations 
 
Ax = b    (1.3.4-1) 
 
Where x, b ∈ RN and A is a N x N real matrix. 
 
As described in section 1.3.1, we interpret A as a linear transformation that maps each 
 v∈ RN    into some Av∈ RN according to the rule 
 
 
 

Av = 

11

a
     
...
     
a
     
a

1N
12

a
     
a
     
...
     
a

21
22

          
:  
          
:   
:

     
a
     
a
...
a
    


N2

N1

2N

NN

v


v

:


v









1

12

N








=

1

va

11
1

va

21

:

va

N1

+
+

va
12
2
va
22

2

...
++
...
++

va
1N
N
va
2N

N

+

va
2N

2

...
++

a

v

N

NN

1








     (1.3.4-2) 

 
 
 

 
 
 
Pictorially, we view the problem of solving Ax = b as finding the (or one of many?) 
vector(s) x∈ RN that maps into a specific b under A. 
 
 
              RN   v  
            0 
 
             x 
 
 
 
 
 
 
Here we have shown that for any real matrix A, the rule for forming Av (1.3.4-3) 
guarantees that 
 
A0=0     (1.3.4-4) 
 

      Av 
    0    RN 
   b 

A 
A 
A 

 
 
 

 
 
 

 
 
 

0


0

:


0


    (1.3.4-5) 








Where 0 is the null vector, 0 = 

 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

A 

A 

RN 

 
v  

      0 

      Av ≠ 0 

We always have one vector, 0, that maps into 0 under A.  Crucial to the question of 
existence and uniqueness of solutions is the existence of any other vectors w ≠ 0 that also 
map into 0 under A. 
 
We define the null space (or kernel) of a real matrix A to be the set of all vectors w∈ RN 
such that Aw = 0.  Pictorially, we view the kernel of A, denoted KA, as 
 
 
 
 
 
        RN 
 
      KA  w 
        0 
 
 
 
 
 
 
 
 
 
We use the concept of the kernel (null space) to prove the following theorems on 
existence/uniqueness of solutions to Ax = b. 
 
 
Theorem 1.3.4.1 
 
Let x∈ RN be a solution to the linear system Ax = b, where b∈ RN, A is an N x N real 
matrix.  If the kernel of A contains only the null vector, i.e. KA = 0, then this solution is 
unique (no other solutions exist). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
Then, 
 

Ay = A(x + v) = Ax + Av     (1.3.4-7) 

 
Since x is a solution, A x = b, and 
 

Proof: 
 
Let x satisfy Ax = b.  Let y be some vector in RN that also satisfies the system of 
equations Ay = b. 
 
If we define v = y – x, we can write this 2nd solution as 
 

y =  x + v     (1.3.4-6) 

Ay = b + Av     (1.3.4-8) 

 
If y is to be a solution as well, then Ay = b.  This can then be the case only if 
 

Av = 0     (1.3.4-9) 

 
Therefore, if x is a solution, every other solution must differ from x by a vector v∈ KA. 
 
Since we have stated that for our matrix A, the only vector in the kernel is the null vector 
0, there are no other solutions y ≠ x to Ax = b. 
 
Q.E.D. = “Quod Erat Demonstrondum” 
 
   “That which was to have been proven” 
 
We have proven a theorem on uniqueness.  We must not prove a theorem on existence. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

=

N

N

Rv
∈∃

 with 
vA

 under the condition that 

To do so, we define the range of A, denoted RA, to be the subset of all vectors y∈ RN 
such that there exists some v∈ RN with Av = y. 
 
 
There exists                       some vector v 
 
RA  {
}y
     (1.3.4-10) 
Ry
∈≡
 
 
 
Every  
y∈ RN 
 
 
Pictorially, we view the range as 
 
 
 
 
 
 
 
 
 
 
No vectors map into the port of RN outside of the range. 
 
 
Theorem 1.3.4.2: 
 
Let A be a real N x N matrix with kernel KA ⊂ RN and Range RA ⊂ RN.  Then 
 

RN 
 
      KA  w 
        0 
 
 

RA 
 
      Av ≠ y 

 
      0 

 
v  

 
A 

 
 

 
 

 

 
 

 
 

 

 
 

A 

 

 

 

 

RN 

 
 

 
 

 
 

 
 

 
 

 
 

 

(I) 
  
 
(II) 

the dimensions of the kernel and of the range satisfy the “dimension theorem” 
dim(KA) + dim(RA) = N     (1.3.4-11) 

If the kernel contains only the null vector 0, dim(KA) = 0.  As the range 
therefore has dimension N, RA = RN, and for every b∈ RN, there exists some 
x∈ RN with Ax = b (existence). 

 
 
 
 
 
 
 
 

Let us use an orthonormal basis {U[1], U[2] , …, U[M] , U[M+1] , …, U[N]} 
For RN such that the 1st M vectors form a basis for the kernel KA. 
 
Since the kernel satisfies all the properties of a vector space itself, we can 
construct the M basis vectors for KA, for example by Gram-Schmidt 
orthogonalization.  Once we have identified these M basis vectors, we can 
continue with Gram-Schmidt orthogonalization to finish the basis set. 
 
We can therefore write any w∈ KA as 
 
W = c1U[1]+ c2U[2]+…+ cMU
[M]     (1.3.4-12) 
 
 
And the dimension of the kernel is obviously M, 
 
dim(KA) = M    (1.3.4-13) 
 
We now write any arbitrary vector v∈ RN as an expansion in the basis, 
 
v = 
 
Then, taking the product with A, 
 
Av = A (
)
M
[
]
]1[
]1[
Uv
Uv
Uv
...
'
'
'
++
+
M
1
1
44444
44444
1
2
3
0
=

     (1.3.4-14) 

Uv
'
M
1
+

UAv
'
N

...
++

...
++

...
++

Uv
'
M

Uv
'
N

Uv
'
1

Uv
'
1

UA

[

M

]1
+

[

M

]1
+

[

M

]

+

v

'
M

1
+

]1[

+

[

N

]

]1[

+

[

N

]

 (1.3.4-15) 

 
We therefore see that any vector Av  ⊂ RA can be written as a linear 
combination of the N – M vectors {AU[M+1], …, AU[N]}. 
 
Therefore dim(RA) = N – M and dim(KA) + dim(RA) = N 
 
Follows directly 

(II) 

Proof: 
 

(I) 

 
 
 
 
 
 
 
 
 
 
 

Taken jointly, theorems 1.3.4.1 and 1.3.4.2 demonstrate that if KA = 0, i.e. only the null 
vector maps into the null vector under A, then Ax = b has a unique solution for all b. 
 
What happens if the kernel of A is not empty, i.e. there exists some w ≠ 0?  Let us 
consider a specific example. 
 
Look at a system with 
 

 
Then for any v∈ R3 
 

 
Writing  

 

 
With  

Therefore 
 

A = 

0    0    0
0    0    0
1    0    0













     (1.3.4-16) 

Av = 

0    0    0
0    0    0
1    0    0













v
v
v







1

2

3







=

0
0
3v













     (1.3.4-17) 

v = v1e[1]+ v2e[2]+ v3e[3],     (1.3.4-18) 

Av = v1Ae[1]+ v2Ae[2]+ v3A e[3]     (1.3.4-19) 

Ae[1] = 

Ae[2] = 

Ae[3] = 

=

= 

e[3] 

= 

0 

= 0

 

=

=

0




0


0




0




0


0




0




0


1





1
0    0    0








0
0    0    0




0
1    0    0








0
0    0    0








1
0    0    0




0
1    0    0








0
0    0    0








0
0    0    0




1
1    0    0








(1.3.4-20) 
 

Av = v10 + v20 + v3e[3] = v3e[3]     (1.3.4-21) 
 
 
This information is “lost” when mapped by A 

We therefore see that for this A, any vector that is a linear combination of e[1] and e[2] is 
part of the kernel, 
 

w= w1e[1] + w2e[2] ∈ KA     (1.3.4-22) 

 
we then can say that KA = span{e[1],e[2]}, and so dim(KA) = z.     (1.3.4-23) 
 

Also since for any v∈ R3, Av = 

3e[3]; therefore RA = span{e[3]}, dim(RA) = 1     
= v

0
0
3v













(1.3.4-24) 
 
As expected from the dimension theorem, dim(KA) + dim(RA) = 3    (1.3.4-25) 
 

Now, does Ax = b have a solution? 
 

- if b∈ RA, i.e. b = 

    (1.3.4-26) , then yes, there is a solution. 

0
0
3b













0
0
3b













 
We easily see that a solution is 
 

x = 

(1.3.4-27), A
x = 

b     (1.3.4-28) 
=

0    0    0
0    0    0
1    0    0













0
0
3b













=

0
0
3b













 
There are however an infinite number of solutions, since any vector  
x + w1e[1] + w2e[2] is also a solution as  

 
A(x + w1e[1] + w2e[2]) = Ax + w1Ae[1] + w2Ae[2] 
                                            =Ax + w10 + w20RA     (1.3.4-29) 
=b 
 

b
b
b

 with either b
1 ≠ 0 or b2 ≠ 0, then Ax = b has no 







1

2

3

- 

if b∉ RA, i.e. b = 

solution. 







 
 
 
 
 

We see therefore that we have the following three possibilities regarding the existence 
and uniqueness of solutions to the linear system Ax = b, A N x N real matrix, b∈ RN. 
 
 
Case I 
 
The kernel of A is empty, i.e. KA = 0.  Then, RA = RN and for all b∈ RN there exists a 
unique solution x. 
 
 
 
Case II 
 
w = 0.  Let dim(KA) =M, and {U[1], U[2], …, U[M]} forms 
There exists w 
 0 for which A
≠
an orthonormal basis KA, 
 
W = c1U[1] + c2U[2] + …+ cMU[M] ∈  KA, Aw = 0     (1.3.4-30) 
 
MUb
[
]2[
]1[
]
Ub
Ub
0
...
If then 
, then b∈ RA and solutions exist, but there are 
=
•=
=
•=
•
an infinite number.  If Ax = b, then A(x + c1U[1]+…+ cMU[M]) = b  (1.3.4-31) as well. 
 
Case III 
 
1≥
Again dim (KA)=M, M  and {
 
Now, for at least on U[j], j = 1, 2, …, M, 
Ax = b has no solution. 
 
While these rules provide insight into existence and uniqueness, to employ them we need: 
1.  A method to determine if KA = 0 from the coefficients of A 
2.  A method to identify basis vectors for KA 

U[1],…U[M]} forms an orthonormal basis for KA. 

jUb
][ ≠
•

0

.  Therefore b∉ RA and the system 

 
 
Point (1) is the subject of the next section.  (2) is discussed in context of eigenvalues. 
 
 
 
 

