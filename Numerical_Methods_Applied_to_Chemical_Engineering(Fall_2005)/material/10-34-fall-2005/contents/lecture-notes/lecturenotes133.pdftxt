1.3.3 Basis sets and Gram-Schmidt Orthogonalization 
 
Before we address the question of existence and uniqueness, we must establish one more 
tool for working with vectors – basis sets. 
 

Let v ∈ RN, with v = 

v


v

:


v


1

2

3

     (1.3.3-1) 








 
We can obviously define the set of N unit vectors 
 

e[1] = 

 

e

[2] = 

   …        

e[N] = 

     (1.3.3-2) 

1


0

0


:


0











0


1

0


:


0











0


0

0


:


1











 
so that we can write v as 
v = v1e[1] + v2e[2] + … + vNe[N]     (1.3.3-3) 
 
As any v ∈ RN can be written in this manner, the set of vectors {e[1], e[2], … e[N]} are said 
to form a basis for the vector space RN. 
 
The same function can be performed by any set of mutually orthogonal vectors, i.e. a set 
of vectors {U[1], U[2], …, U[N]} such that 
 

[j]

•

[k]

U

U

j if
     0
 
This means  that each U[j]  is mutually orthogonal  to all of  the other vectors.   We can  then 
write any v∈ RN as 
 

(1.3.3-4) 

≠

=

k

                           v = 

[1]

ev
'
1

+

ev
'
2

[2]

[N]

ev
'
N

     (1.3.3-5)                       U[1] 

...
++
 

Where we use a prime to denote that 
                                                                                                                         90 deg. 
U[3]        
     (1.3.3-6)                                                  
                                                        
v
v ≠
'
j
j
                                                                                                                  90 deg. 
when comparing the expansions (1.3.3-3) and (1.3.3-5) 
                                                                                                               U[2] 
 
 

Orthogonal basis sets are very easy to use since the coefficients of a vector v∈ RN in the 
expansion are easily determined. 
 
We take the dot product of (1.3.3-5) with any basis vector U[k], k∈ [1,N], 
 

Uv
•

[k]

=

U(v
'
1

[1]

•

[k]
)U

...
++

[k]

U(v
'
k

[k]
)U

...
++

[N]

U(v
'
N

•

[k]
)U

     (1.3.3-6) 

•
 
 

[j]

U

•

U

[k]

[k]

=

U(

•

[k]
)δU

=

[k]

U

jk

2

δ

jk

     (1.3.3-7) 

Because 

with 
 

δ jk

=

j1,


j0,


=
≠

k
k
 

     (1.3.3-8) 

then (1.3.3-6) becomes 

Uv
•

[k]

=

Uv
'
k

2

[k]

v
'
=⇒
k

[k]

2

Uv
•
[k]
U

     (1.3.3-9) 

 
[k]U =1 for all k∈ [1,N], we 
In the special case that all basis vectors are normalized, i.e. 
have an orthonormal basis set, and the coefficients of  v∈ RN are simply the dot products 
with each basis set vector. 
 
Exmaple 1.3.3-1 
 
Consider the orthogonal basis for R3 
 

1


1

0





          




[1]

U

=

 

          

U    

[2]

1


−=

0





          
1




          

       

U

[3]

=

0


0

1









     (1.3.3-10) 

v
v
v







1

2

3



    




what are the coefficients of the expansion 

v

=

Uv
'
1

[1]

+

Uv
'
2

[2]

+

Uv
'
3

[3]

     (1.3.3-11) 

for any v∈ R3, v = 

 

 
 
 
 

First, we check the basis set for orthogonality 
1




1
−


0





= [ 1   1   0]

[2] U
U •

[2]

= (1)(1) + (1)(-1) + (0)(0) = 0 

 

0


0

1









0


0

1









[1] U
U •

[3]

= [ 1   1   0] 

 

[2] U
U •

[3]

= [1   -1   0] 

 
We also have 

= (1)(0) + (1)(0) + (0)(1) = 0     (1.3.3-12) 

= (1)(0) + (-1)(0) + (0)(1) = 0 

2

2

2

[1]U

[2]U

[3]U

=

[1   1   0] 

=

[1   -1   0] 

=

[0   0   1] 

1




1


0




1


−

0


0




0


1





= 2   

= 2 



1




= 1 

So the coefficients of v = 

2

v
1
v
v











1 [v1   v2   v3] 
2

3

1 [v1   v2   v3] 
2

(1.3.3-13) 

 are 

= 

1 (v1 + v2) 
2



1




= 

1 (v1 - v2) 
2

= v

3 







1


1

0


1


−

0


0




0


1





v

'
1

=

[1]

2

Uv
•
[1]
U

 = 

v

'
2

=

[2]

2

Uv
•
[2]
U

 = 

v

'
3

=

[3]

2

Uv
•
[3]
U

 = 

1 [v1   v2   v3] 
1

Although orthogonal basis sets are very convenient to use, a set of N vectors B = {b[1], 
b[2], …, b[N]} need not be mutually orthogonal to be used as a basis – they need merely be 
linearly independent. 
 
 b[1], b[2], …, b[M] ∈ RN.  This set of M vectors is 
Let us consider a set of M N vectors 
≤
said to be linearly independent if 
 

c1b[1] + c2b[2] + … + cMb[M] = 0     implies c1=c2=…=cM=0     (1.3.3-16) 

 
This means that no b[j], j∈ [1,M] can be written as a linear combination of the other M-1 
basis vectors. 
 
For example, the set of 3 vectors for R3 
 

b[1] = 

     

b[2] = 

     

b[3] = 

     (1.3.3-17) 

2
0
0













1


1

0









1


−

0





1




 
is not linearly independent because we can write b[3] as a linear combination of b[1] and 
b[2], 
 

b[1] - b[2] = 

 - 

 = 

    = 

b[3]     (1.3.3-18) 

2
0
0













1


1

0









1


−

0





1




 
Here, a vector v ∈ RN is said to be a linear combination of the vectors b[1], …, b[M] ∈ RN if 
it can be written as 
 

[M]

bv
'
M

     (1.3.3-19) 

v = 

bv
'
1

[1]

[2]

+

bv
'
2

...
++
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

We see that the 3 vectors of (1.3.3-17) do not form a basis for R3 since we cannot express 
any vector v ∈ R3 with  v
b[1], b[2], b[3]} since 
0
as a linear combination of {
3 ≠
 

v = 

1v
'

2
0
0













+ 

2v
'

1


1

0









 +  v

'
3

1


−

0





1




= 

'
2

+

v

'
3

v
'
−
1
v
'
−
3

2v
v
'
2
0













     (1.3.3-20) 

 
We see however that if we instead had the set of 3 linearly independent vectors 
 
 

 



b[1] =  


2
0
0







     

b[2] = 

1


1

0









     

b[3] = 

0
0
2













          (1.3.3-21) 

 
then we could write any v∈ R3 as 
 

2
0
0

v
v
v

1

2

3



















v = 

=  v
'
1






 
(1.3.3-22) defines a set of 3 simultaneous linear equations 
 



















 + 

3v
'

+ 

2v
'

= 

1


1

0



2v
v
'
2
2v







0
0
2

'
1

+

'
3

v

'
2







     (1.3.3-22) 

2v +
=  1v
v
'
'
1
2
2v = 2v  
'
     (1.3.3-23) 
3v
 

32v =
'

v
v
v

1

2

3







 

1v ,
'

2v ,
'

3v
'

, 

=

'
1
'
2
'
3













v
v
v













0
       
1
      
2
0
1
       
0
      
0
1
       
2
      






 
that we must solve for 
 

     v

'
1

=

(v

1

)v
'
2

−
2

     (1.3.3-24) 

v

'
1

=

v
3
2

          
,

     v

'
2

=

          
,v
2
 
 
 
 

We therefore make the following statement: 
 
Any set B of N linearly independent vectors b[1], b[2], …, b[N] ∈ RN can be used as a basis 
for RN. 
 
We can pick any M subset of the linearly independent basis B, and define the span of this 
subset {b[1], b[2], …, b[M]} 
v ∈ RN that can be 
 B as the space of all possible vectors 
⊂
written as 
 
 

v = c1b[1] + c2b[2] + … + cMb[M]     (1.3.3-25) 
 
2
0








For the basis set (1.3.3-21), we choose b[1] =  
0
0



2
0







Then, span { b[1] ,  b[3] } is the set of all vectors v  ∈ R3 that can be written as 
 
     
 

.(1.3.3-26) 

b[3] = 

 and  







= 

c
1

c

3

= c













1

2

3







v
v
v













2
0
2







v = 

+ c3

0
0
2

     (1.3.3-27) 

1b[1] + c3b[3] = c1

2


0

0


 
Therefore, for this case it is easy to see that v ∈  span { b[1] ,  b[3] }, if and only if (“iff”) 
v2 = 0. 
 
Note that if v∈ span{ b[1] ,  b[3] } and w∈  span{ b[1] ,  b[3] }, then automatically  
v + w ∈ span { b[1] ,  b[3] }. 
 
We see then that span{ b[1] ,  b[3] } itself satisfies all the properties of a vector space 
identified in section 1.3.1. 
 
Since span{ b[1] ,  b[3] } 
subspace of R3. 
 
 
 
 
 
 
 
 
 
 

 R⊂ 3 (i.e. it is a subset of R3), we call span{ b[1] ,  b[3] } a 

This concept of basis sets also lets us formally identify the meaning of dimension – this 
will be useful in the establishment of criteria for existence/uniqueness of solutions. 
 
Let us consider a vector space V that satisfies all the properties of a vector space 
identified in section 1.3.1. 
We say that the dimension of V is N if every set of N+1 vectors v[1], v[2], …, v[N+1] ∈ V is 
linearly independent and if there exists some set of N linearly independent vectors  
b[1], …, b[N] ∈  V that forms a basis for V.  We say then that dim(V) = N.     (1.3.3-28) 
 
 
While linearly independent basis sets are completely valid, they are more difficult to use 
than orthogonal basis sets because one must solve a set of N linear algebraic equations to 
find the coefficients of the expansion 
 

v = 

bv
'
1

[1]

[2]

+

bv
'
2

...
++
 

[N]

bv
'
N

          (1.3.3-29) 

'
1
'
2

'
N

















[N]
1
[N]
2

v


v


:

v



b   
...
   
b   
b
[1]
[2]

1
1

b
b   
...
b   
   
[1]
[2]

2
2

          
:
          
:  
:

b
b   
b   
...
   
[2]
[1]


N
N
 
This requires more effort for an orthogonal basis {U[1], … , U[N]} as 
 

v


v

:


v









1

2

N

=

              O(N3) effort to solve for all vj’s         (1.3.3-30) 

[N]
N

v

'
j

=

[j]

Uv
•
[j]
U
U
•

[j]

                          O(N2) effort to find all vj’s 

(1.3.3-9, repeated) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

This provides an impetus to perform Gramm-Schmidt orthogonalization.  We start with a 
linearly independent basis set {b[1], b[2], …, b[N]} for RN.  From this set, we construct an 
orthogonal basis set {U[1], U[2], …, U[N]} through the following procedure: 
 
1.  First, set U[1] = b[1]     (1.3.3-31) 
 
2. Next, we construct U[2] such that 
.  Since U[1] = b[1], and b[2] and b[1] are 
[2]
[1]
U
U
0
•
=
linearly independent, we can form an orthogonal vector U[2] from b[2] by the following 
procedure: 
    
     U[2]    
                    cU[1]               b[2] 
 

 
 
                                                                b[1] = U[1] 
              “subtract” this part  
              from b[2] 
 

 
Then, taking the dot product with U[1],  
 

We write U[2] = b[2] + cU[1]     (1.3.3-32) 

 
Therefore  

[2]

U

•

U

[1]

=

0

= 

b

[2]

[1]

•

U

+

Uc

[1]

•

U

[1]

      (1.3.3-33) 

−

c = 

[1]U
2

[2]b
•
[1]U

     (1.3.3-34) 

 
And our 2nd vector in the orthogonal basis is  






U[2] = b[2] - 

U
•
2
[1]
U

[1]

U

     (1.3.3-35) 







[2]

b

[1]

 
 
 
 
 
 
 
 
 
 

3.  We now form U[3] in a similar manner. 
Since U[2] is a linear combination of b[1] and b[2], we can add a component from b[3] 
direction to form U[3], 
 

U[3] = b[3] + c2U[2] + c1U[1]     (1.3.3-36) 
 
Uc
2

[3]
•b

Uc
1

U

0

= 

•

U

+

+

[1]

[3]

U

•

U

[1]

=

[2]

[1]

[1]

[1]

•

U

     (1.3.3-37) 

 

 

First, we want 
 
 
 
so 
 

 

 

 

 

      = 0 

−

c1 = 

[1]U
2

[3]b
•
[1]U

     (1.3.3-38) 

A similar condition that 

U

[3]

•

c2 = 

 
 yields 
[2]U
2

     (1.3.3-39) 

[2]
U
0
=
[3]b
−
•
[2]U

 
so that the 3rd member of the orthogonal basis set is 
 

U[3] = b[3] - 








[2]U
[3]b
•
2
[2]U








[2]U

−








[1]U
[3]b
•
2
[1]U








[1]U

     (1.3.3-40) 

4.  Continue for U[j], j = 4, 5, …, N where 

1-j
U[j] = b[j] -  ∑
1k
=








[j]b
[k]U
•
2
[k]U








kU

     (1.3.3-41) 

5.  Normalize vectors if desired (we can do this also during construction of 
orthogonal basis set) 

[j]
U ←

[j]

[j]

U
U

     (1.3.3-42) 

 

 

 

 

As an example, let us use this method to generate an orthogonal basis for R3 such that the 
1st member of the basis set is 
 

U[1] = 

     (1.3.3-43) 

1


1

0









 
First, we write a linearly independent basis that is not, in general, orthogonal.  For 
example, we could choose 
 

b[1] = 

     

b[2] = 

     

b[3] = 

1


0

0









0


0

1



              (1.3.3-44) 







1


1

0









 
We now perform Gram-Schmidt orthogonalization, 
 

1.  U[1] = b[1] = 

1


1

0


2.  We next set  







       (1.3.3-45)   

U[2] = b[2] - 








[1]U
[2]b
•
2
[1]U








[1]U

     (1.3.3-35, repeated) 

2

[1]U

= [1   1   0] 

1


1

0









= 2     (1.3.3-46) 

[2] U
b •

[1]

 = [1   0   0] 

1


1

0









=1     (1.3.3-47) 

 

 

so 
 

 
We now calculate  

U[3] = b[3] - 








2

[2]U

= [1/2   -1/2   0] 

−

[2]U



[1]U
[3]b
[2]U
[3]b


•
•


2
2


[1]U
[2]U




(1.3.3-41, repeated) 
1


2








2
 −+





2

=












1
2





−

0

1
2

1
4

+

1
4

1
2

= 

[1]U

 








=

1
2

     (1.3.3-50) 

U[2] = 

1


0

0









-

1
2

1


1

0









1
2





−=






0
















1
2

1


1

0



Note 

[2] U
U •

[1]

 = [1/2   -1/2   0] 

     (1.3.3-48) 

= ½ - ½ = 0     (1.3.3-49) 

[3] U
b •

[2]

= [0   0   1] 











1
2

−

0

= 0     (1.3.3-51) 

1
2











[3] U
b •

[1]

= [0   0   1] 

1


1

0









= 0     (1.3.3-52) 

 

We therefore have merely U[3] = b[3] = 

 

 

0


0

1









    (1.3.3-53) 

Our orthogonal basis set is therefore 

U[1] = 

1


1

0









        

U[2] = 

1
2

−

0











1
2











        U[3] = 

0


0

1









    (1.3.3-54) 

 

 

