Problem Set 9 Parameter Estimation and Statistics 
10.34 Fall 2005, KJ Beers 
 
Ben Wang, Mark Styczynski 
December 9, 2005 

 
Problem 1: 8.A.1 
 
We are asked to set up the linear algebraic equations to obtain a least squares 
parameter estimate.  Our linearized model is of the form: 
 

log
10

=Nu

log
10

αα
+
0
1

log
10

Re

+

α
2

log
10

Pr

  

 

(1) 

 
Using our standard notation, our response, y, is a measured value of Nu 
 
[
]
log
.0;
8986
4261
5098
.1;
.0;
5520
1521
.2;
.0;
9676
.1

Nu

=

=

y

 

log10

(2) 

 
Our input parameters are the values of Re and Pr.  Taking the log of these 
values we use them in our predictor matrix (design matrix) which has the form of: 
 

X

=

 

 

 

 

(3) 

1


1

1


1


1

1



0
1
−
2
−
0
1
−
2
−

3147
.0
−
3147
.0
−
3147
.0
−
4055
.0
4055
.0
4055
.0












log
α
10
0
α
1
α
2













 
And our parameters that we’d like to fit, according to our linear model are: 
 

θ

=

  

 

 

 

(4) 

 
The system of equations to solve, we get from equation (8.23) from Beers: 
 
 

(

XX
T

)
=θ
LS

yX
T

 

 

 

 

(5) 

 
=  and 
This looks very similar to our old friend Ax = b, where 
A
XX T
Now we just use Gaussian elimination to solve this set of equations: 
 

b
yX T = .  

bA
\=θ
LS

 

 

 

 

 

(6) 

 
Working out all the algebra we get the following system of equations: 
 

1


1

1


1


1

1



0
1
−
2
−
0
1
2
−

3147
.0
−
3147
.0
−
3147
.0
−
4055
.0
4055
.0
4055
.0












=

.0
2939


0464
.0
−

.0
3705

−

3996
.0


.0
0615

2581
.0
−














=

 

XXA
T
=

=

1
0
3147
.0







−

1
1
−
3147
.0

−

1
2
−
3147
.0

−

1
0
4055
.0

1
1
−
4055

.0

1


2
−

4055
.0



 

6
6
−
1182

.0

6
−
10
.0
1182

−

.0
1182
1182
.0
−
.0
1491







 

 






 

b

=

1
0
3147
.0







−

1
1
−
3147
.0

−

1
2
−
3147
.0

−

1
0
4055
.0

1
1
−
4055
.0

1


2
−

4055
.0



0801
.0




2420
.1


0526
.0





 
Plugging these values in, we get to: 
 

 

6
6
−
1182

.0

6
−
10
1182
.0

−

.0
1182
1182
.0
−
1491
.0







log
α
0
10
α
1
α
2













=

0801
.0




2420
.1


0526
.0





 

 

(7) 







 
Solving via Gaussian elimination by hand 
 

6
6
−
1182

.0







6
−
10
1182
.0

−

.0
1182
1182
.0
−
1491
.0

0801
.0




2420
.1


0526
.0











 

 

First we determine 

λ
21

=

 
next we determine 

 

A
21
A
11
6
0
1182







.0

−=

1

.  Now we take [row 2] – λ21 [row1] 

6
−
4
1182

.0

−

.0

.0

1182


0

1491



0801
.0


.1
3221

.0
0526









 

λ
31

=

A
31
A
11

=

.0
0197

.  Now we take [row 3] - λ23 [row1] 

6
0
0

.06
−
4
0

.0

1182
0
1468







0801
.0




3221
.1


0510
.0











 

 
We are fortunate that this is all the elimination we need to do.  Using backward 
substitution: 
 

θLS

=

log
α
10
0
α
1
α
2













=

3370
.0
.0
3305
.0
3475













   

 

 

(8) 

  
So these are the parameters that minimize the least squared error.  Now we are 
asked to find the 95% confidence intervals.  The 95% confidence interval can be 
calculated using equation (8.132) from Beers: 
 
[

XXs
T



]
1
−
θ
jjM

=
θθ
j
jM
,

2/1




T
,
2/
αν

(9) 

   

±

 

 
For each of the parameters, all we have to know is the sample variance, the 
appropriate T-distribution value for a given ν and α, and the diagonals of (XTX)-1.   
 
(XTX)-1is calculated from  (
)
)(
=−1
XXXX
T
T
 

I

 

6
6
−
1182
.0

6
−
10
1182
.0

.0
1182
1182
.0
−
1491
.0






 
This is can be rewritten into 3 systems of equations 
 

XX
T
XX
T
XX
T







−

21

(
XX
T
(
XX
T
(
XX
T

)
1
−
11
)
1
−
)
1
−
31








(
(
(

)
1
−
12
)
1
−
22
)
1
−
33

(
(
(

XX
T
XX
T
XX
T

)
1
−
13
)
1
−
)
1
−
33

23








=

001
010
100













 

 
Backward substitution gets you: 
 
)
(
1


−
XX
4193
.0
T


11


)
(


1
−
XX
T
25.0




)
(


1
−
XX
T
1342
.0


−


31




This can repeated for the other “systems”.  This is especially easy because we 
use the same A matrix with the same pivots.  Therefore for the following system: 
 
)
(
−
)
(
1
−
(
)
1
−

.0
1182
1182
.0
−
1491
.0

6
6
−
1182
.0

6
−
10
1182
.0

XX
T
XX
T
XX
T

1
12

=

=

0


1

0















22

32

−

21

 

 





















 
we get: 

 

6
.0
1182
6
−


10
1182
6
.0
−
−

1182
1182
1491
.0
.0
.0

−

Eliminating with the same λ31 λ21 we get 
 
)
(
1
−
11
)
(
1
−
(
)
1
−
31

1182
.06
−
4
0
1468
.0
0

XX
T
XX
T
XX
T

6
0
0

21



























(
(
(














XX
T
XX
T
XX
T

)
1
−
11
)
1
−
)
1
−
31

21








=

1


0

0









 

=







−

1
1
0197
.0







 

1182
.06
6
−


0
4
0

0
1468
.0
0


Backward substitution gets you: 
 
(
XX
T
(
XX
T
(
XX
T

)
1
−
12
)
1
−
)
1
−

32

22








 
The last system yields: 

(
(
(














XX
T
XX
T
XX
T

)
−
)
1
−
)
1
−

1
12

22

32








=

25.0


25.0

0









=

0


1

0









 








 

6
6
−


10
6
−

1182
1182
.0
.0

−

After elimination we get: 
 

1182
.06
6
−


0
4
0

1468
.0
0
0


Upon backward substitution: 
 

.0
1182
1182
.0
−
1491
.0

(
(
(














XX
T
XX
T
XX
T

)
1
−
31
)
1
−
32
)
1
−
33








=

0


0

1









 

(
(
(














XX
T
XX
T
XX
T

1

)
−
)
1
−
)
1
−

31

32

33








=

0
0
1













 

(
XX
T
(
XX
T
(
XX
T

)
1
−
13
)
1
−
)
1
−

33

23















=

−





1342
.0
0
812.6







 

 
We put all these together column by column to get: 
 

(

)
1XX T
=−

−

4193
.0
25.0
1342
.0

−

25.0
25.0
0







.0

.6

1342
0
812







 

 
Now that we are done with the inverse, we look towards the other parameter.  s 
is calculated from: 
 

s

=

1
ν

(

y

N
∑ =
k
1

[ ]
k

−

y
)

(
,
θ

[ ]
k

x

)
)
2

  

 

 

(10) 

 
 α is specified for the appropriate confidence interval desired, here 0.05 (95% 
confidence intervals).  ν can be calculated from: 
 

ν N
=

−

dim

( )
θ

36
−=

 

 

 

 

(11) 

 
The value of Tν,α/2 is calculated from a simple piece of Matlab code or by looking 
it up on a chart of T-values. 
 
Putting all of these values into the equation (9), you get 95% confidence intervals 
on the order of: 
 

conf_intervals = 
 
    0.3244    0.3497 

    0.3208    0.3403 
    0.2964    0.3986 
 
This can be accomplished with the following code: 
 

% benwang_HW9_8A1.m 
% Ben Wang 
% HW#9 Problem #1 
% due 12/9/05 9 am 
  
% Linear Least Squares Fit 
  
% ======= main routine benwang_HW9_8A1.m 
function iflag_main = benwang_HW9_8A1(); 
  
iflag_main = 0; 
  
%PDL> clear graphs, screen etc. general initialization 
clear all; close all; clc; 
  
Nu = [1.9676; .8986; .4261; 2.5098; 1.1521; .5520]; 
Re = [1; 1e-1; 1e-2; 1; 1e-1; 1e-2]; 
Pr = [0.73; 0.73; 0.73; 1.5; 1.5; 1.5]; 
  
X = [ones(6,1) log10(Re) log10(Pr)]; 
  
y = log10(Nu); 
  
A = X'*X; 
b = X'*y; 
  
theta_LS = A\b 
A_inv = inv(A); 
  
% Calculate confidence intervals 
  
alpha = 0.05; 
nu = length(Nu) - length(theta_LS); 
  
y_k = X*theta_LS; 
  
for j = 1:6 
    s_index(j) = (y_k(j) - y(j))^2; 
end 
sum(s_index); 
s = sqrt(1/nu*sum(s_index)); 
t_val = tinv(1-alpha/2,nu); 
  
for i = 1:3     
    CI(i) = t_val*s*sqrt(A_inv(i,i)); 
    conf_intervals(i,1) = theta_LS(i) - CI(i); 
    conf_intervals(i,2) = theta_LS(i) + CI(i); 

 
Problem 8A2 
 
Here we are asked to do problem 8A1 with the help of the regress function in 
Matlab.  This is accomplished with some very simple code, yielding: 
 

end 
  
conf_intervals 
  
return 

theta = 
 
    0.3370 
    0.3305 
    0.3475 
 
 
theta_CI = 
 
    0.3244    0.3497 
    0.3208    0.3403 
    0.2964    0.3986 

 
You will note that you get the same values as in 8A1, with the possible exception 
of minor roundoff error. 
 

% benwang_HW9_8A2.m 
% Ben Wang 
% HW#9 Problem #2 
% due 12/9/05 9 am 
  
% Linear Least Squares Fit 
  
% ======= main routine benwang_HW9_8A2.m 
function iflag_main = benwang_HW9_8A2(); 
  
iflag_main = 0; 
  
%PDL> clear graphs, screen etc. general initialization 
clear all; close all; clc; 
  
Nu = [1.9676; .8986; .4261; 2.5098; 1.1521; .5520]; 
Re = [1; 1e-1; 1e-2; 1; 1e-1; 1e-2]; 
Pr = [0.73; 0.73; 0.73; 1.5; 1.5; 1.5]; 
  
X = [ones(6,1) log10(Re) log10(Pr)]; 
y = log10(Nu); 
alpha = 0.05; 
[theta,theta_CI] = regress(y,X,alpha) 

(
)
αα=Nu
Re
1
0

   

 

 

 

(12) 

(
Pr

) 2
α

%[theta,theta_CI,residuals,res_CI,stats]=regress(y,X,alpha) 
  
return 

 
Problem 8A3 
 
This problem asks us to use the nlinfit function in Matlab to do regression using a 
nonlinear model.  We now use our model that has not been linearized: 
 

 
Matlab provides several functions that help you do regression.  The first is 
nlinfit, which takes as input a design matrix of inputed predictor variables, the 
response variable, a function name that contains the nonlinear model, and an 
initial guess for the values of model parameters.  It returns the least squares 
estimate for the parameters, the residual errors in the fit, as well as the Jacobian, 
which will be used to help calculate the confidence intervals. 
 
The next function used will be nlparci which takes the output from nlinfit 
and uses them to calculate confidence intervals. 
 
When we obtain parameter values and confidence intervals, with nlinfit and 
nlparci, we obtain: 
 

theta = 
 
    2.1857 
    0.3345 
    0.3400 
 
 
theta_CI = 
 
    2.1579    2.2134 
    0.3252    0.3438 
    0.3089    0.3710 

theta = 
 
    0.3396 
    0.3345 
    0.3400 
 

 
If we remember now that, in comparing our nonlinear model to our linear model, 
we have to take the log of our pre-factor α0.  We then get values of: 
 

 
theta_CI = 
 
    0.3340    0.3451 
    0.3252    0.3438 
    0.3089    0.3710 

 
We see that the values are slightly different, but the magnitudes of the 
differences are relatively negligible. 
 

% benwang_HW9_8A3.m 
% Ben Wang 
% HW#9 Problem #3 
% due 12/9/05 9 am 
  
% Nonlinear Least Squares Fit 
  
% ======= main routine benwang_HW9_8A3.m 
function iflag_main = benwang_HW9_8A3(); 
  
iflag_main = 0; 
  
%PDL> clear graphs, screen etc. general initialization 
clear all; close all; clc; 
  
% specify predictors and responses 
Nu = [1.9676; .8986; .4261; 2.5098; 1.1521; .5520]; 
Re = [1; 1e-1; 1e-2; 1; 1e-1; 1e-2]; 
Pr = [0.73; 0.73; 0.73; 1.5; 1.5; 1.5]; 
  
X_pred = [Re Pr]; 
y = Nu; 
  
%initial guess for theta 
theta_0 = [0.3; 1; 1]; 
  
% calculate least squares fit using nonlinear model 
[theta, residuals, Jac] = nlinfit(X_pred,y,@benwang_nlin,theta_0); 
sum(residuals.^2) 
% calculate confidence intervals 
theta_CI = nlparci(theta,residuals,Jac); 
% [y_hat,y_hat_HW] = nlpredci(@benwang_nlin,X_pred,theta,residuals,Jac); 
  
% output theta and 95% confidence intervals 
theta 
theta_CI 
  
% to compare against linear regression, we need to get alpha_0 back into 
% its log form 
  
theta(1) = log10(theta(1)); 
theta_CI(1,1) = log10(theta_CI(1,1)); 

theta_CI(1,2) = log10(theta_CI(1,2)); 
% output theta and 95% confidence intervals 
  
theta 
theta_CI 
  
return 
  
% ==== subroutine that contains model information 
  
function y_hat = benwang_nlin(theta,X_pred) 
  
Re = X_pred(:,1); 
Pr = X_pred(:,2); 
  
alpha_0 = theta(1); 
alpha_1 = theta(2); 
alpha_2 = theta(3); 
  
% nonlinear model 
y_hat = alpha_0.*(Re).^alpha_1.*(Pr).^alpha_2; 
  
return 
 

