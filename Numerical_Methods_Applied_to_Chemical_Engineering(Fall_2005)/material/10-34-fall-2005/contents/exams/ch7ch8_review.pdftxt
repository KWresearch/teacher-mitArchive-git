Ch 7 Probability Theory and Stochastic Simulation: 
 
Frequentist statistics: 
 

N
E
N
)
E
2
W

 

)

)

=

≈

≈

∩

 
 

=
)

-  Probability of observing E:   

 Joint Probability:  
- 
 
-  Expectation: 

(
)1
) (
| EEpEp
 
1
2
1
ν    
N W
exp
∑ =
1
ν
N
exp
(
) (
)2
EEpEp
|
2
1

(
Ep
(
Ep
1
WE
(
(
) (
Bayes’ Theorem:  
 
 
EEpEp
|
1
2
1
- Bayes’ Theorem is general. 
 
 
Definitions: 
 
 
[
]
[
(
)
(
(
)
(
)
(
)
2
-  variance:  
WEWEW
WE
WE
var
2
=
−
=
−
(X, Y independent, var(X+Y) = var(X) + var(Y)) 
- 
)Wvar
(
-  standard deviation: 
 
 
=σ
[
]
]
[
}YEYXEXE
( )
)
(
)
{
-  covariance 
 
YX
cov
,
=
−
−
random variables X and Y 
-  covariance matrix 

]2
)
   

=

(

 

 

 
Important Probability Distributions Definitions: 
 

 

 

 

, for two 

-  Discrete random variable 
o  For 
{
}m
 
XX
X
X
,...,
,
=
i
2
1
o 
(
) =iXN
number of observations of Xi 
o  T is the total number of observations   
o  Probability is definied by: 
o  Normalization is defined by  
-  Continuous random variable 
 
o  This is just the continuous version of the above, defined by 
integrals instead of limits, differentials instead of increments 
hix
o  Normalization condition:  
( )
1  
dxxp
 
=
∫
lox
( )
xE

(
) T
XNM
 
j =
∑
i
=1
(
XN
)
(
XP
i =
T
(
) 1
 
jXP
=

( )
dxxxp

M
∑
i
1
=

=

x

)

 

 

 

 

 

 

 

 

i

hix
∫=
lox

 

o  Expectation 
 
-  Cumulative Probability distribution 
o  Basis of RAND in matlab 

x
o 
(
(
)
)
 
u
dx
xp
XF
'
'
=
= ∫
M
lox
o  u is defined as uniformly distributed 

0

≤≤ u

1

 

 
Bernoulli trials 
 
o  Concept that observed error is the net sum of many small random 
errors 

 
Random Walk Problem 
-  key point: independence of coin tosses 
 
 
-  Main results:  
0=x
 

x =
2

2

nl

 

 
Binomial Distribution 

H

)

=

(
nnP
,

-  probability distribution: 

n


n

n
!

=
(
)!
nn
n
!
−

H
H
H
-  BINORND 
Matlab to generate random number distributed 
 
using binomial distribution 

-  binomial coefficient: 

n
n

)Hnn
−

(
1









)(

Hn

−

p

p

 

 

H

H

H

 
Gaussian (Normal) Distribution 
 

=

-  Take binomial distribution, change into probability of observing net 
displacement after n steps of length l 
n
!
o 
(
)
lnxp
;
,
(
(
lxn
lxn
/
/
 −
 +


!




2
2
-  Evaluate in limit that n(cid:198) ∞, take natural log, and use Stirling’s 
approximation 
-  Algebra, and taylor expand around the ln terms 
-  Taking the exponential and normalizing such that: 

1
2

= 1









  

)

)

 

n

∞
(
)
dxlnxP
;
,
∫
∞−

2

2

 

 

 

=

−

- 

exp

nl=σ
2

)
(
xP
;
σ

x
1




2
2
2
σ
πσ


-  Binomial Distribution of random walk reduces to Gaussian Distribution 
as n(cid:198) ∞ 
-  Central Limit Theorem: sequence of random variables, which are not 
distributed normally, the statistic 
−
µξ
1
o 
N
j
j
.
 
S
∑
n
σ
j
1
=
j

=

n

 

- 

o  random variable: 
2
jξ  with mean 
.jµ  and variance 
jσ  
is normally distributed in the limit that n(cid:198) ∞, with variance = 1 
2


S
1
o 
(
)
SP
n


2
2
π


-  Non-zero Mean (basis of randn) 
)
(
2

x
1
−
µ
(
)
o 
,
2
σµ

2
2
2
σ
πσ


exp

exp





N

−

=

=

−

 

n

 

-  Multivariate Gaussian Distribution (use of covariance matrix) 
[
]
)
{
(
}j
o  Covariance Matrix: 
[
]
]
[
( )
(
)
 
E
E
E
cov
νννν
ν
=
−
−
i
i
j
ij
o  Covariance Matrix is always symmetric and positive definite 
o  For independent components: 
( )
 
I2
cov σν =
o 
[
] TAx
)
( )
( )
(
 
xA
A
cov
cov
cov
=ν
=
o 
if ν is a random vector and c is a constant vector: 
(
)
[
]
[
]c
(
)
( )
( )
o 
T
T
c
c
c
c
c
var
var
cov
cov
ν
=
⋅
ν
=
ν
⋅=
ν
 
1
1
)
(
(
)
)
(


o 
T
 
;
,
1
− µνµν
µν
−
Σ
−
=Σ


)
(
2
N
2/
2


π

exp

P

Σ

−

 
Poisson Distribution 
 

-  Poisson distribution can be used to determine probability of success if 
there are n trials, derived in the limit as n(cid:198) ∞ 
-  Total number of successes in trial is a random variable, which d 
-  Another limiting case of binomial distribution 
(
)
pn
ξ
(
)
;
ξ
!
ξ
-  p = probability of individual success 
-  n = number of trials 
-  ξ = result if success or failure, typically {1,0} with different probabilities 

pne
−

pn
,

- 

P

=

 

 
Boltzmann/Maxwell Distributions 
( )
qE
1
( )
−

o 
qP
exp
 
 
 
=


Q
kT
o  Q is the normalization constant 
o  Replacing E(q) for kinetic energy we arrive at Maxwell Distribution 
2ν


m
o 
( )
ν


kT
2





exp

∝

P

−

 

 
Brownian Dynamics and Stochastic Differential Equations 

x

 

=

 

 

−

 









exp

2

t
2
2
σ

xv =
τ

2R
2
ρ
9
µ

-  velocity autocorrelation function 
o 
( )
(
)
t
τ/
−
tC
C
e
 
0
0
≥
≈
xV
V
V
x
o 
( )
( )
( )tD
VtV
δ2
0 =
x
x
-  Dirac Delta Function 
1
o 
( )
t
lim
δ
2
πσ
0
→
σ
∞
o 
( ) ( )
( )0f
tf
dt
t
=
δ
∫
∞−
-  Langevin equation 
-  Wiener process 
-  Stochastic Differential equations 
o  Explicit Euler SDE method 


dU
1
o 
(
(
( )
)
)


t
t
tx
tx
−∆+
−=
+∆

dx
ζ


Ito’s Stochastic Calculus 
o  Example: Black-Scholes 
o  Fokker-Planck 
o  Einstein Relation 
o  Brownian motion in multiple dimensions 
-  MCMC 
o  Stat Mech example 
o  Metropolis recipe (pg497) 
o  Example: Ising Lattice 
o  Field theory 
o  Monte Carlo Integration 
o  Simulated annealings 
o  Genetic Programming 

( )
tx

- 

[
2

D

]
2/1

(
)t
W
∆

 

 

 
Bayesian Statistics and Parameter Estimation 
 
Goal of this material is to draw conclusions from data (“statistical inference”) and 
estimate parameters.  Basic definitions 
 

-  Predictor variables: x = [x1 x2 x3… xM] 
-  Response variable: y(R) = [y1 y2 y3 … yL] 
-  θ: model parameters 

 
Main goal: match model prediction to that of the observed response by selecting 
θ.  
 
Single-Response Linear Regression 
 

[k]], for 

[k] x2

[k] x3

[k]… xM

-  set of predictor variables, known a priori: x[k] = [x1
the kth experiment 
-  measurement y[k] 
[ ]
[ ]
[ ]
[ ]k
[ ]
k
k
k
 
-  assume a linear model: 
x
x
y
x
...
k
++
=
+
+
+
ββ
β
ε
β
MM
22
11
0
the error in ε[k] is responsible for the difference between model and 
- 
observed 
]T
[
-  define 
 
 
 ... 
ββββθ=
0
1
2
M
(
)
true
 
 
- 
 
response is:  
y
x
θ +
⋅
=
[ ]
(
)
[ ]
true
k
=ˆ
-  model prediction is:  
 
 
 
y
x
k
θ⋅
-  define design matrix X, which contains all information about every 
experiment (with different predictor variables) 
]1[


x
−−−
−−−


]2[
x
−−−
−−−











:
Nx
[

[ ]k
ε

 

−−−

]

−−−

X

=

[ ]
k

[ ]
k

 

 

 
-  vector of predicted responses: 

 
 

y
)

( )
θ

=

=

X

 
θ

]1[

y
)
]2[
y
)

:
]

[

N

y
)

( )
θ
( )
θ






( )
θ











 
Linear Least Squares Regression 
 

[ ]
k

[
]2
= N
( )
[ ] ( )
 
-  minimize sum of squared errors: 
 
y
S
y
k
)
θ
θ
−
∑
k
1
=
-  First derivative = 0, 2nd derivative is > 0, using these conditions with 
above equation you can derive a linear system 
(
)
(
)
1−
 (review point?) 
- 
XX
yX
XX
yX
 (cid:198) 
T
T
T
T
=θ
=θ
LS
LS
-  XTX, contains information about experimental design to probe the 
parameter values 
-  XTX is a real, symmetric matrix that is positive-semidefinite 
-  Solving this is through standard linear solving, or QR decomposition or 
some other method 
-  All this estimates parameters, but does not give us accuracy of our 
estimates 

 
Bayesian view of statistical inference 
 

-  Statement of belief (especially in random number generators) 

 
Bayesian view of single-response regression 
 

|

[ ]
(
)
[ ]k
[ ]
true
k
-  Begin with 
 
y
x
k
θ +
ε
=
⋅
-  When we repeat this experiment multiple times, we get a vector ε 
(
) 0=kE ε
(
)
[ ]
[ ]
[ ]
-  With Gauss-Markov Conditions: 
 
 
cov
,
j
k
2
σδεε
=
kj
-  We also assume that our error is normally distributed  
-  Probability of observing some response y 
N
1
1
(
)
=

−

o 
( )
yp
,
σθ
θ



22
2
σ
π


-  We use Bayes’ Theorem to get probability of θ and σ 
(
) (
)
yp
p
,
|
,
σθσθ
(
)
-  Posterior density: 
 
p
y
|
,
σθ
( )yp
-  p(y) is a normalizing factor 
-  we redefine p(y | θ, σ) to l(θ, σ| y) 
- 
in the Bayesian framework we want to maximize posterior density 
( )
( )
-  Non-informative priors: p(θ,σ)=p(θ)p(σ) 
p
c
1−∝σθp
~θ  

exp

−
σ

=

S

 

N

 

 
Nonlinear least squares 
 

- 

the treatment via least squares still works, we just use numerical 
optimization, utilizing a cost function, to get there: (review point?) 
[
]2
)
(
1
1
N
( )
( )
[ ]
k
[
]
- 
 
 
F
S
xf
y
;
k
θ
θ
θ
=
=
−
∑
t
cos
2
2
k
1
=
-  use of linearized design matrix 
-  Hessians (first order approximation to get to XTX).  Remember to get 
convergence, approximate Hessian needs to be positive-definite. 
-  Levenberg-Marquardt method: ill-conditioned systems 

 
Generating Confidence Intervals 
 

- 

t-statistic 
o 
t

≡

y
(
s
/

  

θ−
)N
t


1
+∝


ν


in the limit that ν approaches infinity, t-distribution reduces to 
Normal distribution 
-  confidence intervals for model parameters 

o 
o 

)
|
ν

(
1
+
ν
2

(
tp

  

−

2

)

{
[
XXs
T

} 2/11
]
−
jj

 

θ
M

o 
o 

T
=
±
θθ
j
jM
2/
,
,
αν
( )θ
 
= N
dim−
ν

 
MCMC in Bayesian Analysis 
 

