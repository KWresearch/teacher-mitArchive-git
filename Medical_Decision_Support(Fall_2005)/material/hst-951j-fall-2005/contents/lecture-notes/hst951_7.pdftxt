Harvard-MIT Division of Health Sciences and Technology 
HST.951J: Medical Decision Support, Fall 2005
Instructors: Professor Lucila Ohno-Machado and Professor Staal Vinterbo 

6.873/HST.951 Medical Decision Support

Spring 2005 

Variable Compression:

Principal Components Analysis

Linear Discriminant Analysis


Lucila Ohno-Machado 


Variable Selection


•  Use few variables 

•  Interpretation is easier


Ranking Variables Univariately


•	 Remove one variable 
from the model at a 
time 
•	 Compare 
performance of [n-1]
model with full [n]
model 
•	 Rank variables 
according to
performance
difference 

Screenshots removed due to copyright reasons.


Figures removed due to copyright reasons. 
Please see: 
Khan, J., et al. "Classification and diagnostic prediction of cancers using gene expression 
profiling and artificial neural networks." Nat Med 7, no. 6 (Jun 2001):  673-9. 

Variable Selection


•	

Ideal: consider all variable combinations

–  Not feasible in most data sets with large number of n variables: 
2
n 
•	 Greedy Forward:

–	 Select most important variable as the “first component”, Select 
other variables conditioned on the previous ones 
–	 Stepwise: consider backtracking 
•	 Greedy Backward: 
–	 Start with all variables and remove one at a time. 
–	 Stepwise: consider backtracking 
•	 Other search methods: genetic algorithms that optimize
classification performance and # variables 

Variable compression


•	 Direction of maximum variability 
– PCA  
•  PCA regression

– LDA 

– (Partial Least Squares) 


correlatio 
n

_ coefficien 
t

σ XY   = ρ
r  = 
σ σ  Y 
X 

VARIANCE 
n
∑ ( X  − 
X 
i 
i = 1 
n − 1 

X 
)( 

σ  = 
XX 
st  _ deviation 
n 
∑ 
X 
( 
i 
1 
= 

i 

i  − X  ) 

COVARIANCE  
n 
∑ ( X  − 
Y 
Y 
X  
)(  i  − 
i 
σ XY =  i = 1 
n − 1 

) 

Xσ 

= 

X 
− 

)(

X 

i 

−

X

)

n 

1 
− 

Y 

Covariance and 
Correlation Matrices 

⎡σXX   σXY   ⎤ 
cov = ⎢ 
⎥ 
⎣σYX   σYY   ⎦
⎡ 1  ρ⎤

corr  = ⎢
⎣ρ  1 ⎥
⎦ 
n 
∑ ( X  − 
i
i =1 
n  − 1 
n 
∑ ( X  − 
X 
X 
)(
i 
σ  =  i =1 
XX 
n  − 1 

σ  = 
XY 

X 

Y 
)(  −
i 

Y 

)


i  − X  ) 

0

X 

Slope from linear regression is asymmetric,  

covariance and ρ are symmetric

β 0  =  y − β x
y  = β + β 1 x

1
0 
Σ ( x  − x )( 
y 
y  = 2 + 4 x 
−
Σ ( x  − x ) 2 
x  =  y / 4 − 2 

β 1  =

y 

) 

y 

.0 35 ⎤ 
⎡  .0 86
cov = ⎢ 
⎥ Σ = 
⎣  .0 35 15.69⎦

.0 96 ⎤

⎡  1 
corr  = ⎢
⎥ 
⎣  .0 96
1  ⎦

x 

Principal Components Analysis


•	 Our motivation: Reduce the number of variables 
so that we can run interesting algorithms 
•	 The goal is to build linear combinations of the 
variables (transformation vectors) 
•	 First component should represent the direction 
with largest variance 
•	 Second component is orthogonal to 
(independent of) the first, and is the next one
with largest variance 
•	 and so on…


Y 

X and Y are not 
independent 
(covariance is not 0)


cov


Y =
 ( X  * 4)
 +
 e

σ 
⎡

XX 
σ
⎢
≠ 0 
⎣

XY 
.0 35

.0 86
⎡

⎢
.0 35 15.69

⎣

.0 96

σ 
XY 
σ
YY 
⎤ 
⎥
⎦


cov


=


≠
 0


⎤

⎥
⎦

=

=
ρ


0 

X1 

Eigenvalues


I is the identity matrix.

A is a square matrix (such as the covariance matrix).

|A - λ I| = 0 

λ  is called the eigenvalue (or characteristic root) of A.


⎡ 1 0⎤  = 0
⎡σ XX  σ XY  ⎤
−λ
⎥
⎢
⎢
⎥ 
⎣ 0 1⎦
⎣σ XY  σ YY 
⎦ 

Eigenvectors

a
a
σ
⎤ 
⎡ 
⎤ 
⎡ 
⎤ 
XY

q= 
⎥
⎢
⎥
⎢
⎥
b 
b 
σ

⎣ 
⎦ 
⎣
⎦ 
⎦

YY

c
c
σ
⎤ 
⎤ 
⎡ 
⎤ 
⎡ 
XY

⎥
⎥
⎢
⎥
⎢
d 
d 
σ

⎦

⎣ 
⎦ 
⎣ 
⎦

YY


m=


σ
⎡ 
XX  
⎢
σ

⎣ 
XY 
σ
⎡ 
XX  
⎢
σ

⎣ 
XY 

q and m are eigenvalues 
[a b]T  and [c d]T  are eigenvectors, they are orthogonal 
(independent of each other, do not contain redundant information) 
The eigenvector associated with the largest eigenvalue will point in 
the direction of largest variance in the data. 
If q > m, then [a b]T  is PC1 

Principal Components

.0 23⎤ 
.0 23⎤ 
.1 
27
.5 12
⎡ ⎤ 
⎡ 
⎡

⎥
⎢
⎢
⎥
⎥
⎢
.0 97
65 
.0 97

.
.5 12 21 
⎣

⎦

⎦

⎣

⎦ 
⎣

. 0 97
.0 97
.1 
27
.5 12
⎡ 
⎡
⎤

⎤

⎡

⎤ 
⎢
⎥
⎢
⎥
⎢
⎥
⎦
23

.0 
65  −  .0

23 
.
.5 12 21 
⎣
−

⎣

⎦

⎣

⎦ 

. 0 05
= 

=


22.87


Total variance is 21.65 + 1.27 = 22.92 

X2

0 

X1

=


, PC


O1  O2  O3

x11 
x
x31
⎡

⎤

21 
⎥
⎢
x32 
x22 
x12 
⎦

⎣
ax11 

bx12 
a 
b

+

⎡

⎤

⎢
⎥
dx12 

cx11 

c 
d

+

⎦

⎣ 
23.0 
97.0 
23.0 
⎡
⎤

⎢
⎥
97.0 
97.0 
23.0 
−

⎦

⎣ 

=

=


x

x

x 

⎡
⎢
⎣
⎡
⎢
⎣

=

Transformed data

a
c
⎤

⎡

⎥
⎢
b 
⎦
d 
⎣

ax 
bx 
+

dx 
cx 
+

22 
x11

x 
97.0 
12 
x11

x 
23.0 
12 

bx32
⎤
⎥
dx32
⎦
x 
97.0 
+

21 
x 
23.0 
−


ax31
+

cx
+

31 
23.0 
97.0 

21 
+

−


x 
x 

22 

21 

21 

22

22

23.0 
97.0 

P
C
2 

P
C
1 

Total variance is 22.92 

Variance of PC1 is 22.87, so it captures 99% of the variance.

PC2 can  be discarded with little loss of information.


x31 
x31

x32
97.0 
+ 
⎤
⎥
32 x 
23 
.0 
− 
⎦
X2 

0 

X1 

PC1 is not at the regression line

PC 
Reg 

Y 

•	 y=4x 
• [a b]T  = [0.23 0.97] 
•	 Transformation is 
0.23x+0.97y 
•	 PC1 goes thru 
(0,0) and (0.23,0.97) 
Its slope is 
–	 0.97/0.23 = 4.217 

•	

0	

X 

PCA regression

•	 Reduce original 
dimensionality n (number of
variables) finding n PCs, such
that n<d 
•	 Perform regression on PCs 

•	 Problem: Direction of greater 
overall variance is not 
necessarily best for
classification 
•	 Solution: Consider also 
direction of greater separation
between two classes 

(not so good) idea: 

Class mean separation

•	 Find means of each 
category 
•	 Draw the line that passes 
through the 2 means 
•	 Project points on the line

(a.k.a. orthogonalize points with 
respect to the line) 
•	 Find point that best 
separates data 

Fisher’s Linear Discriminant 


•	 Use classes to define discrimination line, 
but criterion to maximize is: 
– ratio of (between classes variation) and 
(within classes variation) 
•	 Project all objects into the line

•	 Find point in the line that best separates 
classes 

=


=

scatter 

x 
( 
i 

μ
2

T 

)

cov1


cov 2


Sw  is the sum of scatters

within the classes 
4
 ⎤

1
x 
x
T 
( 
)
)(
⎡ 
− 
−
μ
μ
i 
i
1 
1
= 
⎥
⎢
n 
4  16 
1 
− 
⎣ 
⎦

1.1 
9.3 
x
)(
⎡

⎤

−
−
μ
i
2 
= ⎢
⎥
n 
14 
9.3 
1 
− 
⎦

⎣

k 
∑

p j  cov j  n
( −
 1) 

j  1 = 
4

1
⎡

⎤

⎥
⎢
4  16 
⎦

⎣

9.3

1.1 
⎤

⎡

⎥
⎢
14 
9.3 
⎦

⎣

Sw  =
 S1
 +
 S
2


(99)

(99)

=


5. 

=


5. 

S 

w 

=


S1


S 

2 

Sb  is scatter between the classes


T

Sb  = (μ − μ )(μ − μ )
1 
2
1 
2 
•	 Maximize Sb/Sw (a square d x d matrix,
where d is the number of dimensions) 
•	 Find maximum eigenvalue and respective 
eigenvector 
•	 This is the direction that maximizes Fisher’s 
criterion 
•	 Points are projected over this line 
•	 Calculate distance from every projected point 
to projected class means and decide class 
corresponding to smaller distance 
•	 Assumption: class distributions are normal 

Eigenvector with 

max eigenvalue


Classification Models


•	 Quadratic Discriminant Analysis

•	 Partial Least Squares 
–	 PCA uses X to calculate directions of greater 
variation 
–  PLS uses X and Y to calculate these directions

•	 It is a variation of multiple linear regression 
Var(Xα), 

PCA maximizes 
Corr2(y,Xα)Var(Xα)

PLS maximizes 

•	 Logistic Regression


PCA, PLS, Selection


•	 3 data sets

–	 Singh et al. (Cancer Cell, 2002: 52 cases of benign and  

malignant prostate tumors)

–	 Bhattachajee et al. (PNAS, 2001: 186 cases of different types of 
lung cancer) 
–	 Golub et al. (Science, 1999: 72 cases of acute myeloblastic and 
lymphoblastic leukemia) 

•	 PCA logistic regression 
• PLS  
•	 Forward selection logistic regression


•	 5-fold cross-validation 

Missing Values: 0.001% -
0.02%

Screenshots removed due to copyright reasons.


Classification of Leukemia with Gene Expression


PCA 

Variable 
Selection 

Variable selection from ~2,300 genes


