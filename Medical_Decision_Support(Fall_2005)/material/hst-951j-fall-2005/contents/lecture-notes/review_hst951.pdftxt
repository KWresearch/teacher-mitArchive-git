Harvard-MIT Division of Health Sciences and Technology 
HST.951J: Medical Decision Support, Fall 2005
Instructors: Professor Lucila Ohno-Machado and Professor Staal Vinterbo 

Review of some concepts 

in predictive modeling


Lucila Ohno-Machado, 

Brigham and Women’s Hospital 

Topics


•  Clustering

• MDS  
• Neural nets


•  Decision trees 
•  Linear regression 
•  Logistic regression 
•  Evaluation 
•  Classification trees 
•  Ensembles 
• PCA  

2 x 2 table

(contingency table) 
PPD(cid:173)
PPD+ 

TB 

no TB 

8


3


11 

2 

87 

89 

10


90


100


Probability of TB given PPD- = 2/89




Bayes rule


•  Definition of conditional probability: 

•  P(A|B) = P(AB)/P(B) 

P(B|A) = P(BA)/P(A) 

P(AB) = P(BA)

P(A|B)P(B) = P(B|A)P(A)


P(A|B) = P(B|A)P(A)/P(B)


Sensitivity = 40/50 = .8 
Specificity = 45/50 = .9 

threshold 

“nl” 

“D” 

nl 

45 

5 

50 

D 

10 

40

50 

50 

50 

nl 

TN 

disease 

TP 

0.0 

FN 

FP 

0.6 

1.0


Sensitivity = 30/50 = .6 
Specificity = 1 

threshold 

“nl” 

“D” 

nl 

50 

0 

50 

D 

20 

30

50 

70 

30 

nl 

TN 

disease 

TP 

FN 

0.0 

0.7 

1.0


4
.
0
 
d
l
o
h
s
e
r
h
T

6
.
0
 
d
l
o
h
s
e
r
h
T

 
7
.
0
 
d
l
o
h
s
e
r
h
T

“nl” 

“D” 

“nl” 

“D” 

“nl” 
“D” 

nl 

40 

10 
50 

nl 

45 

5 
50 

nl 

50 
0 
50 

D 

0 

50
50 

D 

10 

40
50 

D 

20 
30
50 

40 
60 

50 

50 

70 
30 

1 

 
y
t
i
v
i
t
i
s
n
e
S

ROC 
curve

0 

1 - Specificity


1 



All possible pairs 0-1


•  Healthy 
0.3 
0.2 
0.5 
0.1 
0.7 

< 

• Sick  
0.8 
0.2 
0.5 
0.7 
0.9 

concordant 
discordant
concordant
concordant
concordant 

All possible pairs 0-1

Systems’ estimates for 

•  Healthy 
0.3 
0.2 
0.5 
0.1 
0.7 

• Sick  
0.8 
0.2 
0.5 
0.7 
0.9 

concordant 
tie
concordant
concordant
concordant 

C - index


•  Concordant 
18 

•  Discordant
4


• Ties

3


C -index =  Concordant + 1/2 Ties =  18 + 1.5

All pairs 
25


Calibration


sum of group = 0.5 

Sorted pairs by systems’ estimates 
0.1 
0.2 
0.2 
0.3 
0.5 
0.5 
0.7 
0.7 
0.8 
0.9 

sum of group = 1.3 

sum of group = 3.1 

1 

1 

1 

Real outcomes

0 
0 
sum = 1
0 
0 
sum = 1
0 
1

1

sum = 3

Prefect calibration 

Calibration 
plot 

1


p

u
o
r
g
 
r
e
p
 
s
e
u
l
a
v
 
d
e
v
r
e
s
b
o
 
f
o
 
g
v
A

0 

Avg of estimates per group  1


Surgery 

No clairvoyant 
No surgery 

EV= 9.5 

Death 
0.05 

EV= 9.5 
Survival 
0.95 

Surgery 
EV= 0 

Death 

Survival 

1 
0 

“Death” 
0.05 

EV= 9.8 

Clairvoyant 

“Survival” 
0.95 
Value of clairvoyance 
= 9.8 - 9.5 = 0.3 

No surgery 
EV= 6 

Surgery 

EV= 10 
No surgery 
EV= 6 

Death 

Survival 

0 
1 

Death 

0 

Full mobility 

10

6 

0 
10

Death 
Full mobility 

Poor mobility 

6

Death 
Full mobility 

Poor mobility 

0 

10

6

 

Sensitivity Analysis


•  Effect of probabilities in the decision

10 

Expected 
Values 

6 

0 

Surgery 

No surgery 

0.25 
P(Death)


0.5


What predictive models do

and evaluate
performance on 
new cases 

Predict this 

-0.2 
0.5 
0.1 
-0.9 
0.4 
0.6 
-0.7 

0.8 
-0.4 
0.2 
0.3 
0.2 
0.3 
-0.4 

Using these


Case 1 

Case 2 

0.7 
0.6 
-0.6 
0 
-0.4 
-0.8 
0.5 

0.6 
0.4 
-0.1 
0 
-0.3 
-0.8 
0.3 

-0.1 
0.6 
0.2 
-0.5 
0.4 
0.7 
-0.7 

? 
? 
? 
? 
? 
? 
? 

Predictive Model Considerations


•  Select a model 
–  Linear, Nonlinear 
–  Parametric, non-parametric 
–  Data separability 
–  Continuous versus discrete (categorical) outcome 

–  Continuous versus discrete variables 
–  One class, multiple classes 
•  Estimate the parameters (i.e., “learn from data”)

• Evaluate  

Predictive Modeling Tenets


•	 Evaluate performance on a set of new cases 

•	 Test set should not be used in any step of building 
the predictive modeling (model selection, 
parameter estimation) 
•	 Avoid overfitting 
–	 “Rule of thumb”: 2-10 times more cases than attributes 
–  Use a portion of the training set for model selection or 
parameter tuning 
•	 Start with simpler models as benchmarks


Desirable properties of models


•	 Good predictive performance (even for non-linearly 
separable data) 
•	 Robustness (outliers are ignored) 
•	 Ability to be interpreted 
–	 Indicate which variables contribute more for the predictions 
–	 Indicate the nature of variable interactions 
–	 Allow visualization 
•	 Be easily applied, be generalizable to other measurement 
instruments, and easily communicated 

correlatio 
n

_ coefficien 
t

σ XY  = ρ
r  = 
σ σ  Y 
X 

X 
)( 

VARIANCE 
n
∑ ( X  − 
X 
i 
σ  =  i = 1 
XX 
n − 1 
st  _ deviation 
n 
∑ 
X 
( 
i 
1 
= 

X 
− 

i 

Xσ 

= 

i  − X  ) 

COVARIANCE

n 
∑ ( X i  − 
X 
Y 
)(  i  −
σ XY =  i = 1 
n − 1 

Y 

) 

)(

X 

−

X

)

i 

n 

1 
− 

Y 

Covariance and 
Correlation Matrices 

⎡σXX   σXY   ⎤ 
cov = ⎢ 
⎥ 
⎣σYX   σYY   ⎦
⎡ 1  ρ⎤

corr  = ⎢
⎣ρ  1 ⎥
⎦ 
n 
∑ ( X  − 
i
i =1 
n  − 1 
n 
∑ ( X  − 
X 
X 
)(
i 
σ  =  i =1 
XX 
n  − 1 

σ  = 
XY 

X 

Y 
)(  −
i 

Y 

)


i  − X  ) 

0

X 

Slope from linear regression is asymmetric, 
covariance and ρ are symmetric 
β 0  =  y − β x
y  = β + β 1 x
1
0

Σ ( x  − x )( 
y 
y  = 2 + 4 x 
−
Σ ( x  − x ) 2 
x  =  y / 4 − 2 

β 1  =

y 

) 

y 

⎡  86.0 
cov = ⎢ 
⎣  35.0 
⎡  1 
corr  = ⎢
⎣  96.0 

35.0 
⎤ 
⎥ Σ = 
69.15 
⎦

96 
.0 
⎤

⎥ 
1  ⎦

x 

Solve system of normal equations

βn  + β1 ∑ x  = ∑ y 
0 

Normal equation 1

β0 ∑ x  + β1 ∑ x  2  = ∑  x y  

Normal equation 2

β0  =  y − β x1 
y 
Σ( x  − x )( 
y 
)− 
Σ( x  − x ) 2 

β1  =

y

Logit Model

1

1 + e − (β +β xi  )
0
1

e 
β 0 +β 1 xi

eβ 0 +β 1 xi  + 1


=pi 

p  = 
i

log

pi
⎡ 
⎤
⎥ = β 0  + β 1 xi 
⎢ 
pi  ⎦
1 − 
⎣
pi 
⎤
log⎡ 
⎥ = ∑β xi

⎢ 
⎣ −  pi  ⎦ 
1 
i


logit


p=1 

x


x


Logistic Regression

•  Good for interpretation 
•  Works well only if data are linearly separable 

• 
Interactions need to be entered manually 
•  Not likely to overfit if # variables is low 

Inputs
34

Age 

* .5 

Gender 

Mitoses 

1 

4 

* .4 

Σ 

* .8 

Output 

0.6 

“Probability
of cancer” 

Coefficients 

Prediction  

p  =  _____1_____
1 + e -(
)
Σ+α

What do coefficients mean?


eβage  = ORage 

Age49  Age50 
 22
 28
45 
52 
 74
 73

 50
97 
 147

Death
Life 
Total

 

 

OR  = 

age 
| 

= 50

death 

|

age 

= 50 

p
death 
1 −  p
p
death 
1 −  p

|

age 

= 49

death 

age 
| 

= 49

What do coefficients mean?


eβ color  = ORcolor 

Blue 
 28
45 
 73

Green 
 22
52 
 74

 50

97

 147


Death
Life 
Total

OR  = 

28 / 45
 =  47 
. 1 

22 / 52 
eβ color  =  47 
. 1 

β color  =  385 
. 0 
1

pblue  = 
)  =  383 
. 0 
1 + e − − 
+  385 
8616 
. 0 
. 0 
(
1

p green  = 
1 + e 8616 
. 0 

=  297 
. 0 

Maximum Likelihood Estimation


• Steps:

– Define expression for the probability of data as 
a function of the parameters 
– Find the values of the parameters that maximize 
this expression 

Likelihood Function


L  = Pr(Y  )

y 
L  = Pr( 
y 
1 , 

2 ,..., y  )

n 
L  = Pr( y  ) Pr( y 
1

n 
y  ) = ∏ Pr( y  )

Pr( 
)... 
n
i 
2 
i  =1 

Complete separation

MLE does not exist (ie, it is infinite) 

βi 

y 

βi+1 

y

x

x 

x 

Logistic Regression

and non-linearly-separable problems

•	 Simple form below cannot deal with it 

•	 Y = 1/(1+exp-(ax1+bx2)) 
•	 Adding interaction terms transforms the 
space such that problem may become 
linearly separable 
•	 Y = 1/(1+exp-(ax1  + bx2  + cx1x2))


Figures removed due to copyright reasons. 

Please see: 

Khan, J., et. al. "Classification and diagnostic prediction of cancers using gene expression 

profiling and artificial neural networks." Nat Med 7, no. 6 (June 2001): 673-9.


Kernel trick


•	 Idea: Nonlinearly project data into higher 
dimensional space with Φ:Rm→H 
•	 Apply linear algorithm in H


Classification Trees


asymmetry 
asymmetry

< 2 

border 
border

R 

detail 
detail

color 
color

A 

border 
border

detail 
detail

<2 

< 2 

“malig” 
“malig” 

“benigh” 
“benigh” 

Y 

detail 
detail

> 10 

detail 
detail

Y 

“malig” 
“malig” 

“benign” 
“benign” 

0- TEST: null  VALUE: null  Num Cases: 700.0 Num Dsrd: 241.0 
  2 - TEST: breath  VALUE:  1 Num Cases: 75.0 Num Dsrd: 1.0 
********PRUNED !!! 
********PRUNED!!!
 1- TEST: breath  VALUE: 0 Num Cases: 625.0 Num Dsrd: 240.0 
 4- TEST: CWtender  VALUE: 1 Num Cases:  11.0 Num Dsrd:  .0
 3- TEST: CWtender  VALUE: 0 Num Cases:  614.0 Num Dsrd: 240.0 
   8 - TEST: age  VALUE: >32 Num Cases: 611.0 Num Dsrd: 240.0 
  10 - TEST: Duration  VALUE: >72 Num Cases: 3.0 Num Dsrd:  .0
  9 - TEST: Duration  VALUE: <=72 Num Cases:  608.0 Num Dsrd: 240.0 
 12- TEST:  Duration  VALUE: >48 Num Cases: 2.0 Num Dsrd: 2.0
 11- TEST: Duration  VALUE: <=48 Num Cases: 606.0 Num Dsrd:  238.0
   14 - TEST: prevang  VALUE: 1 Num Cases: 340.0 Num Dsrd: 92.0
  18 - TEST: Epis  VALUE: 1 Num Cases: 8.0 Num Dsrd: .0
  17 - TEST: Epis  VALUE: 0 Num Cases: 332.0 Num Dsrd: 92.0
 22- TEST:  Worsening  VALUE: >72 Num Cases:  6.0 Num Dsrd:  .0
 21- TEST:  Worsening  VALUE: <=72 Num Cases:  326.0 Num Dsrd: 92.0
   28 - TEST: Duration  VALUE: >36 Num Cases: 3.0 Num Dsrd:  .0
   27 - TEST: Duration  VALUE: <=36 Num Cases:  323.0 Num Dsrd: 92.0
  36 - TEST: Worsening  VALUE: >28 Num Cases: 3.0 Num Dsrd:  2.0
  35 - TEST: Worsening  VALUE: <=28 Num Cases: 320.0 Num Dsrd: 90.0
 44- TEST:  age  VALUE: >55 Num Ca ses: 240.0 Num Dsrd: 81.0 
52 - TEST: Wor sening  VALUE:  >0 Num Cases: 238.0 Num Dsrd: 81.0 
64 - TEST: OldMI  VALUE: 1 Num Cases:  49.0 Num Dsrd: 9.0
  74- TEST:  Smokes  VALUE:  0 Num Cases: 37.0 Num Dsrd:  9.0
 86- TEST: a ge  VALUE: >65 Num Cases:  30.0 Num Dsrd: 5.0

********PRUNED!!!

********PRUNED!!!

 85- TEST: age  VALUE:  <=65 Num Cases: 7.0 Num Dsrd: 4.0
 98- TEST: Worsening  VALUE: >2 Num Cases:  5.0 Num Dsrd: 2.0
 97- TEST: Worsening  VALUE: <=2 Num Cases: 2.0  Num Dsrd: 2.0
  73- TEST:  Smokes  VALUE:  1 Num Cases: 12.0 Num Dsrd:  .0 
63- TEST: OldMI  VALUE: 0 Num Ca ses: 189.0 Num Dsrd: 72.0 
  72- TEST:  Nausea  VALUE: 0 Num Cases: 165.0 Num Dsrd: 57.0
 84- TEST: Duration  VALUE: >16 Num Cases: 3.0 Num D srd: 2.0
 83- TEST: Duration  VALUE: <=16 Num Cases: 162.0 Num Dsrd: 55.0 
********PRUNED!!! 
********PRUNED!!! 
71- TEST: Nausea  VALUE: 1 Num Cases: 24.0 Num D srd:  15 .0
 82- TEST: Back  VALUE: 0 Num Cases: 21.0 Num Dsrd: 15.0 
 94- TEST: post  VALUE: 1 Num Cases: 1.0 Num Dsrd: .0
 93- TEST: post  VALUE: 0 Num Cases: 20.0 Num D srd:  15.0 
 81- TEST: Back  VALUE: 1 Num Cases: 3.0 Num Dsrd: .0  
51 - TEST: Wor sening  VALUE:  <=0 Num Cases: 2.0 Num Dsrd: .0
 43- TEST:  age   VALUE: <=55 Num Cases: 80.0 Num Dsrd:  9.0 
50 - TEST: Wor sening  VALUE:  >1 Num Cases: 68.0 Num Dsrd: 5.0 
********PRUNED!!! 
********PRUNED!!! 
********PRUNED!!! 
********PRUNED!!! 
********PRUNED!!! 
********PRUNED!!! 
********PRUNED!!! 
********PRUNED!!!
  49 - TEST: Wor sening  VALUE: <=1 Num Cases: 12.0 Num Dsrd: 4.0 
60 - TEST: age  VALUE:  >47 Num Cases: 10.0 Num Dsrd: 2.0
  68- TEST:  OldMI  VALUE: 1 Num Ca ses: 1.0 Num Dsrd: 1.0
  67- TEST:  OldMI  VALUE: 0 Num Ca ses: 9.0 Num Dsrd: 1.0 
********PRUNED!!! 
********PRUNED!!!
  59- TEST:  age   VALUE: <=47 Num Cases: 2.0 Num Dsrd: 2.0
   13 - TEST: prevang  VALUE: 0 Num Cases: 266.0 Num Dsrd: 146.0 
  16 - TEST: Duration  VALUE: >0 Num Cases: 259.0 Num Dsrd: 146.0 
 20- TEST:  post  VALUE: 1 Num Ca ses: 13.0 Num Dsrd: 2.0
   26 - TEST: D iabetes  VALUE: 1 Num Cases: 1.0 Num Dsrd: 1.0
   25 - TEST: D iabetes  VALUE: 0 Num Cases: 12.0 Num Dsrd:  1.0 
********PRUNED!!! 
********PRUNED!!!
   19 - TEST: post  VALUE: 0 Num Cases:  246.0 Num Dsrd: 144.0 
   24 - TEST: Nausea  VALUE: 0 Num Ca ses: 202.0 Num Dsrd: 105.0 
  32 - TEST: OldMI  VALUE: 1 Num Cases: 1 3.0 Num Dsrd: 1.0
 42- TEST:  BP  VALUE: 1 Num Ca ses: 1.0 Num Dsrd: 1.0
 41- TEST:  BP  VALUE: 0 Num Ca ses: 1 2.0 Num Dsrd: .0
  31 - TEST: OldMI  VALUE: 0 Num Cases: 1 89.0 Num Dsrd: 104.0 
 40- TEST:  age  VALUE: >37 Num Ca ses: 184.0 Num Dsrd: 103.0 
48 - TEST: Epis  VALUE: 1 Num Cases:  8.0 Num Dsrd: 2.0 
58 - TEST: Duration  VALUE:  >8 Num Cases: 2.0 Num Dsrd: 2.0 
57 - TEST: Duration  VALUE:  <=8 Num Cases: 6.0 Num Dsrd: .0 
47 - TEST: Epis  VALUE: 0 Num Cases:  176.0 Num Dsrd: 101.0 
56 - TEST: Duration  VALUE:  >15 Num Ca ses: 2.0 Num Dsrd: .0 
55 - TEST: Duration  VALUE:  <=15 Num Cases: 174.0 Num Dsrd: 101.0 
  66- TEST:  Lipids  VALUE: 1 Num Cases: 1.0 Num Dsrd:  1.0
  65- TEST:  Lipids  VALUE: 0 Num Cases: 173.0 Num Dsrd: 100.0 
 76- TEST: Sweating  VALUE: 0 Num Ca ses: 73.0 Num Dsrd: 32.0 
********PRUNED!!! 
********PRUNED!!!
 75- TEST: Sweat ing  VALUE: 1 Num Ca ses: 100.0 Num Dsrd: 68.0 
 88- TEST: Duration  VALUE: >8 Num Ca ses: 7.0 Num Dsrd: 2.0 
 104- TEST: Rarm  VALUE: 0 Num Cases: 5.0 Num Dsrd: .0 
 103- TEST: Rarm  VALUE: 1 Num Cases: 2.0 Num Dsrd: 2.0
 87- TEST: Duration  VALUE: <=8 Num Cases: 93.0  Num D srd: 66.0 
********PRUNED!!! 
********PRUNED!!!
   39 - TEST: age  VALUE: <=37 Num Cases: 5.0 Num Dsrd: 1.0
   23 - TEST: Nausea  VALUE: 1 Num Ca ses: 44.0 Num Dsrd: 39.0
  30 - TEST: age  VALUE:  >47 Num Ca ses: 4 1.0 Num Dsrd: 39.0
 38- TEST:  Duration  VALUE: >7 Num Cases: 7.0 Num Dsrd:  5.0 
46 - TEST: Larm  VALUE:  0 Num Cases: 1.0 Num Dsrd: .0 
45 - TEST: Larm  VALUE:  1 Num Cases: 6.0 Num Dsrd: 5.0 
54 - TEST: Rarm  VALUE: 0 Num Cases: 5.0 Num Dsrd: 5.0 
53 - TEST: Rarm  VALUE: 1 Num Cases: 1.0 Num Dsrd: .0
 37- TEST:  Duration  VALUE: <=7 Num Cases: 34.0 Num Dsrd: 34.0
  29 - TEST: age  VALUE:  <=47 Num Cases: 3.0 Num Dsrd:  .0
  15 - TEST: Duration  VALUE: <=0 Num Cases:  7.0 Num Dsrd: .0
 7- TEST:  age  VALUE: <=32 Num Ca ses: 3.0 Num Dsrd: .0 

From perceptrons to  

CART, to multilayer perceptrons

Why? 


“LARGE” data sets 


•	 In predictive modeling, large data sets have 
several cases (with few attributes or 
variables for each case) 
•	 In some domains, “large” data sets with 
several attributes and few cases are subject 
to analysis (predictive modeling) 
•	 The main tenets of predictive modeling 
should be always used 

“Large m  small n” problem


•	 m  variables, n  cases 
•	 Underdetermined systems  

•	 Simple memorization even with simple 
models 
•	 Poor generalization to new data

•	 Overfitting 

Reducing Columns


Some approaches:

•Principal 
Components 
Analysis 
(a component is a 
linear combination of 
variables with 
specific coefficients)

•Variable selection

0.7 
0.6 
-0.6 
0 
-0.4 
-0.8 
0.5 

-0.2 
0.5 
0.1 
-0.9 
0.4 
0.6 
-0.7 

0.8 
-0.4 
0.2 
0.3 
0.2 
0.3 
-0.4 

Principal Component Analysis


•	

•	

Identify direction with greatest variation (combination of 
variables with different weights) 
Identify next direction conditioned on the first one, and so 
on until the variance accounted for is acceptable 

PCA disadvantage


•	 No class information used in PCA

•	 Projected coordinates may be bad for 
classification 

Related technique 


•  Partial Least Squares 
–  PCA uses X to calculate directions of greater variation 
–  PLS uses X and Y to calculate these directions 
•  It is a variation of multiple linear regression 
PCA maximizes 
Var(Xα), 

Corr2(y,Xα)Var(Xα)

PLS maximizes 

Variable Selection


•	 Ideal: consider all variable combinations 

–	 Not feasible: 2n 
–  Greedy Backward: may not work if more variables than 
cases 
•	 Greedy Forward: 
–	 Select most important variable as the “first component” 
–	 Select other variables conditioned on the previous ones 
–	 Stepwise: consider backtracking 
•	 Other search methods: genetic algorithms that 
optimize classification performance and # 
variables 

Simple Forward  

Variable Selection 

•	 Conditional ranking of most important 
variables is possible 
•	 Easy interpretation of resulting LR model

– No artificial axis that is a combination of 
variables as in PCA 
•	 No need to deal with too many columns

•	 Selection based on outcome variable 
–  uses classification problem at hand 

Cross-validation


•	 Several training and test set pairs are 
created 
•	 Results are pooled from all test sets


•	 “Leave-n-out” 
•	 Jackknife (“Leave-1-out”) 

Leave-N/3-out


Training Set 

Model Building


Test Set 

Evaluation


1 23 54 0 1 1 
2 43 23 1 0 1 
3 34 35 0 0 0 
4 20 21 1 1 1 
5 19 03 1 1 0 
6 78 04 0 1 0 
7 98 03 0 1 1 
8 35 05 1 1 1 
9 99 23 0 0 1 
10 23 34 0 0 0 

Bootstrap


•	 Efron (Stanford biostats) late 80’s 
–	 “Pulling oneself up by one’s bootstraps” 
•	 Nonparametric approach to statistical inference

•	 Uses computation  instead of traditional 
distributional assumptions and asymptotic results 
•	 Can be used for non-linear statistics without 
known standard error formulas 

Sample with Replacement

Y4 * 
Y3 * 
Y2 * 
Sample  Y1 * 
*Y 
6.00 
6 
6 
6 
6 
1 
3.75 
-3 
6 
6 
6 
2 
3 
6 
6 
6 
5 
5.75 
.. 
100 
101 
… 
255 
256 

3.5 
3.00 

2.75 
1.25 

3 
6 

5 
3 

5 
5 

3 
3 

-3 
-3 

-3 
3 

6 
-3 

3 
3 

The population is to the sample

as 

the sample is to the bootstrap samples


In practice (as opposed to previous example), 
not all bootstrap samples are selected 

Empirical distribution of Y


-3 

6


Bootstrap Confidence Intervals


•	 Percentile Intervals 
Example 
–  95% CI is calculated by taking 
– Lower = 0.025 x bootstrap replicates 
– Upper = 0.975 x bootstrap replicates 

Bagging


•	 Breiman, 1996 
•	 Derived from bootstrap (Efron, 1993) 

•	 Create classifiers using training sets that are 
bootstrapped (drawn with replacement) 
•	 Average results for each case 


Boosting


•	 A family of methods 
•	 Sequential production of classifiers

•	 Each classifier is dependent on the previous one, 
and focuses on the previous one’s errors 
•	 Examples that are incorrectly predicted in 
previous classifiers are chosen more often or 
weighted more heavily 

Visualization


•	 Capabilities of predictive models in this 
area are limited 
•	 Clustering is often good for visualization, 
but it is generally not very useful to separate 
data into pre-defined categories 
– Hierarchical trees 
–  2-D or 3-D multidimensional scaling plots

– Self-organizing maps 

Visualizing the classification  

potential of selected inputs

•	 Clustering visualization that uses 
classification information may help display 
the separation of the cases in a limited 
number of dimensions 
•	 Clustering without selection of dimensions 
important for classification is less expected 
to display this separation 

Metric spaces


•	 Positivity  d ij   > d ii   = 0
Reflexivity 
•	 Symmetry  d ij   = d ji 

•	 Triangle 
inequality  d ij   ≤ d ih   + d hj 

j 

h 

i 

Figures removed due to copyright reasons. 
Please see: 
Khan, J., et. al. "Classification and diagnostic prediction of cancers using gene expression
Khan, J., et. al. "Classification and diagnostic prediction of cancers using gene expression 
profiling and artificial neural networks." Nat Med 7, no. 6 (June 2001): 673-9.
profiling and artificial neural networks." Nat Med 7, no. 6 (June 2001): 673-9. 

k-means clustering 

(Lloyd’s algorithm)

1.	 Select k (number of clusters) 
2.	 Select k initial cluster centers c1,…,ck 
Iterate until convergence: For each i, 

3.	
1.	 Determine data vectors vi1,…,vin  closest to ci 
(i.e., partition space) 
2.	 Update ci  as ci  = 1/n  (vi1+…+vin  ) 

Figures removed due to copyright reasons. 

Please see: 

Khan, J., et. al. "Classification and diagnostic prediction of cancers using gene expression

Khan, J., et. al. "Classification and diagnostic prediction of cancers using gene expression 
profiling and artificial neural networks." Nat Med 7, no. 6 (June 2001): 673-9.

profiling and artificial neural networks." Nat Med 7, no. 6 (June 2001): 673-9.


Neural Networks 


Inputs 

Hidden Layer 

Outputs 

Age  

34 

Gender  

Mitoses 

2 

4 

.1 
.3 

.6 
.2 

.7 
.2 

.4 

.2 

.5 
.8 

Σ 

Σ 

Σ 

Weights 

Weights 

0.6 

“Probability
of Cancer”

Neural Networks 

Work well even with non(cid:173)
linearly separable data 
Overfitting control: 
•Few weights
•Little training
•Penalty for large weights

Backpropagation algorithm


Classification 

cross-entropy 

Regression 

sum-of-squares 

Some reminders


•	 Simple models may perform at the same 
level of complex ones for certain data sets 
•	 A benchmark can be established with these 
models, which can be easily accessed 
•	 Simple rules may have a role in 
generalizing results to other platforms 
•	 No model can be proved to be best, need to 
try all 

