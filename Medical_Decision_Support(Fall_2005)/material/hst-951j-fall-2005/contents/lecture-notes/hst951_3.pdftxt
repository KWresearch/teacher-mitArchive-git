Harvard-MIT Division of Health Sciences and Technology 
HST.951J: Medical Decision Support, Fall 2005
Instructors: Professor Lucila Ohno-Machado and Professor Staal Vinterbo 

6.873/HST.951 Medical Decision Support 
Fall 2005 

Decision Analysis 
(part 2 of 2) 
Review Linear Regression 

Lucila Ohno-Machado 

Outline


•  Homework clarification 
•  Sensitivity, specificity, prevalence

•  Cost-effectiveness analysis 
•  Discounting cost and utilities 
•  Review of Linear Regression 

2 x 2 table

(contingency table) 

PPD+ 

PPD(cid:173)

TB 

no TB 

8


3


11 

2 

87 

89 

10


90 

100


Prevalence of TB = 10/100 
Sensitivity of test = 8/11 
Specificity of test = 87/89 

threshold 

normal 

Disease 

True 
Negative (TN) 
FN 

FP 

True 
Positive (TP) 

0 

3mm 

10


Sens = TP/TP+FN 

Spec = TN/TN+FP 

PPV = TP/TP+FP 

NPV = TN/TN+FN 

Accuracy = TN +TP 

nl 

D 

“nl” 

TN 

FN 

“D” 

FP 

-


TP

+ 

“nl” 

“D” 

Sens = TP/TP+FN 

40/50 = .8

Spec = TN/TN+FP

45/50 = .9

PPV = TP/TP+FP

40/45 = .89 

NPV = TN/TN+FN 

45/55 = .81 

Accuracy = TN +TP

85/100 = .85


nl 

D 

“nl” 

45 

“D” 

5 

10 

40

“nl” 

“D” 

Sensitivity = 50/50 = 1 
Specificity = 40/50 = 0.8 

threshold 

“nl” 

“D” 

nl 

40 

10 

50 

D 

0 

50

50 

40 

60 

nl 

TN 

disease 

TP 

FP 

0.0 

0.4 

1.0


Sensitivity = 40/50 = .8 
Specificity = 45/50 = .9 

threshold 

“nl” 

“D” 

nl 

45 

5 

50 

D 

10 

40

50 

50 

50 

nl 

TN 

disease 

TP 

0.0 

FN 

FP 

0.6 

1.0


Sensitivity = 30/50 = .6 
Specificity = 1 

threshold 

“nl” 

“D” 

nl 

50 

0 

50 

D 

20 

30

50 

70 

30 

nl 

TN 

disease 

TP 

FN 

0.0 

0.7 

1.0


Cost-effectiveness analysis


•  Comparison of costs with health effects

– Cost per Down case syndrome averted 

– Cost per year of life saved 
•  Perspectives (society, insurer, patient)

•  Comparators 
– Comparison with doing nothing 
– Comparison with “standard of care” 

Discounting costs


•	

It is better to spend $10 next year than today (its 
value will be only $9.52, assuming 5% rate) 
•	 Even better to spend it 2 years from now ($9.07)

•	 For cost-effectiveness analysis spanning 
multiple years, recommended rate is usually 5% 

• C = C0  + C1/(1-r)1  + C2/(1-r)2  + ...

• C0 are costs at time 0 

Discounting utilities


•	 Value for full mobility is 10 today (is it only 9.52 
next year?) 
•	 Should the discount rate be the same as for 
costs? 
If smaller, then it would always be better to wait 
one more year… 

•	

Levels of Evidence

in Evidence-Based Medicine 

US Task Force


•	 Level 1: at least 1 randomized controlled 
trial (RCT) 
•	 Level 2-I: controlled trials (CT) 
•	 Level 2-II: cohort or case-control study

•	 Level 2-III: multiple time series with or 
without the intervention 
•	 Level 3: expert opinions


Examples


•	 Cost per year of life saved, Life 
years/US$1M 
•	 By pass surgery middle-aged man

– $11k/year, 93

•	 CCU for low risk patients 
– $435k/year, 2


Importance of good stratification


•  Bypass surgery 
– Left main disease 
– One vessel disease 

•  CCU for chest pain 
– 5% risk of MI 
– 20% risk of MI 

93 
12 

2 
10 

Intro to Modeling


Univariate Linear Model

•  Y is what we want to predict (dependent variable)

•  X are the predictors (independent variables) 
•  Y=f(X), where f is a linear function 

y 

y  = 1β0  + x1β
1
1 
y  = 1β + x2β
2
0 
1 
… 

x 

Univariate Linear Model


β1  is slope 
0  is intercept 
β

y  = 1β0  + x1

β 
1 
1

y  = 1β + x2

β 
1 
2
0

… 

y 

x 

Multivariate Model


•  Simple model: structure and parameters

•  3 predictors, 4 parameters β 
•  one of the parameters (β0) is a constant 
x12

x11

y

x13

1
=

β
2
 +
β

+
β1

+
β0

1 
3

x22

x21

y

x 
1
=

+
β2

1
 +
β

0
 +
β

β

23  3 


2 

1 
1

⎡

⎢
x 11  x 21

⎢
x 12  x 22

⎢
⎢

⎣ x 13  x 23


β
⎡⎤ 
⎤ 
0

⎥
⎢
⎥
β
⎥
⎢
⎥
1

⎥
⎢
⎥
β
2

⎥ 
⎢

⎥ 
β 
⎣⎦
⎦

3


Notation and Terminology


• xi  is vector of j inputs, 
covariates, independent
variables, or predictors 
for case i (i.e., what wex1
know for all cases) 

•


X is matrix of j x n

xi  column vectors (input 
data for each case) 

age

⎡

⎢
salt 
⎢
smoke

⎢⎣ 

⎤

⎥
⎥
⎥
⎦


30 

⎡

⎢
=
 10 

⎢
1

⎢⎣ 

⎤

⎥
⎥
⎥
⎦


x1
⎡

⎤

⎢
⎥
=
 x2 
⎢
⎥
x3 
⎢
⎣

⎥⎦ 

x11 x21
x12 x22 

x13 x23 


⎡

⎢
⎢
⎢
⎣


T

⎤

=⎥
⎥
⎥⎦


⎡
 x11 x12 x13

⎢
⎣
 x21 x22 x23


⎤

⎥
⎦


Prediction


• y is scalar: output,

i 
dependent variable 
(i.e., what we want to 
predict) 
e.g., mean blood 
pressure 

•  Y is vector of yi 

⎡ predpat1 ⎤
⎡ y1 ⎤ 
⎡ 100⎤
⎥ = 
⎥ = 
⎢ 
⎢
⎥ 
⎢
⎣ predpat2 ⎦
⎣ y2 ⎦ 
⎣ 98  ⎦

Multivariate Linear Model

x12 
x11 
y
1

x13 
β3 
2
 +
β

+
β1

x22 
x21 
y
2

x23  3

2
 +
β

1
 +
β

β


1
=

+
β0

1
=

0 +
β


Y = XTβ, where each x includes a term for 1 (constant) (x10=1,
i 
x20=1, etc.) to be multiplied by the intercept β0
β
⎡

⎤ 
0 
y
1

x10 x11 x12 x13 

⎤

⎡
⎢
⎥
⎡

⎤

β
⎢
⎥
1 
⎥
⎢
⎢
⎥
y
2

x x x x
⎢
⎥
β
⎦

⎣

⎦

⎣

2 
20  21  22  23 
⎢ 
⎥ 
β

⎣

⎦

3 

Regression and Classification


Regression: continuous outcome 
•  E.g., blood pressure 
Y = f(X) 

Classification: categorical outcome

•  E.g., death (binary) 
ˆ (

G =  X G 
)

Loss function


•	 Y and X random variables

f(x) is the model 
•	
•	 L(Y,f(X)) is the loss function (penalty for being 
wrong) 
It is a function of how much to pay for 
discrepancies between Y (real observation) and 
f(X) estimated value for an observation) 
In several cases, we use only the error and 
leave the cost for the decision analysis model 

•	

•	

Regression Problems


Let’s concentrate on simple errors for now:

•	 Expected Prediction Error (EPE): 
[Y-f(X)]2 
|Y-f(X)| 
•	 These two error functions imply that errors 
in both directions are considered the same 
way. 

Univariate Linear Model


yˆ1  = 1β0  + x1β
1

yˆ 2  = 1β + x2β

0
1 

y 

y  = 1β + x1β + error 

1
0 
1

y  = 1β + x2β + error 

2
0 
1


x 

Squared Errors

n 
SSE = ∑ (Y  − Yˆ )

y 
i =1 
Lin e a r r e g r e s sio n  

2

ˆ 
y 
i 

x
=  ββ 
+
i
0 
1

ˆ 
Y 

= 

Xf
(

)

y 

Conditioning on x 

•	 x is a certain value 

y

) = 1 / k ∑ y
i

ˆ ( x 
y 
xi = x )

) =  Ave ( y  | x  =  x )

x 
f 
( 
i
i 
•	 Expectation is
approximated by average 
•	 Conditioning is on x


x 

k -Nearest Neighbors 


y

•	 N is neighborhood 
) = 1 / k  ∑ y
i

ˆ ( x 
y 
xi ∈N k  ( x )

x 
) =  Ave ( y  | x  ∈ N k ( x ))
f 
( 
i
i 
•	 Expectation is approximated by 
average 
•	 Conditioning is on 
neighborhood 

x 

Nearest Neighbors 

•  N is continuous  

neighborhood

Yˆ ( x ) = 1 / n∑  w 
y i	
i 
f ( x ) = 
e 
WeightedAv 
(

| x )

y

y 
i 

•	 Expectation is
approximated by weighted 
average 

w

x 

x


Minimize Sum of Squared Errors


y 

n 
= ∑ [ y − (β + β x )]  =

2

0
1

i = 
1


yˆ i  = β 0  + β xi

1

n
SSE = ∑ ( y −  yˆ ) 2
i = 1 
n

= ∑ ( y 2
i = 
1

n

= ∑ ( y  − 2 yβ − 2 yβ x + β 2  + 2 ββ  x + β 2
2
0 
1
0 
0
1
1

i = 1


− 2 y (β + β x ) + (β + β x ) ) =

2

0
1 
0
1


2
x  )

(derivative wrt β 0) = 0


n 
∑ ( y  − 2 yβ − 2 yβ x + β 2  + 2 β β  x + β  x  )
2
2 
2
1 
0 
1
0 
0
1
i = 1 

∂ SE 
∂β 0 

n
= 2∑ (
i = 1 

−  β + β x ) = 0
y 
+ 
0
1


β n + β 1 ∑ x = ∑ y 
0


Normal equation 1

(derivative wrt β 1) = 0


n 
∑ ( y  − 2 y β − 2 y β x + β 2  + 2 ββ  x + β  x  )
2
2 
2
1 
0 
1
0 
0
1
i = 1 

∂ SE 
∂β 1 

n
= 2∑ (−  x y  + β x + β x 2 ) = 0 
0
1 
i = 1 

β 0 ∑ x + β 1 ∑ x 2  = ∑  x y


Normal equation 2

Solve system of normal equations


βn + β1 ∑ x = ∑ y 
0 
β0 ∑ x + β1 ∑ x 2  = ∑  x y 

Normal equation 1


Normal equation 2


β0  =  y − β x
1

Σ( x − x )( y −  y )

Σ( x − x ) 2


β1  =

y

Limitations of linear regression


•	 Assumes conditional probability p(Y|X) is 
normal 
•	 Assumes equal variance in every X 

•	 It’s linear ☺ 

(but we can always use interaction or 
transformed terms) 

Linear Regression for 

Classification?


y =  p 

yˆ i  = β + βxi
0
1 

y=1 

x 

Linear Probability Model

0
x
ββ +
i
0
1

ˆ
y

i

=

for 0 >= β0 + β1 xi

for 0<= β0 + β1 xi <=1

y=1

1

for β0 + β1 xi >=1

x

Logit Model

x
i

)

p=1

1
(
ββ
+
0
1

−

e
ββ
+
0
1

+
e
ββ
+
0
1

x
i

x
i

+

1

p
i

=

p
i

=

1

e

x
ββ
+
i
0
1

log

log

⎡
⎢
⎣
⎡
⎢
⎣

1

1

p
i
p
−
i
p
i
p
−
i

⎤
=⎥
⎦
⎤
∑=⎥
⎦
i

x
β
i

logit

x

x

Two dimensions

x1

x2

