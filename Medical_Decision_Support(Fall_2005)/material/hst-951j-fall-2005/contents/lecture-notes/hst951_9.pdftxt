Harvard-MIT Division of Health Sciences and Technology 
HST.951J: Medical Decision Support, Fall 2005
Instructors: Professor Lucila Ohno-Machado and Professor Staal Vinterbo 

6.873/HST.951 Medical Decision Support 
Spring 2005 

Artificial 

Neural Networks


Lucila Ohno-Machado 
(with many slides borrowed from Stephan Dreiseitl. Courtesy of Stephan Dreiseitl. Used with permission.) 

Overview 


•  Motivation 
•  Perceptrons 
•  Multilayer perceptrons 
•  Improving generalization

•  Bayesian perspective 

Motivation


Images removed due to copyright reasons. 

benign lesion 
benign lesion

malignant lesion

malignant lesion

Motivation


•	 Human brain 
– Parallel processing 
– Distributed representation 
– Fault tolerant 
– Good generalization capability 
•	 Mimic structure and processing in 
computational model 

Biological Analogy


Dendrites 

+ 
+ 
+ 
-

-

Axon 

Synapses 

Nodes 

Synapses
(weights) 

Perceptrons


01 

11 

10 

00 

11 

10 

00

00 

11 

Input patterns 

Input layer 

Output layer 

00 

00 

00 

01 

10 

10 

11 

11 

11

Sorted 
.patterns 

Activation functions


Perceptrons (linear machines)


Input units 

Cough  Headache 

weights 

No disease  Pneumonia 

Flu  Meningitis 

Output units 

Δ rule 
change weights to

decrease the error


- what we got
what we wanted
error 

Abdominal Pain Perceptron


Male 

Age 

Temp 

WBC 

Intensity  Duration

Pain
Pain


1 

20

37 
 

10 

1

1

adjustable 
weights

Appendicitis Diverticulitis	

0

0

Ulcer 
Pain 
Duodenal  Non-specific
Perforated 
0

1

Cholecystitis 

Obstruction Pancreatitis 
Small Bowel 

0

0

0

AND

y 

w1 
x1 

w2 
x2 

θ  = 0.5 

input  output 
0 
00 
0 
01 
10 
0 
1 
11 

1, for a  > θ
0, for a ≤ θ 

f(a) = 

f(x1w1 + x2w2) = y 
f(0w1 + 0w2) = 0 
f(0w1 + 1w2) = 0 
f(1w1 + 0w2 ) = 0 
f(1w1 + 1w2 ) = 1 
some possible values for w1 and w2 
w1  w2 
0.35 
0.20 
0.40 
0.20 
0.30 
0.25 
0.20 
0.40 

θ 

Single layer neural network


Output 
units 

Output of unit j: 

oj = 1/ (1 + e- ( aj

+θ j)

) 

j 

Input to unit 
j:  aj = Σ wijai 

Input to unit 
i:  ai 
measured value of variable 
i 

i 

Input units 

Single layer neural network

j: 
Output of unit 

Output 

units 

o j = 1/ (1 + e  - (  a j+ θ j) 

j 

Input to unit 

j:  a j  = Σ w ija i 

Input to unit 

i:  a i 
value of variable 

measured 

i 

Input units 

i 

Increasing θ 

1.2 

1 

0.8 

0.6 

0.4 

0.2 

Ser es1 
i
Ser es2 
i
Ser es3 
i

-15  

-10  

-5  

0 

0  

5  

10  

15  

Training: Minimize Error


Input units 

Cough  Headache 

weights 

No disease  Pneumonia 

Flu  Meningitis 

Output units 

Δ rule 
change weights to

decrease the error


- what we got
what we wanted
error 

Error Functions


•  Mean Squared Error (for regression  

problems), where t is target, o is output 
Σ(t - o)2/n 

•  Cross Entropy Error (for binary 

classification)

− Σ(t log o) + (1-t) log (1-o)


oj = 1/ (1 + e- ( aj

+θ j)

) 

Error function


•  Convention: w := (w0,w), x := (1,x) 
•  w0 is “bias” 
• o = f (w •  x)  
•  Class labels ti  ∈{+1,-1} 
•  Error measure 
–  E =  -Σ  ti  (w • x ) 
i
i miscl. 
•  How to minimize E? 

w
o

= 0.5 

1 

y 

θ  = 0.5 

w2 
x2 

w1 
x1 

y 

w1 
x1 

w2 
x2 

Minimizing the Error


Error surface 

initial error 

derivative 

final error 

local minimum 

w

initial 
w

trained

Gradient descent


Error 

Global minimum 

Local minimum 

Perceptron learning


•  Find minimum of E by iterating 
k+1  = wk  – ηgradw E 
w
•  E =  -Σ  ti  (w • x )  ⇒i

i miscl. 
gradw E = -Σ  ti  xi

i miscl.


•  “online” version: pick misclassified x

i 
k+1  = wk  + ηti  xi

w

Perceptron learning


•	 Update rule wk+1  = wk  + ηti  xi

•	 Theorem: perceptron learning converges 
for linearly separable sets 

Gradient descent

•	 Simple function minimization algorithm

•	 Gradient is vector of partial derivatives

•	 Negative gradient is direction of steepest 
descent 

20

10

0
0

1

2

1

3

0

3

2

3

2

1

0

0

1

2

3

Figures by MIT OCW.

Classification Model

Inputs 

Output 

Age 

34 

Gender 

Stage 

1 

4 

Weights 
5 

4 

8 

Σ 

0.6 
“Probability 
of beingAlive” 

Independent 
variables 
x1, x2, x3 

Coefficients 
a, b, c 

Dependent
variable 
p
Prediction 

Terminology 


•  Independent variable = input variable

•  Dependent variable = output variable

•  Coefficients = “weights” 
•  Estimates = “targets” 

•  Iterative step = cycle, epoch 

XOR 

y 

w1 
x1 

w2 
x2 

θ  = 0.5 

input  output 
0 
00 
1 
01 
10 
1 
0 
11 

f(x1w1 + x2w2) = y 
f(0w1 + 0w2) = 0 
f(0w1 + 1w2) = 1 
f(1w1 + 0w2 ) = 1 
f(1w1 + 1w2 ) = 0 
some possible values for w1 and w2 
w1  w2 

θ 

f(a) = 

1, for a  > θ
0, for a ≤ θ 

input  output 
0 
00 
1 
01 
10 
1 
0 
11 

XOR 

y 

w 3 

x 1 

θ  = 0.5 
w5  w 4 
z 
w1  w2 
x 2

θ  = 0.5 

f(w1, w2, w3, w4, w5) 

a possible set of values for ws 
(w1, w2, w3, w4, w5) 

(0.3,0.3,1,1,-2) 

f(a) = 

1, for a  > θ
0, for a ≤ θ 

θ 

input  output 
0 
00 
1 
01 
10 
1 
0 
11 

XOR 


w5 

w6 

θ  = 0.5 for all units 

w1  w3 

w2 

w4 

f(w1, w2, w3, w4, w5 , w6) 

a possible set of values for ws 
(w1, w2, w3, w4, w5 , w6) 

(0.6,-0.6,-0.7,0.8,1,1) 

f(a) = 

1, for a  > θ
0, for a ≤ θ 

θ 

From perceptrons to 

multilayer perceptrons


Why? 


Abdominal Pain
Perforated Non-specific
Duodenal 
Pain
Ulcer
1
0

Cholecystitis
0

Appendicitis Diverticulitis
0
0

Small Bowel
Obstruction
0

Pancreatitis
0

adjustable
weights

1

Male

20

Age

37

10

1

1

Temp

WBC

Pain
Intensity

Pain
Duration

Heart Attack Network


Duration  Intensity  ECG: ST 
Pain elevation Smoker 
Pain 
1

2 

4 

1

Age  Male 

50

1

Myocardial Infarction 
0.8

“Probability” of MI 

Multilayered Perceptrons


Output units 

Output of unit k: 
ok = 1/ (1 + e- ( ak+θk) )

k 

Hidden 
units 

Input to unit k: 
Multilayered
ak = Σwjkoj 
perceptron 
Perceptron 
Output of unit  j: 
oj  = 1/ (1 + e- ( a j+θj) ) 

j 
Input to unit j: aj =  Σwijai 

Input to unit i:  ai 
measured value of variable 

i 

Input units 

i 

Neural Network Model

Inputs 

Age 

34

Gender 

Stage 

2 

4 

.1 
.3

.6 
.2 

.7

.2

.4 

.2 

.5

.8

Σ

Σ 

Σ 

Independent  Weights  Hidden  Weights 
variables 
Layer 

Output


0.6 

“Probability 
of beingAlive” 

Dependent 
variable 
Prediction 

“Combined logistic models”

Inputs 

Age 

34

Gender 

Stage 

2 

4 

.1 

.6 

.7 

.5

.8

Σ 

Independent  Weights  Hidden  Weights 
variables 
Layer 

Output


0.6 

“Probability 
of beingAlive” 

Dependent 
variable 
Prediction 

Inputs


Age 

34 

Gender 

Stage 

2 

4 

.2 

.3 

.2 

.5 

.8 

Σ 

Independent  Weights  Hidden  Weights 
variables 
Layer 

Output


0.6 

“Probability 
of beingAlive” 

Dependent 
variable 
Prediction 

Inputs


Age 

34

Gender 

Stage 

1 

4 

.1 
.3

.6
.2 

.7 
.2

.5

.8

Σ 

Independent  Weights  Hidden  Weights 
variables 
Layer 

Output


0.6 

“Probability 
of beingAlive” 

Dependent 
variable 
Prediction 

Not really,  

no target for hidden units...


Age 

34

Gender 

Stage 

2 

4 

.1 
.3

.6 
.2 

.7

.2

.4 

.2 

.5

.8

Σ

Σ 

Σ 

Independent  Weights  Hidden  Weights 
variables 
Layer 

0.6 

“Probability 
of beingAlive” 

Dependent 
variable 
Prediction 

Hidden Units and Backpropagation


Output units 

-

what we got
what we wanted 
error 

Δ rule 

Δ rule 

Hidden 
units 

Input units


b
a
c
k
p
r
o
p
a
g
a
t
i
o
n
 

Multilayer perceptrons


•  Sigmoidal hidden layer 
•  Can represent arbitrary decision regions

•  Can be trained similar to perceptrons 

ECG Interpretation


QRS amplitude 

R-R interval 

QRS duration 

AVF lead 

S-T elevation 

P-R interval 


SV tachycardia 

Ventricular tachycardia 

LV hypertrophy 

RV hypertrophy 

Myocardial infarction 

Linear Separation

Separate n-dimensional space using one (n - 1)-dimensional space 

Meningitis 
No cough 
Headache 
01 

Flu 
Cough
Headache 
11 

No treatment 
Treatment 

00 
No disease 
No cough
No headache 

10 
Pneumonia 
Cough
No headache 

011 

111 

110 

101 

100


010 

000 

Another way of thinking about 

this…


•  Have data set D = {(x ,t )} drawn from

i
i
probability distribution P(x,t) 
•	 Model P(x,t) given samples D by ANN
with adjustable parameter w 
•  Statistics

analogy:


Maximum Likelihood Estimation


•  Maximize likelihood of data D 
•  Likelihood L = Π p(x ,t ) = Π p(t |x )p(x )

i
i
i
i
i
•  Minimize -log L = -Σ log p(t |x ) -Σ log p(x )

i
i
i
•  Drop second term: does not depend on w

•  Two cases: “regression” and classification


Likelihood for classification

(ie categorical target) 


•  For classification, targets t are class labels

•  Minimize -Σ log p(t |x )

i
i

1-ti

|x ) =  y(x ,w)  ti (1- y(x ,w))  ⇒
•  p(t
i
i
i
i
-log p(t |x ) = -t log y(x ,w) -(1 – t ) * log(1-y(x ,w))
i
i
i
i
i
i
•  Minimizing –log L equivalent to minimizing  

-[Σ t log y(x ,w) +(1 – t ) * log(1-y(x ,w))]

i
i
i
i
(cross-entropy error) 

Likelihood for “regression” 

(ie continuous target)


•	 For regression, targets t are real values

•	 Minimize -Σ log p(t |x )

i
i
•	 p(t |x ) = 1/Z exp(-(y(x ,w) – t )2/(2σ2)) ⇒

i	
i
i
i
-log p(t |x ) = 1/(2σ2) (y(x ,w) – t )2 +log Z 

i
i	
i
i
•	 y(x ,w) is network output
i

•	 Minimizing –log L equivalent to minimizing  

Σ  (y(x ,w) – t )2  (sum-of-squares error)

i	
i

Backpropagation algorithm


•  Minimizing error function 
by gradient descent: 
k+1  = wk  – ηgradw E 
w
•  Iterative gradient  

calculation by  

propagating error signals 

Backpropagation algorithm

Problem: how to set learning rate η?


2

0

-2

2

0

-2

-4

-2

0

2

-4

-2

0

2

Figures by MIT OCW.
Better: use more advanced minimization 
algorithms (second-order information) 

Backpropagation algorithm


Classification 

cross-entropy 

Regression 

sum-of-squares 

Overfitting


Real Distribution 

Overfitted 

Model

Improving generalization


Problem: memorizing (x,t) combinations 
(“overtraining”) 

0.7  0.5  0 
1 
-0.5 
0.9 
-0.2 
-1.2 
1 
0.3  0.6  1 
? 
-0.2 
0.5 

Improving generalization


•	 Need test set to judge performance

•	 Goal: represent information in data set, not 
noise 
•	 How to improve generalization?

– Limit network topology 
– Early stopping 
– Weight decay 

Limit network topology


•  Idea: fewer weights ⇒  less flexibility

•  Analogy to polynomial interpolation:


Limit network topology


Early Stopping


Overfitted model  “Real” model 

Overfitted model 

D
H
C

error 

holdout 

training 

0 

age 

cycles


Early stopping

•	 Idea: stop training when information (but
not noise) is modeled 
•	 Need hold-out set to determine when to 
stop training 

Overfitting


tss 

Overfitted model 

tss   a 

min (Δtss) 
tss   b 

a = hold-out set 

b = training set 

Stopping criterion 

Epochs 

Early stopping


Weight decay

•  Idea: control smoothness of network 

output by controlling size of weights

•  Add term  α||w||2  to error function


Weight decay


Bayesian perspective

•	 Error function minimization corresponds to 
maximum likelihood (ML) estimate: single
best solution wML 
•	 Can lead to overtraining

•	 Bayesian approach: consider weight 
posterior distribution p(w|D). 

Bayesian perspective


•  Posterior = likelihood * prior 
•  p(w|D) = p(D|w) p(w)/p(D) 
•  Two approaches to approximating p(w|D):

– Sampling 
– Gaussian approximation 

Sampling from p(w|D)


prior 

likelihood 

Sampling from p(w|D)


prior  *  likelihood = 

posterior


Bayesian example for 

regression


Bayesian example for 

classification


Model Features

(with strong personal biases) 


Modeling 
Effort 

Examples  Explanat. 
Needed 

Rule-based Exp. Syst. 
Classification Trees 
Neural Nets, SVM 
Regression Models 

Learned Bayesian Nets 
(beautiful when it works) 

high 
low 
low 
high 

low 

high? 
low 
“high” 
high+ 
low 
high 
moderate  moderate 

high+ 

high 

Regression vs. Neural Networks


Y 

Y 

“X1 ” 

“X2 ” 

“X

1X3 ” 

“X1X2X3 ” 

X1  X2  X3  X1X2  X1X3  X2X3 
X1X2X3 
(23 -1) possible combinations 

X1 X2  X3 

Y = a(X1) + b(X2) + c(X3) + d(X1X2) + ... 

Summary


•  ANNs inspired by functionality of brain

•  Nonlinear data model 
•  Trained by minimizing error function 
•  Goal is to generalize well 
•  Avoid overtraining 
•  Distinguish ML and MAP solutions 

Some References


Introductory and Historical Textbooks 
•	 Rumelhart, D.E., and McClelland, J.L. (eds) Parallel Distributed 
Processing. MIT Press, Cambridge, 1986. (H) 
•	 Hertz JA; Palmer RG; Krogh, AS. Introduction to the Theory of 
Neural Computation. Addison-Wesley, Redwood City, 1991. 
•	 Pao, YH. Adaptive Pattern Recognition and Neural Networks. 
Addison-Wesley, Reading, 1989. 
•	 Bishop CM. Neural Networks for Pattern Recognition. Clarendon 
Press, Oxford, 1995. 

