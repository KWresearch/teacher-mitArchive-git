Harvard-MIT Division of Health Sciences and Technology 
HST.951J: Medical Decision Support, Fall 2005
Instructors: Professor Lucila Ohno-Machado and Professor Staal Vinterbo 

6.873/HST.951 Medical Decision Support
 
Spring 2005 

Unsupervised Learning 

An overview of clustering and 
 
other exploratory data analysis  
 
methods 
 

Lucila Ohno-Machado 
 

A few “synonyms”…
 
(cid:132)  Nosography 
(cid:132)  Agminatics 
(cid:132)  Nosology 
(cid:132)  Aciniformics 
(cid:132)  Q-analysis 
(cid:132)  Numerical taxonomy 
(cid:132)  Typology 
(cid:132)  Botryology 
(cid:132)  Clustering 
(cid:132)  Systematics 
(cid:132)  Taximetrics 
(cid:132)  Clumping 
(cid:132)  Morphometrics 

(cid:132)  A multidimensional 
space needs to be 
reduced… 

Supervised Models 
 

Case 1 

Case 2 

age 
0.7 
0.6 
-0.6 
0 
-0.4 
-0.8 
0.5 

test1 
-0.2
0.5
0.1
-0.9
0.4
0.6
-0.7

0.8
0.4
0.2
0.3
0.2
0.3
0.4

Using these 

We are chasing 
 
PARTICULAR

patterns in the 
data…
 
Evaluate against 
 
“gold standard”
 

we predict probability of 
diagnosis, prognosis 

Unsupervised Models
 

Case 1 

Case 2 

age 
0.7 
0.6 
-0.6 
0 
-0.4 
-0.8 
0.5 

test1 
1
0.5
0.1
-0.9
0.4
0.6
-0.7

Cluster 
1
1
2
3
2
2
3

We are chasing 
ANY pattern in 
the data… 
We will need to 
interpret (label) 
the pattern 

Using these 

we put cases into clusters

Exploratory Data Analysis
 
(cid:132)  Goal is to flatten the dimensions of data to the 
spaces that we are familiar with (2-D and 3-D) 
(cid:132)  We can “see” the data in these dimensions and 
extract patterns 

(cid:132)  We are looking for clusters of data with similar 
characteristics overall 
(cid:132)  Hypothesis generation versus hypothesis testing 
(cid:132)  Fishing expedition versus confirmatory analysis 

Outline
 
(cid:132)  Proximity
 
(cid:132)  Distance Metrics 
(cid:132)  Similarity Measures 
(cid:132)  Clustering 
(cid:132)  Hierarchical Clustering 
(cid:132)  Agglomerative 
(cid:132)  K-means 
(cid:132)  Multidimensional Scaling 

Spatial relations
 
(cid:132)  Distance and dissimilarity 
(cid:132)  E.g. Euclidean distance, perceived 
difference 
(cid:132)  Proximity and similarity measures 
(cid:132)  E.g. correlation coefficient  Distance matrix 
House  Harvard  MIT  BWH 

House 
Harvard 
MIT 
BWH 

15 
18 
10 

4 
3 

5 

Unsupervised Learning
 

Raw 
Data 

Distance 
or 
Similarity 
Matrix 

Dimensionality 
Reduction 
•MDS 

Clustering 
•Hierarchical 
•Non-hierar. 

Graphical 
Representation 

“Validation” 
•Internal 
•External 

Algorithms, (dis)similarity measures, 
and graphical representations 

(cid:132)	  Most algorithms are not necessarily linked to a 
particular metric or (dis)similarity measure 
(cid:132)	  Also not necessarily linked to a particular graphical 
representation 
(cid:132)	  Cluster techniques were popular in the 50/60s 
(psychology experiments) 
(cid:132)	  There has been recent interest in biomedicine 
because of the emergence of high throughput 
technologies 
(cid:132)	  Old algorithms have been rediscovered and renamed 

Metrics (distances) 
 

i 

j 

Minkowski r-metric

(cid:132)  K dimensional data 
K
 ∑

⎧
=
 
⎨
⎩

 k =1


(cid:132)
  Euclidean 

d 

ij 

xik  −
 x
 jk
 

d

ij

=

d

ij

=

K
⎧
∑
⎨
⎩
k
1
=

K
⎧
∑
⎨
⎩
k
1
=

x
ik

−

x

jk

x
ik

−

x

jk

1 
2

⎫
2 
⎬
⎭

1
1

1

⎫
⎬
⎭

1

r

r

⎫
⎬
⎭

j 

i 

Minkowski r-metric 

(cid:132)  K dimensional data 
K
 ∑
 xik

⎧
=
 
⎨
⎩

 k =1

Euclidean 

d
 ij 

(cid:132) 

(cid:132) 

Manhattan 
  d
 ij
 
(cid:132)  (city-block) 


Generalized  d 

ij 

(cid:132)
 

K
 
⎧
=
  ∑
 xik 
⎨
⎩

=
 k  1 
K
⎧
=
  ∑
 xik
 
⎨
⎩

=
 k  1 

−


x
 jk

−


x
 jk 

x
 jk 
−
 

1 
2

⎫
2
 
⎬
⎭

 
1
1

⎫
1
 
⎬
⎭


 
1

r 

r

⎫
⎬
⎭


 





Metric spaces 
 
(cid:132)  Positivity  d ij   > d ii   = 0

Reflexivity 
(cid:132)  Symmetry  d ij   = d ji  
(cid:132)  Triangle 
inequality  d ij   ≤ d ih   + d hj
 

j 

h 

i 

More metrics
 
(cid:132)  Ultrametric  d ij  ≤ max[d ih  , d hj   ] 
replaces 
d ij  ≤ d ih  + d hj 

j

i 
h 
(cid:132)  Four-point  d hi  + d jk  ≤ max[(d hj  + d ik  ), (d hk  + d ij   )] 
additive 
replaces 
condition  d ij  ≤ d ih  + d hj 

Similarity measures
 
(cid:132)  Similarity function 
(cid:132)  For binary, “shared attributes” 
i j

t 
j 
i 
1 
2 ×1 

,  j ) = 

i s 
( 

i s 
( 

,  j ) = 

it  = [1,0,1]
j  = [0,0,1] 
t

Variations…
 
(cid:132)  Fraction of dattributes shared
 
i j
 
t 
,  j ) = 
(i s 
d 
(cid:132)  Tanimoto coefficient
 
i j
t 
i i  +  j j  − i j 
t
t
t
1 
2 + 1 − 1 

it  = [1,0,1] 
jt  = [0,0,1]

,  j ) = 

i s 
( 

i s 
(
 

,  j ) = 

Popular similarity measures
 
(cid:132)  Correlation
 
(cid:132)  Linear 
(cid:132)  Rank 
(cid:132)  Entropy-based
 
(cid:132)  Mutual information, based on the P(i|j) 
(cid:132)  Ad-hoc 
(cid:132)  Human perception 

Clustering
 

Hierarchical Clustering 
 
(cid:132)  Agglomerative Technique (average link) 
(cid:132)  Step 1: “Merge” 2 closest cases into a cluster 
(cid:132)  Step 2: Define cluster representative (e.g. , cluster 
means) as a “case” and remove the individual 
cases that compose the cluster 
(cid:132)  Go to step 1 until all cases are linked 

(cid:132)  Visualization 
(cid:132)  Dendrogram, Tree, Venn diagram 

Data Visualization
 

 
y
t
i
r
a
l
i
m
i
s
s
i
d

Figure by MIT OCW.


Hierarchical Clustering on Small 
 
Round Blood Cell Tumours
 

RMS 

BL 

NB 

EWS
 

Linkages
 
(cid:132)	  Average-linkage: proximity to the mean 
(centroid) 
(cid:132)	  Single-linkage: proximity to the closest 
element in another cluster 
(cid:132)	  Complete-linkage: proximity to the most 
distant element 

Mean Linkage
 
(cid:132)	  Assign case 
according to 
proximity to 
the mean 
(centroid) of 
another cluster 

x  x 

x 

x 

O O 

O 
O 

Single Linkage
 

(cid:132)	  Assign case 
according to 
proximity to 
the closest 
element in 
another cluster 

x  x 

x 

x 

O O 

O 
O 

Complete Linkage
 

(cid:132)	  Assign case 
according to 
proximity to 
the most 
distant 
element 

x  x 

x 

x 

O O 

O 
O 

Additive Trees
 
(cid:132)  Commonly the minimum spanning tree 
(cid:132)  Nearest neighbor approach to 
hierarchical clustering 

k-means clustering 
 
(Lloyd’s algorithm)
 
1.	  Select k(number of clusters) 
2.	  Select kinitial cluster centers c 
1,…,c 
k
Iterate until convergence: For each i, 
 
3.	 
1. 	  Determine data vectors vi1,…,vin closest 
to ci  (i.e., partition space) 
2. 	  Update ci as ci= 1/n(vi1+…+vin ) 

k-means clustering example
 

k-means clustering example
 

k-means clustering example
 

Common mistakes 
 
(cid:132)  Refer to dendrograms as meaning 
“hierarchical clustering” in general 
(cid:132)  Misinterpretation of tree-like graphical 
representations 
(cid:132)  Ill definition of clustering criterion 
(cid:132)  Declare a clustering algorithm as “best” 
(cid:132)  Expect classification model from clusters 
(cid:132)  Expect robust results with little/poor data 

Dimensionality Reduction 
 

Multidimensional Scaling 
 
(cid:132)	  Geometrical models
 
(cid:132)	  Uncover structure or pattern in 
observed proximity matrix 
(cid:132)	  Objective is to determine both 
dimensionality  d and the position of 
points in the d-dimensional space 

Classic Multidimensional Scaling
 

(cid:132)	  Also known as principal coordinates 
analysis (because it is principal 
components analysis) ☺ 
(cid:132)	  From distances, find coordinates
 
(cid:132)	  Constrain origin to centroid of data 

Metric and non-metric MDS
 
(cid:132)  Metric (Torgerson 1952) 
(cid:132)  Non-metric (Shepard 1961) 
(cid:132)  Estimates nonlinear form of the monotonic 
function 
sij  =  f mon  (d ij  )
 

Stress and goodness-of-fit
 
Stress 
Goodness of fit
 

(cid:132)  20 
(cid:132)  10 
(cid:132)  5 
(cid:132)  2.5 
(cid:132)  0 

(cid:132)  Poor 
(cid:132)  Fair 
(cid:132)  Good 
(cid:132)  Excellent 
(cid:132)  Perfect 

Figures removed due to copyright reasons. 
 
Please see: 
 
Khan, J., et al. "Classification and diagnostic prediction of cancers using gene expression 
 
profiling and artificial neural networks." Nat Med 7, no. 6 (Jun 2001): 673-9.
 

Visualization


(cid:132)	 Clustering is often 
good for visualization, 
but it is generally not 
very useful to 
separate data into 
pre-defined 
categories 
(cid:132)	 But there are 
counterexamples… 

Figures removed due to copyright reasons. 
 
Please see: 
 
Khan, J., et al. "Classification and diagnostic

prediction of cancers using gene expression (cid:10)

profiling and artificial neural networks."  Nat Med 7, no. 6 

(Jun 2001): 673-9.






Figures removed due to copyright reasons. 
 
Please see: 
 
Khan, J., et al. "Classification and diagnostic prediction of cancers using gene expression 
 
profiling and artificial neural networks." Nat Med 7, no. 6 (Jun 2001): 673-9. 
 

