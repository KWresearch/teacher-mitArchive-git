Harvard-MIT Division of Health Sciences and Technology 
HST.951J: Medical Decision Support, Fall 2005
Instructors: Professor Lucila Ohno-Machado and Professor Staal Vinterbo 

6.873/HST.951 Medical Decision Support

Fall 2005 


Logistic Regression  

Maximum Likelihood Estimation


Lucila Ohno-Machado 


Risk Score of Death from Angioplasty 

Unadjusted Overall Mortality Rate = 2.1% 

3000 

2500 

 
s
e
s
a
C
 
f
o
 
r
e
b
m
u
N

2000 

1500 

1000 

500 

0 

62% 

Number 
of Cases 

53.6% 

Mortality 
Risk 

26% 

21.5% 

0.4% 

1.4% 

0  to 2  

3  to 4  

12.4% 

7.6% 
2.2% 

2.9% 

7  to 8  
5  to 6  
Risk Score  Ca tegory 

1.6% 

1.3% 

9  to 10  

>10  

60% 

50% 

40% 

30% 

20% 

10% 

0% 

Linear Regression 

Ordinary Least Squares (OLS)


Minimize Sum of Squared Errors 
(SSE) 
n  data points 
i  is the subscript for each point 

yˆ i  = β0  + β xi1 
n 
n
SSE  = ∑ ( y  − yˆ i )  = ∑ [ y  − (β  + βxi )]
2
2
i 
i
0
1 
i =1 
i =1 

y 

x3

x4 
x2 
x1 

x 

Logit


=pi 

p  = 
i

1 
1 + e − (β +β xi  )
0
1

e 
β0 +β1 xi

eβ0 +β1 xi  + 1 

log

pi
⎤
⎡ 
⎥ = β01 + β1 xi
⎢ 
pi  ⎦
1 − 
⎣

y

logit

x 

x 

Increasing β


1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8

0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6

0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4

0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

1.2
1.2
1.2
1.2
1.2
1.2
1.2
1.2
1.2
1.2
1.2
1.2
1.2

1
1
1
1
1
1
1
1
1
1
1
1
1

0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8

0.6
0.6
0.6
0.6
0.6
0.6
0.6
0.6
0.6
0.6
0.6
0.6
0.6

0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4

0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2

0
0
0
0
0
0
0
0
0
0
0
0
0

1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2
1 .2

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8
0 .8

Series 1
Series 1
Series 1
Series 1
Series 1

0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
0 .6
Ser ies1
Ser ies1
Ser ies1
Ser ies1
Ser ies1
Ser ies1

Series 1
Series 1
Series 1
Series 1
Series 1

0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4
0 .4

0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2
0 .2

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10

20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20

30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30

0
0
0
0
0
0
0
0
0
0
0
0
0

10
10
10
10
10
10
10
10
10
10
10
10
10

20
20
20
20
20
20
20
20
20
20
20
20
20

30
30
30
30
30
30
30
30
30
30
30
30
30

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10

20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20

30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30

•  Baseline case 

Finding β0


1

pi  = 
1 + e − (β  )0 

Blue(1)  Green(0) 
 50
 22
 28
45 
52 
97 

 147

 74
 73

Death
Life 
Total

 

297.0 

1

= 
1 + e − (β  )

0 

β 0 

.0

− = 

8616

Odds ratio


•  Odds: p/(1-p) 
•  Odds-ratio 

Blue 
 28
45 
 73

Green 
 22
52 
 74

 50
97 
 147

Death
Life 
Total

=OR 

 

  OR  =

death 
blue 
|

p 
death 
|blue 
1 − p 
p

death 
| green  
1 − p
death 
|
28 / 45

22 / 52 

green  

=  47.1 

What do coefficients mean?


eβ color  = ORcolor  

Blue 
 28
45 
 73

Green 
 22
52 
 74

 50

97 
 147

Death
Life 
Total

OR  = 

28 / 45 
22 / 52

eβ color   =  47 
.1

β color  =  385.0

1 
pblue  = 
1 + e − − 
( 
.0 
8616
1 
p green  = 
1 + e 8616
.0 

 

=  47 
.1


)  =  383.0 

+  385.0 

=  297
.0 

What do coefficients mean?


eβage  = ORage  

Age49  Age50 
 22
 28
45 
52 
 74
 73

 50
97 
 147

Death
Life 
Total

 

 

OR  = 

age 

=50

death 
| 

age 

=50

p
death 
| 
1 − p
p
death 
| 
1 − p

age 

= 49

death 
|

age 

= 49

Why not search using OLS?

x3 
y 

yˆ i  = β0  + β xi

1

n 
SSE  = ∑ ( y  − yˆ i ) 
i

i =1


log

pi
⎤
⎡ 
⎥ = β01 + β1 xi

⎢ 
pi  ⎦
1 − 
⎣

2


x4 

x2

x1

logit


x


x


P(model | data) ?


1 
pi  = 
1 + e  − ( β β 1 xi )
0 + 

If only intercept is allowed, which
value would it have?

y 

y

x

x 

x 

P (data | model) ?

P(data|model) = [P(model | data) P(data)] / P(model)


When comparing models: 

P(model): assume all the same (ie, chances of
being a model with high coefficients the same as
low, etc) 

P(data): assume it is the same 
Then, 
P(data | model) α P(model | data) 

Maximum Likelihood Estimation


•	 Maximize P(data | model)

•	 Maximize the probability that we would 
observe what we observed (given 
assumption of a particular model) 
•	 Choose the best parameters from the 
particular model 

logit 

x 

Maximum Likelihood Estimation


•  Steps:

– Define expression for the probability of data 
as a function of the parameters 
– Find the values of the parameters that 

maximize this expression


Likelihood Function


L  = Pr(Y  )

y 
L  = Pr(  
1 , 

y 

2 ,...,  y  )

n 
L  = Pr(   y  ) Pr( y 
1

)... 

2 

Pr( 

n 
n  ∏ 
y  )

Pr( 
= 
i 
1 
= 

i y 

)

Likelihood Function

Binomial 


Pr(  

n 
y  ) = ∏ Pr(  y  )

n
i 
i  =1 

2 

L  = Pr(Y  )

2 ,...,  y  )

y 
L  = Pr( 
1 , 
n 
L  = Pr(  y  ) Pr(  y 
1

)... 

y 

Pr(  y  = 1) =  p
i
i 
Pr(  y  = 0) = (1 −  p  )

i
i 

Pr( 

y 
i 

)

= 

p
i

y 

i 

1(

−

p
i 

y 

i

1)
−

Likelihood Function


yi 

(1 −  p )

1− yi
i


i


(1 −  p )

i


n 
n
L = ∏ Pr(  y  ) = ∏ p 
i 
i
i = 1

i = 1 
y 
pi 
⎞
n  ⎛ 
L = ∏ ⎜⎜ 
⎟⎟ 
i = 1  ⎝ (1 −  p ) ⎠ 
i 
⎛
∑
⎜⎜
1(
⎝
i
∑
x
(
)
−
β
i
i

log

log

log

y

i

y

i

L

=

L

=

log

1(

−

p
i

)

p
⎞
+⎟⎟
i
p
)
−
⎠
i
∑
log
1(
i

∑
i
e
+

ix
β

)

Since model is the logit

Log Likelihood Function


yi 

(1 − p  )
1− yi
i


n
n 
L = ∏ Pr(  y  ) = ∏ p 
i 
i
i = 1 
i = 1

y

p 
n  ⎛ 
⎞ 
L = ∏ ⎜⎜ 
i = 1  ⎝ (1 − pi ) ⎟⎟
i 
⎠ 
p 
⎛ 
⎞
⎟⎟ + ∑ log(1 − p  )

log  L = ∑ y  log⎜⎜
i
i 
i

⎝ (1 − p  ) ⎠ 
i
i 
i 

i


(1 − pi )

Log Likelihood Function


p 
⎞

⎛ 
⎟⎟ + ∑ log(1 − p  )

log  L  = ∑ y  log⎜⎜
i
i 
i

⎝ (1 − p  ) ⎠ 
i 
i
i

log  L  = ∑ y  (βxi  ) − ∑ log(1 + e 
βxi )

i

i

i

Since model is the logit 

Maximize 


log  L  = ∑ y  (βxi  ) − ∑ log(1 + e 
i

i
i


βxi )


∂

L
log
∂
β

=

∑
i

xy
i
i

−

∑
i

ˆ
xy
i
i

=

0

ˆ
y

i

=

1
e
ix
−+
β

1

Not easy to solve because 
y-hat is non-linear, need to 
use iterative methods: most 
popular is Newton-Raphson

Maximize 


log  L = ∑ y  (βxi ) − ∑ log(1 + e 
i

i
i


βxi  )


∂ log  L 
= ∑ y xi  − ∑ y
ˆ  x  = 0

i 
i 
i 
∂β 
i
i 
1 
yˆ i  = 
1 + e −βxi 

Not easy to solve because
y-hat is non-linear, need to
use iterative methods: most 
popular is Newton-Raphson 

Newton-Raphson


•	 Start with random or zero βs

•	 “walk” in the “direction” that maximizes 
MLE 
– how big a step (Gradient or Score)

– direction 

Maximizing the LogLikelihood


Log 
Likelihood 

βi+1  βi 
First iteration LL 
Initial LL 

Maximizing the LogLikelihood


Log 
Likelihood 

βi+1  βi 
Second iteration LL 

New Initial LL 

Similar iterative method  to Minimizing the  

Error in Gradient Descent (neural nets)


Error surface 

initial error 

negative 

derivative 

final error 

local minimum 

winitial wtrained 

positive change


Newton-Raphson Algorithm

log  L  = ∑ y  (β xi  ) − ∑ log(1 + e 
β xi  )

i

i
i


U  (β ) =

∂ log  L 
∂β 

= ∑  x y 
i
i 
i 

− ∑ ˆ
x y 
i

i 
i 

I  (β ) =

∂ 2 log  L 
β β  ' 
∂ 
∂ 

= −∑ 
' 
i  ˆ  (1 − y ˆ ) 
x x i
y 
i
i

i


Gradient


Hessian

β j  + 1  = β j  − I 

− 1

(β j  )U  (β j  ) 

a step 

Convergence 

•


Criterion 
βj +1  − βj  <  0001
.

βj 

•


Convergence problems: complete and quasi-
complete separation 

Complete separation


MLE does not exist (ie, it is infinite) 

βi 

y 

βi+1 

y

x

x 

x 

Quasi-complete separation


Same values for predictors, different 
outcomes 

y 

x 

No (quasi)complete separation

is fine to find MLE


y 

x


How good is the model?


•	 Is it better than predicting the same prior 
probability for everyone? (ie, model with 
just β0) 
•	 How well do the training data fit?

•	 How well does is generalize? 

Generalized likelihood-ratio test


• Are  β 1, β 2, …, β  different from 0?

n

n
n 
L = ∏ Pr(  y  ) = ∏ p 
yi 
(1 − p  )
1− yi
i

i 
i
i = 1 
i = 1

log  L = ∑ [ y  log  p  + (1 − y  )
i
i
i 
i

L

L  +  log 
G − = 
log 
2 
2 
o 
1


log(  − p  )] 

1
i


G has χ 2  distribution

cross  − entropy  _  error   − =  ∑ [ y  log  p  + (1 − y  )
i
i
i 
i


log(  − p  )] 

1
i


AIC, SC, BIC


•	 To compare models

•	 Akaike’s Information Criterion, k 
parameters 
AIC= − log 
k 
L
 + 2

2 

•	 Schwartz Criterion, Bayesian Information 
Criterion, n  cases 
BIC= − log 
L 
2 

logn


+


k 

Summary

•	 Maximum Likelihood Estimation is used in 
finding parameters for models 
•	 MLE maximizes the probability that the 
data obtained would have been generated
by the model 
•	 Coming up: goodness-of-fit (how good are 
the predictions?) 
– How well do the training data fit? 

– How well does is generalize? 

