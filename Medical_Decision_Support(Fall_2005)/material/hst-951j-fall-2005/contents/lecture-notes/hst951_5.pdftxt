Harvard-MIT Division of Health Sciences and Technology 
HST.951J: Medical Decision Support, Fall 2005

Instructors: Professor Lucila Ohno-Machado and Professor Staal Vinterbo 

6.873/HST.951 Medical Decision Support 
Spring 2004


Evaluation  

Lucila Ohno-Machado 

Outline


Calibration and 
Discrimination 
• AUCs  
•  H-L statistic 

Strategies: 
–  Cross-validation 
–  Bootstrap 
•  Decomposition of  

error 
– Bias  
– Variance 

Main Concepts

•  Example of  a Medical Classification System

•  Discrimination 
–  Discrimination: sensitivity, specificity, PPV, NPV,

accuracy, ROC curves, areas, related concepts

•  Calibration 
–  Calibration curves 
–  Hosmer and Lemeshow goodness-of-fit 

Example I


Modeling the Risk of  Major In-Hospital 
Complications Following Percutaneous 
Coronary Interventions 

Frederic S. Resnic, Lucila Ohno-Machado, Gavin J. Blake, 
Jimmy Pavliska, Andrew Selwyn, Jeffrey J. Popma 
[Simplified risk score models accurately predict the risk of 
major in-hospital complications following percutaneous 
coronary intervention. 
Am J Cardiol. 2001 Jul 1;88(1):5-9.] 

Dataset:  Attributes Collected 


History 

Presentation  Angiographic 

Procedural 

Operator/Lab 

acute MI 
age 
primary 
gender 
rescue 
diabetes 
iddm 
CHF class 
history CABG  angina class 
Baseline 
Cardiogenic 
shock 
creatinine 
CRI 
failed CABG 
ESRD 
hyperlipidemia 

Data Source: 
Medical Record 
Clinician Derived 
Other 

number lesions  annual volume 
occluded 
lesion type 
device experience 
multivessel 
(A,B1,B2,C)  number stents  daily volume 
lab device 
stent types (8) 
graft lesion 
closure device 
vessel treated 
experience 
unscheduled case 
ostial 
gp 2b3a 
antagonists 
dissection post 
rotablator 
atherectomy 
angiojet 
max pre stenosis 
max post stenosis 
no reflow 

Study Population


Development Set 
1/97-2/99 
2,804 

Validation Set 
3/99-12/99 
1,460 

Cases 

Women 
Age > 74yrs 
Acute MI 
Primary 
Shock 

909  (32.4%) 
595  (21.2%) 
250 
(8.9%) 
(5.6%) 
156 
62 
(2.2%) 

433  (29.7%) 
308  (22.5%) 
144 
(9.9%) 
(6.5%) 
95 
20 
(1.4%) 

(5.5%) 
80 
777  (53.2%) 

p=.066 
p=.340 
p=.311 
p=.214 
p=.058 
p=.298 
p<.001 

p=.110 
p=.739 

Class 3/4 CHF 
gp IIb/IIIa antagonist 

(6.3%) 
176 
1,005   (35.8%) 

Death 
67 
Death, MI, CABG (MACE)  177 

(2.4%) 
(6.3%)  

24  (1.6%) 
96  (6.6%) 

ROC Curves: Death Models

Validation Set:  1460 Cases 

1.00 

0.90 

0.80 

0.70 

0.60 

0.50 

0.40 

0.30 

0.20 

0.10 

 
y
t
i
v
i
t
i
s
n
e
S

LR 
Score 
aNN  

ROC Area 
LR: 
0.840 
Score:  0.855 
aNN:  0.835 

ROC = 0.50 

0.00 
0.00 

0.20 

0.60 
0.40 
1 - Specificity 

0.80 

1.00 

Risk Score of Death:  BWH Experience

Unadjusted Overall Mortality Rate = 2.1% 

 
s
e
s
a
C
 
f
o
 
r
e
b
m
u
N

3000 

2500 

2000 

1500 

1000 

500 

0 

Number 
of Cases 

53.6% 

Mortality 
Risk 

21.5% 

12.4% 

2.2% 

0  to 2  

3  to 4  

7  to 8  
5  to 6  
Risk Score  Ca tegory 

9  to 10  

>10  

60% 

50% 

40% 

30% 

20% 

10% 

0% 

Evaluation Indices


General indices


•  Brier score (a.k.a. mean squared error)


2

Σ(e - o )
i
i
n 

e = estimate (e.g., 0.2)

o = observation (0 or 1) 

n = number of cases


Discrimination Indices


Discrimination


•  The system can “somehow” differentiate  

between cases in different categories


•  Binary outcome is a special case:  

– diagnosis (differentiate sick and healthy 
individuals) 
– prognosis (differentiate poor and good 
outcomes) 

Discrimination of  

Binary Outcomes

•	 Real outcome (true outcome, also known as “gold
standard”) is 0 or 1, estimated outcome is usually a
number between 0 and 1 (e.g., 0.34)
“True”

Estimate 
0.3	
0 
0 
0.2	
1 
0.5	
0.1	
0 
In practice, classification into category 0 or 1 is based on
Thresholded Results (e.g., if output or probability > 0.5 
then consider “positive”) 
–	 Threshold is arbitrary 

•	

threshold 

normal 

Disease 

True 
Negative (TN) 
FN 

FP 

True 
Positive (TP) 

0 

e.g. 0.5 

1.0


Sens = TP/TP+FN 

Spec = TN/TN+FP 

PPV = TP/TP+FP 

NPV = TN/TN+FN 

Accuracy = TN +TP 

nl 

D 

“nl” 

TN 

FN 

“D” 

FP 

-


TP

+ 

“nl” 

“D” 

Sens = TP/TP+FN 

40/50 = .8

Spec = TN/TN+FP

45/50 = .9

PPV = TP/TP+FP

40/45 = .89 

NPV = TN/TN+FN 

45/55 = .81 

Accuracy = TN +TP

85/100 = .85


nl 

D 

“nl” 

45 

“D” 

5 

10 

40

“nl” 

“D” 

Sensitivity = 50/50 = 1 
Specificity = 40/50 = 0.8 

threshold 

“nl” 

“D” 

nl 

40 

10 

50 

D 

0 

50

50 

40 

60 

nl 

TN 

disease 

TP 

FP 

0.0 

0.4 

1.0


Sensitivity = 40/50 = .8 
Specificity = 45/50 = .9 

threshold 

“nl” 

“D” 

nl 

45 

5 

50 

D 

10 

40

50 

50 

50 

nl 

TN 

disease 

TP 

0.0 

FN 

FP 

0.6 

1.0


Sensitivity = 30/50 = .6 
Specificity = 1 

threshold 

“nl” 

“D” 

nl 

50 

0 

50 

D 

20 

30

50 

70 

30 

nl 

TN 

disease 

TP 

FN 

0.0 

0.7 

1.0


4
.
0
 
d
l
o
h
s
e
r
h
T

6
.
0
 
d
l
o
h
s
e
r
h
T

 
7
.
0
 
d
l
o
h
s
e
r
h
T

“nl” 

“D” 

“nl” 

“D” 

“nl” 
“D” 

nl 

40 

10 
50 

nl 

45 

5 
50 

nl 

50 
0 
50 

D 

0 

50
50 

D 

10 

40
50 

D 

20 
30
50 

40 
60 

50 

50 

70 
30 

1 

 
y
t
i
v
i
t
i
s
n
e
S

ROC 
curve

0 

1 - Specificity


1 



1 

 
y
t
i
v
i
t
i
s
n
e
S

ROC 
curve 

 
s
d
l
o
h
s
e
r
h
T
 
l
l
A

0 

1 - Specificity 

1 

45 degree line: 
no discrimination 

1


 
y
t
i
v
i
t
i
s
n
e
S

0 

1 - Specificity


1 

45 degree line: 
no discrimination 

1


 
y
t
i
v
i
t
i
s
n
e
S

Area under ROC: 

0.5 

0 

1 - Specificity 

1 

Perfect 
discrimination 

1


 
y
t
i
v
i
t
i
s
n
e
S

0 

1 - Specificity


1 

Perfect 
discrimination 

1 

 
y
t
i
v
i
t
i
s
n
e
S

Area under ROC: 

1 

0 

1 - Specificity 

1


1 

 
y
t
i
v
i
t
i
s
n
e
S

ROC 
curve 
Area = 0.86

0 

1 - Specificity 

1


What is the area under the  

ROC? 

•	 An estimate of the discriminatory performance of the 
system 
the real outcome is binary, and systems’ estimates are  

–	
continuous (0 to 1)

–	 all thresholds are considered 

•	 Usually a good way to describe the discrimination if there
is no particular trade-off between false positives and
false negatives (unlike in medicine…) 
–	 Partial areas can be compared in this case 

Simplified Example

Systems’ estimates for 10 patients

“Probability of being sick” 
“Sickness rank” 
(5 are healthy, 5 are sick): 

0.3 
0.2 
0.5 

0.1 

0.7 

0.8 

0.2 

0.5 

0.7 

0.9 


Estimates per class


•  Healthy (real outcome is 0) 
0.3 
0.2 
0.5 
0.1 
0.7 

•  Sick (real outcome is1) 

0.8 
0.2 
0.5 
0.7 
0.9 

All possible pairs 0-1


•  Healthy 
0.3 
0.2 
0.5 
0.1 
0.7 

< 

•  Sick 
0.8 
0.2 
0.5 
0.7 
0.9 

concordant 
discordant
concordant
concordant
concordant

All possible pairs 0-1

Systems’ estimates for 

•  Healthy 
0.3 
0.2 
0.5 
0.1 
0.7 

•  Sick 
0.8 
0.2 
0.5 
0.7 
0.9 

concordant
tie
concordant
concordant
concordant 

C - index


•  Concordant 
18

•  Discordant 
4 

• Ties

3


C -index =  Concordant + 1/2 Ties =  18 + 1.5

All pairs 
25


1 

 
y
t
i
v
i
t
i
s
n
e
S

ROC 
curve 
Area = 0.78

0 

1 - Specificity 

1


ROC Curves: Death Models

Validation Set:  1460 Cases 

1.00 

0.90 

0.80 

0.70 

0.60 

0.50 

0.40 

0.30 

0.20 

0.10 

 
y
t
i
v
i
t
i
s
n
e
S

LR 
Score 
aNN  

ROC Area 
LR: 
0.840 
Score:  0.855 
aNN:  0.835 

ROC = 0.50 

0.00 
0.00 

0.20 

0.60 
0.40 
1 - Specificity 

0.80 

1.00 

Calibration Indices


Discrimination and Calibration


•	 Discrimination measures how much the 
system can discriminate between cases 
with gold standard ‘1’ and gold standard ‘0’ 
•	 Calibration measures how close the 
estimates are to a “real” probability 
•	 “If the system is good in discrimination, 
calibration can be fixed” 

Calibration 


•  System can reliably estimate probability of

– a diagnosis 
– a prognosis 

•  Probability is close to the “real” probability


What is the “real” probability?


•	 Binary events are YES/NO (0/1) i.e., 
probabilities are 0 or 1 for a given individual 
•  Some models produce continuous (or quasi-

continuous estimates for the binary events)

•	 Example: 

–  Database of patients with spinal cord injury, and a 
model that predicts whether a patient will ambulate or
not at hospital discharge 
–	 Event is 0: doesn’t walk or 1: walks

–  Models produce a probability that patient will walk: 
0.05, 0.10, ... 

How close are the estimates to the 

“true” probability for a patient?

•	 “True” probability can be interpreted as 
probability within a set of similar patients 
•	 What are similar patients? 

–	 Clones 
–  Patients who look the same (in terms of variables 
measured) 
–	 Patients who get similar scores from models

–	 How to define boundaries for similarity? 

Estimates and Outcomes


•  Consider pairs of 
– estimate and true outcome 
0.6 and 1
0.2 and 0
0.9 and 0
– And so on… 

Calibration 


Sorted pairs by systems’ estimates 
outcomes 
0.1 
0.2 
0.2 
0.3 
0.5 
0.5 
0.7 
0.7 
0.8 
0.9 

sum of group = 1.3 

sum of group = 0.5 

sum of group = 3.1 

Real 

sum = 1 

sum = 1 

sum = 3 

0 
0 
1 
0 
0 
1 
0 
1 
1 
1 

Regression line 

Linear 
Regression 
and 
450  line 

1


p

u
o
r
g
 
r
e
p
 
s
e
u
l
a
v
 
d
e
v
r
e
s
b
o
 
f
o
 
g
v
A

0 

Avg of estimates per group  1


Goodness-of-fit

Sort systems’ estimates, group, sum, chi-square 

Estimated 
0.1 
0.2 
0.2 
0.3 
0.5 
0.5 
0.7 
0.7 
0.8 
0.9 

sum of group = 0.5 

sum of group = 1.3 

sum of group = 3.1 

χ2 = Σ [(observed - estimated)2/estimated] 

Observed 
0 
0 
1 
0 
0 
1 
0 
1 
1 
1 

sum = 3 

sum = 1 

sum = 1 

Hosmer-Lemeshow C-hat  

Groups based on n-iles (e.g., terciles), n-2 d.f. training, n  d.f. test 
Measured Groups 
Estimated 
0.1 
0.2 
0.2  sum = 0.5 
0.3 
0.5 
0.5  sum = 1.3 
0.7 
0.7 
0.8 
0.9  sum = 3.1 

Observed 
0 
0 
1  sum = 1 
0 
0 
1  sum = 1 
0 
1 
1 
1  sum = 3 

Hosmer-Lemeshow H-hat  

Groups based on n  fixed thresholds (e.g., 0.3, 0.6, 0.9), n-2 d.f.

Measured Groups 
Estimated 
0.1 
0.2 
0.2 
0.3  sum = 0.8 
0.5 
0.5  sum = 1.0 
0.7 
0.7 
0.8 
0.9  sum = 3.1 

Observed 
0 
0 
1 
0 sum = 1 
0 
1  sum = 1 
0 
1 
1 
1  sum = 3 

Decomposition of Error


The “ideal” model generates data D.

A “learned” model is learned from D.

Once learned, model M  is fixed.

After learning, I  and M  are conditionally  

independent given D. 

D 

f 

y 

I 

f’

M

y^ 

Decomposition of Error


A and B binary (y-hat and y-ideal) 

= 1 − ∑ 
P  
A  
= B 
= 1 − ∑ 
P  
A  
B 
=

(

AB  

|


D 

AB  
( 

D 
| 

) = 
) = 1 − ∑ 
A  P 
(
B 
A 
=

D 

f 

B 

f’

A 

|

B P D 
)
(

|

D 

) =


Decomposition of Error


A represents classification from learned model 
B represents classification from “ideal” 

D 

f 

B 

f’

A 

) = 

) + 0 + 0 + 0

= 

|

D 

) = 

B P D  
| 
(
)

= 1 − ∑ 
A P 
(
= B 
A 
= 1 − ∑ 
B P  A P 
(
(
)
1 ⎤ 
⎡ 1
− ∑ 
B P A  P 
(
)
( 
+  ⎥⎦
⎢⎣ 2  2 
1 
1
=  + − ∑ 
B P A P 
(
)
(
2 
2

) + [∑  AB 
P 
( 

) −∑  AB 
P 
( 

⎡ 1 
)]+ ⎢⎣ 2 ∑  A P 
)  − 
(
2 

⎡ 1 
1 ∑  A P 
2 ⎤
+ ⎢⎣ 2 ∑  B P  
(
(
)  ⎥⎦ 
2

1 
)  − ∑  B P  
( 
2
2 

) 2 ⎤ =
⎥⎦

Decomposition of Error 

− ∑ 
B P A P 
(
)
(

= 

+ 

1 
1 
2 
2 
− =  1[∑ 
B P A P 
(
)
(

− 

1 ∑ 
1 ∑ 
( A P 
( A P 
2)
2)
2 
2 
1 
[1 − ∑  B P 
(
2

1 ∑ 
1 ∑ 
( B P 
( B P 
2 
2 
1
[∑  A P 
)  − ∑  AB 
P 
(
(
2
2 

) ] + 
2 

+ 

− 

2)

2)

= 

) + ∑  B P 
(

)  ] =
2

)

+ ∑ 
P 

( AB 

)

−∑ 
P 

( AB 

)

+ 

)] + 

1 
[1 − ∑  A P 
(
) ] + 
2 
2

) − ∑  AB 
P 
( 
=0 

=  {[1 − ∑  A P 
1 
)  ]+ [1 − ∑  B P 
(
(
2 
2 
error 
variance 

)  ]+ ∑ [ A P 
) −  B P 
(
(
2 

)] }

2 

bias 

