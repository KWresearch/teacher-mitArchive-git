1
00:00:00 --> 00:00:01
 
 

2
00:00:01 --> 00:00:02
The following content
is provided

3
00:00:02 --> 00:00:04
by MIT open courseware under
a creative commons license.

4
00:00:04 --> 00:00:08
Additional information about
our license, and MIT open

5
00:00:08 --> 00:00:15
courseware, in general, is
available at ocw.mit.edu.

6
00:00:15 --> 00:00:17
PROFESSOR: Good afternoon.
 

7
00:00:17 --> 00:00:22
 
 

8
00:00:22 --> 00:00:31
In response to public demand
here-- let's see how many

9
00:00:31 --> 00:00:34
people do you-- again, the
colors may have faded by

10
00:00:34 --> 00:00:37
now, but check it when
you tilt your head.

11
00:00:37 --> 00:00:38
The effect changes.
 

12
00:00:38 --> 00:00:44
 
 

13
00:00:44 --> 00:00:48
How many people still have
some sort of an effect?

14
00:00:48 --> 00:00:51
It's really nice to know
that this course has some

15
00:00:51 --> 00:00:53
lasting impact on people.
 

16
00:00:53 --> 00:01:00
 
 

17
00:01:00 --> 00:01:08
Maybe we should try posting the
Powerpoint on the web, because

18
00:01:08 --> 00:01:10
then if you want to build up
your own McCullough effect

19
00:01:10 --> 00:01:14
you can do this and
entertain yourselves.

20
00:01:14 --> 00:01:18
 
 

21
00:01:18 --> 00:01:24
If you have been following the
story thus far, you should

22
00:01:24 --> 00:01:32
basically have the idea that at
the front of the visual system,

23
00:01:32 --> 00:01:39
or of any sensory system,
you've got all sorts of

24
00:01:39 --> 00:01:41
information coming in.
 

25
00:01:41 --> 00:01:46
That the job early processes
in the visual systems do.

26
00:01:46 --> 00:01:49
The sorts of things that the
early parts of visual cortex do

27
00:01:49 --> 00:01:53
and say, oh look, there's a
little line at this point

28
00:01:53 --> 00:01:54
in the visual field.
 

29
00:01:54 --> 00:01:58
Oh look, there's a dot
there, and a line here,

30
00:01:58 --> 00:02:00
and it's moving like that.
 

31
00:02:00 --> 00:02:03
Little bits of information
all over the place.

32
00:02:03 --> 00:02:06
Too much of it for
you to handle.

33
00:02:06 --> 00:02:11
So, last time I talked about
this bottleneck of attention

34
00:02:11 --> 00:02:17
that allows only some of it
through to processes that

35
00:02:17 --> 00:02:21
would do things like,
say, recognition.

36
00:02:21 --> 00:02:24
And, then, I'm going to run
out of places to draw.

37
00:02:24 --> 00:02:26
We'll take a little
detour here.

38
00:02:26 --> 00:02:32
Somewhere up here, you're
going to get perception.

39
00:02:32 --> 00:02:38
And the job of today's lecture
is to convince you that that

40
00:02:38 --> 00:02:42
percept, your current
understanding of the world,

41
00:02:42 --> 00:02:46
is always the result of
many, many inferences.

42
00:02:46 --> 00:02:52
Many, many guesses of what the
nature of the world might be.

43
00:02:52 --> 00:02:59
Because, not only is there too
much information coming in,

44
00:02:59 --> 00:03:03
there's also too little
information coming to specify

45
00:03:03 --> 00:03:06
exactly what's going on
out there in the world.

46
00:03:06 --> 00:03:08
Consider just a couple of
the problems -- I think

47
00:03:08 --> 00:03:10
are they on the hand out?
 

48
00:03:10 --> 00:03:12
Couple of them are-- well,
one of the obvious ones.

49
00:03:12 --> 00:03:15
It's the world is 3D.
 

50
00:03:15 --> 00:03:17
Your job is to figure
out what's going on

51
00:03:17 --> 00:03:18
out in the world.
 

52
00:03:18 --> 00:03:20
The input is inherently 2D.
 

53
00:03:20 --> 00:03:25
That retina that you've
got is a 2D surface.

54
00:03:25 --> 00:03:28
So, if you are seeing 3D,
you are recovering that

55
00:03:28 --> 00:03:34
3D information from
essentially 2D input.

56
00:03:34 --> 00:03:40
You're collecting information
about light intensities.

57
00:03:40 --> 00:03:42
You don't care about
light intensity.

58
00:03:42 --> 00:03:46
You care about surface
properties in the world.

59
00:03:46 --> 00:03:49
So, everything patch that
you're seeing -- if you're

60
00:03:49 --> 00:03:57
looking at this spot right
here, what you're seeing is the

61
00:03:57 --> 00:04:03
product of the surface, the
properties of the surface, and

62
00:04:03 --> 00:04:05
the properties of the
illuminant, the properties of

63
00:04:05 --> 00:04:07
what's lighting it up.
 

64
00:04:07 --> 00:04:09
You don't care about the
properties of the illuminate.

65
00:04:09 --> 00:04:15
You want to recover just the
properties of the surface.

66
00:04:15 --> 00:04:18
So how do you successfully
ignore the properties

67
00:04:18 --> 00:04:18
of illumination?
 

68
00:04:18 --> 00:04:21
We'll say a little
bit about that.

69
00:04:21 --> 00:04:25
The world you're looking is
an essentially stable world.

70
00:04:25 --> 00:04:27
I mean things move around
in it, but the whole world

71
00:04:27 --> 00:04:28
doesn't jump around.
 

72
00:04:28 --> 00:04:32
But, you're looking at
it from an inherently

73
00:04:32 --> 00:04:34
unstable vantage point.
 

74
00:04:34 --> 00:04:35
You're moving around.
 

75
00:04:35 --> 00:04:38
And more to the point, even if
you're salk still, the way

76
00:04:38 --> 00:04:42
you're looking at the world is
you're moving your eyes around.

77
00:04:42 --> 00:04:44
Try this for a moment.
 

78
00:04:44 --> 00:04:46
Look at the lower left hand
corner of the screen.

79
00:04:46 --> 00:04:49
 
 

80
00:04:49 --> 00:04:51
Well, actually that might be a
little large, so look at the

81
00:04:51 --> 00:04:54
lower left hand corner this
McCollough test pattern.

82
00:04:54 --> 00:04:56
Look at the lower right hand
corner of the McCollough

83
00:04:56 --> 00:04:58
test pattern.
 

84
00:04:58 --> 00:05:03
Did the McCollough effect test
pattern jump when you did that?

85
00:05:03 --> 00:05:04
No.
 

86
00:05:04 --> 00:05:05
It didn't.
 

87
00:05:05 --> 00:05:06
Why didn't it jump?
 

88
00:05:06 --> 00:05:10
Because when you're looking
here-- do I have a

89
00:05:10 --> 00:05:12
laser pointer today?
 

90
00:05:12 --> 00:05:12
No.
 

91
00:05:12 --> 00:05:17
Oh well -- when you're looking
here, the bulk of that square

92
00:05:17 --> 00:05:19
is to the right of fixation.
 

93
00:05:19 --> 00:05:21
When you're looking here,
the bulk that image is to

94
00:05:21 --> 00:05:22
the left the fixation.
 

95
00:05:22 --> 00:05:25
So it's in two different
spots on the retina.

96
00:05:25 --> 00:05:27
If I put something here, and
something here, on your retina,

97
00:05:27 --> 00:05:28
it'll look like it's moving.
 

98
00:05:28 --> 00:05:31
Why didn't that look
like it's moving?

99
00:05:31 --> 00:05:36
The reason is, that when you
tell your eyes to move, you

100
00:05:36 --> 00:05:43
send a copy of that command, in
effect, to visual centers of

101
00:05:43 --> 00:05:48
your brain saying, look, I just
told my eyes to move, kindly

102
00:05:48 --> 00:05:52
ignore the resulting smear.
 

103
00:05:52 --> 00:05:54
In fact, I want you
to do two things.

104
00:05:54 --> 00:06:00
I want you-- you, your visual
system-- I want you to shut

105
00:06:00 --> 00:06:06
down during the course of the
eye movement, and I want you to

106
00:06:06 --> 00:06:10
compensate for the fact that
everything is been displaced.

107
00:06:10 --> 00:06:13
You can see what would happen
if that was not the case,

108
00:06:13 --> 00:06:17
by taking your finger,
and poking your eye.

109
00:06:17 --> 00:06:20
 
 

110
00:06:20 --> 00:06:24
On your eyelid, you should
try this, because it's more

111
00:06:24 --> 00:06:26
interesting if you actually
try this, it if you

112
00:06:26 --> 00:06:27
wiggle your eyeball.
 

113
00:06:27 --> 00:06:30
You can do it slowly, or
you can do it quickly.

114
00:06:30 --> 00:06:32
Look at me, and poke
your eye ball around.

115
00:06:32 --> 00:06:37
You will notice that all
your friends look funny.

116
00:06:37 --> 00:06:40
But, you'll notice that
things are jumping around.

117
00:06:40 --> 00:06:41
Why is that?
 

118
00:06:41 --> 00:06:44
Well look.
 

119
00:06:44 --> 00:06:47
Millions of years of evolution
did not provide you with a

120
00:06:47 --> 00:06:52
mechanism that said, I am
now going to poke my eye.

121
00:06:52 --> 00:06:56
Please send a copy of that
signal to the visual centers

122
00:06:56 --> 00:06:57
of the brain, saying
to cancel that out.

123
00:06:57 --> 00:06:59
There's no cancellation
signal here.

124
00:06:59 --> 00:07:02
And so, you see the
image moving around.

125
00:07:02 --> 00:07:06
You don't see the image moving
around when you move your eyes

126
00:07:06 --> 00:07:10
normally, because you're
compensating for it.

127
00:07:10 --> 00:07:13
All right, so you're collecting
all this information.

128
00:07:13 --> 00:07:17
You're doing your best
to register it in a

129
00:07:17 --> 00:07:19
stable kind of a way.
 

130
00:07:19 --> 00:07:19
Oh look at that.
 

131
00:07:19 --> 00:07:21
It also says I'm going to
demonstrate the vestibular

132
00:07:21 --> 00:07:22
ocular reflex.
 

133
00:07:22 --> 00:07:25
 
 

134
00:07:25 --> 00:07:29
Let me do that, just because
you might as well get to use

135
00:07:29 --> 00:07:32
your fingers some more.
 

136
00:07:32 --> 00:07:35
You also want the world not
to jump around too much when

137
00:07:35 --> 00:07:36
you're moving your head.
 

138
00:07:36 --> 00:07:41
One of the things you do very
reflexively is, if you rotate

139
00:07:41 --> 00:07:43
you take your head one way,
your eyes counter

140
00:07:43 --> 00:07:44
rotate the other.
 

141
00:07:44 --> 00:07:47
That's a very quick reflex.
 

142
00:07:47 --> 00:07:49
If you want to see how
quick it is, try this.

143
00:07:49 --> 00:07:51
Hold your finger out
in front of you.

144
00:07:51 --> 00:07:55
Look at your finger, and move
your head back and forth, and

145
00:07:55 --> 00:07:58
just keep your eyes
on the finger.

146
00:07:58 --> 00:08:01
No problem, right?
 

147
00:08:01 --> 00:08:02
You know you can do that.
 

148
00:08:02 --> 00:08:05
Now, at the same speed, move
your fingers, and try to keep

149
00:08:05 --> 00:08:07
your eye on the finger.
 

150
00:08:07 --> 00:08:10
Can't do it.
 

151
00:08:10 --> 00:08:16
It doesn't work, because that
tracking movement -- there

152
00:08:16 --> 00:08:17
are more neurons involved.
 

153
00:08:17 --> 00:08:20
It's a slower process.
 

154
00:08:20 --> 00:08:25
The vestibular ocular reflect
is a very short latency kind

155
00:08:25 --> 00:08:29
of a reflex designed to keep
the input relatively stable.

156
00:08:29 --> 00:08:32
So, you take all that lovely
input in, you got all these

157
00:08:32 --> 00:08:35
little bits information all
over the place, and then you've

158
00:08:35 --> 00:08:39
gotta make your best guess
about what it is that

159
00:08:39 --> 00:08:40
you're looking at.
 

160
00:08:40 --> 00:08:42
The reason that this is an
interesting picture, it's in

161
00:08:42 --> 00:08:45
the book, by the way, so when
you can't see here, you can go

162
00:08:45 --> 00:08:48
and study it in the book, until
you can see the dalmatian

163
00:08:48 --> 00:08:50
dog that really is there.
 

164
00:08:50 --> 00:08:52
How many people can see it now?
 

165
00:08:52 --> 00:08:55
Oh, we got most of them.
 

166
00:08:55 --> 00:08:59
His head is right above my
finger, front paws, back paws.

167
00:08:59 --> 00:09:05
He's on a road sloping from
lower left to upper right.

168
00:09:05 --> 00:09:11
The point of a picture like
this, is that it's slows down

169
00:09:11 --> 00:09:14
the process of inference
enough, that you can sort

170
00:09:14 --> 00:09:15
of feel it happened.
 

171
00:09:15 --> 00:09:20
Normally, when I look out at
you, for instance, my visual

172
00:09:20 --> 00:09:24
system kicks up one, and only
one, interpretation of what I'm

173
00:09:24 --> 00:09:27
looking at, so rapidly, that
I never notice all the

174
00:09:27 --> 00:09:29
work that's involved.
 

175
00:09:29 --> 00:09:32
The purpose of this picture,
and really the purpose of this

176
00:09:32 --> 00:09:36
lecture, is to show some of
the work that's involved.

177
00:09:36 --> 00:09:41
So what you've got here is
a bunch of isolated little

178
00:09:41 --> 00:09:44
black and white regions.
 

179
00:09:44 --> 00:09:47
In order to figure out what's
going on here, you've got to

180
00:09:47 --> 00:09:50
decide who goes together.
 

181
00:09:50 --> 00:09:51
How are we going to decide
who goes together?

182
00:09:51 --> 00:09:53
Well lets think of this in
the context of a bunch

183
00:09:53 --> 00:09:55
of little line segments.
 

184
00:09:55 --> 00:09:58
 
 

185
00:09:58 --> 00:10:01
You can, sort of, decide that
some of these guys might go

186
00:10:01 --> 00:10:03
with some of the other ones
but what I'm going to do, is

187
00:10:03 --> 00:10:08
rotate all of them by 90
degrees, if I recall.

188
00:10:08 --> 00:10:09
No.
 

189
00:10:09 --> 00:10:10
Maybe some other orientation.
 

190
00:10:10 --> 00:10:15
But anyway, now they are the
same lines on the screen, but

191
00:10:15 --> 00:10:18
now some of them hang
together, right?

192
00:10:18 --> 00:10:22
Got that sort of potato
shape thing there.

193
00:10:22 --> 00:10:23
Why?
 

194
00:10:23 --> 00:10:26
What makes these guys go along
with each other now in a way

195
00:10:26 --> 00:10:27
that they didn't before?
 

196
00:10:27 --> 00:10:35
Well, if you're a little chunk
of brain, whose job it is to

197
00:10:35 --> 00:10:38
figure out where contours are
out in the world, what you're

198
00:10:38 --> 00:10:41
getting from earlier in the
visual system, is word that

199
00:10:41 --> 00:10:43
there's a little bit of contour
here, a little bit of a contour

200
00:10:43 --> 00:10:45
here, a little bit
of a contour here.

201
00:10:45 --> 00:10:47
I wonder if those
should go together?

202
00:10:47 --> 00:10:49
Now how do we decide that
the little bits of contour

203
00:10:49 --> 00:10:51
might go together?
 

204
00:10:51 --> 00:10:53
Well, you might do
something like this.

205
00:10:53 --> 00:11:00
 
 

206
00:11:00 --> 00:11:06
If you've got a line here, and
you're asking what's the best

207
00:11:06 --> 00:11:15
bet about if this is really a
piece of a continuing contour?

208
00:11:15 --> 00:11:17
Where's this likely to go next?
 

209
00:11:17 --> 00:11:20
Well, it might make a hair pin
turn and go off that way.

210
00:11:20 --> 00:11:21
Doesn't seem really likely.
 

211
00:11:21 --> 00:11:27
More likely, it's going to go
off in something like the

212
00:11:27 --> 00:11:28
direction that it's pointing.
 

213
00:11:28 --> 00:11:33
And that turns out to be
the potato shaped ones.

214
00:11:33 --> 00:11:37
This is a very rule
governed behavior.

215
00:11:37 --> 00:11:42
If you've got a bunch a little
line segments, as long as the

216
00:11:42 --> 00:11:47
deviation here isn't more than
about, as I recall, 30 degrees

217
00:11:47 --> 00:11:51
or so, from co-linear, you're
willing to string those

218
00:11:51 --> 00:11:54
together pretty happily.
 

219
00:11:54 --> 00:11:57
If it starts to be more
than, that your unlikely

220
00:11:57 --> 00:11:59
to string them together.
 

221
00:11:59 --> 00:12:02
The beginning of an effort
to tie little pieces of

222
00:12:02 --> 00:12:08
information together
into larger structures.

223
00:12:08 --> 00:12:09
All right, what
do you see here?

224
00:12:09 --> 00:12:12
 
 

225
00:12:12 --> 00:12:13
Two lines crossing each other.
 

226
00:12:13 --> 00:12:17
A reasonable interpretation.
 

227
00:12:17 --> 00:12:19
Though, it's not the only
possible interpretation

228
00:12:19 --> 00:12:20
of this.
 

229
00:12:20 --> 00:12:22
I mean, it could be
something like this.

230
00:12:22 --> 00:12:27
Two birds kissing each other,
or something like that.

231
00:12:27 --> 00:12:40
Why do you see and, in fact,
this is something like that.

232
00:12:40 --> 00:12:43
And you see that as an x with
two lines crossing each other,

233
00:12:43 --> 00:12:58
but if I provide enough other
details here, it's a sea lion?

234
00:12:58 --> 00:12:59
I don't know what it is.
 

235
00:12:59 --> 00:13:02
 
 

236
00:13:02 --> 00:13:05
The point is, he's
still a lousy artist.

237
00:13:05 --> 00:13:09
It hasn't gotten any better.
 

238
00:13:09 --> 00:13:10
The point is more
or less the same.

239
00:13:10 --> 00:13:18
You've got this little process
that's worrying about that

240
00:13:18 --> 00:13:25
little piece of line segment,
gets to this junction, and is

241
00:13:25 --> 00:13:28
busy doing the three roads
diversion of yellow wood

242
00:13:28 --> 00:13:31
kind of thing, trying to
decide which way to go.

243
00:13:31 --> 00:13:36
And, it's guessing that all
else being equal, I should

244
00:13:36 --> 00:13:37
probably go with that.
 

245
00:13:37 --> 00:13:47
 
 

246
00:13:47 --> 00:13:50
It sometimes goes by the
name of good continuation.

247
00:13:50 --> 00:13:53
It's one of a variety of so-
called grouping rules that were

248
00:13:53 --> 00:13:58
developed first by the Gestalt
psychologists starting in the

249
00:13:58 --> 00:14:03
early part of the twentieth
century, and they're really

250
00:14:03 --> 00:14:10
rules for figuring out how bits
of the scene might

251
00:14:10 --> 00:14:12
hang together.
 

252
00:14:12 --> 00:14:17
You can see a similar sort
of process going on here.

253
00:14:17 --> 00:14:19
You can see that as three
isolated line segments,

254
00:14:19 --> 00:14:21
but you probably don't.
 

255
00:14:21 --> 00:14:26
You see that as a curvy
lines that occluded, right?

256
00:14:26 --> 00:14:28
If I suddenly reveal
this, you don't go, ooh.

257
00:14:28 --> 00:14:31
Amazing!
 

258
00:14:31 --> 00:14:33
Kind of what I
thought was there.

259
00:14:33 --> 00:14:37
That's also highly
rule governed.

260
00:14:37 --> 00:14:40
 
 

261
00:14:40 --> 00:14:50
If you've got a line segment
and you've got another line

262
00:14:50 --> 00:14:54
segment, you're perfectly
happy to see this one and

263
00:14:54 --> 00:14:56
this one is connected.
 

264
00:14:56 --> 00:14:59
 
 

265
00:14:59 --> 00:15:00
Well how about this.
 

266
00:15:00 --> 00:15:01
This will do.
 

267
00:15:01 --> 00:15:04
If I put one up here.
 

268
00:15:04 --> 00:15:06
All the more if I erase
the stuff in between.

269
00:15:06 --> 00:15:11
 
 

270
00:15:11 --> 00:15:13
That, you're less likely
to see as connected.

271
00:15:13 --> 00:15:16
Now why are you less likely
to see that as connected?

272
00:15:16 --> 00:15:22
The rule turns out to be, that
if I can connect two lines

273
00:15:22 --> 00:15:29
with a smooth curve,
I'm in business.

274
00:15:29 --> 00:15:30
I'll be willing to see
those as connected.

275
00:15:30 --> 00:15:33
But, if I have to put an
inflection in it to make

276
00:15:33 --> 00:15:36
it work, then it doesn't
look as convincing.

277
00:15:36 --> 00:15:39
I mean it's not that I deny the
possibility this could ever

278
00:15:39 --> 00:15:48
connect with that, but if I
give people a bunch of stimuli

279
00:15:48 --> 00:15:54
like this, and ask good
connection or a bad connection?

280
00:15:54 --> 00:15:57
The good connections are the
ones that can be done can

281
00:15:57 --> 00:15:58
with a single smooth curve.
 

282
00:15:58 --> 00:15:59
That can't quite.
 

283
00:15:59 --> 00:16:01
I think you probably have
to get an inflection

284
00:16:01 --> 00:16:02
in there somewhere.
 

285
00:16:02 --> 00:16:07
And if you have to inflict
the curve, it fails.

286
00:16:07 --> 00:16:13
People report it as looking
less convincingly continuous.

287
00:16:13 --> 00:16:16
All right, it's
voting times here.

288
00:16:16 --> 00:16:19
Here we've got a whole bunch
isolated guys, but they do

289
00:16:19 --> 00:16:21
seem to have something
to do with each other.

290
00:16:21 --> 00:16:26
If you had to pick, this being
organized by columns or rows,

291
00:16:26 --> 00:16:28
how many vote for columns?
 

292
00:16:28 --> 00:16:31
How many vote for rows?
 

293
00:16:31 --> 00:16:33
Ain't much to chose, right?
 

294
00:16:33 --> 00:16:38
But, if I do this, OK, how
many vote for columns?

295
00:16:38 --> 00:16:40
How many vote for rows?
 

296
00:16:40 --> 00:16:42
So, we've now skewed it
very heavily in the

297
00:16:42 --> 00:16:44
direction of columns.
 

298
00:16:44 --> 00:16:46
And, all that I've
done is change the

299
00:16:46 --> 00:16:49
proximity of elements.
 

300
00:16:49 --> 00:16:55
Once the distance from one item
to the next item in a vertical

301
00:16:55 --> 00:16:59
direction is closer than to the
items in the horizontal

302
00:16:59 --> 00:17:05
direction, it's another one
just these gestalt grouping

303
00:17:05 --> 00:17:09
rules of proximity takes over,
and says, well, all else being

304
00:17:09 --> 00:17:12
equal, if I had to guess who
goes with who, the guys that

305
00:17:12 --> 00:17:13
are close to each other.
 

306
00:17:13 --> 00:17:17
They probably go
with each other.

307
00:17:17 --> 00:17:21
Multiple rules operate at the
same time, so I'll keep the

308
00:17:21 --> 00:17:24
proximity rule working here.
 

309
00:17:24 --> 00:17:28
Now, if you have to vote,
how many vote for columns?

310
00:17:28 --> 00:17:30
How many votes for rows?
 

311
00:17:30 --> 00:17:34
So now, I've skewed it very
heavily in the direction of

312
00:17:34 --> 00:17:39
rows, even though the
proximity rule, is still

313
00:17:39 --> 00:17:40
going for columns.
 

314
00:17:40 --> 00:17:43
Those things are closer to
each other in a vertical

315
00:17:43 --> 00:17:44
direction that horizontal.
 

316
00:17:44 --> 00:17:48
In this case, the similarity
is trumping that.

317
00:17:48 --> 00:17:51
You can balance these
off against each other.

318
00:17:51 --> 00:17:54
How similar to does it need to
be to compensate for a two to

319
00:17:54 --> 00:17:59
one difference in distance for
instance or something like

320
00:17:59 --> 00:18:01
that, but the important
point here is, that what

321
00:18:01 --> 00:18:04
you're trying to do.
 

322
00:18:04 --> 00:18:07
These are, you know,
demonstration versions of

323
00:18:07 --> 00:18:10
presumably what you're
doing all the time.

324
00:18:10 --> 00:18:20
I'm looking out there, and I'm
seeing regions of redness.

325
00:18:20 --> 00:18:24
I see sort of disconnected
regions of redness, but

326
00:18:24 --> 00:18:26
I'm guessing, they're
all part of her top.

327
00:18:26 --> 00:18:32
It's your top there.
 

328
00:18:32 --> 00:18:33
It's you.
 

329
00:18:33 --> 00:18:34
No, not her.
 

330
00:18:34 --> 00:18:36
That's pink, you're wearing.
 

331
00:18:36 --> 00:18:37
The woman behind you.
 

332
00:18:37 --> 00:18:40
Well, the woman next
to you is also red.

333
00:18:40 --> 00:18:49
But, in this case, you can see
the role of proximity here.

334
00:18:49 --> 00:18:51
So, the similarity thing is
telling me, all those red

335
00:18:51 --> 00:18:53
things are tied together,
and I'm making it into sort

336
00:18:53 --> 00:18:55
of one piece of clothing.
 

337
00:18:55 --> 00:19:00
The proximity thing is saying,
well I don't think her red top,

338
00:19:00 --> 00:19:03
and her red top are
the same red top.

339
00:19:03 --> 00:19:05
That would be a very
weird assumption.

340
00:19:05 --> 00:19:07
On the other hand, the guy
she's sitting next to

341
00:19:07 --> 00:19:09
is also wearing red.
 

342
00:19:09 --> 00:19:12
So maybe they're just
wearing one garment.

343
00:19:12 --> 00:19:13
No.
 

344
00:19:13 --> 00:19:15
I'm probably not going to come
up with that assumption either.

345
00:19:15 --> 00:19:21
But what my visual system is
doing, is continuously trying

346
00:19:21 --> 00:19:29
to cut the world up into
meaningful chunks, that

347
00:19:29 --> 00:19:33
are going to be worth
subsequent analysis.

348
00:19:33 --> 00:19:35
I don't want to go off
and analyze every little

349
00:19:35 --> 00:19:37
pixel in this scene.
 

350
00:19:37 --> 00:19:40
I don't have the brain
power to do that.

351
00:19:40 --> 00:19:43
I want to have meaningful
chunks that are worth

352
00:19:43 --> 00:19:46
analyzing, so here I might
decide to the meaningful

353
00:19:46 --> 00:19:48
chunks were the rows.
 

354
00:19:48 --> 00:19:49
They are a something or other.
 

355
00:19:49 --> 00:19:52
 
 

356
00:19:52 --> 00:19:53
OK.
 

357
00:19:53 --> 00:19:57
 
 

358
00:19:57 --> 00:20:00
There's a couple of reasons why
you need to do this grouping

359
00:20:00 --> 00:20:06
business over little
elements in the world.

360
00:20:06 --> 00:20:09
One of them-- I was sort of
cartooning over here-- which is

361
00:20:09 --> 00:20:12
that early in the visual
system, the chunks of the brain

362
00:20:12 --> 00:20:15
that are looking at it bits of
the world are only looking at

363
00:20:15 --> 00:20:16
very teeny bits, and you're
going to have to tie

364
00:20:16 --> 00:20:17
those together.
 

365
00:20:17 --> 00:20:22
The other reason, is that
out in the world, contours

366
00:20:22 --> 00:20:24
don't behave well.
 

367
00:20:24 --> 00:20:27
They tend to do awkward things
like disappear on you.

368
00:20:27 --> 00:20:39
And you don't want to get the
idea that these are-- if I've

369
00:20:39 --> 00:20:43
got a contour like that, if for
some reason bits of it are

370
00:20:43 --> 00:20:47
deleted, maybe because there's
an occluder, or maybe because

371
00:20:47 --> 00:20:50
something just bad happened in
the image, you don't want to

372
00:20:50 --> 00:20:53
lose this whole structure,
because bits of it

373
00:20:53 --> 00:20:55
have been deleted.
 

374
00:20:55 --> 00:21:00
So you have a lot clever
mechanisms designed to help you

375
00:21:00 --> 00:21:07
find where the edges are of
things out there in the scene.

376
00:21:07 --> 00:21:10
And, putting the bits
of edges together into

377
00:21:10 --> 00:21:11
a long coherent one.
 

378
00:21:11 --> 00:21:13
If you play with Photoshop,
you can go and find

379
00:21:13 --> 00:21:14
the edges I think.
 

380
00:21:14 --> 00:21:17
Isn't there like a find
edges filter, or something?

381
00:21:17 --> 00:21:19
So, try that sometime.
 

382
00:21:19 --> 00:21:28
Do find edges on an image of,
say, a person, and you'll see

383
00:21:28 --> 00:21:30
that it comes up with a lot of
edges that you recognize as

384
00:21:30 --> 00:21:33
being related to this
person, but its fragmented

385
00:21:33 --> 00:21:34
all over the place.
 

386
00:21:34 --> 00:21:38
And, in fact, getting your
computer to figure out which

387
00:21:38 --> 00:21:43
bits go with each other is
a tricky piece of work.

388
00:21:43 --> 00:21:44
It's tricky for you too.
 

389
00:21:44 --> 00:21:46
You just don't know that
it's tricky, because,

390
00:21:46 --> 00:21:47
it works all the time.
 

391
00:21:47 --> 00:21:47
All right.
 

392
00:21:47 --> 00:21:52
So, this beautifully boring
stimulus is there, because

393
00:21:52 --> 00:21:59
you see this beautiful
vertical contour, right?

394
00:21:59 --> 00:22:02
Now, all I'm going to do, I'm
going to leave it there, but

395
00:22:02 --> 00:22:05
I'm going to put a new
background on it.

396
00:22:05 --> 00:22:07
Isn't that lovely?
 

397
00:22:07 --> 00:22:08
Why is that interesting?
 

398
00:22:08 --> 00:22:12
Well, among the reasons it
is interesting, is because

399
00:22:12 --> 00:22:15
it's the same gray stuff
that was there before.

400
00:22:15 --> 00:22:17
So, the top of that bar,
and the bottom of that

401
00:22:17 --> 00:22:20
bar are still identical.
 

402
00:22:20 --> 00:22:24
All four of those little
rectangles are all identical.

403
00:22:24 --> 00:22:27
Even though they no
longer look identical.

404
00:22:27 --> 00:22:31
Because what the system
is doing, is-- actually

405
00:22:31 --> 00:22:31
can I do this?
 

406
00:22:31 --> 00:22:33
I forget what I programmed.
 

407
00:22:33 --> 00:22:35
Oh, there we go.
 

408
00:22:35 --> 00:22:38
little key's going to
turn into that one.

409
00:22:38 --> 00:22:40
Isn't that fun?
 

410
00:22:40 --> 00:22:41
Get back there.
 

411
00:22:41 --> 00:22:43
There we go.
 

412
00:22:43 --> 00:22:47
-- but my real point -- so what
that is, is a simultaneous

413
00:22:47 --> 00:22:49
contrast affect, by the way.
 

414
00:22:49 --> 00:22:53
This square looks bright,
because it's surrounded

415
00:22:53 --> 00:22:55
by darker stuff.
 

416
00:22:55 --> 00:22:57
That square looks dark,
because it's surrounded

417
00:22:57 --> 00:22:58
by brighter stuff.
 

418
00:22:58 --> 00:23:04
And even though this bar is
continuous with it's gray level

419
00:23:04 --> 00:23:08
from bottom to top, it picks up
it's apparent brightness

420
00:23:08 --> 00:23:13
from the immediately
surrounding contours.

421
00:23:13 --> 00:23:17
The interesting aspect of this
from the point of view of

422
00:23:17 --> 00:23:25
understanding were edges are,
is that if the bar is brighter

423
00:23:25 --> 00:23:29
than the background here, and
darker than the background up

424
00:23:29 --> 00:23:33
there, there must be a place in
the middle where it's gone.

425
00:23:33 --> 00:23:35
Where there is no contour.
 

426
00:23:35 --> 00:23:36
But you don't see that.
 

427
00:23:36 --> 00:23:39
 
 

428
00:23:39 --> 00:23:45
I was too busy making this
little thing move, and

429
00:23:45 --> 00:23:46
I forgot to do that.
 

430
00:23:46 --> 00:23:47
Oh well.
 

431
00:23:47 --> 00:23:50
It's not a small region.
 

432
00:23:50 --> 00:23:53
There's a fairly sizable region
of that middle there where

433
00:23:53 --> 00:23:56
there is no physical contour.
 

434
00:23:56 --> 00:23:58
But you fill it in.
 

435
00:23:58 --> 00:24:05
You know, in some fashion,
that contour is there.

436
00:24:05 --> 00:24:07
That's called a subjective
contour, where you're

437
00:24:07 --> 00:24:11
completing a contour that
doesn't have any real

438
00:24:11 --> 00:24:12
support in the image.
 

439
00:24:12 --> 00:24:15
That's the piece that your
Photoshop filter will have a

440
00:24:15 --> 00:24:17
hard time doing, by the way.
 

441
00:24:17 --> 00:24:24
Oh, we can also do a second
order affect here, that's

442
00:24:24 --> 00:24:26
kind of -- well, no,
we'll do that later.

443
00:24:26 --> 00:24:27
Oh.
 

444
00:24:27 --> 00:24:27
Come on.
 

445
00:24:27 --> 00:24:28
Go away.
 

446
00:24:28 --> 00:24:31
You've moved often enough.
 

447
00:24:31 --> 00:24:32
OK.
 

448
00:24:32 --> 00:24:34
Let's continue the
same point here.

449
00:24:34 --> 00:24:37
All right, everybody sees
this rectangle, right?

450
00:24:37 --> 00:24:43
And what are those
black things?

451
00:24:43 --> 00:24:46
 
 

452
00:24:46 --> 00:24:47
Three quarter circles.
 

453
00:24:47 --> 00:24:47
Yes.
 

454
00:24:47 --> 00:24:50
The literalists figured out
that these are Pacmen or three

455
00:24:50 --> 00:24:52
quarter circles, or something
like that, but you

456
00:24:52 --> 00:24:54
didn't really see
that when it came up.

457
00:24:54 --> 00:24:58
You said, oh that's a rectangle
sitting on top of four circles

458
00:24:58 --> 00:25:01
of some variety, and, in fact,
you still see it as a rectangle

459
00:25:01 --> 00:25:04
sitting on top of four circles.
 

460
00:25:04 --> 00:25:08
In fact, you're probably
reasonably convinced that

461
00:25:08 --> 00:25:09
you can see the contour.
 

462
00:25:09 --> 00:25:12
 
 

463
00:25:12 --> 00:25:13
That's weird.
 

464
00:25:13 --> 00:25:15
I'm reasonably convinced
that there's an interesting

465
00:25:15 --> 00:25:19
artifact on the screen
that's creating contours.

466
00:25:19 --> 00:25:20
I don't know what that's about.
 

467
00:25:20 --> 00:25:21
It looks better over there.
 

468
00:25:21 --> 00:25:24
It looks less bogus over there.
 

469
00:25:24 --> 00:25:26
You can probably see
the whole rectangle.

470
00:25:26 --> 00:25:31
The white on white
borders are not there.

471
00:25:31 --> 00:25:34
There's simply is no
physical contour there.

472
00:25:34 --> 00:25:36
There may be here, because
the project's doing

473
00:25:36 --> 00:25:38
something mutant.
 

474
00:25:38 --> 00:25:41
But, there's certainly no
contour there, even though

475
00:25:41 --> 00:25:44
the rectangle at the center
looks somewhat brighter.

476
00:25:44 --> 00:25:49
Again, what you're doing is,
making a guess about what is it

477
00:25:49 --> 00:25:52
that actually created that
image that's landing

478
00:25:52 --> 00:25:53
on my retina now?
 

479
00:25:53 --> 00:25:57
It could be a little
conference of pac-people.

480
00:25:57 --> 00:26:00
Four little three quarter
circles that got together

481
00:26:00 --> 00:26:02
to talk to each other.
 

482
00:26:02 --> 00:26:05
But that doesn't seem the
most likely possibility.

483
00:26:05 --> 00:26:10
What seems more likely here, is
that it's a white rectangle

484
00:26:10 --> 00:26:16
sitting on top of
four black circles.

485
00:26:16 --> 00:26:21
And you end up completing
that contour.

486
00:26:21 --> 00:26:23
Or this circle for that matter.
 

487
00:26:23 --> 00:26:26
What are you doing here?
 

488
00:26:26 --> 00:26:33
You don't need to have fancy
computer graphics to do this.

489
00:26:33 --> 00:26:37
One of the advantages of the
material in this particular

490
00:26:37 --> 00:26:44
lecture is that it provides
great material for doodling

491
00:26:44 --> 00:26:47
in other lectures.
 

492
00:26:47 --> 00:26:49
So, if you like subjective
contours, you can

493
00:26:49 --> 00:26:50
make your own.
 

494
00:26:50 --> 00:26:53
 
 

495
00:26:53 --> 00:27:00
And, so how's that look?
 

496
00:27:00 --> 00:27:04
Got a subjective contour there?
 

497
00:27:04 --> 00:27:05
It's not perfectly circular.
 

498
00:27:05 --> 00:27:07
That's OK.
 

499
00:27:07 --> 00:27:11
But what seems to happen is
that you generate a hypothesis

500
00:27:11 --> 00:27:15
that says, the whole problem I
got here, is contours don't

501
00:27:15 --> 00:27:17
just end in the world,
they tend to continue.

502
00:27:17 --> 00:27:22
Well, if this guy's continuing,
well, what happened here?

503
00:27:22 --> 00:27:22
Well.
 

504
00:27:22 --> 00:27:26
Maybe it ran into another
edge, it's being hidden.

505
00:27:26 --> 00:27:29
All else being equal, it
probably ran into an edge

506
00:27:29 --> 00:27:31
that's orthogonal to the
direction it's going,

507
00:27:31 --> 00:27:33
let's guess that.
 

508
00:27:33 --> 00:27:36
And, if we guess a whole bunch
of little orthogonal edges,

509
00:27:36 --> 00:27:39
we're back to that earlier
demonstration with a bunch

510
00:27:39 --> 00:27:41
of little line segments.
 

511
00:27:41 --> 00:27:43
I can tie those little
bits together.

512
00:27:43 --> 00:27:46
They make a kind of
a circle thing.

513
00:27:46 --> 00:27:50
So I end up seeing that
imaginary circle.

514
00:27:50 --> 00:27:53
And, in fact, I'm going to
start filling in the contour

515
00:27:53 --> 00:27:55
all the way around.
 

516
00:27:55 --> 00:27:58
That suggests, by the way, that
if I was to tilt all these

517
00:27:58 --> 00:28:04
lines a little bit off of
straight radial, so that the

518
00:28:04 --> 00:28:09
virtual line segment was not
forming a nice, neat circle,

519
00:28:09 --> 00:28:12
that the impression of a
subjective circle

520
00:28:12 --> 00:28:13
would get weaker.
 

521
00:28:13 --> 00:28:15
And you can decide whether
or not that's true here.

522
00:28:15 --> 00:28:18
So, see this circle?
 

523
00:28:18 --> 00:28:25
Now, the question is, we'll
give you three choices here.

524
00:28:25 --> 00:28:27
Stronger, weaker or
about the same?

525
00:28:27 --> 00:28:31
How vote that this one is
stronger than the previous one?

526
00:28:31 --> 00:28:34
How many vote for
just about the same?

527
00:28:34 --> 00:28:36
How many vote for weaker?
 

528
00:28:36 --> 00:28:38
Just about the same.
 

529
00:28:38 --> 00:28:39
That's a boring demo.
 

530
00:28:39 --> 00:28:43
I'll have to change
that next year.

531
00:28:43 --> 00:28:44
Because, it didn't
work well enough.

532
00:28:44 --> 00:28:47
 
 

533
00:28:47 --> 00:28:50
OK.
 

534
00:28:50 --> 00:28:53
Another example.
 

535
00:28:53 --> 00:28:55
If your following along on
the notes, we have now

536
00:28:55 --> 00:28:59
gotten to the so-called
Craik-O'Brien-Cornsweet

537
00:28:59 --> 00:29:05
illusion, named after Craik,
O'Brien and Cornsweet. suites.

538
00:29:05 --> 00:29:14
We're continuing the edge
business, but, now, what I want

539
00:29:14 --> 00:29:18
to do is tie that into that
topic that I mentioned early in

540
00:29:18 --> 00:29:24
the lecture, about how it is
that, what you're interested in

541
00:29:24 --> 00:29:26
is things the surface
properties in the world, and

542
00:29:26 --> 00:29:28
you're not interested
in lighting.

543
00:29:28 --> 00:29:31
Lighting is very boring.
 

544
00:29:31 --> 00:29:33
So, what do you see here?
 

545
00:29:33 --> 00:29:36
 
 

546
00:29:36 --> 00:29:45
Some bold soul described this
complicated image, boy,

547
00:29:45 --> 00:29:50
slow group, gray thing.
 

548
00:29:50 --> 00:29:51
Two gray things.
 

549
00:29:51 --> 00:29:56
 
 

550
00:29:56 --> 00:29:57
One is darker.
 

551
00:29:57 --> 00:30:01
Oh boy, I'm going to bring
my pliers next time.

552
00:30:01 --> 00:30:02
This is like pulling teeth.
 

553
00:30:02 --> 00:30:03
Yes.
 

554
00:30:03 --> 00:30:09
AUDIENCE: It's like the top
of the pyramid, and there's

555
00:30:09 --> 00:30:11
shading on the other side.
 

556
00:30:11 --> 00:30:12
PROFESSOR: Oh.
 

557
00:30:12 --> 00:30:13
Yeah.
 

558
00:30:13 --> 00:30:15
Ok, so the light's coming
from the left, or something.

559
00:30:15 --> 00:30:16
OK.
 

560
00:30:16 --> 00:30:16
Yeah.
 

561
00:30:16 --> 00:30:20
That's a complicated
inference about these.

562
00:30:20 --> 00:30:20
OK.
 

563
00:30:20 --> 00:30:25
But what you don't
particularly see, is this.

564
00:30:25 --> 00:30:30
If I take the edge out, if I
take that edge away from the

565
00:30:30 --> 00:30:32
middle here, what you
discover is the whole

566
00:30:32 --> 00:30:35
thing is the same gray.
 

567
00:30:35 --> 00:30:40
That's not obvious.
 

568
00:30:40 --> 00:30:43
If you were to draw the
luminance profile, drag a photo

569
00:30:43 --> 00:30:47
detector across this thing,
what you would get

570
00:30:47 --> 00:30:49
is something like.
 

571
00:30:49 --> 00:30:49
Which side is bright?
 

572
00:30:49 --> 00:30:50
OK.
 

573
00:30:50 --> 00:30:50
That side bright.
 

574
00:30:50 --> 00:30:55
So it rises, then drops
across the edge, and then

575
00:30:55 --> 00:30:58
rises back like that.
 

576
00:30:58 --> 00:31:02
So, that if you take out the
actual edge, it's equal

577
00:31:02 --> 00:31:05
on the two sides.
 

578
00:31:05 --> 00:31:07
So, that's kind of weird.
 

579
00:31:07 --> 00:31:14
Why does it look like, now,
they didn't believe me.

580
00:31:14 --> 00:31:17
They thought I was doing
something evil here.

581
00:31:17 --> 00:31:23
 
 

582
00:31:23 --> 00:31:23
Here we go.
 

583
00:31:23 --> 00:31:23
Look.
 

584
00:31:23 --> 00:31:26
That's why I had this thing
slowly sneak up, so you

585
00:31:26 --> 00:31:32
could, well, semi slowly,
fast-ly sneak up.

586
00:31:32 --> 00:31:37
What's going on here is that
the visual system knows

587
00:31:37 --> 00:31:40
something about edges
and about lighting.

588
00:31:40 --> 00:31:46
Edges in the world tend
to be fairly abrupt.

589
00:31:46 --> 00:31:50
Lightning changes, so it's
bright here, and dimmer over

590
00:31:50 --> 00:31:55
here, lighting changes
tend to be fairly gradual.

591
00:31:55 --> 00:31:59
So, if what I'm interested in
is seeing what's on the

592
00:31:59 --> 00:32:08
surface, as opposed to seeing
the product of boring light

593
00:32:08 --> 00:32:09
and shade variations.
 

594
00:32:09 --> 00:32:18
What I might want to do is to
look for abrupt changes, and

595
00:32:18 --> 00:32:21
to, in effect, suppress
gradual changes.

596
00:32:21 --> 00:32:24
And that's what's
going on here.

597
00:32:24 --> 00:32:27
 
 

598
00:32:27 --> 00:32:30
We might as well do an
entertaining second

599
00:32:30 --> 00:32:32
order effect here.
 

600
00:32:32 --> 00:32:35
 
 

601
00:32:35 --> 00:32:38
Remember that negative
after image thing?

602
00:32:38 --> 00:32:42
You know, look at red, you see
green, and stuff like that?

603
00:32:42 --> 00:32:44
What I should be able to do
here, is produce a negative

604
00:32:44 --> 00:32:50
version of this affect, where
you'll end up seeing this side

605
00:32:50 --> 00:32:54
is dark and this side is light,
even though I'm not going to

606
00:32:54 --> 00:32:57
change anything over
here, over here.

607
00:32:57 --> 00:33:02
Stare at that center
line, right.

608
00:33:02 --> 00:33:05
Stare rigorously at the center
line, and keep staring.

609
00:33:05 --> 00:33:13
And then when I do that,
so, this is two illusions

610
00:33:13 --> 00:33:15
concentrates on top of that.
 

611
00:33:15 --> 00:33:18
Do it again, was that?
 

612
00:33:18 --> 00:33:18
All right.
 

613
00:33:18 --> 00:33:25
For the people incapable of
following instructions the

614
00:33:25 --> 00:33:27
first time, try it again.
 

615
00:33:27 --> 00:33:35
So stare at the center and
hold your fixation there.

616
00:33:35 --> 00:33:37
Actually, people who got it the
first time, keep staring at the

617
00:33:37 --> 00:33:39
center, but the people who got
it the first time, can try

618
00:33:39 --> 00:33:41
something tricky-er which is
move your fixation, a little

619
00:33:41 --> 00:33:44
bit, and you'll change the
proportion that looks

620
00:33:44 --> 00:33:45
light or dark.
 

621
00:33:45 --> 00:33:49
 
 

622
00:33:49 --> 00:33:51
Isn't that fun?
 

623
00:33:51 --> 00:33:54
You should understand
why that works.

624
00:33:54 --> 00:33:57
If you don't, write yourself
a little note on your paper

625
00:33:57 --> 00:34:02
saying I don't understand why
that works and figure it out.

626
00:34:02 --> 00:34:02
All right.
 

627
00:34:02 --> 00:34:05
So the important thing is that
what you're trying to do here,

628
00:34:05 --> 00:34:09
is you're trying to get rid of
information about the light.

629
00:34:09 --> 00:34:11
You don't care about
the light levels.

630
00:34:11 --> 00:34:13
What you care about is what's
going on in the world.

631
00:34:13 --> 00:34:17
Now Ted Adelson in the brain
and cogs department here has

632
00:34:17 --> 00:34:22
exploited this fact brilliantly
in a variety of gorgeous demos.

633
00:34:22 --> 00:34:24
One of which is this.
 

634
00:34:24 --> 00:34:29
It is extremely difficult, even
if you've seen this before, to

635
00:34:29 --> 00:34:32
convince yourself that the gray
levels of A and B

636
00:34:32 --> 00:34:34
are identical.
 

637
00:34:34 --> 00:34:35
Which they are.
 

638
00:34:35 --> 00:34:39
In fact, it's so difficult that
even if I stick a bar across it

639
00:34:39 --> 00:34:43
that's clearly the same thing,
it's still kind of hard to

640
00:34:43 --> 00:34:45
see that as identical.
 

641
00:34:45 --> 00:34:47
Your brain wants to do
all sorts of things to

642
00:34:47 --> 00:34:51
deny that possibility.
 

643
00:34:51 --> 00:34:52
What's going on here?
 

644
00:34:52 --> 00:34:57
What's going on is that virtual
cylinder is casting a virtual

645
00:34:57 --> 00:35:02
shadow, that you are
busy discounting.

646
00:35:02 --> 00:35:05
You're saying, I don't
care about the shadow.

647
00:35:05 --> 00:35:07
There's a checker board.
 

648
00:35:07 --> 00:35:08
I know about checker boards.
 

649
00:35:08 --> 00:35:11
Checker boards to go dark
square, light square.

650
00:35:11 --> 00:35:12
I can figure this out.
 

651
00:35:12 --> 00:35:15
That means B is a light square,
because it's surrounded by dark

652
00:35:15 --> 00:35:18
squares, and A is dark square
because it's surrounded

653
00:35:18 --> 00:35:20
by light squares.
 

654
00:35:20 --> 00:35:26
And, it's a particularly lovely
example of, among other things,

655
00:35:26 --> 00:35:31
this ability to get rid of
information about

656
00:35:31 --> 00:35:32
the illumination.
 

657
00:35:32 --> 00:35:35
This isn't to say that what you
do is somehow just run some

658
00:35:35 --> 00:35:37
sort of, throw away the light
source information, and

659
00:35:37 --> 00:35:41
don't do anything with it.
 

660
00:35:41 --> 00:35:42
Get back there.
 

661
00:35:42 --> 00:35:45
 
 

662
00:35:45 --> 00:35:46
What's it say?
 

663
00:35:46 --> 00:35:47
Cow.
 

664
00:35:47 --> 00:35:49
 
 

665
00:35:49 --> 00:35:53
There's only white and
black on that screen.

666
00:35:53 --> 00:35:59
Look at the C and ask where
that contour is coming from.

667
00:35:59 --> 00:36:03
That contour, particularly
the outside of that C,

668
00:36:03 --> 00:36:06
has extremely little
support in the image.

669
00:36:06 --> 00:36:09
What you are doing is
making an inference.

670
00:36:09 --> 00:36:12
This time, you're using
the shadow information.

671
00:36:12 --> 00:36:16
You don't want to
see the shadow.

672
00:36:16 --> 00:36:19
How many people see it not only
as cow, also as an embossed

673
00:36:19 --> 00:36:22
word cow, sticking
out a little bit?

674
00:36:22 --> 00:36:24
The reason you're seeing
the cow at all, is you're

675
00:36:24 --> 00:36:27
assuming that those black
things are shadows.

676
00:36:27 --> 00:36:30
If those black things are
shadows, it follows the light

677
00:36:30 --> 00:36:31
is coming from the upper left.
 

678
00:36:31 --> 00:36:36
If the light is coming from the
upper left, well, we can go and

679
00:36:36 --> 00:36:42
figure out what shape object
must have been producing

680
00:36:42 --> 00:36:46
those shadows, and the
answer spells cow.

681
00:36:46 --> 00:36:49
But, if you were to take
a look at the C, is

682
00:36:49 --> 00:36:50
about the clearest.
 

683
00:36:50 --> 00:36:52
Well, I don't know, the O
is pretty good too, and so

684
00:36:52 --> 00:36:53
the W, for that matter.
 

685
00:36:53 --> 00:36:57
Anyway, looking any of those
guys, and look at the shapes of

686
00:36:57 --> 00:37:00
the black bits, which is the
only thing that stands out from

687
00:37:00 --> 00:37:01
the background, of course.
 

688
00:37:01 --> 00:37:02
There's nothing else there,
except the black bits

689
00:37:02 --> 00:37:04
on a white background.
 

690
00:37:04 --> 00:37:09
None of those bits
say C or O or W.

691
00:37:09 --> 00:37:12
It's a construction based
on the assumption that the

692
00:37:12 --> 00:37:14
black bits are shadows.
 

693
00:37:14 --> 00:37:20
You use that sort of
information all the time to

694
00:37:20 --> 00:37:22
do things like see faces.
 

695
00:37:22 --> 00:37:25
These are so called Mooney
faces, named after

696
00:37:25 --> 00:37:26
a guy named Mooney.
 

697
00:37:26 --> 00:37:30
 
 

698
00:37:30 --> 00:37:36
You can tell me a lot about the
curvature of these faces, even

699
00:37:36 --> 00:37:40
though, again, there's nothing
on the screen except for black

700
00:37:40 --> 00:37:42
regions and white regions.
 

701
00:37:42 --> 00:37:47
None of which are, themselves,
particularly faced shape.

702
00:37:47 --> 00:37:49
I mean look at the eye.
 

703
00:37:49 --> 00:37:51
Are any of those eyes actually
shaped -- look at the guy on

704
00:37:51 --> 00:37:56
the right-- I mean his eye,
he's only got one apparently,

705
00:37:56 --> 00:38:00
that eye looks like, I don't
know, a mutant bunny

706
00:38:00 --> 00:38:03
or something.
 

707
00:38:03 --> 00:38:10
If I just presented the eye
piece on the on the guy on the

708
00:38:10 --> 00:38:14
right, that black glob that's
defining his eye, if I just

709
00:38:14 --> 00:38:17
presented that in isolation,
nobody would say,

710
00:38:17 --> 00:38:18
oh yeah, sure.
 

711
00:38:18 --> 00:38:20
That's an eye.
 

712
00:38:20 --> 00:38:23
You'd all be saying dalmatian
dog before, mutant bunny

713
00:38:23 --> 00:38:25
this time, or something.
 

714
00:38:25 --> 00:38:32
And, it relies on these
assumptions about shadow, is

715
00:38:32 --> 00:38:33
what is what you're doing here.
 

716
00:38:33 --> 00:38:36
 
 

717
00:38:36 --> 00:38:40
It survives inversion
reasonably well.

718
00:38:40 --> 00:38:46
But, those don't look
like faces very much.

719
00:38:46 --> 00:38:50
What went wrong?
 

720
00:38:50 --> 00:38:53
You might think, I know that
shadows aren't red and blue.

721
00:38:53 --> 00:38:55
But that's actually
not the problem.

722
00:38:55 --> 00:38:57
Here they look
pretty good, right.

723
00:38:57 --> 00:39:02
Those faces look ok, but
these faces look lousy.

724
00:39:02 --> 00:39:04
Why do they look lousy?
 

725
00:39:04 --> 00:39:07
 
 

726
00:39:07 --> 00:39:08
Somebody raise a hand,
or something, yeah

727
00:39:08 --> 00:39:09
there's a theory.
 

728
00:39:09 --> 00:39:11
AUDIENCE: The shadows are
lighte than the [INAUDIBLE]

729
00:39:11 --> 00:39:14
PROFESSOR: Yes, if you make the
shadow regions lighter than

730
00:39:14 --> 00:39:19
the lit regions, the brain
says, that's not a shadow.

731
00:39:19 --> 00:39:20
I don't care about shadows.
 

732
00:39:20 --> 00:39:24
I don't want to see shadows as
entities in their own right

733
00:39:24 --> 00:39:25
particularly, most of the time.
 

734
00:39:25 --> 00:39:27
But I'll tell you one thing
I know about shadows.

735
00:39:27 --> 00:39:30
Shadows are darker
than the other stuff.

736
00:39:30 --> 00:39:31
If the shadows are lighter
than the other stuff,

737
00:39:31 --> 00:39:33
it's not shadow.
 

738
00:39:33 --> 00:39:37
It's something else,
and something weird.

739
00:39:37 --> 00:39:38
So this doesn't work.
 

740
00:39:38 --> 00:39:39
But that works.
 

741
00:39:39 --> 00:39:41
Oh, I suppose the fact that
it said on this one, shadow

742
00:39:41 --> 00:39:44
must be darker, might have
tipped some people off.

743
00:39:44 --> 00:39:47
 
 

744
00:39:47 --> 00:39:48
OK.
 

745
00:39:48 --> 00:39:50
Let's see here.
 

746
00:39:50 --> 00:39:54
I think what I will do -- this
makes it very natural break

747
00:39:54 --> 00:39:57
point -- where it says Mooney
face and we'll go on to this

748
00:39:57 --> 00:40:00
question about making the best
guess you can make in the

749
00:40:00 --> 00:40:03
context of going from 2D to 3D.
 

750
00:40:03 --> 00:40:08
D But before we go on to
that, let's us lets take our

751
00:40:08 --> 00:40:11
brief, stretch your limbs
kind of break, here.

752
00:40:11 --> 00:42:03
 
 

753
00:42:03 --> 00:42:03
OK.
 

754
00:42:03 --> 00:42:07
Let us gather back
together here.

755
00:42:07 --> 00:42:11
 
 

756
00:42:11 --> 00:42:14
By the way, it looks like it's
getting to be that time in the

757
00:42:14 --> 00:42:24
term, where people are abusing
their natural sleep mechanism

758
00:42:24 --> 00:42:27
that I will talk about
later on in the term.

759
00:42:27 --> 00:42:32
But looking around at this
crowd, I would say that

760
00:42:32 --> 00:42:39
you're not getting the
seven to eight you need.

761
00:42:39 --> 00:42:47
Or, if you are, maybe you
really need ten and you're

762
00:42:47 --> 00:42:49
catching the extra to 2 here.
 

763
00:42:49 --> 00:42:57
 
 

764
00:42:57 --> 00:43:01
I've been talking about sort of
little, almost like atomic

765
00:43:01 --> 00:43:05
small scale examples, of these
sort of inferences

766
00:43:05 --> 00:43:06
that you make.
 

767
00:43:06 --> 00:43:10
And, now what I want to do, is
sort of head for the larger

768
00:43:10 --> 00:43:12
picture of how you make
an inference about

769
00:43:12 --> 00:43:14
the whole scene.
 

770
00:43:14 --> 00:43:19
I'm not going to get all the
way there, and there are many

771
00:43:19 --> 00:43:21
realm I could talk
about this in.

772
00:43:21 --> 00:43:24
So I'm going to talk about it
in one restricted area, which

773
00:43:24 --> 00:43:29
is this question of going from
a 2D image to 3D inferences

774
00:43:29 --> 00:43:30
about the world.
 

775
00:43:30 --> 00:43:35
Something that you do
automatically, all the time.

776
00:43:35 --> 00:43:38
I want to explain a bit
about how you do it.

777
00:43:38 --> 00:43:42
You will see, on your hand out,
this is very useless, blank

778
00:43:42 --> 00:43:45
region, that says one
two, three, four, five.

779
00:43:45 --> 00:43:48
I'm going to go through
a series of depth cues.

780
00:43:48 --> 00:43:50
Probably more than
five of them.

781
00:43:50 --> 00:43:52
And, that's what's
supposed to go in there.

782
00:43:52 --> 00:43:56
They're all lots of different
sources of information that you

783
00:43:56 --> 00:44:03
use to go from the 2D world to
the 3D world, many of them seen

784
00:44:03 --> 00:44:06
here in this lovely piece of
renaissance art that we

785
00:44:06 --> 00:44:07
will come up back to.
 

786
00:44:07 --> 00:44:11
But, here's a much more
boring piece of art.

787
00:44:11 --> 00:44:12
What do you see?
 

788
00:44:12 --> 00:44:15
 
 

789
00:44:15 --> 00:44:16
Circle, square and a diamond.
 

790
00:44:16 --> 00:44:19
And, then there's the clever
person, who's trying to figure

791
00:44:19 --> 00:44:21
out, I've describe them
as pac-man before,

792
00:44:21 --> 00:44:22
but these aren't.
 

793
00:44:22 --> 00:44:25
But, you see a circle,
square, and a diamond.

794
00:44:25 --> 00:44:27
You don't have any serious
difficulty inferring

795
00:44:27 --> 00:44:30
that -- triangle, sorry.
 

796
00:44:30 --> 00:44:33
You don't see this.
 

797
00:44:33 --> 00:44:37
For present purposes, the
important point here is,

798
00:44:37 --> 00:44:41
you can also tell me
their depth order.

799
00:44:41 --> 00:44:44
It's in some sense so obvious
that you never think about it,

800
00:44:44 --> 00:44:49
but it's a very important
source of information about

801
00:44:49 --> 00:44:54
depth order, that you get
simply because you know that

802
00:44:54 --> 00:44:55
solid object occlude
each other.

803
00:44:55 --> 00:45:02
So you firmly believe that I am
standing in front the screen.

804
00:45:02 --> 00:45:05
I could had, well no, I
couldn't have, it'd be

805
00:45:05 --> 00:45:09
theoretically possible that I
had suddenly cut a cunningly

806
00:45:09 --> 00:45:15
wolf shaped hole in the screen
and I'm A, very large, and

807
00:45:15 --> 00:45:20
B, standing over towards
east campus somewhere.

808
00:45:20 --> 00:45:23
And, you're looking at me
through this set of holes.

809
00:45:23 --> 00:45:27
No.
 

810
00:45:27 --> 00:45:32
You automatically leap to the
assumption that if A looks

811
00:45:32 --> 00:45:34
like it's occluding
B, A in front of B.

812
00:45:34 --> 00:45:37
 
 

813
00:45:37 --> 00:45:38
Ah, my bunnies.
 

814
00:45:38 --> 00:45:39
I don't know what
happened to them.

815
00:45:39 --> 00:45:40
They got kind of pixellated.
 

816
00:45:40 --> 00:45:50
But, all right, which
bunnies are closer to you?

817
00:45:50 --> 00:45:53
The big bunnies are
closer to you.

818
00:45:53 --> 00:45:54
Right.
 

819
00:45:54 --> 00:45:56
Why do you think the big
bunnies are closer to you?

820
00:45:56 --> 00:45:58
Because, you're making an
inference that bunnies

821
00:45:58 --> 00:46:01
are roughly bunny sized.
 

822
00:46:01 --> 00:46:08
And, in the same way, I'm
currently making the assumption

823
00:46:08 --> 00:46:11
that you guys are all more
or less people sized.

824
00:46:11 --> 00:46:14
If I did not make that
assumption, I would come to

825
00:46:14 --> 00:46:17
some very odd inferences
about the current view

826
00:46:17 --> 00:46:19
that I'm looking at.
 

827
00:46:19 --> 00:46:22
So, the people in the front row
-- there's a person in the

828
00:46:22 --> 00:46:26
front row -- her head is about
two degrees of visual angle.

829
00:46:26 --> 00:46:31
Remember 360 degrees around my
head each degree it's about my

830
00:46:31 --> 00:46:34
thumb, so her head takes
up about two degrees.

831
00:46:34 --> 00:46:38
And, let's see, there's this
guy in the cheap seats back

832
00:46:38 --> 00:46:44
there, his head is only
about half a degree.

833
00:46:44 --> 00:46:48
I could make the assumption
that he's a pinhead.

834
00:46:48 --> 00:46:49
A guy with a real small
had sitting out the

835
00:46:49 --> 00:46:51
deep back out there.
 

836
00:46:51 --> 00:46:53
Well actually, I wouldn't
make the assumption that he

837
00:46:53 --> 00:46:54
was sitting in the back.
 

838
00:46:54 --> 00:46:57
He's a pin head sitting at the
same distance as large head

839
00:46:57 --> 00:47:00
woman here in the front.
 

840
00:47:00 --> 00:47:02
But that's dumb.
 

841
00:47:02 --> 00:47:02
Right?
 

842
00:47:02 --> 00:47:04
Your visual system
knows that's dumb.

843
00:47:04 --> 00:47:06
Your visual system knows people
are roughly people's sized.

844
00:47:06 --> 00:47:09
Not exactly people sized, but
roughly people sized, and if I

845
00:47:09 --> 00:47:12
see a bunch of small things
there and a bunch of big things

846
00:47:12 --> 00:47:15
here, odds are that this
is closer than that.

847
00:47:15 --> 00:47:18
And that's part of what's
giving me my current inference

848
00:47:18 --> 00:47:22
that I'm looking at a tilted
plane of people in purple seats

849
00:47:22 --> 00:47:25
is this information about size.
 

850
00:47:25 --> 00:47:28
If I organize the bunnies the
way you guys are organized, I

851
00:47:28 --> 00:47:31
got a much clearer sensation
of depth from this

852
00:47:31 --> 00:47:32
texture radiant.
 

853
00:47:32 --> 00:47:36
So, now you should be able
to see sort of tilted

854
00:47:36 --> 00:47:38
rabbit plane, right?
 

855
00:47:38 --> 00:47:40
Even though you know
objectively, it's just

856
00:47:40 --> 00:47:42
sitting flat on the
screen, it looks tilted.

857
00:47:42 --> 00:47:43
Is that a hand up there?
 

858
00:47:43 --> 00:47:44
That was a hand.
 

859
00:47:44 --> 00:47:54
 
 

860
00:47:54 --> 00:47:56
Sorry I missed that.
 

861
00:47:56 --> 00:47:57
My previous image.
 

862
00:47:57 --> 00:48:00
We can do that.
 

863
00:48:00 --> 00:48:07
AUDIENCE: [INAUDIBLE]
 

864
00:48:07 --> 00:48:07
PROFESSOR: Yeah.
 

865
00:48:07 --> 00:48:09
That's another possibility.
 

866
00:48:09 --> 00:48:13
It could would be that.
 

867
00:48:13 --> 00:48:18
And, in fact, it is,
just a flat image.

868
00:48:18 --> 00:48:20
And you actually sound like
you're getting a sort of a

869
00:48:20 --> 00:48:24
hybrid of a tilted plane with
bunnies of a range of sizes.

870
00:48:24 --> 00:48:28
Actually, we can see
that combination here.

871
00:48:28 --> 00:48:33
So, we got a whole bunch
of big bunnies, and

872
00:48:33 --> 00:48:36
two little bunnies.
 

873
00:48:36 --> 00:48:41
Which is the smallest
bunny in this display?

874
00:48:41 --> 00:48:43
The bottom right bunny, right?
 

875
00:48:43 --> 00:48:47
These guys are
identical in size.

876
00:48:47 --> 00:48:48
It's a very minimal display.
 

877
00:48:48 --> 00:48:50
There's a much more vivid
version of this illusion

878
00:48:50 --> 00:48:53
in the book as I recall.
 

879
00:48:53 --> 00:48:58
It's a very minimal
version of the illusion.

880
00:48:58 --> 00:49:09
Because you assume, if
you are -- All right.

881
00:49:09 --> 00:49:15
So I still can't draw. -- If
I'm looking at two bunnies, if

882
00:49:15 --> 00:49:22
I'm looking at a ground plane,
closer is also lower

883
00:49:22 --> 00:49:26
in the visual field.
 

884
00:49:26 --> 00:49:28
So, you make an automatic
assumption that was what was

885
00:49:28 --> 00:49:32
giving this woman that notion
of a fairly tilted plane in

886
00:49:32 --> 00:49:34
the first bunny example.
 

887
00:49:34 --> 00:49:38
You make the assumption that
the bottom of the image is

888
00:49:38 --> 00:49:40
closer to you then the
top of the image.

889
00:49:40 --> 00:49:45
Well, if the bottom of the
image -- let's go a back here--

890
00:49:45 --> 00:49:52
if the bottom of the image is
closer then the top, and these

891
00:49:52 --> 00:49:55
two bunny images are the same
size, if this guy's closer,

892
00:49:55 --> 00:49:57
it must be really small.
 

893
00:49:57 --> 00:50:00
 
 

894
00:50:00 --> 00:50:03
Suppose I took the guy
from the back row here.

895
00:50:03 --> 00:50:08
His whole upper body fills sort
of the top joint of my thumb

896
00:50:08 --> 00:50:09
in visual angle terms.
 

897
00:50:09 --> 00:50:13
If I moved that image to the
front, and sat him in a seat

898
00:50:13 --> 00:50:18
here, he would be about the
size of this woman's

899
00:50:18 --> 00:50:20
upper arms.
 

900
00:50:20 --> 00:50:23
And, I would think, that's
a really small guy.

901
00:50:23 --> 00:50:25
 
 

902
00:50:25 --> 00:50:29
She could wear him on her
--instead of wearing your heart

903
00:50:29 --> 00:50:35
on your sleeve, you could wear
the whole guy on your sleeve.

904
00:50:35 --> 00:50:36
So, that's what's
going on here.

905
00:50:36 --> 00:50:40
You're making the assumption
that small bunny one is

906
00:50:40 --> 00:50:41
closer than small bunny two.
 

907
00:50:41 --> 00:50:43
They're the same image size.
 

908
00:50:43 --> 00:50:46
You therefore infer that out
in the world, this must be a

909
00:50:46 --> 00:50:49
really small bunny, and that
one is just a reasonably

910
00:50:49 --> 00:50:50
small bunny.
 

911
00:50:50 --> 00:50:54
Now the bunny part turns out
to be not that critical.

912
00:50:54 --> 00:51:00
Here you can also see a nice
plane going off into the

913
00:51:00 --> 00:51:04
distance with objects that
are clearly not meaningful.

914
00:51:04 --> 00:51:04
Right.
 

915
00:51:04 --> 00:51:06
Big stuff, front and low.
 

916
00:51:06 --> 00:51:13
Small stuff high in the image
and, you simply infer this is

917
00:51:13 --> 00:51:16
close, and stuff up there is
far, and you get a nice

918
00:51:16 --> 00:51:19
impression of a tilted plane.
 

919
00:51:19 --> 00:51:21
 
 

920
00:51:21 --> 00:51:28
This is one of the cues that
is interestingly variable

921
00:51:28 --> 00:51:29
depending on where you're from.
 

922
00:51:29 --> 00:51:33
 
 

923
00:51:33 --> 00:51:36
The atmosphere scatters light,
particularly water in the

924
00:51:36 --> 00:51:39
atmosphere scatters light,
that's why the sky is blue.

925
00:51:39 --> 00:51:42
The result is that objects
that are far away tend to

926
00:51:42 --> 00:51:44
be both hazier and bluer.
 

927
00:51:44 --> 00:51:49
Something that you can see in
all sorts of works of art.

928
00:51:49 --> 00:51:51
Go to the museum, you can see
artists take advantage of

929
00:51:51 --> 00:51:54
this right, left and center.
 

930
00:51:54 --> 00:51:58
And, you can probably get
even in my pathetically

931
00:51:58 --> 00:52:00
reduced version of it.
 

932
00:52:00 --> 00:52:02
You probably get a sensation
of depth here that

933
00:52:02 --> 00:52:04
you don't get here.
 

934
00:52:04 --> 00:52:09
The geographic aspect of it,
-- anybody here from Arizona?

935
00:52:09 --> 00:52:12
Doesn't work well in Arizona.
 

936
00:52:12 --> 00:52:16
I know this because I went to a
meeting in Tucson and it was

937
00:52:16 --> 00:52:19
boring, so I went out for a
walk, and I saw this hill, and

938
00:52:19 --> 00:52:22
I said to the guy who was at
the street corner with me, how

939
00:52:22 --> 00:52:25
long would it take me
to walk to that hill?

940
00:52:25 --> 00:52:29
And he said, three
days, four days?

941
00:52:29 --> 00:52:32
 
 

942
00:52:32 --> 00:52:37
It's like 50 miles away and
it's like 6000 feet high.

943
00:52:37 --> 00:52:41
But it was extremely crisp.
 

944
00:52:41 --> 00:52:44
And, there's no
water in Arizona.

945
00:52:44 --> 00:52:45
I don't know why
people live there.

946
00:52:45 --> 00:52:47
It's like hot all the time.
 

947
00:52:47 --> 00:52:48
And there's no water.
 

948
00:52:48 --> 00:52:54
But anyway, aerial perspective
cues don't work, so not only

949
00:52:54 --> 00:52:58
are you thirsty, put your
short one depth cue.

950
00:52:58 --> 00:53:01
Anyway, this works much better
in a humid setting that in a

951
00:53:01 --> 00:53:06
non humid setting, but the
point is that again you know

952
00:53:06 --> 00:53:09
about-- this is your visual
system making use of the

953
00:53:09 --> 00:53:12
physics of the situation in
order to infer something about

954
00:53:12 --> 00:53:13
the depth of the situation.
 

955
00:53:13 --> 00:53:16
 
 

956
00:53:16 --> 00:53:20
You also know about the
geometry of the world.

957
00:53:20 --> 00:53:25
So, this is an extremely
limited picture.

958
00:53:25 --> 00:53:30
If I say, this is a highway
going off to infinity somewhere

959
00:53:30 --> 00:53:34
in Arizona, or something like
that, that's not

960
00:53:34 --> 00:53:36
a great picture.
 

961
00:53:36 --> 00:53:37
But you can believe that.
 

962
00:53:37 --> 00:53:43
Because you know implicitly
that parallel lines in the

963
00:53:43 --> 00:53:47
world, if they're in depth,
will look like they are

964
00:53:47 --> 00:53:51
converging towards a
vanishing point somewhere.

965
00:53:51 --> 00:53:54
Now it is sometimes claimed
that this is known as linear

966
00:53:54 --> 00:53:59
perspective, that linear
perspective was discovered by

967
00:53:59 --> 00:54:02
artists during the renaissance.
 

968
00:54:02 --> 00:54:03
That's only sort of true.
 

969
00:54:03 --> 00:54:06
What happened in the
renaissance, was that they made

970
00:54:06 --> 00:54:10
this knowledge explicit, and
became able to use it for

971
00:54:10 --> 00:54:13
instance to make
their art works.

972
00:54:13 --> 00:54:17
But, your cat and your
lizard and stuff know

973
00:54:17 --> 00:54:19
about linear perspective.
 

974
00:54:19 --> 00:54:20
They just know it implicitly.
 

975
00:54:20 --> 00:54:26
The same way they knew about
arial perspective and size

976
00:54:26 --> 00:54:29
clues and occlusion cues,
and things like that.

977
00:54:29 --> 00:54:33
It wasn't that we woke up one
day in Renaissance, Italy

978
00:54:33 --> 00:54:36
and suddenly we could
use this depth cue.

979
00:54:36 --> 00:54:41
What we figured out was how to
paint with this depth cue.

980
00:54:41 --> 00:54:43
And you could do all
sorts of amusing things

981
00:54:43 --> 00:54:44
with the depth cue.
 

982
00:54:44 --> 00:54:46
So, for instance.
 

983
00:54:46 --> 00:54:49
Lines look more or less
the same size, right.

984
00:54:49 --> 00:54:54
 
 

985
00:54:54 --> 00:54:58
Let me see if I can
change that here.

986
00:54:58 --> 00:55:13
 
 

987
00:55:13 --> 00:55:14
All right.
 

988
00:55:14 --> 00:55:19
Even though we're using
crude materials, let's do

989
00:55:19 --> 00:55:23
a forced choice vote here.
 

990
00:55:23 --> 00:55:26
If you know they're at the same
size, so it's boring to ask

991
00:55:26 --> 00:55:27
if they're the same size.
 

992
00:55:27 --> 00:55:31
But if you had to vote bigger
or smaller, how many people

993
00:55:31 --> 00:55:36
would vote that the bottom
line now looks bigger?

994
00:55:36 --> 00:55:39
How many would vote that the
bottom line now look smaller.

995
00:55:39 --> 00:55:43
Well, I guess that worked,
cheap chalk and all.

996
00:55:43 --> 00:55:49
This is an illusion known
as the Ponzo illusion.

997
00:55:49 --> 00:55:53
There are a number of ways to
account for it, but one of the

998
00:55:53 --> 00:55:56
intuitively appealing ones, at
least, is to say, what this is

999
00:55:56 --> 00:56:00
doing, even though you're not
particularly seeing it as a

1000
00:56:00 --> 00:56:03
depth cue, is it's telling the
chunks of your brain that are

1001
00:56:03 --> 00:56:09
trying to figure out 3D, I see
these two converging lines.

1002
00:56:09 --> 00:56:10
If they're parallel lines
in the world, they must

1003
00:56:10 --> 00:56:12
be going off into depth.
 

1004
00:56:12 --> 00:56:14
If they're going off into
depth, then this thing is

1005
00:56:14 --> 00:56:17
further away than this one.
 

1006
00:56:17 --> 00:56:19
Well, if this one is further
away, and they are the

1007
00:56:19 --> 00:56:21
same size on my retina,
this must be bigger.

1008
00:56:21 --> 00:56:25
If that's not intuitively
obvious to you, think about

1009
00:56:25 --> 00:56:28
this as train tracks.
 

1010
00:56:28 --> 00:56:32
So here are train tracks going
off into the distance, and ask

1011
00:56:32 --> 00:56:40
yourself, which maiden here,
tied of to the tracks

1012
00:56:40 --> 00:56:43
is in more distress?
 

1013
00:56:43 --> 00:56:47
It's obvious that this must
be the bigger person if we

1014
00:56:47 --> 00:56:51
interpret this as train tracks
going off into the distance.

1015
00:56:51 --> 00:56:53
Even though we know that
they're essentially

1016
00:56:53 --> 00:56:55
the same size.
 

1017
00:56:55 --> 00:56:58
You're getting the results
of the inference are then

1018
00:56:58 --> 00:57:02
influencing what you see.
 

1019
00:57:02 --> 00:57:04
They are influencing other
inferences that you

1020
00:57:04 --> 00:57:08
make about the image.
 

1021
00:57:08 --> 00:57:13
Now you can exploit these rules
of linear perspective in

1022
00:57:13 --> 00:57:18
much more elaborate
fashion then that.

1023
00:57:18 --> 00:57:24
And the great master of
that game is Escher.

1024
00:57:24 --> 00:57:29
Here is one of
Escher's pictures.

1025
00:57:29 --> 00:57:33
What you want to do again, I
think the image looks rather

1026
00:57:33 --> 00:57:37
sharper up on these sides
guys, but ask yourself where

1027
00:57:37 --> 00:57:40
the vanishing point is.
 

1028
00:57:40 --> 00:57:44
So, look at the first
floor of that structure.

1029
00:57:44 --> 00:57:46
And, it's pretty clear that
those railings are converging

1030
00:57:46 --> 00:57:49
to a vanishing point off
to the right somewhere.

1031
00:57:49 --> 00:57:52
Well, now look at
the top floor.

1032
00:57:52 --> 00:57:54
That's converting to
vanishing point off to

1033
00:57:54 --> 00:57:57
the left somewhere.
 

1034
00:57:57 --> 00:58:00
And, then when you tried it put
the whole thing together,

1035
00:58:00 --> 00:58:03
you've got a structure
the doesn't quite hang

1036
00:58:03 --> 00:58:05
together quite right.
 

1037
00:58:05 --> 00:58:06
This tells you a
couple of things.

1038
00:58:06 --> 00:58:10
Thing one it tells you Escher
was a very clever draftsman.

1039
00:58:10 --> 00:58:13
Thing two that it tells you
is that you do a lot of

1040
00:58:13 --> 00:58:18
these calculations about
perspective very locally.

1041
00:58:18 --> 00:58:23
What you do is you say, your
attending, let's say, to the

1042
00:58:23 --> 00:58:26
lower floor there, and you
say, yeah, this all adds up.

1043
00:58:26 --> 00:58:28
It makes sense.
 

1044
00:58:28 --> 00:58:30
And, then you direct your
attention to the upper floor.

1045
00:58:30 --> 00:58:31
And it all adds up.
 

1046
00:58:31 --> 00:58:32
It makes sense.
 

1047
00:58:32 --> 00:58:35
It's only when you try to
combine all of that across

1048
00:58:35 --> 00:58:38
whole image, that you realize
that the whole image somehow

1049
00:58:38 --> 00:58:40
doesn't quite make sense.
 

1050
00:58:40 --> 00:58:43
And, that's how Escher can do
things like, have staircases

1051
00:58:43 --> 00:58:48
that always go up, and water
falls that fall apparently in

1052
00:58:48 --> 00:58:50
an infinite loop and
things like that.

1053
00:58:50 --> 00:58:53
Grab yourself your favorite
Escher website and or your

1054
00:58:53 --> 00:58:57
favorite Escher book, and you
can watch him manipulating

1055
00:58:57 --> 00:58:59
these depth cues endlessly.
 

1056
00:58:59 --> 00:59:01
It's great entertainment.
 

1057
00:59:01 --> 00:59:04
 
 

1058
00:59:04 --> 00:59:14
Now what I want to do is to
bring together the themes

1059
00:59:14 --> 00:59:19
of the lectures to this
point in single demo.

1060
00:59:19 --> 00:59:23
At the moment, that doesn't
look like much of nothing,

1061
00:59:23 --> 00:59:27
except that, well, I don't
know, what does it look like?

1062
00:59:27 --> 00:59:28
Cubes.
 

1063
00:59:28 --> 00:59:29
All right.
 

1064
00:59:29 --> 00:59:35
That's interesting, because
I don't see no cubes!

1065
00:59:35 --> 00:59:39
Where's that inference
coming from?

1066
00:59:39 --> 00:59:42
What you're really have is
a bunch of Y junctions.

1067
00:59:42 --> 00:59:44
And you know about Y junctions.
 

1068
00:59:44 --> 00:59:46
Those are probably corners.
 

1069
00:59:46 --> 00:59:49
And, they look like they might
be kind of cube-y corners.

1070
00:59:49 --> 00:59:51
But, what I'm going to do
is rotate each of those.

1071
00:59:51 --> 00:59:56
 
 

1072
00:59:56 --> 01:00:03
Now, this is cool stimulus
for a variety of reasons.

1073
01:00:03 --> 01:00:06
First of all, now you're
really seeing a cube.

1074
01:00:06 --> 01:00:07
Right?
 

1075
01:00:07 --> 01:00:08
No problem.
 

1076
01:00:08 --> 01:00:10
Second of all, there
are two cubes.

1077
01:00:10 --> 01:00:13
 
 

1078
01:00:13 --> 01:00:17
There's the cube, with
its face pointing.

1079
01:00:17 --> 01:00:18
Let's try this.
 

1080
01:00:18 --> 01:00:24
There's that face, pointing
down and to the right.

1081
01:00:24 --> 01:00:30
 
 

1082
01:00:30 --> 01:00:35
There's that face pointing
up and to the left.

1083
01:00:35 --> 01:00:36
So, you've got two cubes.
 

1084
01:00:36 --> 01:00:38
This is an ambigious
by stable figure.

1085
01:00:38 --> 01:00:40
It's known as a Necker cube.
 

1086
01:00:40 --> 01:00:43
 
 

1087
01:00:43 --> 01:00:46
We can quickly draw
one of those.

1088
01:00:46 --> 01:00:49
 
 

1089
01:00:49 --> 01:00:50
Endless fun for doodling again.
 

1090
01:00:50 --> 01:00:55
You can make yourself
ambiguous figures instantly.

1091
01:00:55 --> 01:00:58
That figure by itself is known
as the Necker cube after

1092
01:00:58 --> 01:00:59
a guy named Necker.
 

1093
01:00:59 --> 01:01:02
 
 

1094
01:01:02 --> 01:01:05
So, you're inferring two cubes.
 

1095
01:01:05 --> 01:01:08
You're inferring
one of two cubes.

1096
01:01:08 --> 01:01:11
The cube isn't
particularly there.

1097
01:01:11 --> 01:01:14
The black stuff -- all you're
seeing is the verticies of

1098
01:01:14 --> 01:01:20
this cube-- but you're still
managing to infer the rest of

1099
01:01:20 --> 01:01:24
the cube You can probably see
the lines of the cube in

1100
01:01:24 --> 01:01:27
the black region, right.
 

1101
01:01:27 --> 01:01:30
In fact, you can see the
intersection, if you straight

1102
01:01:30 --> 01:01:33
up from here, you can see the
intersection of two lines

1103
01:01:33 --> 01:01:34
that aren't there.
 

1104
01:01:34 --> 01:01:39
 
 

1105
01:01:39 --> 01:01:42
I can make those lines go away.
 

1106
01:01:42 --> 01:01:46
Now take a look at that cube,
and imagine that what you're

1107
01:01:46 --> 01:01:52
looking at is a cube -- sort
of a wire frame cube-- that's

1108
01:01:52 --> 01:01:56
behind a sheet of sort
of black swiss cheese.

1109
01:01:56 --> 01:01:57
You're looking at
it through holes.

1110
01:01:57 --> 01:02:00
Can you get it to
go back there?

1111
01:02:00 --> 01:02:03
If you get it to go back there,
you can hold it back there, you

1112
01:02:03 --> 01:02:06
probably noticed that the
subjective contours pretty

1113
01:02:06 --> 01:02:08
much disappear on you.
 

1114
01:02:08 --> 01:02:09
Why is that?
 

1115
01:02:09 --> 01:02:13
Well, if it's behind, there's
no reason that you should

1116
01:02:13 --> 01:02:14
be seeing those contour.
 

1117
01:02:14 --> 01:02:15
They would be invisible.
 

1118
01:02:15 --> 01:02:18
And so the invisible
contours become invisible.

1119
01:02:18 --> 01:02:21
Now if you bring the cube back
in front in your perception,

1120
01:02:21 --> 01:02:24
you'll see, oh yeah, now if
that cube's floating in front.

1121
01:02:24 --> 01:02:26
I ought to be able to
see the whole cube.

1122
01:02:26 --> 01:02:28
And now, I can see the
subjective contours.

1123
01:02:28 --> 01:02:34
So you can make the subjective
contours contingent on which

1124
01:02:34 --> 01:02:39
particular interpretation you
care to give to the image.

1125
01:02:39 --> 01:02:44
So, I think this illustrates
very nicely the notion that

1126
01:02:44 --> 01:02:51
what you are seeing is your
current hypothesis about what

1127
01:02:51 --> 01:02:55
might be generating the
image on the screen.

1128
01:02:55 --> 01:02:58
Another thing that it points
out, is that you are only

1129
01:02:58 --> 01:03:02
willing to entertain at
limited set of hypothesis.

1130
01:03:02 --> 01:03:07
It is extremely hard to
look at this and see it

1131
01:03:07 --> 01:03:08
as however many it is.
 

1132
01:03:08 --> 01:03:14
Eight little disks with
chicken feed in them.

1133
01:03:14 --> 01:03:16
With little Y's in
them of some sort.

1134
01:03:16 --> 01:03:18
It's very, very
hard to see that.

1135
01:03:18 --> 01:03:21
Even though that is perfectly
consistent with the

1136
01:03:21 --> 01:03:22
image hypothesis.
 

1137
01:03:22 --> 01:03:28
You are out there trying to
make a guess about the world.

1138
01:03:28 --> 01:03:31
And, you're not willing to
entertain all of them.

1139
01:03:31 --> 01:03:33
At least most of them are
immediately relegated to the

1140
01:03:33 --> 01:03:35
realm of the very unlikely.
 

1141
01:03:35 --> 01:03:38
 
 

1142
01:03:38 --> 01:03:43
And, normally out in the world,
what happens is that a single

1143
01:03:43 --> 01:03:47
hypothesis immediately pops to
the four, and you accept it.

1144
01:03:47 --> 01:03:50
In weird situations like
this, you can entertain

1145
01:03:50 --> 01:03:51
a few of them.
 

1146
01:03:51 --> 01:03:55
 
 

1147
01:03:55 --> 01:03:58
You immediately narrow down
the realm of possibilities to

1148
01:03:58 --> 01:04:01
a few, not to an infinity.
 

1149
01:04:01 --> 01:04:04
Even though there's an infinite
number of possible ways

1150
01:04:04 --> 01:04:07
to generate this thing.
 

1151
01:04:07 --> 01:04:10
Shadows I've already
shown are a depth cue.

1152
01:04:10 --> 01:04:14
The reason for putting this
nice piece of renaissance art

1153
01:04:14 --> 01:04:18
up there, is to point out that,
while shadows are a depth cue,

1154
01:04:18 --> 01:04:21
you're not actually terribly
picky about the physics

1155
01:04:21 --> 01:04:24
of the situation.
 

1156
01:04:24 --> 01:04:26
At least not the global
physics of the situation.

1157
01:04:26 --> 01:04:29
So where's the sun here?
 

1158
01:04:29 --> 01:04:30
This is an outdoor scene.
 

1159
01:04:30 --> 01:04:33
Where's the sun coming from?
 

1160
01:04:33 --> 01:04:36
Well, if you look at the people
in the lower left -- the people

1161
01:04:36 --> 01:04:39
on the ground plane there --
it's pretty clear that the sun

1162
01:04:39 --> 01:04:41
must be down and to
the left somewhere.

1163
01:04:41 --> 01:04:42
Right?
 

1164
01:04:42 --> 01:04:46
Well, look at the shadow
underneath that portico.

1165
01:04:46 --> 01:04:51
Well, the sun must be up and
to the right there somewhere.

1166
01:04:51 --> 01:04:56
There's no consistent source of
illumination in this image, but

1167
01:04:56 --> 01:05:00
your perfectly willing to use
the shadow information to

1168
01:05:00 --> 01:05:01
give you depth information.
 

1169
01:05:01 --> 01:05:04
It's giving it to you locally.
 

1170
01:05:04 --> 01:05:07
The fact that it doesn't add up
globally, doesn't bother you.

1171
01:05:07 --> 01:05:11
And it doesn't even bother you
to the extent that it bothers

1172
01:05:11 --> 01:05:14
you in the Escher picture
where the building was

1173
01:05:14 --> 01:05:15
actually impossible.
 

1174
01:05:15 --> 01:05:20
Here, you don't recognize the
impossibility at all, unless

1175
01:05:20 --> 01:05:23
it's pointed out
to you directly.

1176
01:05:23 --> 01:05:27
And, finally I should add to
the list, three more than are

1177
01:05:27 --> 01:05:29
rather hard to demonstrate.
 

1178
01:05:29 --> 01:05:35
That are hard to put up
just as Powerpoint slides.

1179
01:05:35 --> 01:05:38
When people think about depth
perception, if they think

1180
01:05:38 --> 01:05:41
about depth perception, it's
stereopsis binocular vision

1181
01:05:41 --> 01:05:44
that they typically think of.
 

1182
01:05:44 --> 01:05:46
Your two eyes are in two
different places in your

1183
01:05:46 --> 01:05:50
head for most of us.
 

1184
01:05:50 --> 01:05:53
If you blink from eye to eye,
or just cover your one eye

1185
01:05:53 --> 01:05:55
after the other, it's actually
better if you hold one finger

1186
01:05:55 --> 01:05:58
out in front of you, and
blink from eye to eye.

1187
01:05:58 --> 01:06:02
You'll see the image in the
two eyes is not the same.

1188
01:06:02 --> 01:06:04
The difference in those
two images is highly

1189
01:06:04 --> 01:06:06
geometrically regular.
 

1190
01:06:06 --> 01:06:09
And you make use of that
regularity as a depth cue.

1191
01:06:09 --> 01:06:11
It's called binocular
disparity.

1192
01:06:11 --> 01:06:12
Very useful depth cue.
 

1193
01:06:12 --> 01:06:17
It's what's giving you the
magic eye demos that you get.

1194
01:06:17 --> 01:06:19
Those posters that if you
cross your eyes just right,

1195
01:06:19 --> 01:06:21
they jump out in depth.
 

1196
01:06:21 --> 01:06:22
Those still around?
 

1197
01:06:22 --> 01:06:25
 
 

1198
01:06:25 --> 01:06:27
It's a very useful depth cue.
 

1199
01:06:27 --> 01:06:29
It's a little on the overrated
side, because it's a

1200
01:06:29 --> 01:06:31
lot of fun to study.
 

1201
01:06:31 --> 01:06:34
People who think that binocular
vision is the be all and end

1202
01:06:34 --> 01:06:40
all depth cue, should cover one
eye, and ask if the world

1203
01:06:40 --> 01:06:41
suddenly looks very flat.
 

1204
01:06:41 --> 01:06:44
It looks a little flatter, but
I can still perfectly well

1205
01:06:44 --> 01:06:46
tell who's in front of who.
 

1206
01:06:46 --> 01:06:49
On the other hand, if you want
to see what's stereo is doing

1207
01:06:49 --> 01:06:53
for you, on a beautiful day
like today, go outside, lie

1208
01:06:53 --> 01:06:58
under a tree, close one eye,
and look up into the branches,

1209
01:06:58 --> 01:06:59
and try to figure out which
twigs are in front

1210
01:06:59 --> 01:07:00
of other twigs.
 

1211
01:07:00 --> 01:07:02
You'll have very
hard time doing it.

1212
01:07:02 --> 01:07:05
Open the eye, and the
whole thing will jump

1213
01:07:05 --> 01:07:06
out at you in depth.
 

1214
01:07:06 --> 01:07:09
That's the sort of information
that stereo is giving you.

1215
01:07:09 --> 01:07:11
You don't have to
do it with stereo.

1216
01:07:11 --> 01:07:15
Motion parralax to jump to the
bottom of that list, is a

1217
01:07:15 --> 01:07:20
similar sort of geometric clue.
 

1218
01:07:20 --> 01:07:24
If I'm here, and then I'm
here, the image changes in a

1219
01:07:24 --> 01:07:26
geometrically regular way.
 

1220
01:07:26 --> 01:07:29
You guys are sliding around on
my retina in such a way that

1221
01:07:29 --> 01:07:32
these guys are moving more than
you guys out in the

1222
01:07:32 --> 01:07:34
back on my retina.
 

1223
01:07:34 --> 01:07:37
And, I know that, again
implicitly, like I know linear

1224
01:07:37 --> 01:07:40
praralax and I can use
that to inferred depth.

1225
01:07:40 --> 01:07:43
Try this under the tree, and
you can see the same thing.

1226
01:07:43 --> 01:07:46
Close one eye, look up
into the branches.

1227
01:07:46 --> 01:07:47
The branches look
relatively flat.

1228
01:07:47 --> 01:07:50
Now rather than opening this
eye, just move your head back

1229
01:07:50 --> 01:07:53
and forth, and the tree will
jump out at you in depth.

1230
01:07:53 --> 01:07:55
Actually quite striking, you
should try this sometime.

1231
01:07:55 --> 01:07:58
I don't know if anybody
ever does take me up

1232
01:07:58 --> 01:07:59
on this suggestion.
 

1233
01:07:59 --> 01:08:03
So if you actually try it, let
me know so that I know that

1234
01:08:03 --> 01:08:05
somebody actually tried it.
 

1235
01:08:05 --> 01:08:11
Oh and vergence is another one
of these geometrical cues.

1236
01:08:11 --> 01:08:12
Hold your finger out
in front of you.

1237
01:08:12 --> 01:08:14
Look at your finger.
 

1238
01:08:14 --> 01:08:18
Now move the finger towards
you, holding it as a single

1239
01:08:18 --> 01:08:20
finger as long as you can.
 

1240
01:08:20 --> 01:08:23
So I can watch you
go cross eyed.

1241
01:08:23 --> 01:08:27
What you are doing is
converging your eyes.

1242
01:08:27 --> 01:08:30
And, if you could move your
finger further out, you would

1243
01:08:30 --> 01:08:32
be diverging your eyes.
 

1244
01:08:32 --> 01:08:35
Well, what you can think of
that as doing, is taking

1245
01:08:35 --> 01:08:40
like a pair of sticks and
pointing them at the object.

1246
01:08:40 --> 01:08:44
It's your visual axis in
the sense, you're pointing

1247
01:08:44 --> 01:08:46
at the object there.
 

1248
01:08:46 --> 01:08:49
And the angle formed by those
sticks is narrower if you're

1249
01:08:49 --> 01:08:52
looking far away, then it is
if you're looking close up.

1250
01:08:52 --> 01:08:55
And you have a fairly
impoverished ability to use

1251
01:08:55 --> 01:08:57
that as a depth cue too.
 

1252
01:08:57 --> 01:09:01
If you were a chameleon you'd
be much better at this.

1253
01:09:01 --> 01:09:05
Chameleons have eyes the move
independently and are very

1254
01:09:05 --> 01:09:10
sensitive to the angle that
their eyes are pointing.

1255
01:09:10 --> 01:09:15
And, in fact, you've seen the
Animal Planet kind of videos

1256
01:09:15 --> 01:09:19
where the chameleon's tongue
goes out the length of its

1257
01:09:19 --> 01:09:21
body, and it grabs a fly
or something like that.

1258
01:09:21 --> 01:09:23
How does that it know
where the fly is?

1259
01:09:23 --> 01:09:26
It knows by measuring
the angle of its eyes.

1260
01:09:26 --> 01:09:27
How do we know that?
 

1261
01:09:27 --> 01:09:31
Well, we know that because,
somebody went and put

1262
01:09:31 --> 01:09:36
glasses on chameleon
that diverged the eye.

1263
01:09:36 --> 01:09:44
So, in order to point it's
eyes at the fly, it had the

1264
01:09:44 --> 01:09:46
angle wrong, basically.
 

1265
01:09:46 --> 01:09:49
So, you put a fly on a popsicle
stick you put the glasses on

1266
01:09:49 --> 01:09:51
the chameleon and then your
film the chameleon's tongue,

1267
01:09:51 --> 01:09:53
and the chameleon
keeps missing.

1268
01:09:53 --> 01:09:56
 
 

1269
01:09:56 --> 01:09:59
If I put those prisms on your
eyes, you will adapt, and you

1270
01:09:59 --> 01:10:02
will eventually be able
to catch the fly again.

1271
01:10:02 --> 01:10:05
Should you be so inclined.
 

1272
01:10:05 --> 01:10:08
The chameleon turns out to be a
less adaptable creature than

1273
01:10:08 --> 01:10:11
you, and will not adapt.
 

1274
01:10:11 --> 01:10:14
You can do the same
game with chickens.

1275
01:10:14 --> 01:10:15
Put a pair of prisms
 

1276
01:10:15 --> 01:10:17
on your eyes to divert
everything off say fifteen

1277
01:10:17 --> 01:10:21
degrees to the left, and then
if I tell you pick this up,

1278
01:10:21 --> 01:10:23
you'll reach fifteen degrees
in the wrong direction.

1279
01:10:23 --> 01:10:25
But eventually you'll learn.
 

1280
01:10:25 --> 01:10:27
Put the prisms on a chicken.
 

1281
01:10:27 --> 01:10:28
Put some grain down.
 

1282
01:10:28 --> 01:10:30
Here are the grains here.
 

1283
01:10:30 --> 01:10:31
The chicken sees it over there.
 

1284
01:10:31 --> 01:10:32
Chickens pecking over there.
 

1285
01:10:32 --> 01:10:35
Chickens not getting any grain.
 

1286
01:10:35 --> 01:10:39
Chicken will do that
forever and apparently not

1287
01:10:39 --> 01:10:44
learn to get it right.
 

1288
01:10:44 --> 01:10:45
Oh, good.
 

1289
01:10:45 --> 01:10:51
I left myself with enough time
to talk about inferences in

1290
01:10:51 --> 01:10:55
a more global sense of
combining information

1291
01:10:55 --> 01:10:56
from across the senses.
 

1292
01:10:56 --> 01:11:01
So, you've got this job to try
to figure out what's going on,

1293
01:11:01 --> 01:11:04
I've been talking about doing
that specifically with

1294
01:11:04 --> 01:11:06
the visual system.
 

1295
01:11:06 --> 01:11:11
But, you're collecting
information from multiple

1296
01:11:11 --> 01:11:15
sources, and taking whatever
the best information is, so if

1297
01:11:15 --> 01:11:20
I show you a movie, you'll get
captured by the visual

1298
01:11:20 --> 01:11:25
information, and you're
perfectly happy to hear the

1299
01:11:25 --> 01:11:29
words coming out of the mouth
of the guy on the screen, even

1300
01:11:29 --> 01:11:31
though it's coming out of
some speaker on the side.

1301
01:11:31 --> 01:11:34
And it doesn't matter if
they got fancy dolby stereo

1302
01:11:34 --> 01:11:35
or something like that.
 

1303
01:11:35 --> 01:11:41
Use a cheap, simple speaker
sitting off to the side and

1304
01:11:41 --> 01:11:43
you'll still hear it as coming
out of the guy's mouth

1305
01:11:43 --> 01:11:43
if you're watching it.
 

1306
01:11:43 --> 01:11:46
So you're combining information
from multiple senses.

1307
01:11:46 --> 01:11:51
The particular example I
thought I would discuss with

1308
01:11:51 --> 01:11:59
you, is the ever pleasant
example of motion sickness.

1309
01:11:59 --> 01:12:08
So, when I first came to
graduate school, my lab was

1310
01:12:08 --> 01:12:16
doing research for NASA on the
effects of looking at large

1311
01:12:16 --> 01:12:18
fields that were rotating.
 

1312
01:12:18 --> 01:12:20
Sort of Omni theater stuff.
 

1313
01:12:20 --> 01:12:23
If you look at a whole field
that's rotating counter clock

1314
01:12:23 --> 01:12:25
wise around your line of
sight, you feel like you're

1315
01:12:25 --> 01:12:27
rotating clock wise.
 

1316
01:12:27 --> 01:12:31
Meantime guys up at Brandeis
who we were collaborating with,

1317
01:12:31 --> 01:12:34
were doing the same sort of
thing, but their depended

1318
01:12:34 --> 01:12:37
measure was how long it
took you to throw up.

1319
01:12:37 --> 01:12:39
Fortunately, that was
not my introduction

1320
01:12:39 --> 01:12:42
to graduate school.
 

1321
01:12:42 --> 01:12:47
But, it will make you sick.
 

1322
01:12:47 --> 01:12:49
I might as well collect
some data here.

1323
01:12:49 --> 01:12:52
How many people have ever
been motion sick here?

1324
01:12:52 --> 01:12:53
OK.
 

1325
01:12:53 --> 01:12:56
What made you sick?
 

1326
01:12:56 --> 01:12:56
Motion.
 

1327
01:12:56 --> 01:12:56
Yes.
 

1328
01:12:56 --> 01:12:59
Thank you.
 

1329
01:12:59 --> 01:13:03
Could were get a little more
specific, while keeping within

1330
01:13:03 --> 01:13:06
the realm of good taste here?
 

1331
01:13:06 --> 01:13:11
 
 

1332
01:13:11 --> 01:13:12
Oh.
 

1333
01:13:12 --> 01:13:14
Reading on a bumpy bus.
 

1334
01:13:14 --> 01:13:15
That's one good example.
 

1335
01:13:15 --> 01:13:17
Anybody got another good one?
 

1336
01:13:17 --> 01:13:39
 
 

1337
01:13:39 --> 01:13:40
We'll take one more here.
 

1338
01:13:40 --> 01:13:43
 
 

1339
01:13:43 --> 01:13:45
Oh, spinning around
in a circle.

1340
01:13:45 --> 01:13:46
Did you get sick while you're
spinning around in circle,

1341
01:13:46 --> 01:13:49
or after you stopped?
 

1342
01:13:49 --> 01:13:50
After you stopped.
 

1343
01:13:50 --> 01:13:54
And, then when you stopped, you
felt like you were spinning

1344
01:13:54 --> 01:13:56
around in the other direction.
 

1345
01:13:56 --> 01:13:58
So when you're spinning
yourself around in the circle,

1346
01:13:58 --> 01:14:02
-- you have, in your inner
ears, these tubes filled with

1347
01:14:02 --> 01:14:11
fluid, -- and quick drawing
here, quick bit of vestibular

1348
01:14:11 --> 01:14:18
physiology --need a nice fat
piece of chalk -- oh, it's

1349
01:14:18 --> 01:14:26
yellow -- you've got these
tubes inside your ears that are

1350
01:14:26 --> 01:14:34
filled with a fluid, and inside
a little space in there,

1351
01:14:34 --> 01:14:37
are these hairs.
 

1352
01:14:37 --> 01:14:40
If you bend the hairs, it sends
a signal off to your brain,

1353
01:14:40 --> 01:14:42
that's the transducer to the
equivalent of the photo

1354
01:14:42 --> 01:14:43
receptors from the eyes.
 

1355
01:14:43 --> 01:14:50
 
 

1356
01:14:50 --> 01:14:53
You can imagine taking a bucket
and starting to move, the fluid

1357
01:14:53 --> 01:14:56
stays behind for a little
while, and sloshes around.

1358
01:14:56 --> 01:14:58
That's why its hard to carry
buckets of liquid around

1359
01:14:58 --> 01:14:59
if they are too full.
 

1360
01:14:59 --> 01:15:03
So if you rotate your head,
the fluid tends to stay put.

1361
01:15:03 --> 01:15:07
And, it moves over the hairs
bending them and telling you

1362
01:15:07 --> 01:15:08
that you move your head.
 

1363
01:15:08 --> 01:15:10
That's what's telling you
about this sort of motion.

1364
01:15:10 --> 01:15:14
Well, you spin around for
awhile, and the fluid in here

1365
01:15:14 --> 01:15:17
eventually catches up with
you and starts moving.

1366
01:15:17 --> 01:15:19
Then you stop.
 

1367
01:15:19 --> 01:15:24
And the fluid keeps going,
and you say, oh no.

1368
01:15:24 --> 01:15:27
The other way you can do this,
by the way-- so it's very

1369
01:15:27 --> 01:15:30
carefully calibrated system,
turns out that alcohol is

1370
01:15:30 --> 01:15:34
lighter then this fluid.
 

1371
01:15:34 --> 01:15:38
If you drink, the reason you
get dizzy, it's not because you

1372
01:15:38 --> 01:15:41
pickled your brain, which is
also true, but because you've

1373
01:15:41 --> 01:15:44
dilute it this fluid with
alcohol and this stuff

1374
01:15:44 --> 01:15:45
is uncalibrated.
 

1375
01:15:45 --> 01:15:48
And, now, you move your
head a little, and the

1376
01:15:48 --> 01:15:49
brain, says, oh, man.
 

1377
01:15:49 --> 01:15:51
We just moved a lot.
 

1378
01:15:51 --> 01:15:52
Don't move that head.
 

1379
01:15:52 --> 01:15:57
 
 

1380
01:15:57 --> 01:16:00
What she was pointing out, is
that what turns out to be the

1381
01:16:00 --> 01:16:06
great stimulus for making
yourself motion sick, is a

1382
01:16:06 --> 01:16:11
mismatch between the
information that in particular

1383
01:16:11 --> 01:16:15
this vesticular system, this
balance system, and your

1384
01:16:15 --> 01:16:16
eyes are giving you.
 

1385
01:16:16 --> 01:16:21
So the example given here
of reading on the bus

1386
01:16:21 --> 01:16:24
is a marvelous example.
 

1387
01:16:24 --> 01:16:28
Because what you're doing, in
order to read, you're holding

1388
01:16:28 --> 01:16:31
this thing so it's a relatively
stable visual stimulus.

1389
01:16:31 --> 01:16:34
Nothing's happening here.
 

1390
01:16:34 --> 01:16:36
But your vestibular system
is saying [?___bumbidingdi

1391
01:16:36 --> 01:16:39
bingdaeda--___?] lots
of stuff is happening.

1392
01:16:39 --> 01:16:43
And your digestive system is
busy saying [? Blech. ?]

1393
01:16:43 --> 01:16:46
Same thing happens
on a plane, right?

1394
01:16:46 --> 01:16:50
The problem on a plane is that
when the plane bounces up

1395
01:16:50 --> 01:16:53
and down, I mean short of
catastrophic bouncing up and

1396
01:16:53 --> 01:16:55
down, when a plane hits
turbulence and it's bouncing up

1397
01:16:55 --> 01:16:58
and down, what do you see?
 

1398
01:16:58 --> 01:17:00
Nothing.
 

1399
01:17:00 --> 01:17:01
Right.
 

1400
01:17:01 --> 01:17:02
There's nothing
happening visually.

1401
01:17:02 --> 01:17:03
What do you feel?
 

1402
01:17:03 --> 01:17:06
You feel [? bompitdumitbomp. ?]
 

1403
01:17:06 --> 01:17:09
and you know, [? bloop. ?]
 

1404
01:17:09 --> 01:17:14
And, the interesting question
is why'd you get sick?

1405
01:17:14 --> 01:17:18
It's fine to say that the
mismatch of visual and

1406
01:17:18 --> 01:17:22
vestibular information turns
out to be nauseating.

1407
01:17:22 --> 01:17:25
But, why should it
make you sick?

1408
01:17:25 --> 01:17:29
The answer is, it's
another inference.

1409
01:17:29 --> 01:17:31
So, the next question then
is what's the inference?

1410
01:17:31 --> 01:17:32
What are you guessing?
 

1411
01:17:32 --> 01:17:36
 
 

1412
01:17:36 --> 01:17:39
You're guessing that the
airline food was lousy.

1413
01:17:39 --> 01:17:40
Yeah.
 

1414
01:17:40 --> 01:17:42
Yeah, you're guessing
that you were poisoned.

1415
01:17:42 --> 01:17:44
Why you guessing that
you were poisoned?

1416
01:17:44 --> 01:17:47
 
 

1417
01:17:47 --> 01:17:50
It's the mismatch
that's critical.

1418
01:17:50 --> 01:17:56
If you just make your vision
strange, I could show you weird

1419
01:17:56 --> 01:18:01
stuff you hadn't seen before,
and you wouldn't throw up.

1420
01:18:01 --> 01:18:04
And, I can also bounce you
around, and until I do

1421
01:18:04 --> 01:18:07
fairly dramatic stuff,
you won't throw up.

1422
01:18:07 --> 01:18:11
But, if I mismatch what your
visual system is telling you

1423
01:18:11 --> 01:18:15
about your body, and what your
vestibular system is telling

1424
01:18:15 --> 01:18:19
you, what this is,is a
protection of sorts

1425
01:18:19 --> 01:18:21
against neuro-toxins.
 

1426
01:18:21 --> 01:18:23
How do poisons work?
 

1427
01:18:23 --> 01:18:27
Well some of them work by
attacking your nervous system.

1428
01:18:27 --> 01:18:28
Lot of work by attack
your nervous system.

1429
01:18:28 --> 01:18:33
How are you, the owner of the
nervous system, going to know,

1430
01:18:33 --> 01:18:37
you're going to think, I
can't integrate anymore.

1431
01:18:37 --> 01:18:40
 
 

1432
01:18:40 --> 01:18:43
I can no longer remember
that I love my mother.

1433
01:18:43 --> 01:18:46
The sorts of things that
immediately present themselves

1434
01:18:46 --> 01:18:50
to you as you are being
poisoned, are, your senses are

1435
01:18:50 --> 01:18:56
coming unglued from each other,
and so if your eyes are saying,

1436
01:18:56 --> 01:18:59
bong-de-bong bong, and your
vestibular system is saying

1437
01:18:59 --> 01:19:04
that your body infers that you
have been poisoned, and a

1438
01:19:04 --> 01:19:06
useful idea, if you've been
poisoned, is to get rid of

1439
01:19:06 --> 01:19:08
whatever you just ate.
 

1440
01:19:08 --> 01:19:13
So, why does this happen on
airplanes and stuff like that?

1441
01:19:13 --> 01:19:17
You weren't built to be flying
around at 30,000 feet bouncing

1442
01:19:17 --> 01:19:18
around in the clouds.
 

1443
01:19:18 --> 01:19:22
It just wasn't what's nature
set you up to do, and so you

1444
01:19:22 --> 01:19:24
get the unfortunate situation
where even though you haven't

1445
01:19:24 --> 01:19:27
been poisoned, you get sick.
 

1446
01:19:27 --> 01:19:28
OK.
 

1447
01:19:28 --> 01:19:29
Enough of that cheery topic.
 

1448
01:19:29 --> 01:19:32
 
 

