1
00:00:00 --> 00:00:01
 
 

2
00:00:01 --> 00:00:04
The following content is
provided by MIT OpenCourseWare

3
00:00:04 --> 00:00:07
under a Creative
Commons license.

4
00:00:07 --> 00:00:10
Additional information about
our license, and MIT

5
00:00:10 --> 00:00:15
OpenCourseWare in general, is
available at ocw.mit.edu.

6
00:00:15 --> 00:00:15
PROFESSOR:
 

7
00:00:15 --> 00:00:18
Good afternoon.
 

8
00:00:18 --> 00:00:23
Before I forget, today's
topic is cognition.

9
00:00:23 --> 00:00:28
And in part, a good chunk of
this lecture is going to be

10
00:00:28 --> 00:00:34
based on work done by Kahneman
and Tversky, who won the Nobel

11
00:00:34 --> 00:00:38
Prize for a bunch of this
work some years back.

12
00:00:38 --> 00:00:41
A couple of years back, now.
 

13
00:00:41 --> 00:00:44
The reason that is of
particular interest is that

14
00:00:44 --> 00:00:48
Danny Kahneman is here
tomorrow speaking.

15
00:00:48 --> 00:00:51
E-51, 4 o'clock, right?
 

16
00:00:51 --> 00:00:55
So if you happen to be a,
interested in the material and

17
00:00:55 --> 00:00:59
b, at leisure at 4 o'clock
tomorrow, I'd go hear him.

18
00:00:59 --> 00:01:04
He's a very good speaker and
there aren't that many folks

19
00:01:04 --> 00:01:07
who I get to talk about in
psychology who win

20
00:01:07 --> 00:01:08
Nobel Prizes.
 

21
00:01:08 --> 00:01:10
So, you should go hear him.
 

22
00:01:10 --> 00:01:13
 
 

23
00:01:13 --> 00:01:16
That's one characteristic
of today's lecture.

24
00:01:16 --> 00:01:18
I realize that the other
characteristic of today's

25
00:01:18 --> 00:01:25
lecture is that I get to tell
you a bunch of things that

26
00:01:25 --> 00:01:31
are applicable to the
forthcoming election.

27
00:01:31 --> 00:01:34
And to politics in general.
 

28
00:01:34 --> 00:01:39
This particular swath I'm
cutting through, the large area

29
00:01:39 --> 00:01:45
of cognition, includes a bunch
of topics that if you've

30
00:01:45 --> 00:01:49
watched the debates thus far,
or if you go and watch the

31
00:01:49 --> 00:01:53
debate that's Friday, right,
the next one's Friday evening,

32
00:01:53 --> 00:01:56
you'll be able to see some
of the stuff I'm talking

33
00:01:56 --> 00:02:00
about today in action.
 

34
00:02:00 --> 00:02:04
There are two large parts to
what I want to talk about.

35
00:02:04 --> 00:02:09
And this is described, at least
in rough detail, in this

36
00:02:09 --> 00:02:12
abstract at the top
of the handout.

37
00:02:12 --> 00:02:18
We can make the distinction
between the way, using the

38
00:02:18 --> 00:02:22
computer as a model for how the
brain works is sort of a common

39
00:02:22 --> 00:02:26
game, rather like using the
camera as a model for the eye.

40
00:02:26 --> 00:02:29
There are important
differences.

41
00:02:29 --> 00:02:32
One of the important
differences that I'll talk

42
00:02:32 --> 00:02:37
about, that I started talking
about last time, is that if you

43
00:02:37 --> 00:02:42
call up a document off your
laptop, you are expecting that

44
00:02:42 --> 00:02:45
thing pulled out of memory to
be exactly what you

45
00:02:45 --> 00:02:47
put into memory.
 

46
00:02:47 --> 00:02:52
As we saw, as we saw towards
the end of the last

47
00:02:52 --> 00:02:55
lecture, that's not true
about human memory.

48
00:02:55 --> 00:02:57
I'll elaborate on that a bit.
 

49
00:02:57 --> 00:03:02
The other thing you expect is
that your computer will do

50
00:03:02 --> 00:03:04
mathematical and logical
operations correctly

51
00:03:04 --> 00:03:07
and quickly.
 

52
00:03:07 --> 00:03:11
And that also turns out to be
not particularly characteristic

53
00:03:11 --> 00:03:15
of human cognitive behavior.
 

54
00:03:15 --> 00:03:17
For interesting reasons that I
will describe in the second

55
00:03:17 --> 00:03:18
part of the lecture.
 

56
00:03:18 --> 00:03:23
So let's start with this issue,
this point, that says that

57
00:03:23 --> 00:03:28
recall is not just taking
stuff out of storage.

58
00:03:28 --> 00:03:33
Last time, I talked about a
couple of examples of that.

59
00:03:33 --> 00:03:36
And those are the ones labeled
as old on the handout.

60
00:03:36 --> 00:03:38
There's the context
of remembering.

61
00:03:38 --> 00:03:44
If you are depressed, your
memories of childhood are

62
00:03:44 --> 00:03:48
more gloomy than if you
are in a good mood.

63
00:03:48 --> 00:03:53
There is retriever's bias, that
was the example I was giving

64
00:03:53 --> 00:03:59
last time, with these
experiments where the knife in

65
00:03:59 --> 00:04:03
a simulated altercation that
you see, the knife might move

66
00:04:03 --> 00:04:07
from one person's hand to
another person's hand in your

67
00:04:07 --> 00:04:10
memory, depending on the
particular biases and

68
00:04:10 --> 00:04:12
associations in your memory.
 

69
00:04:12 --> 00:04:18
Now, let me tell you about a
few more ways in which memory

70
00:04:18 --> 00:04:23
is not just like pulling a
document off the hard

71
00:04:23 --> 00:04:24
drive somewhere.
 

72
00:04:24 --> 00:04:28
One of these is what I've
labeled on the handout as

73
00:04:28 --> 00:04:31
failures of source monitoring.
 

74
00:04:31 --> 00:04:35
Source monitoring is what we're
busy making sure you do

75
00:04:35 --> 00:04:37
correctly on your papers.
 

76
00:04:37 --> 00:04:42
Telling us where the
information comes from.

77
00:04:42 --> 00:04:45
And evaluating the
information on the basis

78
00:04:45 --> 00:04:46
of where it comes from.
 

79
00:04:46 --> 00:04:53
It turns out that that's not
as easy to do as one might

80
00:04:53 --> 00:04:55
think it ought to be.
 

81
00:04:55 --> 00:04:58
Indeed, I got this quote from
Euripedes on the handout that

82
00:04:58 --> 00:05:01
says man's most valuable trait
is a judicious sense of

83
00:05:01 --> 00:05:03
what not to believe.
 

84
00:05:03 --> 00:05:08
If you hear something that's
clearly false, is it

85
00:05:08 --> 00:05:12
possible for you to not
be influenced by that.

86
00:05:12 --> 00:05:14
This turns out to have been a
question that philosophers

87
00:05:14 --> 00:05:15
have worried about.
 

88
00:05:15 --> 00:05:19
Descartes thought that the
answer to the question was yes.

89
00:05:19 --> 00:05:24
If you hear a statement that's
clearly false, you can label it

90
00:05:24 --> 00:05:28
as clearly false and, in
a sense, put it aside.

91
00:05:28 --> 00:05:31
Spinoza, in contrast, thought
that it would be very

92
00:05:31 --> 00:05:32
nice if that were true.
 

93
00:05:32 --> 00:05:34
But he doubted that
it really was.

94
00:05:34 --> 00:05:38
This has subsequently been
tested experimentally.

95
00:05:38 --> 00:05:40
And I don't have enough
colored chalk around,

96
00:05:40 --> 00:05:42
but I'll simulate it.
 

97
00:05:42 --> 00:05:45
Here's what Dan Gilbert
and his colleagues

98
00:05:45 --> 00:05:47
did a few years back.
 

99
00:05:47 --> 00:05:50
They did an experiment
where you read little

100
00:05:50 --> 00:05:54
biographies of people.
 

101
00:05:54 --> 00:06:00
And the material you're
reading is color-coded.

102
00:06:00 --> 00:06:03
And so you're reading along.
 

103
00:06:03 --> 00:06:05
And then some of it, here,
we'll use straight lines

104
00:06:05 --> 00:06:06
for a different color.
 

105
00:06:06 --> 00:06:09
Some of it is red.
 

106
00:06:09 --> 00:06:11
If it's red it's a lie.
 

107
00:06:11 --> 00:06:12
Don't believe it.
 

108
00:06:12 --> 00:06:16
But if it's green, it's true.
 

109
00:06:16 --> 00:06:20
And if it's red it's a
lie, and true, and so on.

110
00:06:20 --> 00:06:22
Read a paragraph of this stuff.
 

111
00:06:22 --> 00:06:24
We're going to make sure that
you've read it because we're

112
00:06:24 --> 00:06:26
going to test you afterwards
and make sure that you

113
00:06:26 --> 00:06:28
actually know what it said.
 

114
00:06:28 --> 00:06:31
Because it's very boring
to discover that you can

115
00:06:31 --> 00:06:35
ignore lies if you don't
ever hear the lies.

116
00:06:35 --> 00:06:39
But we know you've heard
these various lies.

117
00:06:39 --> 00:06:43
And then what we'll do is
ask you about this person.

118
00:06:43 --> 00:06:48
The metric they were using was
a scale between college student

119
00:06:48 --> 00:06:50
and criminal, I believe.
 

120
00:06:50 --> 00:06:53
And you were asked how
criminal you thought

121
00:06:53 --> 00:06:55
these particular people.
 

122
00:06:55 --> 00:06:56
So you're reading along.
 

123
00:06:56 --> 00:07:01
And so-and-so is taking 1803.
 

124
00:07:01 --> 00:07:05
And steals calculators.
 

125
00:07:05 --> 00:07:10
And does the problem sets
reasonably reliably.

126
00:07:10 --> 00:07:12
And then fences
the calculators.

127
00:07:12 --> 00:07:15
And so on, right?
 

128
00:07:15 --> 00:07:21
The result is that even though
you know that the statements

129
00:07:21 --> 00:07:28
are false, your opinion of
this hypothetical person is

130
00:07:28 --> 00:07:30
lowered by negative lies.
 

131
00:07:30 --> 00:07:33
And, reciprocally, raised
by positive lies.

132
00:07:33 --> 00:07:42
You can't completely discount
the information that you're

133
00:07:42 --> 00:07:45
hearing, even if you
know it's a lie.

134
00:07:45 --> 00:07:48
Now, if it is not immediately
obvious to you that this has

135
00:07:48 --> 00:07:53
direct relevance to the current
political campaign, you can ask

136
00:07:53 --> 00:07:56
yourself about the behavior you
can see in this campaign, or in

137
00:07:56 --> 00:07:59
any other campaign that you
ever care to subject

138
00:07:59 --> 00:08:00
yourself to.
 

139
00:08:00 --> 00:08:07
Which is, you get charges
that are made, typically by

140
00:08:07 --> 00:08:10
surrogates of the candidate.
 

141
00:08:10 --> 00:08:14
So a clear example, this here
would be the charges about

142
00:08:14 --> 00:08:19
Kerry's Vietnam behavior,
brought by the Swift Boat

143
00:08:19 --> 00:08:23
people, not directly
by the Bush campaign.

144
00:08:23 --> 00:08:25
And I should be able to
think of a flip side.

145
00:08:25 --> 00:08:29
Oh, the National Guard
stuff for Bush.

146
00:08:29 --> 00:08:31
Regardless of the truth, we're
not going to get into the

147
00:08:31 --> 00:08:32
truth assertion of this.
 

148
00:08:32 --> 00:08:36
But, in both cases the
really nasty attacks are

149
00:08:36 --> 00:08:37
raised by surrogates.
 

150
00:08:37 --> 00:08:45
If it is then proved to be a
lie, then the candidate can

151
00:08:45 --> 00:08:52
have the great and noble
opportunity to run around the

152
00:08:52 --> 00:08:58
country saying, look, people
should not be saying that John

153
00:08:58 --> 00:09:01
Kerry molests small animals.
 

154
00:09:01 --> 00:09:03
It's a lie that he
molests small animals.

155
00:09:03 --> 00:09:07
Anybody who says that
should given a dope slap.

156
00:09:07 --> 00:09:09
And I would never say
anything like, John Kerry

157
00:09:09 --> 00:09:11
molests small animals.
 

158
00:09:11 --> 00:09:15
And I will tell all of my
people not to say that John

159
00:09:15 --> 00:09:17
Kerry molests small animals.
 

160
00:09:17 --> 00:09:21
And you can go home with this
righteous feeling around you

161
00:09:21 --> 00:09:24
saying that I've done the right
thing of condemning this smear.

162
00:09:24 --> 00:09:27
Not only that, I've spread it
around a bit, and the dumb

163
00:09:27 --> 00:09:29
suckers are going
to believe it.

164
00:09:29 --> 00:09:32
It's not going to swing the
election one way the other, but

165
00:09:32 --> 00:09:34
it's going to push things.
 

166
00:09:34 --> 00:09:39
And political campaigns do
this absolutely routinely.

167
00:09:39 --> 00:09:43
Perhaps not out of a completely
Machiavellian understanding of

168
00:09:43 --> 00:09:46
the psychology, but because
they know it works

169
00:09:46 --> 00:09:47
in some fashion.
 

170
00:09:47 --> 00:09:48
I don't know that they're
sitting around reading Dan

171
00:09:48 --> 00:09:54
Gilbert's papers to figure
this sort of thing out.

172
00:09:54 --> 00:09:57
This has applications
elsewhere in your life, too.

173
00:09:57 --> 00:10:01
It's very important to
understand -- I'm going to take

174
00:10:01 --> 00:10:06
zinc because it's going to keep
me from getting colds.

175
00:10:06 --> 00:10:08
Now, why do I think that?
 

176
00:10:08 --> 00:10:14
Is that because my aunt June,
who's the medical wacko of the

177
00:10:14 --> 00:10:15
family thinks that's true?
 

178
00:10:15 --> 00:10:18
Or is that because I read it in
the Journal of the American

179
00:10:18 --> 00:10:19
Medical Association?
 

180
00:10:19 --> 00:10:22
It makes a difference where
the information comes from.

181
00:10:22 --> 00:10:24
So, some sort of an
ability to evaluate the

182
00:10:24 --> 00:10:25
source is important.
 

183
00:10:25 --> 00:10:29
And the fact that we're not
very good at it is problematic.

184
00:10:29 --> 00:10:33
A particular subset of the
problems with source monitoring

185
00:10:33 --> 00:10:35
is what's known as [? crypt ?]
 

186
00:10:35 --> 00:10:36
amnesia.
 

187
00:10:36 --> 00:10:39
And that's the problem where
you don't remember where

188
00:10:39 --> 00:10:42
your own ideas come from.
 

189
00:10:42 --> 00:10:50
The canonical form of this, in
a setting like MIT -- actually,

190
00:10:50 --> 00:10:54
this is the part of the
lecture for the TAs.

191
00:10:54 --> 00:10:57
The canonical version of this
is you go to your doctoral

192
00:10:57 --> 00:11:01
adviser with a bright idea.
 

193
00:11:01 --> 00:11:03
Yeah, I got this cool
idea for new experiment.

194
00:11:03 --> 00:11:07
Your doctoral adviser looks
at you, says, that's stupid.

195
00:11:07 --> 00:11:08
And you go away.
 

196
00:11:08 --> 00:11:10
You know, you're
kind of devastated.

197
00:11:10 --> 00:11:12
Next week, in lab meeting.
 

198
00:11:12 --> 00:11:16
Your doctoral adviser has
this really bright new idea.

199
00:11:16 --> 00:11:18
It's your idea.
 

200
00:11:18 --> 00:11:20
He thinks it's his idea.
 

201
00:11:20 --> 00:11:24
He does not, he's being a
-- probably not, being a

202
00:11:24 --> 00:11:28
slimeball to say I'm going
to steal their ideas.

203
00:11:28 --> 00:11:31
It's, you actually forget
where it comes from.

204
00:11:31 --> 00:11:34
And in the absence of a clear
memory of where the idea came

205
00:11:34 --> 00:11:39
from, if I've got an idea in my
head about my research, gee, I

206
00:11:39 --> 00:11:40
wonder where that came from.
 

207
00:11:40 --> 00:11:42
It came from -- I think they
come from my shower head.

208
00:11:42 --> 00:11:45
Because all the good ideas I
have always come in the shower.

209
00:11:45 --> 00:11:48
But, most people tend to
think it comes from them.

210
00:11:48 --> 00:11:51
I just cite the shower.
 

211
00:11:51 --> 00:11:55
But this is actually a very
good argument, by the way, for

212
00:11:55 --> 00:11:59
taking copious notes in a
lab book when you work in a.

213
00:11:59 --> 00:12:03
Lab Because if it ever gets to
the point of an argument, you

214
00:12:03 --> 00:12:06
want to be able to say, look,
in this dated entry from

215
00:12:06 --> 00:12:12
October 5th, 10,000 and --
2004, it might take you a long

216
00:12:12 --> 00:12:15
time to get your doctorate.
 

217
00:12:15 --> 00:12:17
here's the idea.
 

218
00:12:17 --> 00:12:19
I wrote down this idea, and
then I wrote down this

219
00:12:19 --> 00:12:22
tear-stained comment where
you told me I was a doofus.

220
00:12:22 --> 00:12:25
But, anyway, I really do
think this was my idea.

221
00:12:25 --> 00:12:27
And I ought to be an author
on the paper that you

222
00:12:27 --> 00:12:30
just submitted about it.
 

223
00:12:30 --> 00:12:31
Anyway, it's a serious problem.
 

224
00:12:31 --> 00:12:36
Another place where that shows
up is there are recurring

225
00:12:36 --> 00:12:40
scandals in the sort of
academic book-publishing world

226
00:12:40 --> 00:12:44
where some well-respected
author is proven to have

227
00:12:44 --> 00:12:47
taken a chunk out of
somebody else's book.

228
00:12:47 --> 00:12:49
Or something very close to a
chunk out of somebody else

229
00:12:49 --> 00:12:50
book, and publishing it.
 

230
00:12:50 --> 00:12:52
Some people are doing
that because they're

231
00:12:52 --> 00:12:55
sloppy, slimy people.
 

232
00:12:55 --> 00:12:57
Other people are doing
it because you read

233
00:12:57 --> 00:12:59
a lot of stuff.
 

234
00:12:59 --> 00:13:01
Maybe you took good
notes, maybe you didn't.

235
00:13:01 --> 00:13:04
And then when you start
writing, this clever

236
00:13:04 --> 00:13:07
phrase comes to you -- or
maybe just a pedestrian

237
00:13:07 --> 00:13:08
phrase comes to mind.
 

238
00:13:08 --> 00:13:10
But a phrase comes to mind.
 

239
00:13:10 --> 00:13:13
And you simply have forgotten
the source of that phrase.

240
00:13:13 --> 00:13:16
Which is that you got it from
somebody else's writing, and

241
00:13:16 --> 00:13:21
then -- it's a strong argument
for making sure that you keep

242
00:13:21 --> 00:13:22
very clear track of where
you're getting your

243
00:13:22 --> 00:13:23
material from.
 

244
00:13:23 --> 00:13:26
And it's part of why we beat
so hard on the notion of

245
00:13:26 --> 00:13:31
referencing in papers
in a course like this.

246
00:13:31 --> 00:13:35
That's what [? crypt ?]
 

247
00:13:35 --> 00:13:39
amnesia is, in any case.
 

248
00:13:39 --> 00:13:42
So, those are failures
of source monitoring.

249
00:13:42 --> 00:13:46
It is also possible, to move on
to the next one, to have

250
00:13:46 --> 00:13:53
memories that feel like
memories but just are not true.

251
00:13:53 --> 00:13:56
Again, Liz Loftus has done
some of the most interesting

252
00:13:56 --> 00:13:59
research on this.
 

253
00:13:59 --> 00:14:03
One of her experiments, another
one of these car crash

254
00:14:03 --> 00:14:06
experiments, remember the
experiment of, how fast did the

255
00:14:06 --> 00:14:10
car smash, or bump, or tap,
or whatever it was,

256
00:14:10 --> 00:14:11
into the other car?
 

257
00:14:11 --> 00:14:12
Well, here's another one.
 

258
00:14:12 --> 00:14:15
You watch another one of these
videos of a car getting

259
00:14:15 --> 00:14:19
into an accident.
 

260
00:14:19 --> 00:14:21
And that part of the problem
is that the car went straight

261
00:14:21 --> 00:14:23
through a yield sign.
 

262
00:14:23 --> 00:14:27
Later, you are given
information that suggests that

263
00:14:27 --> 00:14:29
it might have been a stop sign.
 

264
00:14:29 --> 00:14:32
Can do this in a variety
of ways, including ways

265
00:14:32 --> 00:14:35
that tell you that that
is bad information.

266
00:14:35 --> 00:14:39
Another source
monitoring problem.

267
00:14:39 --> 00:14:43
But, given later information
that it was a stop sign, not a

268
00:14:43 --> 00:14:48
yield sign, people will
incorrectly remember it -- not

269
00:14:48 --> 00:14:50
everybody, but you will get a
percentage of people -- who

270
00:14:50 --> 00:14:53
will now misremember the
original scene as having

271
00:14:53 --> 00:14:56
had a stop sign in it.
 

272
00:14:56 --> 00:15:01
If you ask what color was the
sign, they'll say red, even

273
00:15:01 --> 00:15:05
though a yield sign would be
yellow, and was yellow in the

274
00:15:05 --> 00:15:07
in the actual video
that you saw.

275
00:15:07 --> 00:15:11
So you've recreated, you've
now put together the true

276
00:15:11 --> 00:15:15
memory with some false
later information.

277
00:15:15 --> 00:15:17
And it's corrupted
that true memory.

278
00:15:17 --> 00:15:22
Now, that's of obviously some
interest in terms of getting

279
00:15:22 --> 00:15:25
testimony in court and
things like that.

280
00:15:25 --> 00:15:33
The place where this has become
extremely controversial and a

281
00:15:33 --> 00:15:40
real problem, is the issue, is
it possible to recover memories

282
00:15:40 --> 00:15:42
that were suppressed?
 

283
00:15:42 --> 00:15:44
Suppose something really
bad happens to you

284
00:15:44 --> 00:15:44
when you're a kid.
 

285
00:15:44 --> 00:15:49
Is it possible to suppress
that memory and not have

286
00:15:49 --> 00:15:51
it available to you
until some later point.

287
00:15:51 --> 00:15:57
At which point you now
remember this memory.

288
00:15:57 --> 00:15:58
Is that possible?
 

289
00:15:58 --> 00:16:00
The evidence suggests that it
may indeed be possible to not

290
00:16:00 --> 00:16:02
think about something
for a very long time.

291
00:16:02 --> 00:16:05
Have it bubble up
into your memory.

292
00:16:05 --> 00:16:09
Sadly, the evidence is also
strong that it is possible for

293
00:16:09 --> 00:16:13
stuff to bubble up into your
memory that isn't true.

294
00:16:13 --> 00:16:17
Under the right circumstances.
 

295
00:16:17 --> 00:16:21
If what's bubbling up into
memory is memories of, for

296
00:16:21 --> 00:16:24
instance, childhood sexual
abuse, for which there were a

297
00:16:24 --> 00:16:28
large number of criminal cases
about a decade ago, now, it's a

298
00:16:28 --> 00:16:31
really important question to
know whether these

299
00:16:31 --> 00:16:33
are real memories.
 

300
00:16:33 --> 00:16:38
And it is deeply unclear
that there's any nice, neat

301
00:16:38 --> 00:16:39
way to know the answer.
 

302
00:16:39 --> 00:16:43
Now, look, obviously you can't,
one of the reasons it's very

303
00:16:43 --> 00:16:46
unclear, is you can't do
the real experiment.

304
00:16:46 --> 00:16:51
You can't take a little kid,
abuse the little kid and say,

305
00:16:51 --> 00:16:53
let's come back 20 years and
see if they recover the memory.

306
00:16:53 --> 00:16:57
I mean, it doesn't take a
genius to figure out that

307
00:16:57 --> 00:16:58
that's not going to work.
 

308
00:16:58 --> 00:17:06
But, Liz Loftus has a very nice
experiment that points in the

309
00:17:06 --> 00:17:12
direction of this
sort of problem.

310
00:17:12 --> 00:17:18
Here's what she did: you go and
interview families about, let's

311
00:17:18 --> 00:17:20
take one specific example.
 

312
00:17:20 --> 00:17:23
Getting lost in a mall.
 

313
00:17:23 --> 00:17:30
Did so-and-so, the
designated subject.

314
00:17:30 --> 00:17:31
You are --
 

315
00:17:31 --> 00:17:33
AUDIENCE: Olga.
 

316
00:17:33 --> 00:17:33
PROFESSOR: Olga.
 

317
00:17:33 --> 00:17:35
Did you ever get
lost in the mall?

318
00:17:35 --> 00:17:36
AUDIENCE: [UNINTELLIGIBLE]
 

319
00:17:36 --> 00:17:36
PROFESSOR: OK, good.
 

320
00:17:36 --> 00:17:37
We won't ask her that.
 

321
00:17:37 --> 00:17:41
But we'll ask her family
whether Olga ever got

322
00:17:41 --> 00:17:42
lost in the mall.
 

323
00:17:42 --> 00:17:44
And they'll say, no, no,
no, it never happened.

324
00:17:44 --> 00:17:45
OK, good.
 

325
00:17:45 --> 00:17:46
Got a brother or sister?
 

326
00:17:46 --> 00:17:47
AUDIENCE: Yeah.
 

327
00:17:47 --> 00:17:47
PROFESSOR: Good.
 

328
00:17:47 --> 00:17:48
What have you got?
 

329
00:17:48 --> 00:17:49
AUDIENCE: Brother.
 

330
00:17:49 --> 00:17:50
PROFESSOR: Brother.
 

331
00:17:50 --> 00:17:54
So now we call her brother
aside here, and say, we're

332
00:17:54 --> 00:17:56
going to do a little
experiment here.

333
00:17:56 --> 00:18:00
Go ask Olga if she remembers
the time that she got

334
00:18:00 --> 00:18:03
lost in the mall.
 

335
00:18:03 --> 00:18:05
And you do this with a lot
of, Olga might look at

336
00:18:05 --> 00:18:08
her brother and say, no.
 

337
00:18:08 --> 00:18:11
And that's the end of that
particular experiment.

338
00:18:11 --> 00:18:15
But some percentage of people,
given brother who seems to

339
00:18:15 --> 00:18:18
think that this happened at
some point, will, yeah,

340
00:18:18 --> 00:18:19
I remember that.
 

341
00:18:19 --> 00:18:20
Well, tell me about it.
 

342
00:18:20 --> 00:18:23
Well, you know how
this story goes.

343
00:18:23 --> 00:18:26
Even if you were never lost in
the mall, we could ask Olga.

344
00:18:26 --> 00:18:28
She could generate the
story pretty successfully.

345
00:18:28 --> 00:18:30
It'll be something like,
well, you know, I was there

346
00:18:30 --> 00:18:31
with my parents, right?
 

347
00:18:31 --> 00:18:35
And I was looking at these
toys, like, in the window.

348
00:18:35 --> 00:18:38
And then I followed
these, I was little,

349
00:18:38 --> 00:18:39
I was really little.
 

350
00:18:39 --> 00:18:40
So you don't get lost in
the mall when you're

351
00:18:40 --> 00:18:41
18 or something.
 

352
00:18:41 --> 00:18:44
So, I was little.
 

353
00:18:44 --> 00:18:45
I'm following these
legs around.

354
00:18:45 --> 00:18:47
And it turns out to
be the wrong legs.

355
00:18:47 --> 00:18:50
I thought it was my mom
and it was somebody else.

356
00:18:50 --> 00:18:54
And I started to cry, and then
this big man came and he took

357
00:18:54 --> 00:18:57
me to the security guys.
 

358
00:18:57 --> 00:19:02
And they announced it over the
PA system, that I was lost.

359
00:19:02 --> 00:19:06
And my mother came back --
people fill in really good,

360
00:19:06 --> 00:19:08
detailed stories of this.
 

361
00:19:08 --> 00:19:13
That are apparently invented
out of -- well, not out of

362
00:19:13 --> 00:19:15
really whole cloth, I suppose.
 

363
00:19:15 --> 00:19:19
They're invented out of the
understanding that you have

364
00:19:19 --> 00:19:23
of the possibility of
being lost in the mall.

365
00:19:23 --> 00:19:25
And the childhood fear of
being lost in the mall.

366
00:19:25 --> 00:19:27
But it's not a real memory.
 

367
00:19:27 --> 00:19:29
Feels like a real memory.
 

368
00:19:29 --> 00:19:32
Subjects in Loftus's
experiments, confronted later

369
00:19:32 --> 00:19:37
with the reality that this was
all a setup, were often quite

370
00:19:37 --> 00:19:41
surprised -- Sure I was
never lost in the mall?

371
00:19:41 --> 00:19:43
No, says Olga's brother.
 

372
00:19:43 --> 00:19:46
Liz Loftus just told
me to tell you that.

373
00:19:46 --> 00:19:49
And, oh, ah, hm, that's
a little weird.

374
00:19:49 --> 00:19:52
But, the point is it is
possible to generate things

375
00:19:52 --> 00:19:57
that feel like memories
that are not memories.

376
00:19:57 --> 00:19:58
Yes.
 

377
00:19:58 --> 00:19:58
AUDIENCE: What about
 

378
00:19:58 --> 00:20:04
remembering dreams, supposing
something happened in a dream

379
00:20:04 --> 00:20:09
that could happen, and you
remember that as a [INAUDIBLE]

380
00:20:09 --> 00:20:12
PROFESSOR: If you get routinely
confused about what happens in

381
00:20:12 --> 00:20:17
your dreams and what happens in
reality, that's known in

382
00:20:17 --> 00:20:20
the trade as a failure
of reality testing.

383
00:20:20 --> 00:20:25
And is not considered a
good psychiatric sign.

384
00:20:25 --> 00:20:30
But, sure, those sorts
of things are possible.

385
00:20:30 --> 00:20:36
Particularly when you end up
having desperately realistic

386
00:20:36 --> 00:20:38
kinds of dreams, the sorts of
things where these, sort of,

387
00:20:38 --> 00:20:40
reality testing things do.
 

388
00:20:40 --> 00:20:44
Some dirt is, how many people,
I'll probably ask this again

389
00:20:44 --> 00:20:45
when we talk about
sleep and dreams.

390
00:20:45 --> 00:20:46
But, never mind.
 

391
00:20:46 --> 00:20:49
How many people have
had the dream of their

392
00:20:49 --> 00:20:51
alarm clock going off?
 

393
00:20:51 --> 00:20:55
Dreamed that they turned off
their alarm clock and got up.

394
00:20:55 --> 00:21:02
Only to then discover,
oh, man, I'm in bed.

395
00:21:02 --> 00:21:05
And I'm supposed to be in
my psych recitation or

396
00:21:05 --> 00:21:06
something like that.
 

397
00:21:06 --> 00:21:11
Or, at least tried to give
that a story to their TA.

398
00:21:11 --> 00:21:18
So, yeah, there are possible
sources of confusion like that.

399
00:21:18 --> 00:21:25
The last item on this list of
sources of possible confusion

400
00:21:25 --> 00:21:27
I think, says something like
becoming plastic again.

401
00:21:27 --> 00:21:30
This is new stuff which I
will come to momentarily.

402
00:21:30 --> 00:21:32
AUDIENCE: [UNINTELLIGIBLE]
 

403
00:21:32 --> 00:21:33
PROFESSOR: That's OK.
 

404
00:21:33 --> 00:21:36
AUDIENCE: Is there a way to
distinguish between these

405
00:21:36 --> 00:21:39
false memories or if
they've actually occurred?

406
00:21:39 --> 00:21:43
[UNINTELLIGIBLE]
 

407
00:21:43 --> 00:21:45
If you don't know them
[UNINTELLIGIBLE]

408
00:21:45 --> 00:21:46
comes and says, I think I was
sexually abused as a child, is

409
00:21:46 --> 00:21:51
there a way to determine,
is this, like --

410
00:21:51 --> 00:21:55
PROFESSOR: There is, at the
present, time no ironclad

411
00:21:55 --> 00:21:57
way to know that.
 

412
00:21:57 --> 00:22:02
I mean, sometimes you can go
and get evidence -- there's no

413
00:22:02 --> 00:22:04
ironclad way to know simply
from the memory testimony.

414
00:22:04 --> 00:22:08
There may be ways to find out
by talking to other witnesses

415
00:22:08 --> 00:22:09
or something like that.
 

416
00:22:09 --> 00:22:13
There have been various efforts
to look, for instance, at brain

417
00:22:13 --> 00:22:17
activity and see whether or not
there's a difference

418
00:22:17 --> 00:22:18
in the pattern.
 

419
00:22:18 --> 00:22:20
So, you do this at the lab.
 

420
00:22:20 --> 00:22:24
Create a false memory and a
real memory, and see if you can

421
00:22:24 --> 00:22:29
see the difference somehow in a
FMRI scan or something

422
00:22:29 --> 00:22:31
of that sort.
 

423
00:22:31 --> 00:22:33
There are hints that
maybe you can get there.

424
00:22:33 --> 00:22:35
It's like lie detectors.
 

425
00:22:35 --> 00:22:39
A fascinating topic for a whole
lecture that I'm not giving.

426
00:22:39 --> 00:22:45
There is no ironclad way of
telling a lie from the truth

427
00:22:45 --> 00:22:47
by wiring anybody up.
 

428
00:22:47 --> 00:22:51
Regardless of what
the FBI tells you.

429
00:22:51 --> 00:22:53
Actually, I shouldn't be
telling you this, because the

430
00:22:53 --> 00:22:56
only reason that these things
work is because people

431
00:22:56 --> 00:22:57
think they work.
 

432
00:22:57 --> 00:22:59
If you think that the lie
detector's really going to

433
00:22:59 --> 00:23:03
work, then if you're lying,
you tend to get kind of more

434
00:23:03 --> 00:23:05
nervous than if you don't
think, than if you're

435
00:23:05 --> 00:23:06
telling the truth.
 

436
00:23:06 --> 00:23:08
And that's what
they can monitor.

437
00:23:08 --> 00:23:13
But a really good pathological
liar, or a well-trained agent

438
00:23:13 --> 00:23:16
is apparently pretty good
at getting past your

439
00:23:16 --> 00:23:18
average lie detector.
 

440
00:23:18 --> 00:23:22
Lots of people, you can get
serious defense money by having

441
00:23:22 --> 00:23:25
a brilliant new idea about how
to tell truth from lie and real

442
00:23:25 --> 00:23:27
memory from false memory.
 

443
00:23:27 --> 00:23:29
There's a lot of
interest in doing that.

444
00:23:29 --> 00:23:35
But, no ironclad way of
doing it at the present.

445
00:23:35 --> 00:23:39
Makes for great sci-fi novels,
though, right what's going to

446
00:23:39 --> 00:23:43
happen when we finally figure
out that we can actually stick

447
00:23:43 --> 00:23:46
this thing on your head and
tell what your memories,

448
00:23:46 --> 00:23:49
whether your memories
are true or not.

449
00:23:49 --> 00:23:52
But at the moment
it's still sci-fi.

450
00:23:52 --> 00:23:59
Not quite sci-fi, but still
very new, is the indication.

451
00:23:59 --> 00:24:01
Remember, I talked about
consolidation last time.

452
00:24:01 --> 00:24:04
This process of making a
fragile memory into something

453
00:24:04 --> 00:24:06
that will last for
years and years.

454
00:24:06 --> 00:24:10
There is evidence at least from
rats in a fairly restricted

455
00:24:10 --> 00:24:16
range of studies, that when you
recall a memory, when you bring

456
00:24:16 --> 00:24:20
it back out of, say, long-term
memory into that working

457
00:24:20 --> 00:24:23
memory, that it becomes
plastic again.

458
00:24:23 --> 00:24:26
And that perhaps some of
the distortions in memory

459
00:24:26 --> 00:24:28
can arise at that point.
 

460
00:24:28 --> 00:24:29
Because the memory
is again plastic.

461
00:24:29 --> 00:24:33
It needs to be reconsolidated
in order to be, in

462
00:24:33 --> 00:24:35
effect, restored.
 

463
00:24:35 --> 00:24:38
So you bring it out.
 

464
00:24:38 --> 00:24:41
It's now vulnerable again
in ways that it wasn't

465
00:24:41 --> 00:24:44
before you remembered it.
 

466
00:24:44 --> 00:24:49
My guess is that if you come
back to the course ten years

467
00:24:49 --> 00:24:53
from now, that there'll be much
more to be said about that.

468
00:24:53 --> 00:24:58
That's new and interesting.
 

469
00:24:58 --> 00:25:00
But the general point
should be clear enough.

470
00:25:00 --> 00:25:05
Which is that recall out of
memory, retrieval out of

471
00:25:05 --> 00:25:07
memory, is an active
reconstruction.

472
00:25:07 --> 00:25:10
It has multiple pitfalls in it
that might cause what you are

473
00:25:10 --> 00:25:15
pulling out as a memory to be
distorted in some

474
00:25:15 --> 00:25:20
fashion or other.
 

475
00:25:20 --> 00:25:24
Now, the second part, the
larger part, of what I want to

476
00:25:24 --> 00:25:30
talk about today is the sense
in which we are not simple

477
00:25:30 --> 00:25:35
logic engines in the way that
our computers, most

478
00:25:35 --> 00:25:36
of the time, are.
 

479
00:25:36 --> 00:25:42
Now, one way to think about
that is to think about a

480
00:25:42 --> 00:25:47
distinction between what's
called narrative thought

481
00:25:47 --> 00:25:50
and propositional thought.
 

482
00:25:50 --> 00:25:53
Propositional thought,
and this runs parallel.

483
00:25:53 --> 00:25:55
Narrative thought and
episodic memory.

484
00:25:55 --> 00:25:57
Propositional thought
and semantic memory.

485
00:25:57 --> 00:25:59
They travel together.
 

486
00:25:59 --> 00:26:07
So, propositional thought is
the sort of thought that your

487
00:26:07 --> 00:26:09
computer would be good at.
 

488
00:26:09 --> 00:26:17
Moving symbols around, is x
bigger than y, and problem set

489
00:26:17 --> 00:26:20
kind of thinking is mostly
propositional thought.

490
00:26:20 --> 00:26:26
Narrative thought is thought
that has a story-like

491
00:26:26 --> 00:26:28
aspect to it.
 

492
00:26:28 --> 00:26:31
Perhaps an episodic memory
type aspect to it.

493
00:26:31 --> 00:26:41
So, an example of narrative
thought might be do you want

494
00:26:41 --> 00:26:48
to go to the party that is
happening on Saturday night

495
00:26:48 --> 00:26:52
at Alpha Beta Gamma house,
or something like that.

496
00:26:52 --> 00:26:53
How do you figure that out?
 

497
00:26:53 --> 00:26:57
Well you probably don't do some
symbolic cost-benefit analysis.

498
00:26:57 --> 00:27:02
You probably imagine
yourself there.

499
00:27:02 --> 00:27:04
Would I be having fun?
 

500
00:27:04 --> 00:27:06
Who's going to be there.
 

501
00:27:06 --> 00:27:07
You'd run a scenario.
 

502
00:27:07 --> 00:27:10
And that's the sort of
narrative thought.

503
00:27:10 --> 00:27:14
Turns out that narrative
thought has a power in our own

504
00:27:14 --> 00:27:21
decision-making that is capable
of running across, running

505
00:27:21 --> 00:27:24
over, the simple facts
of the situation.

506
00:27:24 --> 00:27:28
Even when we know the
facts correctly.

507
00:27:28 --> 00:27:32
Risk assessment is
one example of this.

508
00:27:32 --> 00:27:33
So, forced choice.
 

509
00:27:33 --> 00:27:35
You gotta pick one
of the other.

510
00:27:35 --> 00:27:38
Don't just sit on
your hands here.

511
00:27:38 --> 00:27:44
Which makes you more nervous:
taking a ride in the car

512
00:27:44 --> 00:27:45
or flying in an airplane?
 

513
00:27:45 --> 00:27:47
How many vote for
riding in the car?

514
00:27:47 --> 00:27:51
How many vote for
flying an airplane.

515
00:27:51 --> 00:27:55
As usual half the people can't
figure out that forced choice,

516
00:27:55 --> 00:27:57
pick one or the other, means
you have to raise your hand

517
00:27:57 --> 00:28:00
for one of these two options.
 

518
00:28:00 --> 00:28:02
At least if you're
moderately alert.

519
00:28:02 --> 00:28:06
But anyway, plane
wins big-time.

520
00:28:06 --> 00:28:10
How many people, let's just ask
the propositional thought, or

521
00:28:10 --> 00:28:13
the semantic memory nugget.
 

522
00:28:13 --> 00:28:17
Which is more dangerous?
 

523
00:28:17 --> 00:28:24
Flying in a car, that's
only -- that's the

524
00:28:24 --> 00:28:27
Harry Potter version.
 

525
00:28:27 --> 00:28:29
Flying a plane or
driving a car.

526
00:28:29 --> 00:28:31
How many vote for plane?
 

527
00:28:31 --> 00:28:32
How many vote for car?
 

528
00:28:32 --> 00:28:36
Everybody knows the
proposition here.

529
00:28:36 --> 00:28:41
It's riskier to, you can
measure this in all sorts

530
00:28:41 --> 00:28:42
of different ways.
 

531
00:28:42 --> 00:28:44
Per mile, per time.
 

532
00:28:44 --> 00:28:47
It's it's riskier
traveling in a car than

533
00:28:47 --> 00:28:51
traveling in a plane.
 

534
00:28:51 --> 00:28:53
But people are much more
nervous about planes.

535
00:28:53 --> 00:28:54
Why is that?
 

536
00:28:54 --> 00:28:55
There are multiple reasons.
 

537
00:28:55 --> 00:29:00
But perhaps one of the driving
ones is that the narrative

538
00:29:00 --> 00:29:05
surrounding planes, and
particularly surrounding plane

539
00:29:05 --> 00:29:08
crashes is sufficiently vivid
that it overrides your

540
00:29:08 --> 00:29:12
knowledge of the mere
facts of the matter.

541
00:29:12 --> 00:29:17
If and when a plane sadly
goes down, that's big news.

542
00:29:17 --> 00:29:19
A lot of people tend
to get killed.

543
00:29:19 --> 00:29:23
It makes big news in
the papers and on TV.

544
00:29:23 --> 00:29:25
Highly salient.
 

545
00:29:25 --> 00:29:33
The fact that there's a steady
drumbeat of car crashes, unless

546
00:29:33 --> 00:29:36
it happens to be very local
to you, doesn't have any

547
00:29:36 --> 00:29:38
particular impact on you.
 

548
00:29:38 --> 00:29:41
And the raw statistic
doesn't much matter.

549
00:29:41 --> 00:29:44
In the same way, this
turns out to be true in

550
00:29:44 --> 00:29:47
course catalog land.
 

551
00:29:47 --> 00:29:52
So that the MIT course
evaluation guide, when it

552
00:29:52 --> 00:29:58
comes out, has vast piles
of cool statistics.

553
00:29:58 --> 00:30:00
What people thought of
this particular course.

554
00:30:00 --> 00:30:02
And then it tends -- I haven't
seen the most recent one.

555
00:30:02 --> 00:30:06
But typically it has a
paragraph of comments.

556
00:30:06 --> 00:30:10
One of my favorites over the
years, so-and-so is to the

557
00:30:10 --> 00:30:16
teaching of physics as
Einstein is to the

558
00:30:16 --> 00:30:18
National Football League.
 

559
00:30:18 --> 00:30:21
 
 

560
00:30:21 --> 00:30:23
That's a very salient
bit of narrative.

561
00:30:23 --> 00:30:26
And it turns out that those
sort of comments have much more

562
00:30:26 --> 00:30:28
impact than all the statistics.
 

563
00:30:28 --> 00:30:33
In spite of the fact that the
nice rational you knows that

564
00:30:33 --> 00:30:37
one person's brilliant catty
comment, or brilliantly

565
00:30:37 --> 00:30:41
positive comment, is presumably
much less valuable than

566
00:30:41 --> 00:30:43
the statistical average
across 300 students.

567
00:30:43 --> 00:30:46
But the narrative,
again, overwhelms.

568
00:30:46 --> 00:30:51
You can watch this on Friday,
when you watch the debate.

569
00:30:51 --> 00:30:59
You can be guaranteed that
somebody is going to, it'll

570
00:30:59 --> 00:31:01
probably be either Kerry
or Bush, actually.

571
00:31:01 --> 00:31:09
That Kerry or Bush will take
some boring statistics

572
00:31:09 --> 00:31:11
and personalize it.
 

573
00:31:11 --> 00:31:12
They do this all the time.
 

574
00:31:12 --> 00:31:17
It's become a sort of it a
trope, a standard item in the

575
00:31:17 --> 00:31:20
State of the Union message
where the President always

576
00:31:20 --> 00:31:22
put a couple of heroes.
 

577
00:31:22 --> 00:31:24
They probably are, but, you
know, heroes up in the balcony

578
00:31:24 --> 00:31:28
there so that he can point to
them and say, look at this guy.

579
00:31:28 --> 00:31:29
Look at that guy.
 

580
00:31:29 --> 00:31:33
And in the debate it'll be,
there'll be some discussion

581
00:31:33 --> 00:31:37
of employment statistics.
 

582
00:31:37 --> 00:31:40
And presumably this
would be Kerry.

583
00:31:40 --> 00:31:42
Because he'll be beating
on Bush about it.

584
00:31:42 --> 00:31:44
I don't care about what
you say about the economy

585
00:31:44 --> 00:31:46
recovering or whatever.
 

586
00:31:46 --> 00:31:50
What I care about is
poor little Olga.

587
00:31:50 --> 00:31:54
She got lost in the mall, and
then her father lost her job as

588
00:31:54 --> 00:31:58
a mall security guy because
of the decisions that

589
00:31:58 --> 00:32:01
your government, your
administration made.

590
00:32:01 --> 00:32:05
And what am I going to
say to poor little Olga.

591
00:32:05 --> 00:32:08
Why?
 

592
00:32:08 --> 00:32:11
It's very sad about poor little
Olga, who's now been lost in

593
00:32:11 --> 00:32:16
the mall and she's never going
to sit in the front again.

594
00:32:16 --> 00:32:22
But there are, like, 260
million people, whatever it is

595
00:32:22 --> 00:32:23
in the country at the moment.
 

596
00:32:23 --> 00:32:27
It's very sad about poor
little whoever it is.

597
00:32:27 --> 00:32:30
What's really important are the
broad economic statistics.

598
00:32:30 --> 00:32:35
Broad economic statistics are
wonderful propositional

599
00:32:35 --> 00:32:36
reasoning kinds of stuff.
 

600
00:32:36 --> 00:32:38
But are really boring.
 

601
00:32:38 --> 00:32:40
Unless you're an economist.
 

602
00:32:40 --> 00:32:42
But poor little Olga
is really riveting.

603
00:32:42 --> 00:32:44
And poor little Olga
is what you'll hear

604
00:32:44 --> 00:32:47
about in the debates.
 

605
00:32:47 --> 00:32:52
We should do a debriefing
about this on Monday.

606
00:32:52 --> 00:32:54
If anybody listens to the
debate and hears any of these

607
00:32:54 --> 00:32:56
little tidbits, send me email.
 

608
00:32:56 --> 00:32:59
 
 

609
00:32:59 --> 00:33:01
In case I miss it.
 

610
00:33:01 --> 00:33:03
And then we can we about this.
 

611
00:33:03 --> 00:33:07
Now, this can get
studied in the lab.

612
00:33:07 --> 00:33:08
Here, take a look.
 

613
00:33:08 --> 00:33:12
On the handout, there's a thing
called the framing demo.

614
00:33:12 --> 00:33:15
Says that there's an
unusual flu coming.

615
00:33:15 --> 00:33:18
It's expected to kill 600
people in the U.S., and there

616
00:33:18 --> 00:33:20
are two things you can do.
 

617
00:33:20 --> 00:33:23
There's Program A
and Program B.

618
00:33:23 --> 00:33:28
Take a quick look, decide on
Program A or Program B, and

619
00:33:28 --> 00:33:31
then we'll collect a little
data and talk about it here.

620
00:33:31 --> 00:33:37
 
 

621
00:33:37 --> 00:33:41
Don't cheat off your
neighbor's test.

622
00:33:41 --> 00:33:42
OK.
 

623
00:33:42 --> 00:33:46
Everybody manage to come
up with an answer here?

624
00:33:46 --> 00:33:46
OK.
 

625
00:33:46 --> 00:33:49
So, let's see for starters.
 

626
00:33:49 --> 00:33:51
How many people went for A?
 

627
00:33:51 --> 00:33:54
 
 

628
00:33:54 --> 00:33:55
How many people went for B?
 

629
00:33:55 --> 00:33:58
 
 

630
00:33:58 --> 00:34:01
How many people don't know
what they're going for?

631
00:34:01 --> 00:34:03
OK, well it looks
about evenly divided.

632
00:34:03 --> 00:34:04
Boy, that's really boring.
 

633
00:34:04 --> 00:34:06
Well, it would be really
boring, except that there

634
00:34:06 --> 00:34:11
are two versions of
the handout out there.

635
00:34:11 --> 00:34:19
So, Version One of the handout
says that if Program A is

636
00:34:19 --> 00:34:22
adopted, uh-oh, I don't know
which Version One says.

637
00:34:22 --> 00:34:25
Well, it doesn't matter which
I call Version One, does it?

638
00:34:25 --> 00:34:31
Version One says, 400
people will die.

639
00:34:31 --> 00:34:39
Version Two says 200
people will be saved.

640
00:34:39 --> 00:34:41
 
 

641
00:34:41 --> 00:34:43
Did I get the language
right there?

642
00:34:43 --> 00:34:46
What you will notice it, if
the net is 600 here, these

643
00:34:46 --> 00:34:48
are identical, right?
 

644
00:34:48 --> 00:34:51
There's no difference except
in the way that it's phrased.

645
00:34:51 --> 00:34:56
Or the way the
question is framed.

646
00:34:56 --> 00:35:03
There's a similar change to the
second option, B in the die --

647
00:35:03 --> 00:35:07
in this person, it's described
as a 1/3 chance that no-one

648
00:35:07 --> 00:35:12
will die and a 2/3 chance that
600 hundred people will die.

649
00:35:12 --> 00:35:14
Do you know about the
notion of expected value?

650
00:35:14 --> 00:35:18
You can calculate the expected
value of that option, which

651
00:35:18 --> 00:35:32
will be 1/3 chance that no-one
will die and a 2/3 chance

652
00:35:32 --> 00:35:38
that 600 people will die.
 

653
00:35:38 --> 00:35:42
And then that's going to be 400
people dying, again, here.

654
00:35:42 --> 00:35:48
So all the options are
mathematically identical.

655
00:35:48 --> 00:35:52
Now, how many people have
Version One, the 400 hundred

656
00:35:52 --> 00:35:55
people will die thing?
 

657
00:35:55 --> 00:35:58
OK, how many people have the
200 people will die thing?

658
00:35:58 --> 00:35:59
OK, good.
 

659
00:35:59 --> 00:36:02
So, it's roughly
evenly divided.

660
00:36:02 --> 00:36:05
Now, what I want to do is, I
want to get the A and B for

661
00:36:05 --> 00:36:06
these two different versions.
 

662
00:36:06 --> 00:36:10
If you have the 400 people
will die version, how many

663
00:36:10 --> 00:36:14
people opted for Option A?
 

664
00:36:14 --> 00:36:17
How many people
opted for Option B?

665
00:36:17 --> 00:36:22
OK, so B beats A, and actually
it beat A big time here.

666
00:36:22 --> 00:36:26
If you have the 200 people
will be saved version, how

667
00:36:26 --> 00:36:29
many people went for A?
 

668
00:36:29 --> 00:36:31
How many people went for B?
 

669
00:36:31 --> 00:36:35
OK, in that case A beats B
not quite as massively.

670
00:36:35 --> 00:36:40
But, clearly this manipulation
has flipped the answer.

671
00:36:40 --> 00:36:44
It ain't math that's doing it,
because the math is the same.

672
00:36:44 --> 00:36:48
What's changing here is what
Kahneman and Tversky called

673
00:36:48 --> 00:36:51
the framing of the problem.
 

674
00:36:51 --> 00:36:53
But really it's the narrative
around the problem.

675
00:36:53 --> 00:36:57
What happens is,
you read Option A.

676
00:36:57 --> 00:36:59
400 people will die.
 

677
00:36:59 --> 00:37:01
That's terrible.
 

678
00:37:01 --> 00:37:04
Well, the second possibility in
this case, the B is, woah,

679
00:37:04 --> 00:37:07
there's something we could do.
 

680
00:37:07 --> 00:37:11
That probabilistically might
save some of those people.

681
00:37:11 --> 00:37:12
That sounds OK.
 

682
00:37:12 --> 00:37:13
I'll opt for this.
 

683
00:37:13 --> 00:37:18
As opposed to this one,
200 people will be saved.

684
00:37:18 --> 00:37:20
Well, that's better
than nothing.

685
00:37:20 --> 00:37:23
And the alternative, B, here.
 

686
00:37:23 --> 00:37:27
You read the part that
says, everybody might die.

687
00:37:27 --> 00:37:30
Hm, doesn't sound too good.
 

688
00:37:30 --> 00:37:32
And so you flip it.
 

689
00:37:32 --> 00:37:34
So what you're doing here is
the way that you ask the

690
00:37:34 --> 00:37:38
question changes the
answer you get.

691
00:37:38 --> 00:37:42
Well, we can go back to the
politics game here just fine.

692
00:37:42 --> 00:37:44
Pollsters do this all the time.
 

693
00:37:44 --> 00:37:47
The neutral pollsters
desperately try not to do it.

694
00:37:47 --> 00:37:50
But campaign polls, when
they're trying to get the

695
00:37:50 --> 00:37:57
correct answer out, who would
you rather have for President,

696
00:37:57 --> 00:38:03
a strong leader who has ruled
the free world with a firm

697
00:38:03 --> 00:38:08
hand for four years, or a
flip-flopping senator from a

698
00:38:08 --> 00:38:11
liberal East Coast state?
 

699
00:38:11 --> 00:38:14
Or you could ask the question,
who would you like to have as

700
00:38:14 --> 00:38:19
your President for the next
four years, a guy who barely

701
00:38:19 --> 00:38:21
made it out of Yale and has
driven the country into a

702
00:38:21 --> 00:38:26
ditch, or a guy who actually
got decent grades at Yale and

703
00:38:26 --> 00:38:30
is a national war hero and
has really good hair?

704
00:38:30 --> 00:38:34
 
 

705
00:38:34 --> 00:38:40
You can move the question
around, by the way, and

706
00:38:40 --> 00:38:42
not just in polling.
 

707
00:38:42 --> 00:38:44
But the way you frame issues.
 

708
00:38:44 --> 00:38:47
I mean, talk about
framing issues.

709
00:38:47 --> 00:38:55
If you frame the war in Iraq as
part of the War on Terror, and

710
00:38:55 --> 00:38:58
an integral part of the War on
Terror asking whether you want

711
00:38:58 --> 00:39:01
to keep doing that, is
different than taking the same

712
00:39:01 --> 00:39:07
thing and calling it a
distraction from the War

713
00:39:07 --> 00:39:09
on Terror, for example.
 

714
00:39:09 --> 00:39:12
The facts remain
sort of similar.

715
00:39:12 --> 00:39:14
It's harder to do the
experiment in a nice, clean,

716
00:39:14 --> 00:39:18
controlled -- here, we can make
all the math come out right.

717
00:39:18 --> 00:39:21
In a political debate, there's
an argument about the actual

718
00:39:21 --> 00:39:22
facts, too, of course.
 

719
00:39:22 --> 00:39:25
But the way that you present
the facts, the way you frame

720
00:39:25 --> 00:39:30
the facts, is designed to get
the answer that you want

721
00:39:30 --> 00:39:33
out of those facts.
 

722
00:39:33 --> 00:39:36
There's a beautiful round of
this in today's paper, because

723
00:39:36 --> 00:39:39
there's the report of the
whatever they're called, Iraq

724
00:39:39 --> 00:39:42
Commission that was looking for
weapons and didn't

725
00:39:42 --> 00:39:43
find any weapons.
 

726
00:39:43 --> 00:39:47
And if you read the Kerry camp
statements about what that

727
00:39:47 --> 00:39:50
means, and what the Bush
camp says it means.

728
00:39:50 --> 00:39:54
The Bush camp is very big
on intention this morning.

729
00:39:54 --> 00:39:59
He really, the report that
Saddam wanted weapons.

730
00:39:59 --> 00:40:03
And so the Bush camp is busy
saying, he wanted weapons and

731
00:40:03 --> 00:40:05
he would have gotten them
as soon as he could.

732
00:40:05 --> 00:40:08
And the Kerry camp is looking
at the same facts and

733
00:40:08 --> 00:40:11
saying, we all want stuff.
 

734
00:40:11 --> 00:40:14
Man, he didn't have anything.
 

735
00:40:14 --> 00:40:14
So.
 

736
00:40:14 --> 00:40:16
Anyway, you can have
lots of fun Friday.

737
00:40:16 --> 00:40:19
You can watch them
do this one, too.

738
00:40:19 --> 00:40:21
All right, so.
 

739
00:40:21 --> 00:40:25
The narrative that you
put around a story.

740
00:40:25 --> 00:40:27
The narrative that you put
around the story does not

741
00:40:27 --> 00:40:29
influence your computer.
 

742
00:40:29 --> 00:40:34
You can tell your computer
about 1/3 times 600 plus

743
00:40:34 --> 00:40:37
2/3 times minus 600,
any way you like.

744
00:40:37 --> 00:40:40
You can use big font, you can
use little font, you can use

745
00:40:40 --> 00:40:43
you want, it's going to come
out with the same answer.

746
00:40:43 --> 00:40:45
Not true of the way that you
think about these things.

747
00:40:45 --> 00:40:51
You're going to get different
answers depending on how

748
00:40:51 --> 00:40:53
you frame the question.
 

749
00:40:53 --> 00:40:56
Let me do a different example.
 

750
00:40:56 --> 00:40:58
That's not my handout, I
got to see what's actually

751
00:40:58 --> 00:41:00
on the handout here.
 

752
00:41:00 --> 00:41:03
So, right underneath that,
underneath the bit about

753
00:41:03 --> 00:41:05
framing, there's another demo.
 

754
00:41:05 --> 00:41:09
I will, in advance, promise
you that this is the same

755
00:41:09 --> 00:41:12
on all the handouts.
 

756
00:41:12 --> 00:41:14
Because otherwise you're going
to sit there trying to figure

757
00:41:14 --> 00:41:16
out what's the weird thing.
 

758
00:41:16 --> 00:41:18
So, this is the same
on all the handouts.

759
00:41:18 --> 00:41:22
I'm also aware that I have six
slots between most probable

760
00:41:22 --> 00:41:23
and least probable.
 

761
00:41:23 --> 00:41:27
And only five options
to go in there.

762
00:41:27 --> 00:41:29
Deal with it.
 

763
00:41:29 --> 00:41:31
Here's what you've got to do.
 

764
00:41:31 --> 00:41:32
You've got this guy Henry.
 

765
00:41:32 --> 00:41:34
Henry is a short, slim man.
 

766
00:41:34 --> 00:41:35
He likes to read poetry.
 

767
00:41:35 --> 00:41:36
He's been active in
environmental and

768
00:41:36 --> 00:41:37
feminist causes.
 

769
00:41:37 --> 00:41:41
And Henry is a what?
 

770
00:41:41 --> 00:41:44
And so there are these
five options here.

771
00:41:44 --> 00:41:47
What you want to do is
rank order them from most

772
00:41:47 --> 00:41:49
likely to least likely.
 

773
00:41:49 --> 00:41:53
 
 

774
00:41:53 --> 00:41:56
And then we'll talk
about that a bit.

775
00:41:56 --> 00:42:07
Let's see how that -- No, you
can't copy off your neighbor.

776
00:42:07 --> 00:42:08
You know Henry.
 

777
00:42:08 --> 00:42:16
 
 

778
00:42:16 --> 00:42:17
I should have brought Henry in.
 

779
00:42:17 --> 00:42:21
Just grabbed some character
and said, this is Henry.

780
00:42:21 --> 00:42:21
All right.
 

781
00:42:21 --> 00:42:24
How many people need
more time for this?

782
00:42:24 --> 00:42:26
OK, a couple of people
still working on it.

783
00:42:26 --> 00:42:35
 
 

784
00:42:35 --> 00:42:36
All right.
 

785
00:42:36 --> 00:42:42
Well, we'll just go up.
 

786
00:42:42 --> 00:42:47
Let's look at a couple of
particular comparisons on here.

787
00:42:47 --> 00:42:50
 
 

788
00:42:50 --> 00:42:55
How many people put down,
let's look at A and B.

789
00:42:55 --> 00:42:56
Whoops.
 

790
00:42:56 --> 00:43:00
That looks
propositional thought.

791
00:43:00 --> 00:43:04
 
 

792
00:43:04 --> 00:43:06
All gone.
 

793
00:43:06 --> 00:43:09
OK, let's look at A
and B, for example.

794
00:43:09 --> 00:43:14
How many people said A was
more probable than B?

795
00:43:14 --> 00:43:17
How many people said B was
more probable than A?

796
00:43:17 --> 00:43:18
All right.
 

797
00:43:18 --> 00:43:19
B wins.
 

798
00:43:19 --> 00:43:27
And if I've got the right
one here, so ivy beats

799
00:43:27 --> 00:43:31
truck big-time, right?
 

800
00:43:31 --> 00:43:33
All right.
 

801
00:43:33 --> 00:43:38
How many truck drivers are
there in the country?

802
00:43:38 --> 00:43:38
AUDIENCE: A lot.
 

803
00:43:38 --> 00:43:39
PROFESSOR: A lot.
 

804
00:43:39 --> 00:43:44
10 to the -- I don't know
the answer to this, but?

805
00:43:44 --> 00:43:45
5?
 

806
00:43:45 --> 00:43:48
4 is only a thousand.
 

807
00:43:48 --> 00:43:48
AUDIENCE: 5.
 

808
00:43:48 --> 00:43:53
PROFESSOR: 5 is probably-- it
may be 6, but let's go with 5.

809
00:43:53 --> 00:43:55
OK.
 

810
00:43:55 --> 00:43:58
How many Ivy League classics
professors are there

811
00:43:58 --> 00:43:58
in the country?
 

812
00:43:58 --> 00:44:04
 
 

813
00:44:04 --> 00:44:07
How many Ivy League
schools are there?

814
00:44:07 --> 00:44:07
AUDIENCE: Eight.
 

815
00:44:07 --> 00:44:09
PROFESSOR: Eight to ten,
something like that.

816
00:44:09 --> 00:44:11
Maybe, if you're lucky,
there's ten classics

817
00:44:11 --> 00:44:13
professors at each.
 

818
00:44:13 --> 00:44:16
So that's going to get
you to 10 to the 2nd.

819
00:44:16 --> 00:44:21
Is it a thousand times more
likely that some random guy

820
00:44:21 --> 00:44:26
that you meet, who happens
to be -- well, let's phrase

821
00:44:26 --> 00:44:26
this a different way.
 

822
00:44:26 --> 00:44:30
If you took all, whatever this
is, a hundred thousand truck

823
00:44:30 --> 00:44:31
drivers in the country.
 

824
00:44:31 --> 00:44:32
I'm sure there are
more than that.

825
00:44:32 --> 00:44:34
But let's just suppose there
are only a hundred thousand

826
00:44:34 --> 00:44:36
truck drivers in the country.
 

827
00:44:36 --> 00:44:42
Are there fewer than a hundred
of them who might be,

828
00:44:42 --> 00:44:46
for instance, short,
poetry-loving feminists?

829
00:44:46 --> 00:44:49
 
 

830
00:44:49 --> 00:44:54
And that assumes that every one
of the Ivy League professors is

831
00:44:54 --> 00:44:59
a short -- there's no Ivy
Leagues professors who are

832
00:44:59 --> 00:45:02
big, burly guys with
tattoos and stuff.

833
00:45:02 --> 00:45:04
Or women, for that matter.
 

834
00:45:04 --> 00:45:09
Probably half of them are
women, so this is already --

835
00:45:09 --> 00:45:11
What you've done here is
you've ignored what's

836
00:45:11 --> 00:45:13
known as the base rate.
 

837
00:45:13 --> 00:45:20
All else being equal, it's more
likely that, if I just say,

838
00:45:20 --> 00:45:24
here's Henry, is it more likely
that Henry is a truck

839
00:45:24 --> 00:45:26
driver or an Ivy League
classics professor?

840
00:45:26 --> 00:45:28
It's way, way, way more likely
that he's a truck driver.

841
00:45:28 --> 00:45:32
In fact, it's so much more
likely that he's a truck driver

842
00:45:32 --> 00:45:34
that it's probably more likely.
 

843
00:45:34 --> 00:45:35
It's almost undoubtedly
more likely.

844
00:45:35 --> 00:45:39
Even for this description
of Henry the amazing

845
00:45:39 --> 00:45:42
feminist, whatever he was.
 

846
00:45:42 --> 00:45:45
You see, this is what's known
as the base rate fallacy.

847
00:45:45 --> 00:45:50
People do not tend to take into
account the base rate in the

848
00:45:50 --> 00:45:53
population when they're
thinking about this.

849
00:45:53 --> 00:45:56
I'm asking how likely it
is, and you're answering

850
00:45:56 --> 00:45:58
a different question.
 

851
00:45:58 --> 00:46:04
You're answering the question,
how typical is Henry of your

852
00:46:04 --> 00:46:10
image, of your stereotype, of
a truck driver versus of

853
00:46:10 --> 00:46:14
an Ivy League professor?
 

854
00:46:14 --> 00:46:17
And realize that even if
-- let's suppose your

855
00:46:17 --> 00:46:19
stereotype is right.
 

856
00:46:19 --> 00:46:23
What's the chance that Henry
could be a truck driver?

857
00:46:23 --> 00:46:25
Maybe it's only 1 in 100.
 

858
00:46:25 --> 00:46:27
All right, if it's 1 in
100, there's still 10

859
00:46:27 --> 00:46:29
to the 3rd of them.
 

860
00:46:29 --> 00:46:32
And we're still swamping
the population here.

861
00:46:32 --> 00:46:37
So, failing to take into
account the base rate is

862
00:46:37 --> 00:46:40
going to lead you to the
wrong answer there's.

863
00:46:40 --> 00:46:44
Let's look at another
piece of these data.

864
00:46:44 --> 00:46:52
Let's look at --
 

865
00:46:52 --> 00:46:55
Is it more likely, A and E.
 

866
00:46:55 --> 00:46:59
 
 

867
00:46:59 --> 00:47:00
A, E.
 

868
00:47:00 --> 00:47:05
How many people said that
A was more likely than E?

869
00:47:05 --> 00:47:07
How many people said that
E was more likely than A?

870
00:47:07 --> 00:47:09
Yeah, well this is this
is Kahneman got the

871
00:47:09 --> 00:47:10
Nobel Prize, you see.
 

872
00:47:10 --> 00:47:12
Because he said that, too.
 

873
00:47:12 --> 00:47:13
Well, he didn't say that.
 

874
00:47:13 --> 00:47:14
He said, that's what
you would say.

875
00:47:14 --> 00:47:15
Or he found, anyway.
 

876
00:47:15 --> 00:47:19
So E is much more likely.
 

877
00:47:19 --> 00:47:24
Remember, like, third
grade set theory stuff?

878
00:47:24 --> 00:47:29
Let's make the little
picture here.

879
00:47:29 --> 00:47:30
All right.
 

880
00:47:30 --> 00:47:34
The set of all truck
drivers, right?

881
00:47:34 --> 00:47:36
The set of, I can't even
remember what E is.

882
00:47:36 --> 00:47:43
E's like Mensa Ivy League
truck drivers or something.

883
00:47:43 --> 00:47:51
It's some small little
set, it's a subset of A.

884
00:47:51 --> 00:47:57
So, for starters it's a little
unlikely that the chance of

885
00:47:57 --> 00:48:00
being in here could be greater
than the chance of

886
00:48:00 --> 00:48:04
being in here.
 

887
00:48:04 --> 00:48:06
The best it's going
to be is equal.

888
00:48:06 --> 00:48:06
In some fashion.
 

889
00:48:06 --> 00:48:09
 
 

890
00:48:09 --> 00:48:10
Here's Henry.
 

891
00:48:10 --> 00:48:13
I'm meeting some random Henry.
 

892
00:48:13 --> 00:48:16
What's the chance
that he's in E?

893
00:48:16 --> 00:48:22
It's just not -- Again, you're
answering a question that says

894
00:48:22 --> 00:48:26
something about typical
rather than probable.

895
00:48:26 --> 00:48:30
And you're ignoring what you
would perfectly well know,

896
00:48:30 --> 00:48:34
of course, if we drew it
out in set theory terms.

897
00:48:34 --> 00:48:40
You can't be more likely that
something is in here, if the

898
00:48:40 --> 00:48:43
larger set includes that.
 

899
00:48:43 --> 00:48:45
That doesn't work really well.
 

900
00:48:45 --> 00:48:52
So, people are very
bad at base rates.

901
00:48:52 --> 00:48:57
Is this a bad -- is the purpose
of this lecture to say

902
00:48:57 --> 00:48:59
we're all morons here?
 

903
00:48:59 --> 00:49:00
No, not really.
 

904
00:49:00 --> 00:49:05
The faults we have, the
problems that we have in the

905
00:49:05 --> 00:49:11
way that we reason about
problems like this, reflect --

906
00:49:11 --> 00:49:16
What Kahneman and Tversky
talked about were, if I'm

907
00:49:16 --> 00:49:19
spelling it right, heuristics.
 

908
00:49:19 --> 00:49:24
Sort of, mental shortcuts
that do a certain

909
00:49:24 --> 00:49:28
amount of work for us.
 

910
00:49:28 --> 00:49:31
Our job isn't actually out
there in the world to figure

911
00:49:31 --> 00:49:38
out set theory problems.
 

912
00:49:38 --> 00:49:42
Our problem is to get
home safely at night.

913
00:49:42 --> 00:49:45
So, you're walking down
the street at night.

914
00:49:45 --> 00:49:49
And it's a dark kind of street.
 

915
00:49:49 --> 00:49:53
And you see this big
guy with a stick.

916
00:49:53 --> 00:49:57
And he's walking down the same
side of the street as you.

917
00:49:57 --> 00:49:59
Do you cross the street?
 

918
00:49:59 --> 00:50:02
Well, if you're in a Kahneman
and Tversky kind of experiment

919
00:50:02 --> 00:50:05
you say, I know what I'm
supposed to do here.

920
00:50:05 --> 00:50:08
I'm supposed to think hard
about the base rates.

921
00:50:08 --> 00:50:11
Is it more likely, how many
people are there in the world

922
00:50:11 --> 00:50:15
who are bad, evil, people with
sticks that are going

923
00:50:15 --> 00:50:18
to, like, hit me.
 

924
00:50:18 --> 00:50:21
Versus, how many people are
there who are, like, guys

925
00:50:21 --> 00:50:24
coming home from a softball
game with their bat, or

926
00:50:24 --> 00:50:26
something like that?
 

927
00:50:26 --> 00:50:29
And they're nice
people, mostly.

928
00:50:29 --> 00:50:30
Or at least even if they're
bad, evil people, they're

929
00:50:30 --> 00:50:33
not really going to hit me.
 

930
00:50:33 --> 00:50:36
The answer is, there's a very
small population of people

931
00:50:36 --> 00:50:41
out there with sticks
wanting to hit you.

932
00:50:41 --> 00:50:44
But, in the occasion that
you get that wrong, it's a

933
00:50:44 --> 00:50:47
really bad mistake to make.
 

934
00:50:47 --> 00:50:54
And so, a mental shortcut that
says not, what's the base rate

935
00:50:54 --> 00:50:59
here, but is this typical of a
situation that could be

936
00:50:59 --> 00:51:02
dangerous, sort of,
flipping it around.

937
00:51:02 --> 00:51:04
Could this be a bad thing.
 

938
00:51:04 --> 00:51:08
If I make a mistake and he's a
really a nice person, so you

939
00:51:08 --> 00:51:09
might feel a little hurt that I
crossed to the other

940
00:51:09 --> 00:51:10
side of the street.
 

941
00:51:10 --> 00:51:11
What's the big deal.
 

942
00:51:11 --> 00:51:14
If he's a guy with a bat and a
nail in it and he's going to

943
00:51:14 --> 00:51:15
poke me in the head, and
terrible things are

944
00:51:15 --> 00:51:17
going to happen.
 

945
00:51:17 --> 00:51:19
I'm not going to worry
about the base rates here.

946
00:51:19 --> 00:51:27
And your cognitive hardware
seems to have been set up under

947
00:51:27 --> 00:51:32
those sort of constraints more
than it was set up under what

948
00:51:32 --> 00:51:38
might be optimal public
policy solving constraints.

949
00:51:38 --> 00:51:45
So, this is why you can then
run a political campaign where

950
00:51:45 --> 00:51:49
one side or the other spends a
lot of time conjuring up images

951
00:51:49 --> 00:51:53
of bad guys with sticks,
of some variety or other.

952
00:51:53 --> 00:51:56
Because you hear about
the bad guy with the

953
00:51:56 --> 00:51:58
stick often enough.
 

954
00:51:58 --> 00:52:01
And if this guy promises he's
going to save you from the bad

955
00:52:01 --> 00:52:03
guy with the stick, well,
you'd better go with that.

956
00:52:03 --> 00:52:06
At least, that's an
appealing thought.

957
00:52:06 --> 00:52:07
And it works for them.
 

958
00:52:07 --> 00:52:15
It works as a
political technique.

959
00:52:15 --> 00:52:19
All right, this is all -- who
knows about Henry the, some

960
00:52:19 --> 00:52:21
weird truck driver that I
invented or something

961
00:52:21 --> 00:52:22
like that?
 

962
00:52:22 --> 00:52:29
How about a realm where
things really ought to be

963
00:52:29 --> 00:52:32
logical and mathematical.
 

964
00:52:32 --> 00:52:34
Which would be money.
 

965
00:52:34 --> 00:52:36
I mean, if there was ever going
to be something that ought to

966
00:52:36 --> 00:52:40
just work out in terms of the
math, you would think

967
00:52:40 --> 00:52:41
it would be money.
 

968
00:52:41 --> 00:52:45
And this is, of course, the
reason why there is no

969
00:52:45 --> 00:52:47
Nobel Prize in psychology.
 

970
00:52:47 --> 00:52:49
But there is a Nobel
Prize in economics.

971
00:52:49 --> 00:52:53
And that's what
Kahneman, in fact, won.

972
00:52:53 --> 00:52:57
Kaheman and Tversky would
have won it, but Tversky,

973
00:52:57 --> 00:52:59
unfortunately, died young.
 

974
00:52:59 --> 00:53:03
And you don't win the
Nobel Prize posthumously.

975
00:53:03 --> 00:53:06
So, Kahneman and Tversky
also looked at the way that

976
00:53:06 --> 00:53:08
people reason about money.
 

977
00:53:08 --> 00:53:11
This is something that
goes back long before

978
00:53:11 --> 00:53:13
Kahneman and Tversky.
 

979
00:53:13 --> 00:53:17
And, I can perhaps illustrate
one of the older bits

980
00:53:17 --> 00:53:18
with an example.
 

981
00:53:18 --> 00:53:22
Let's suppose that you're
going to buy a toy

982
00:53:22 --> 00:53:27
car for your cousin.
 

983
00:53:27 --> 00:53:30
You like your cousin, just in
case you were wondering here.

984
00:53:30 --> 00:53:32
So you're buying this toy
car for your cousin.

985
00:53:32 --> 00:53:41
And there are two toy cars
out there at the moment.

986
00:53:41 --> 00:53:46
They are, for present purposes,
functionally equivalent

987
00:53:46 --> 00:53:51
on all dimensions of kid
loveliness or something.

988
00:53:51 --> 00:54:01
There's the midnight blue, I
don't know, Model PT roadster

989
00:54:01 --> 00:54:06
or something from Toys R Us.
 

990
00:54:06 --> 00:54:17
And the midnight green Model
PT roadster from Kids

991
00:54:17 --> 00:54:18
Are Toys, or something.
 

992
00:54:18 --> 00:54:27
 
 

993
00:54:27 --> 00:54:30
So, the difference
is the color.

994
00:54:30 --> 00:54:32
The green color is cooler.
 

995
00:54:32 --> 00:54:38
You know your cousin actually
likes the green one better.

996
00:54:38 --> 00:54:39
It's a bit cooler.
 

997
00:54:39 --> 00:54:46
Anyway, the midnight blue one
costs $12 and the midnight

998
00:54:46 --> 00:54:49
green one costs $40.
 

999
00:54:49 --> 00:54:53
How many people are going
to buy one of them.

1000
00:54:53 --> 00:54:54
You have to pick one, again.
 

1001
00:54:54 --> 00:54:58
How many pick the blue one?
 

1002
00:54:58 --> 00:55:00
How many people pick
the green one?

1003
00:55:00 --> 00:55:00
OK.
 

1004
00:55:00 --> 00:55:03
You guys have nice
cousins, that's good.

1005
00:55:03 --> 00:55:08
All right, the blue
one wins big-time.

1006
00:55:08 --> 00:55:09
All right.
 

1007
00:55:09 --> 00:55:14
You now have magically
graduated from MIT.

1008
00:55:14 --> 00:55:16
Congratulations.
 

1009
00:55:16 --> 00:55:18
You got the job.
 

1010
00:55:18 --> 00:55:22
You're now going
to buy the car.

1011
00:55:22 --> 00:55:24
And so, you're going to buy
-- you're going to buy

1012
00:55:24 --> 00:55:27
a real PT roadster.
 

1013
00:55:27 --> 00:55:31
and And they come in
two different colors.

1014
00:55:31 --> 00:55:34
Amazingly, for today's
purposes, they come in midnight

1015
00:55:34 --> 00:55:35
blue and midnight green.
 

1016
00:55:35 --> 00:55:41
The midnight blue
one costs $30,012.

1017
00:55:41 --> 00:55:48
The midnight green
one costs $30,040.

1018
00:55:48 --> 00:55:50
You like the green one better.
 

1019
00:55:50 --> 00:55:54
How many people are going
to buy the green one?

1020
00:55:54 --> 00:55:58
OK, so you graduated from
MIT with this brain.

1021
00:55:58 --> 00:56:00
It's the same.
 

1022
00:56:00 --> 00:56:02
So, obviously it's now
going very much in

1023
00:56:02 --> 00:56:02
the other direction.
 

1024
00:56:02 --> 00:56:06
It's the same $28.
 

1025
00:56:06 --> 00:56:08
What's your problem here?
 

1026
00:56:08 --> 00:56:13
AUDIENCE: [UNINTELLIGIBLE]
 

1027
00:56:13 --> 00:56:15
PROFESSOR: You were going to --
 

1028
00:56:15 --> 00:56:18
AUDIENCE: You're already paying
over $30,000 so the $20

1029
00:56:18 --> 00:56:21
just seems very trivial.
 

1030
00:56:21 --> 00:56:22
PROFESSOR: Yeah.
 

1031
00:56:22 --> 00:56:23
It's the same $20.
 

1032
00:56:23 --> 00:56:25
This is absolutely
right, of course.

1033
00:56:25 --> 00:56:27
But yeah.
 

1034
00:56:27 --> 00:56:35
AUDIENCE: [INAUDIBLE]
 

1035
00:56:35 --> 00:56:36
PROFESSOR: The toy car is
probably going to last you

1036
00:56:36 --> 00:56:38
least as long as the roadster.
 

1037
00:56:38 --> 00:56:42
 
 

1038
00:56:42 --> 00:56:45
I keep trying to fine-tune this
to make this, you're not going

1039
00:56:45 --> 00:56:49
to buy 15 cars here, and --
We're assuming a one-time

1040
00:56:49 --> 00:56:52
purchase here, a
one-time purchase here.

1041
00:56:52 --> 00:56:57
And they're each going to
last three years, or 100,000

1042
00:56:57 --> 00:56:59
miles, or whichever comes
first, or something.

1043
00:56:59 --> 00:56:59
Yeah.
 

1044
00:56:59 --> 00:57:04
AUDIENCE: [UNINTELLIGIBLE]
 

1045
00:57:04 --> 00:57:07
PROFESSOR: That
much more money?

1046
00:57:07 --> 00:57:08
All right, all right.
 

1047
00:57:08 --> 00:57:11
Would your answer have
changed radically if I

1048
00:57:11 --> 00:57:14
say, congratulations you
have just graduated.

1049
00:57:14 --> 00:57:17
It's also your
cousin's birthday.

1050
00:57:17 --> 00:57:21
You'd all of a sudden be --
Boy, there must me sort

1051
00:57:21 --> 00:57:23
of a U-shaped function.
 

1052
00:57:23 --> 00:57:28
Because by the time you get to
a parent or something, ask your

1053
00:57:28 --> 00:57:31
parents, who graduated
a while ago.

1054
00:57:31 --> 00:57:35
Hey, Mommy, can I have the
midnight green car, it only

1055
00:57:35 --> 00:57:39
costs three times what
the other one cost.

1056
00:57:39 --> 00:57:42
Oh, actually I just sort
of gave away the answer.

1057
00:57:42 --> 00:57:44
It's the three times issue.
 

1058
00:57:44 --> 00:57:46
And you were alluding
to this already.

1059
00:57:46 --> 00:57:51
That money is perceived, even
though it's obviously the same

1060
00:57:51 --> 00:57:56
$28, that people think about
money in ratio terms and

1061
00:57:56 --> 00:57:57
not in absolute terms.
 

1062
00:57:57 --> 00:58:01
This was actually picked
up by Bernoulli a couple

1063
00:58:01 --> 00:58:03
of hundred years ago.
 

1064
00:58:03 --> 00:58:04
I don't know which Bernoulli.
 

1065
00:58:04 --> 00:58:08
It turns out that there were
buckets of Bernoullis.

1066
00:58:08 --> 00:58:13
Five Bernoullis, and
they were all geniuses.

1067
00:58:13 --> 00:58:24
So, but one of the Bernoullis
basically asked, what is the

1068
00:58:24 --> 00:58:26
psychological value of money?
 

1069
00:58:26 --> 00:58:29
Actually, in Kahneman and
Tversky language, that's often

1070
00:58:29 --> 00:58:34
called the utility of money as
a function of actual money.

1071
00:58:34 --> 00:58:41
And what Bernoulli asserted was
that the utility of money, the

1072
00:58:41 --> 00:58:45
psychological value of the
money, basically went

1073
00:58:45 --> 00:58:49
up with log money.
 

1074
00:58:49 --> 00:58:56
So it's basically preserving
ratios and not the absolute.

1075
00:58:56 --> 00:59:01
It's not preserving
the absolute values.

1076
00:59:01 --> 00:59:06
Which, by the way, when you go
out to buy that first car when

1077
00:59:06 --> 00:59:08
you have all this money that
you're apparently going to

1078
00:59:08 --> 00:59:10
have when you graduate.
 

1079
00:59:10 --> 00:59:16
The guy selling you the
car knows all about this.

1080
00:59:16 --> 00:59:24
And he knows that, do you want
the plain vanilla one, or do

1081
00:59:24 --> 00:59:28
you want the PT roadster
with the really cool

1082
00:59:28 --> 00:59:30
racing stripe on it.
 

1083
00:59:30 --> 00:59:37
And this is part of our cool
new graduate package, of stuff

1084
00:59:37 --> 00:59:42
that cost us $3.95 and we'll
crank the price up by $500

1085
00:59:42 --> 00:59:46
here, because on $30,000,
who can tell, right?

1086
00:59:46 --> 00:59:48
And he knows.
 

1087
00:59:48 --> 00:59:51
He knows that it's the
same $28 here and here.

1088
00:59:51 --> 00:59:54
And his job is to get as
many of those $28 out

1089
00:59:54 --> 00:59:57
of you as possible.
 

1090
00:59:57 --> 01:00:01
And will take advantage
of exactly this fact.

1091
01:00:01 --> 01:00:05
If somebody said, I'll put a
little stripe on your toy car

1092
01:00:05 --> 01:00:09
for an extra $28, well
it's a smaller car.

1093
01:00:09 --> 01:00:13
I'll put two stripes on it
because it's a little car.

1094
01:00:13 --> 01:00:15
You'll say, get away from me.
 

1095
01:00:15 --> 01:00:20
But just listen to yourself go
for the options when you go off

1096
01:00:20 --> 01:00:25
and buy that car eventually.
 

1097
01:00:25 --> 01:00:27
All right.
 

1098
01:00:27 --> 01:00:30
let us take a momentary break.
 

1099
01:00:30 --> 01:00:33
And then I will say a word
more about the oddities

1100
01:00:33 --> 01:00:35
of money here.
 

1101
01:00:35 --> 01:01:42
[RANDOM NOISE]
 

1102
01:01:42 --> 01:01:43
OK.
 

1103
01:01:43 --> 01:01:47
 
 

1104
01:01:47 --> 01:02:04
Let us contemplate a little
more of the way in which the

1105
01:02:04 --> 01:02:06
-- now, the reason
this is important.

1106
01:02:06 --> 01:02:10
The reason that this is the
sort of stuff that eventually

1107
01:02:10 --> 01:02:14
gets -- that where working this
out is worth the attention

1108
01:02:14 --> 01:02:22
of the Nobel committee in
economics is that economists,

1109
01:02:22 --> 01:02:26
for a long time, had this
notion, had a psychological

1110
01:02:26 --> 01:02:30
theory, in effect, of what
people were doing economically.

1111
01:02:30 --> 01:02:35
And it was the notion of a
rational consumer, a rational

1112
01:02:35 --> 01:02:38
person who's making decisions
that were in a sense

1113
01:02:38 --> 01:02:40
propositional.
 

1114
01:02:40 --> 01:02:44
That they were doing the math
to the best of their ability,

1115
01:02:44 --> 01:02:46
and working things
out on that basis.

1116
01:02:46 --> 01:02:50
And this the Kahneman and
Tversky program of research

1117
01:02:50 --> 01:02:54
tells you is, the very
interesting constraints

1118
01:02:54 --> 01:02:56
on that rationality.
 

1119
01:02:56 --> 01:03:00
The ways in which it sort of
systematically deviates from

1120
01:03:00 --> 01:03:03
what your rational computer
might think about the

1121
01:03:03 --> 01:03:04
same kind of question.
 

1122
01:03:04 --> 01:03:07
So, consider the following.
 

1123
01:03:07 --> 01:03:13
Let's think about doing
some gambling games.

1124
01:03:13 --> 01:03:15
Where this is a one-shot game.
 

1125
01:03:15 --> 01:03:16
You only get a chance to
play this game once.

1126
01:03:16 --> 01:03:17
I got a coin.
 

1127
01:03:17 --> 01:03:19
It's a fair coin.
 

1128
01:03:19 --> 01:03:28
If I flip it and it comes up
heads, I give you a penny.

1129
01:03:28 --> 01:03:31
If it comes up tails,
you give me a penny.

1130
01:03:31 --> 01:03:33
How many people are willing
to play that game?

1131
01:03:33 --> 01:03:38
 
 

1132
01:03:38 --> 01:03:39
Few people.
 

1133
01:03:39 --> 01:03:42
AUDIENCE: [UNINTELLIGIBLE]
 

1134
01:03:42 --> 01:03:42
PROFESSOR: No, no.
 

1135
01:03:42 --> 01:03:43
One shot.
 

1136
01:03:43 --> 01:03:45
One time.
 

1137
01:03:45 --> 01:03:49
The whole game is
this one penny.

1138
01:03:49 --> 01:03:54
How many people are willing to
play one round of this game?

1139
01:03:54 --> 01:03:58
AUDIENCE: Give me the
quarter instead.

1140
01:03:58 --> 01:04:00
PROFESSOR: Oh, she wants
to play for real money.

1141
01:04:00 --> 01:04:01
OK.
 

1142
01:04:01 --> 01:04:02
That's fine.
 

1143
01:04:02 --> 01:04:04
Let's do that instead.
 

1144
01:04:04 --> 01:04:06
So.
 

1145
01:04:06 --> 01:04:08
Most people are willing
to play that, though.

1146
01:04:08 --> 01:04:10
By this point in the lecture a
variety of people are starting

1147
01:04:10 --> 01:04:14
to wonder what is that I'm
revealing about myself

1148
01:04:14 --> 01:04:17
if I say I'll do this.
 

1149
01:04:17 --> 01:04:23
So, the way you write that, you
could write that as, you've got

1150
01:04:23 --> 01:04:30
a 0.5 chance of winning a penny
and a 0.5 chance of

1151
01:04:30 --> 01:04:32
losing a penny.
 

1152
01:04:32 --> 01:04:36
And the expected value of this
is 0.5 -- you can figure

1153
01:04:36 --> 01:04:37
that out, it's 0.
 

1154
01:04:37 --> 01:04:39
It's a fair game.
 

1155
01:04:39 --> 01:04:42
OK, let's play it again.
 

1156
01:04:42 --> 01:04:44
We'll flip, we'll play for her.
 

1157
01:04:44 --> 01:04:45
Flip the coin.
 

1158
01:04:45 --> 01:04:48
Heads, I give you $100.
 

1159
01:04:48 --> 01:04:51
Tails, you give me $100.
 

1160
01:04:51 --> 01:04:52
How many people want to play?
 

1161
01:04:52 --> 01:04:56
 
 

1162
01:04:56 --> 01:04:58
Your computer doesn't care
much about this, because

1163
01:04:58 --> 01:05:00
the expected value
remains 0, right?

1164
01:05:00 --> 01:05:04
What's the problem?
 

1165
01:05:04 --> 01:05:08
AUDIENCE: [UNINTELLIGIBLE]
 

1166
01:05:08 --> 01:05:10
PROFESSOR: People
are risk averse.

1167
01:05:10 --> 01:05:13
Yeah, that's certainly one.
 

1168
01:05:13 --> 01:05:15
That's what you find here.
 

1169
01:05:15 --> 01:05:20
But why don't you
once want to play?

1170
01:05:20 --> 01:05:26
AUDIENCE: [UNINTELLIGIBLE]
 

1171
01:05:26 --> 01:05:28
PROFESSOR: No, the expected
value is just a thing

1172
01:05:28 --> 01:05:31
defined in math land
or statistics land.

1173
01:05:31 --> 01:05:35
It's zero on this.
 

1174
01:05:35 --> 01:05:38
But the two, you could either
win or lose, it's true.

1175
01:05:38 --> 01:05:39
You're not going
to come out at 0.

1176
01:05:39 --> 01:05:43
But if we played it for
everybody in the classroom, if

1177
01:05:43 --> 01:05:45
I had a lot of $100 bills.
 

1178
01:05:45 --> 01:05:47
Gee, I wouldn't want
to play that game.

1179
01:05:47 --> 01:05:47
Yeah.
 

1180
01:05:47 --> 01:05:51
AUDIENCE: The expected utility
of playing is less than the

1181
01:05:51 --> 01:05:55
expected utility
of not playing.

1182
01:05:55 --> 01:05:55
PROFESSOR: Hmm.
 

1183
01:05:55 --> 01:05:57
That sounded cool.
 

1184
01:05:57 --> 01:06:00
 
 

1185
01:06:00 --> 01:06:02
But I'm not being quick
enough to figure this out.

1186
01:06:02 --> 01:06:04
What did you --
 

1187
01:06:04 --> 01:06:07
AUDIENCE: I'm saying that
winning $100 is less

1188
01:06:07 --> 01:06:07
good than losing $100.
 

1189
01:06:07 --> 01:06:07
PROFESSOR: Thank you.
 

1190
01:06:07 --> 01:06:11
That's exactly right.
 

1191
01:06:11 --> 01:06:14
It turns out that this
curve is not symmetrical

1192
01:06:14 --> 01:06:16
around the origin.
 

1193
01:06:16 --> 01:06:21
That negative money, the
utility of negative money,

1194
01:06:21 --> 01:06:25
tends to be somewhat steeper
and more linear than

1195
01:06:25 --> 01:06:28
positive money.
 

1196
01:06:28 --> 01:06:33
With the result that, exactly
what the gentleman said.

1197
01:06:33 --> 01:06:41
That losing, that the absolute
value of losing $100 on this

1198
01:06:41 --> 01:06:51
scale is greater than the
absolute value of gaining $100.

1199
01:06:51 --> 01:06:54
I mean, one way to intuit this
is, there's plenty of room

1200
01:06:54 --> 01:06:56
for you to gain $100.
 

1201
01:06:56 --> 01:06:58
You can do that all the time.
 

1202
01:06:58 --> 01:07:02
Do you have $100 handy that
you can afford to lose?

1203
01:07:02 --> 01:07:04
Maybe not.
 

1204
01:07:04 --> 01:07:06
It would just hurt a lot more.
 

1205
01:07:06 --> 01:07:11
And if we decide -- And now we
can -- There are some versions

1206
01:07:11 --> 01:07:13
of this -- it's not that you'll
never play a game for

1207
01:07:13 --> 01:07:14
this kind of stakes.
 

1208
01:07:14 --> 01:07:17
I mean, if I say
I'll flip a coin.

1209
01:07:17 --> 01:07:22
Heads, you give me a penny,
tails, I give you $100, how

1210
01:07:22 --> 01:07:24
many people want to play?
 

1211
01:07:24 --> 01:07:26
And the other ones didn't
figure out what I

1212
01:07:26 --> 01:07:29
said, presumably.
 

1213
01:07:29 --> 01:07:34
So, somewhere between that
extreme and the even game is

1214
01:07:34 --> 01:07:37
the point at which we would
measure how risk

1215
01:07:37 --> 01:07:38
averse you are.
 

1216
01:07:38 --> 01:07:40
It'd be different for
different people.

1217
01:07:40 --> 01:07:46
I mean, would you play if
heads, one time play, heads

1218
01:07:46 --> 01:07:48
you give me $50, tails
I give you $100?

1219
01:07:48 --> 01:07:52
Now, some people would
be willing to play.

1220
01:07:52 --> 01:07:53
He's good, he's got $50 handy.
 

1221
01:07:53 --> 01:07:55
Or do you have a question?
 

1222
01:07:55 --> 01:07:55
No, you were just --
 

1223
01:07:55 --> 01:07:56
AUDIENCE: [UNINTELLIGIBLE]
 

1224
01:07:56 --> 01:07:57
198.
 

1225
01:07:57 --> 01:07:58
PROFESSOR: 198, OK.
 

1226
01:07:58 --> 01:08:01
He's got -- talk to him later.
 

1227
01:08:01 --> 01:08:05
He's got more money than
he knows what to do with.

1228
01:08:05 --> 01:08:10
But, in any case, there will be
a balance point at which your

1229
01:08:10 --> 01:08:14
-- where the loss and
the gain balance out.

1230
01:08:14 --> 01:08:18
And then you'd be
presumably willing to pay.

1231
01:08:18 --> 01:08:23
Strange things happen
at the extremes.

1232
01:08:23 --> 01:08:28
And that's the basis for a
willingness to play lotteries.

1233
01:08:28 --> 01:08:39
So, if you have a wager that
says, heads you give me $1,

1234
01:08:39 --> 01:08:47
tails, I give you $10 million,
but the probability of coming

1235
01:08:47 --> 01:08:55
up tails equal 10 to the
minus, I don't know, 80

1236
01:08:55 --> 01:08:56
or something like that.
 

1237
01:08:56 --> 01:08:59
 
 

1238
01:08:59 --> 01:09:01
I'm not getting the right
numbers, particularly,

1239
01:09:01 --> 01:09:03
for a state lottery.
 

1240
01:09:03 --> 01:09:06
But it's not -- it's
that kind of thing.

1241
01:09:06 --> 01:09:08
The reason people run state
lotteries is because

1242
01:09:08 --> 01:09:09
they want your dollars.
 

1243
01:09:09 --> 01:09:14
Not because they're interested
in giving you a pile of money.

1244
01:09:14 --> 01:09:17
So, even so by the time you get
these very extreme kinds of

1245
01:09:17 --> 01:09:23
gambles, as long as the cost is
sort of minimal down here and

1246
01:09:23 --> 01:09:26
the potential payoff is huge,
for instance, you're willing to

1247
01:09:26 --> 01:09:30
pay radically unfair games that
you would never play in

1248
01:09:30 --> 01:09:37
the range of, in the
middle kind of range.

1249
01:09:37 --> 01:09:40
If I tell you, well all
right, we can make this

1250
01:09:40 --> 01:09:48
just a tenfold difference.
 

1251
01:09:48 --> 01:09:52
So if we play a game that --
you can figure this out.

1252
01:09:52 --> 01:09:54
At the extremes you're
willing to risk $1 to

1253
01:09:54 --> 01:09:55
make $100 million.
 

1254
01:09:55 --> 01:09:57
 
 

1255
01:09:57 --> 01:09:59
Even if the odds
are against you.

1256
01:09:59 --> 01:10:02
In a more moderate
game, you would not be

1257
01:10:02 --> 01:10:04
willing to do that.
 

1258
01:10:04 --> 01:10:07
And it's that sort of
reasoning, or lack thereof,

1259
01:10:07 --> 01:10:12
that allows lotteries
to make their money.

1260
01:10:12 --> 01:10:17
Even -- it's probably also
confounded there by the fact

1261
01:10:17 --> 01:10:20
it it's not clear that the
population as a whole really

1262
01:10:20 --> 01:10:23
understands that this
is a sucker's game.

1263
01:10:23 --> 01:10:26
That the state is actually in
the business of taking your

1264
01:10:26 --> 01:10:30
money from you, not just
redistributing the wealth.

1265
01:10:30 --> 01:10:35
But even people who perfectly
well understand that their

1266
01:10:35 --> 01:10:38
chances are less than even of
winning, even if they were to

1267
01:10:38 --> 01:10:42
buy 10 to the 80th tickets,
continue to buy lottery

1268
01:10:42 --> 01:10:43
tickets and continue to.
 

1269
01:10:43 --> 01:10:46
It's a strange way to raise
money for your state,

1270
01:10:46 --> 01:10:49
but never mind.
 

1271
01:10:49 --> 01:10:55
So, money is like other
aspects of reasoning.

1272
01:10:55 --> 01:11:00
Is subject to the sorts of
narrative stories that say, it

1273
01:11:00 --> 01:11:07
hurts more to lose $100 than
it feels good to gain $100.

1274
01:11:07 --> 01:11:12
The sort of narrative that
is beyond just the simple

1275
01:11:12 --> 01:11:15
math of the situation.
 

1276
01:11:15 --> 01:11:20
Let me -- OK, I have time
to do one more example.

1277
01:11:20 --> 01:11:23
Which is an interestingly
illustrative one.

1278
01:11:23 --> 01:11:24
And involves these cards.
 

1279
01:11:24 --> 01:11:26
Which I believe I
put on the handout.

1280
01:11:26 --> 01:11:30
This is known as the Wason
selection task, because good

1281
01:11:30 --> 01:11:32
old Wason developed it.
 

1282
01:11:32 --> 01:11:35
And, oh look, the rules
are properly described

1283
01:11:35 --> 01:11:36
on the handout.
 

1284
01:11:36 --> 01:11:39
You've got four cards here.
 

1285
01:11:39 --> 01:11:43
Each card has a letter on
one side and a number

1286
01:11:43 --> 01:11:45
on the other side.
 

1287
01:11:45 --> 01:11:48
You know that.
 

1288
01:11:48 --> 01:11:53
What you want to know is,
and here's the rule.

1289
01:11:53 --> 01:12:03
The rule is, if the card has a
vowel on one side, then it has

1290
01:12:03 --> 01:12:06
an odd number on
the other side.

1291
01:12:06 --> 01:12:10
That's the rule that
we want to check.

1292
01:12:10 --> 01:12:15
Which cards, what's the minimum
and full set of cards that you

1293
01:12:15 --> 01:12:20
need to flip to check whether
that rule pertains for

1294
01:12:20 --> 01:12:22
this set of cards.
 

1295
01:12:22 --> 01:12:23
That's the question.
 

1296
01:12:23 --> 01:12:27
And so we'll vote on each
card as we go along.

1297
01:12:27 --> 01:12:32
How many people want to flip E?
 

1298
01:12:32 --> 01:12:32
All right.
 

1299
01:12:32 --> 01:12:36
Lots and lots of people
want to flip E.

1300
01:12:36 --> 01:12:39
How many people want
to flip the S?

1301
01:12:39 --> 01:12:41
Hm.
 

1302
01:12:41 --> 01:12:43
Couple of people
want to flip the S.

1303
01:12:43 --> 01:12:44
How many people want
to flip the 7?

1304
01:12:44 --> 01:12:47
 
 

1305
01:12:47 --> 01:12:49
A bunch of people
want to flip the 7.

1306
01:12:49 --> 01:12:52
How many people want
to flip the 4?

1307
01:12:52 --> 01:12:57
About the same bunch of
people want to flip the 4.

1308
01:12:57 --> 01:13:01
And some of the same -- I
probably, well, it's too

1309
01:13:01 --> 01:13:05
complicated to collect all the
details of how many cards

1310
01:13:05 --> 01:13:07
people think you need to flip.
 

1311
01:13:07 --> 01:13:12
The answer is, you need to flip
two and only two cards here.

1312
01:13:12 --> 01:13:15
And before I tell you which
ones it is, I should say that

1313
01:13:15 --> 01:13:19
like many of the demos in this
lecture, this is the sort

1314
01:13:19 --> 01:13:23
of thing that drives
MIT students nuts.

1315
01:13:23 --> 01:13:26
Because they -- it's
a cognition lecture,

1316
01:13:26 --> 01:13:27
it's a logic lecture.
 

1317
01:13:27 --> 01:13:28
It's got, like,
mathy things in it.

1318
01:13:28 --> 01:13:29
And it's logic.
 

1319
01:13:29 --> 01:13:30
And that's why I'm here.
 

1320
01:13:30 --> 01:13:33
And I'm getting them all weird
and wrong, and, and, and, and,

1321
01:13:33 --> 01:13:35
and I'm going to flunk
all my courses.

1322
01:13:35 --> 01:13:38
And I'm going to
die, or something.

1323
01:13:38 --> 01:13:41
I mean.
 

1324
01:13:41 --> 01:13:45
I should note that one of the
characteristics of the Wason

1325
01:13:45 --> 01:13:48
selection task is that
everybody gets it wrong.

1326
01:13:48 --> 01:13:50
I don't remember if it's Wason
in originally who did it.

1327
01:13:50 --> 01:13:55
But he tried this out on
logicians and they blow it.

1328
01:13:55 --> 01:13:59
And if you're feeling, like,
depressed after this, you take

1329
01:13:59 --> 01:14:08
this to the calculus TA next
hour, and see if she can do it.

1330
01:14:08 --> 01:14:16
So, as people correctly figure,
you've got to pull this guy.

1331
01:14:16 --> 01:14:18
Because if it's got an
even number on the

1332
01:14:18 --> 01:14:20
other side you're dead.
 

1333
01:14:20 --> 01:14:21
And you don't have to do
anything because we don't

1334
01:14:21 --> 01:14:27
know -- there's no assertion
about what non-vowels are.

1335
01:14:27 --> 01:14:29
You don't have to flip him.
 

1336
01:14:29 --> 01:14:33
Because it doesn't say, it
says -- here, let's do

1337
01:14:33 --> 01:14:34
this in good logic terms.
 

1338
01:14:34 --> 01:14:37
Which for mysterious reasons
always talks about P.

1339
01:14:37 --> 01:14:43
The assertion is, if P, then Q.
 

1340
01:14:43 --> 01:14:46
Then odd.
 

1341
01:14:46 --> 01:14:47
If P then Q.
 

1342
01:14:47 --> 01:14:50
If you know Q, who cares.
 

1343
01:14:50 --> 01:14:52
It doesn't say, if Q then P.
 

1344
01:14:52 --> 01:14:56
So this could be an S
without violating the rule.

1345
01:14:56 --> 01:15:00
This guy, on the other
hand, is not Q.

1346
01:15:00 --> 01:15:01
This guy's not P.
 

1347
01:15:01 --> 01:15:03
Everybody figured out we
don't care about not-P.

1348
01:15:03 --> 01:15:08
Not-Q is the other one that
you do need to check.

1349
01:15:08 --> 01:15:12
Because if this has an E
on the other side, the

1350
01:15:12 --> 01:15:15
statement is false.
 

1351
01:15:15 --> 01:15:16
The rule is false.
 

1352
01:15:16 --> 01:15:20
So you've got to check for an
E on the other side of this.

1353
01:15:20 --> 01:15:24
So, there are now three
people here who say, I've

1354
01:15:24 --> 01:15:24
got it, I've got it.
 

1355
01:15:24 --> 01:15:26
And good for you.
 

1356
01:15:26 --> 01:15:27
That's nice.
 

1357
01:15:27 --> 01:15:31
People routinely do
very badly on this.

1358
01:15:31 --> 01:15:37
And the interesting question
is, is why -- well, actually,

1359
01:15:37 --> 01:15:39
I'm not sure that's an
interesting question.

1360
01:15:39 --> 01:15:40
Why you do that.
 

1361
01:15:40 --> 01:15:48
The interesting fact is that
people do very well on other

1362
01:15:48 --> 01:15:52
versions of exactly
this problem.

1363
01:15:52 --> 01:15:59
So let's set up another
version of this problem.

1364
01:15:59 --> 01:16:06
We've got somebody
with a -- a beer.

1365
01:16:06 --> 01:16:09
 
 

1366
01:16:09 --> 01:16:13
We've got somebody with a soda.
 

1367
01:16:13 --> 01:16:17
We've got an 18-year-old.
 

1368
01:16:17 --> 01:16:20
And we've got a 22-year-old.
 

1369
01:16:20 --> 01:16:28
The rule is, you have
to be 22 to drink.

1370
01:16:28 --> 01:16:29
Well, 21.
 

1371
01:16:29 --> 01:16:33
You have to be
over 21 to drink.

1372
01:16:33 --> 01:16:36
Which of these cards
do we need to check?

1373
01:16:36 --> 01:16:40
So do we need to
check this one?

1374
01:16:40 --> 01:16:41
Yeah, sure, right.
 

1375
01:16:41 --> 01:16:45
If this sucker's an
18-year-old, he's going down.

1376
01:16:45 --> 01:16:47
Do we need to check this one?
 

1377
01:16:47 --> 01:16:49
We don't care about that.
 

1378
01:16:49 --> 01:16:51
Do we need to check this one?
 

1379
01:16:51 --> 01:16:52
Yeah.
 

1380
01:16:52 --> 01:16:53
We need to check this one?
 

1381
01:16:53 --> 01:16:55
No, we don't care about that.
 

1382
01:16:55 --> 01:16:59
People are perfect at
this, essentially.

1383
01:16:59 --> 01:17:01
Sometimes if I phrase it
right you can manage to get

1384
01:17:01 --> 01:17:02
yourself a little confused.
 

1385
01:17:02 --> 01:17:05
So if you got it wrong don't
sit there and say, I'll

1386
01:17:05 --> 01:17:05
never drink again.
 

1387
01:17:05 --> 01:17:13
 
 

1388
01:17:13 --> 01:17:18
So, people are perfectly
good at this sort of

1389
01:17:18 --> 01:17:20
real world example.
 

1390
01:17:20 --> 01:17:23
It turns out that they're
not good at all real

1391
01:17:23 --> 01:17:24
world examples.
 

1392
01:17:24 --> 01:17:28
I could, I won't bother trying
to generate another real

1393
01:17:28 --> 01:17:30
world example here.
 

1394
01:17:30 --> 01:17:35
Leda Cosmides has argued that
what people are good about

1395
01:17:35 --> 01:17:38
reasoning about is cheating.
 

1396
01:17:38 --> 01:17:42
Is somebody getting goodies
that they're not supposed to

1397
01:17:42 --> 01:17:48
Get Is this 18-year-old
getting a drink when I

1398
01:17:48 --> 01:17:50
can't get a drink yet?
 

1399
01:17:50 --> 01:17:53
I'm going to keep
good track of him.

1400
01:17:53 --> 01:17:59
And the guy drinking this beer
looks kind of young to me.

1401
01:17:59 --> 01:18:07
Whereas, that four -- yeah,
nobody cares about that.

1402
01:18:07 --> 01:18:10
And if you reframe the problem,
there are plenty of ways to

1403
01:18:10 --> 01:18:13
reframe this, to put things
like, oh, there's a version

1404
01:18:13 --> 01:18:16
about, if you're in Seattle
it must be raining.

1405
01:18:16 --> 01:18:18
Examples like that.
 

1406
01:18:18 --> 01:18:20
People bomb that all
over the place.

1407
01:18:20 --> 01:18:23
There's nothing at stake there.
 

1408
01:18:23 --> 01:18:26
Cosmides and a variety of the
evolutionary psych people

1409
01:18:26 --> 01:18:33
argue that your cognitive
capabilities have been shaped

1410
01:18:33 --> 01:18:36
by the problems that you need
to solve out there

1411
01:18:36 --> 01:18:36
in the world.
 

1412
01:18:36 --> 01:18:39
Is somebody getting goodies
they shouldn't be getting?

1413
01:18:39 --> 01:18:42
Is the guy with the stick going
to hit me with the stick?

1414
01:18:42 --> 01:18:43
That kind of thing.
 

1415
01:18:43 --> 01:18:45
It would be lovely
if that was true.

1416
01:18:45 --> 01:18:46
It might be true.
 

1417
01:18:46 --> 01:18:50
The problem is that the
research subsequent to

1418
01:18:50 --> 01:18:52
this has made the issue
more complicated.

1419
01:18:52 --> 01:18:57
It is not -- there are cheating
examples that people have

1420
01:18:57 --> 01:19:01
cooked up that bomb.
 

1421
01:19:01 --> 01:19:03
And there are non-cheating
examples that people

1422
01:19:03 --> 01:19:05
successfully manage to solve.
 

1423
01:19:05 --> 01:19:09
But what does seem to be pretty
clear is that we are not built

1424
01:19:09 --> 01:19:12
to solve abstract problems.
 

1425
01:19:12 --> 01:19:16
That's why you have to
show up at MIT to learn

1426
01:19:16 --> 01:19:18
that kind of stuff.
 

1427
01:19:18 --> 01:19:21
Nobody goes to MIT
to take courses in

1428
01:19:21 --> 01:19:25
carding guys at bars.
 

1429
01:19:25 --> 01:19:26
You come with that.
 

1430
01:19:26 --> 01:19:29
You can figure that
out for yourself.

1431
01:19:29 --> 01:19:32
It's when you have
-- unfortunately.

1432
01:19:32 --> 01:19:36
The problem of running the
country turns out to be

1433
01:19:36 --> 01:19:40
probably a lot more like
this than like this.

1434
01:19:40 --> 01:19:43
But the decision is
made like this.

1435
01:19:43 --> 01:19:45
So, everybody go watch
the debate and tell

1436
01:19:45 --> 01:19:47
me what you find.
 

