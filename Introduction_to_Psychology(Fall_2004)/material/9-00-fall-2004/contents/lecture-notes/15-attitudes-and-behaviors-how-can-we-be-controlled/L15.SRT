1
00:00:00 --> 00:00:01
 
 

2
00:00:01 --> 00:00:03
The following content is
provided by MIT OpenCourseWare

3
00:00:03 --> 00:00:05
under a Creative
Commons license.

4
00:00:05 --> 00:00:08
Additional information
about our license and

5
00:00:08 --> 00:00:19
MIT OpenCourseWare is
available at ocw.MIT.edu.

6
00:00:19 --> 00:00:21
PROFESSOR: Good afternoon.
 

7
00:00:21 --> 00:00:26
 
 

8
00:00:26 --> 00:00:30
For the past couple of
lectures, what I've been doing

9
00:00:30 --> 00:00:39
is using love and romance as
the way to talk about broad

10
00:00:39 --> 00:00:43
issues like evolutionary
psychology that could be talked

11
00:00:43 --> 00:00:48
about with a wide range
of other examples.

12
00:00:48 --> 00:00:53
Love and romance just happen to
provide a particularly good set

13
00:00:53 --> 00:00:56
of examples for that
particular topic.

14
00:00:56 --> 00:01:02
I'm going to do the same thing
now with attitude formation

15
00:01:02 --> 00:01:07
and the links between
attitudes and behavior.

16
00:01:07 --> 00:01:13
But I'm going to switch from
the love in relationships

17
00:01:13 --> 00:01:19
topic, to the topic of racist
or prejudicial behavior

18
00:01:19 --> 00:01:21
and attitudes.
 

19
00:01:21 --> 00:01:24
Again, not because that's the
only set of attitudes that are

20
00:01:24 --> 00:01:30
interesting or important, but
because it makes for

21
00:01:30 --> 00:01:35
an interesting path
through this material.

22
00:01:35 --> 00:01:39
So, when you read the book,
get the topics discussed and

23
00:01:39 --> 00:01:41
rather more general terms.
 

24
00:01:41 --> 00:01:46
And, here, I'll discuss them
in the specific terms of

25
00:01:46 --> 00:01:51
this particular problem.
 

26
00:01:51 --> 00:01:54
I should note at the outset
what it says about halfway

27
00:01:54 --> 00:01:59
down, I see the first page of
the handout, which is, I'm

28
00:01:59 --> 00:02:05
going to do my best to give
an explanation about why

29
00:02:05 --> 00:02:15
prejudiced attitudes are easy
to come by and, are readily

30
00:02:15 --> 00:02:18
comprehensible in terms of
psychological processes that we

31
00:02:18 --> 00:02:20
actually know something about.
 

32
00:02:20 --> 00:02:23
But explaining these things
is not the same thing

33
00:02:23 --> 00:02:24
as excusing them.
 

34
00:02:24 --> 00:02:27
 
 

35
00:02:27 --> 00:02:32
You can have a society that
says, look, all else being

36
00:02:32 --> 00:02:36
equal, racial prejudice and
particularly behaviors based on

37
00:02:36 --> 00:02:42
prejudices, based on gender
race, national origin,

38
00:02:42 --> 00:02:47
religion, that those
sort of biases are bad.

39
00:02:47 --> 00:02:50
And when they lead to
behavior, we want to

40
00:02:50 --> 00:02:51
change that behavior.
 

41
00:02:51 --> 00:02:54
 
 

42
00:02:54 --> 00:02:57
That's quite separate, not
unrelated to, but it's separate

43
00:02:57 --> 00:02:59
from the question of how you
would explain it

44
00:02:59 --> 00:03:00
psychologically.
 

45
00:03:00 --> 00:03:04
It's important to remember
that explanation is not

46
00:03:04 --> 00:03:06
the same thing as excuse.
 

47
00:03:06 --> 00:03:09
Because I don't want people
going out of the lecture

48
00:03:09 --> 00:03:13
saying, my psych professor
explained that it's really,

49
00:03:13 --> 00:03:18
really easy to develop
prejudicial attitudes,

50
00:03:18 --> 00:03:20
and that's OK.
 

51
00:03:20 --> 00:03:29
It's the that's OK part -- I
also put on the handout a pair

52
00:03:29 --> 00:03:31
of quotes from the early days
of the civil rights movement;

53
00:03:31 --> 00:03:39
one from Eisenhower saying that
you can't legislate morality,

54
00:03:39 --> 00:03:43
and a response from Martin
Luther King saying, well maybe

55
00:03:43 --> 00:03:47
not, but you can legislate
the moral behavior.

56
00:03:47 --> 00:03:50
That's really the
social policy point.

57
00:03:50 --> 00:03:56
If you decide you don't like
something that biology,

58
00:03:56 --> 00:04:00
psychology or whatever is
pushing people towards, you

59
00:04:00 --> 00:04:04
simply have to do something to
make it harder for them to go

60
00:04:04 --> 00:04:07
where that tendency
might push them.

61
00:04:07 --> 00:04:12
Well, in any case, what I'm
going to do, is work a story

62
00:04:12 --> 00:04:16
about the development of
prejudicial attitudes that's

63
00:04:16 --> 00:04:23
got four factors here
listed at the top.

64
00:04:23 --> 00:04:24
And, I work through
each of them.

65
00:04:24 --> 00:04:31
I hope you can see how they're
tied together to make prejudice

66
00:04:31 --> 00:04:35
a very available option to us.
 

67
00:04:35 --> 00:04:40
Why this sort of thing happens
to us with some regularity.

68
00:04:40 --> 00:04:45
The first factor that I list
there is ethinocentrism.

69
00:04:45 --> 00:04:48
That's the tendency to
think that your group

70
00:04:48 --> 00:04:50
is the best group.
 

71
00:04:50 --> 00:04:55
We're number one kind of thing.
 

72
00:04:55 --> 00:04:59
If you want to come up with,
say, an evolutionary psych

73
00:04:59 --> 00:05:04
argument for why this might
happen, it's kind of trivial.

74
00:05:04 --> 00:05:07
If you've got some notion that
you want to get your genes into

75
00:05:07 --> 00:05:13
the next generation, then you
might as well favor the people

76
00:05:13 --> 00:05:15
who are more closely
related to you.

77
00:05:15 --> 00:05:23
So you should be more favorable
to humans than to mice.

78
00:05:23 --> 00:05:27
You should be likely to be more
favorable to people within your

79
00:05:27 --> 00:05:32
group, and even more so to
people within your family, out

80
00:05:32 --> 00:05:37
of this sort of fairly
straightforward application of

81
00:05:37 --> 00:05:40
evolutionary theory
for example.

82
00:05:40 --> 00:05:44
The more interesting
aspect psychologically

83
00:05:44 --> 00:05:49
is how easy it is to get
ethinocentric effects.

84
00:05:49 --> 00:05:53
So to get the we're
number one effect.

85
00:05:53 --> 00:05:57
Interesting evidence for this
comes from these minimal

86
00:05:57 --> 00:06:03
attachment experiments that
called, minimal group

87
00:06:03 --> 00:06:05
affiliation experiments.
 

88
00:06:05 --> 00:06:05
There's a lot of them.
 

89
00:06:05 --> 00:06:10
Let me describe a
couple to you.

90
00:06:10 --> 00:06:11
So here's an experiment.
 

91
00:06:11 --> 00:06:16
You come into the lab, and
we're going to do an assessment

92
00:06:16 --> 00:06:21
of your taste in abstract art.
 

93
00:06:21 --> 00:06:25
And, we're going to show you a
bunch of abstract pictures.

94
00:06:25 --> 00:06:28
And, you're going to say how
much you like it on a scale of

95
00:06:28 --> 00:06:30
one to seven or something.
 

96
00:06:30 --> 00:06:34
And, then the feedback you're
going to get from this at the

97
00:06:34 --> 00:06:43
end is that you like the work
of Paul Clay, one abstract

98
00:06:43 --> 00:06:49
artist, better than you like
the work of Wassily Kandinsky,

99
00:06:49 --> 00:06:51
another abstract artist.
 

100
00:06:51 --> 00:06:53
Or it might be the
other way around.

101
00:06:53 --> 00:06:55
I don't even remember, by
the way, whether they were

102
00:06:55 --> 00:06:59
actually using real Clay
and Kandinsky pictures.

103
00:06:59 --> 00:07:01
But you're going to get told
that you're in the Clay

104
00:07:01 --> 00:07:04
group or Kandinsky group.
 

105
00:07:04 --> 00:07:10
This is not a group assignment
where there is a lot at stake.

106
00:07:10 --> 00:07:13
 
 

107
00:07:13 --> 00:07:17
Let us suppose that you're
in the Clay group.

108
00:07:17 --> 00:07:19
You've been assigned
to the Clay group.

109
00:07:19 --> 00:07:22
In the second part of the
experiment, you're playing

110
00:07:22 --> 00:07:24
some sort of a game.
 

111
00:07:24 --> 00:07:27
I don't quite remember
what the story was.

112
00:07:27 --> 00:07:36
And, it ends up with you being
able to operate under one

113
00:07:36 --> 00:07:39
of two pay- off rules.
 

114
00:07:39 --> 00:07:45
In one rule, you're going to
give the other group one

115
00:07:45 --> 00:07:52
buck, and your going
to get two bucks.

116
00:07:52 --> 00:07:54
That's one possibility.
 

117
00:07:54 --> 00:07:59
The other possibility is a rule
that gets you three bucks

118
00:07:59 --> 00:08:02
and give them four bucks.
 

119
00:08:02 --> 00:08:04
So, this is you.
 

120
00:08:04 --> 00:08:08
This is them.
 

121
00:08:08 --> 00:08:15
You've got a choice between
option A, and option B.

122
00:08:15 --> 00:08:21
Clearly, the rational choice,
from your vantage point is

123
00:08:21 --> 00:08:25
option B, because you get three
bucks, and not two bucks.

124
00:08:25 --> 00:08:31
But, in fact, there's a bias
towards picking this option.

125
00:08:31 --> 00:08:32
Why is that?
 

126
00:08:32 --> 00:08:36
Well, if you get two bucks,
you getting more then them.

127
00:08:36 --> 00:08:39
 
 

128
00:08:39 --> 00:08:41
Here, I'm going to get more
than I would have got there,

129
00:08:41 --> 00:08:43
but these guys are going
to get a whole bunch more.

130
00:08:43 --> 00:08:46
Why should I give those
Kandinsky lovers

131
00:08:46 --> 00:08:47
a bunch of stuff?
 

132
00:08:47 --> 00:08:54
 
 

133
00:08:54 --> 00:09:00
Maybe it turns out that after
you figure that these Kandinsky

134
00:09:00 --> 00:09:04
sorts really are a different
sort of scummy kind of person,

135
00:09:04 --> 00:09:06
who you really ought
to stick it to.

136
00:09:06 --> 00:09:08
Well, in fact, there's
no difference between

137
00:09:08 --> 00:09:08
these groups.
 

138
00:09:08 --> 00:09:11
The group assignment
is completely random.

139
00:09:11 --> 00:09:15
So any such distinction that
you had made in your own mind

140
00:09:15 --> 00:09:18
is meaningless but maybe
you didn't know that.

141
00:09:18 --> 00:09:23
So we can re-run this whole
experiment by saying get rid

142
00:09:23 --> 00:09:24
of this silly cover story.
 

143
00:09:24 --> 00:09:26
We're going to flip a coin.
 

144
00:09:26 --> 00:09:30
You're in group B.
 

145
00:09:30 --> 00:09:30
B.
 

146
00:09:30 --> 00:09:31
A.
 

147
00:09:31 --> 00:09:31
A.
 

148
00:09:31 --> 00:09:32
B.
 

149
00:09:32 --> 00:09:32
B.
 

150
00:09:32 --> 00:09:32
B.
 

151
00:09:32 --> 00:09:34
B.
 

152
00:09:34 --> 00:09:35
Look guys, it's random.
 

153
00:09:35 --> 00:09:38
There is nothing
differentiating your group

154
00:09:38 --> 00:09:40
from the other group.
 

155
00:09:40 --> 00:09:41
Guess what?
 

156
00:09:41 --> 00:09:42
You get the same result.
 

157
00:09:42 --> 00:09:45
I think it does get a little
bit weaker, but not much.

158
00:09:45 --> 00:09:53
So, just the act of being in
a group, causes you to be

159
00:09:53 --> 00:09:59
inclined to favor that group.
 

160
00:09:59 --> 00:10:03
You are also inclined
to think that group

161
00:10:03 --> 00:10:06
membership is diagnostic.
 

162
00:10:06 --> 00:10:11
 
 

163
00:10:11 --> 00:10:13
Let me describe a different
experiment actually, as the

164
00:10:13 --> 00:10:15
easiest way to describe this.
 

165
00:10:15 --> 00:10:19
Suppose I put up a bunch of
dots, and we're above the

166
00:10:19 --> 00:10:23
subitizing range
here, obviously.

167
00:10:23 --> 00:10:23
OK.
 

168
00:10:23 --> 00:10:24
Quickly.
 

169
00:10:24 --> 00:10:27
How many dots are they there?
 

170
00:10:27 --> 00:10:28
I don't know.
 

171
00:10:28 --> 00:10:31
You could guess.
 

172
00:10:31 --> 00:10:33
You get lucky, and be right on.
 

173
00:10:33 --> 00:10:35
But you'd either be above
or below, so you're going

174
00:10:35 --> 00:10:36
to do this for awhile.
 

175
00:10:36 --> 00:10:38
Guess how many dots there are.
 

176
00:10:38 --> 00:10:51
And, we are going to declare
that you are an overestimater

177
00:10:51 --> 00:10:52
or an underestimater.
 

178
00:10:52 --> 00:10:58
So, that's the group
assignment in this case.

179
00:10:58 --> 00:11:00
Now the interesting thing in
this experiment is that you've

180
00:11:00 --> 00:11:06
done it with another person.
 

181
00:11:06 --> 00:11:20
And, the possibilities are that
it could be two guys, both

182
00:11:20 --> 00:11:22
of them are overestimaters.
 

183
00:11:22 --> 00:11:26
It could be two guys, one of
them an overestimater, one

184
00:11:26 --> 00:11:28
of them an underestimater.
 

185
00:11:28 --> 00:11:31
Similarly for females right?
 

186
00:11:31 --> 00:11:33
 
 

187
00:11:33 --> 00:11:37
They could both be
underestimaters and the

188
00:11:37 --> 00:11:39
critical conditions are.
 

189
00:11:39 --> 00:11:44
It could be a male, and
a, female who, are both

190
00:11:44 --> 00:11:47
overestimaters both labelled
as overestimaters or both

191
00:11:47 --> 00:11:48
labelled as underestimaters.
 

192
00:11:48 --> 00:11:56
And the critical condition is
male overestimater, female

193
00:11:56 --> 00:11:58
underestimater or the
other way around.

194
00:11:58 --> 00:12:04
But the important point here is
that the male is identified

195
00:12:04 --> 00:12:08
as one, the female is
identified as the other.

196
00:12:08 --> 00:12:10
So you've got people in
all these various groups.

197
00:12:10 --> 00:12:11
I don't know.
 

198
00:12:11 --> 00:12:13
Somebody can figure out how
many groups there must be

199
00:12:13 --> 00:12:14
for the full design here.
 

200
00:12:14 --> 00:12:16
Got people in all those groups.
 

201
00:12:16 --> 00:12:19
And, now you ask a question,
at the end of the whole

202
00:12:19 --> 00:12:22
experiment, do you think
there's a sex difference, a

203
00:12:22 --> 00:12:27
systematic sex difference
between men and

204
00:12:27 --> 00:12:30
women on this task?
 

205
00:12:30 --> 00:12:32
What do you think the sex
difference is, if there's

206
00:12:32 --> 00:12:35
a sex difference?
 

207
00:12:35 --> 00:12:41
All these groups, all the same
sex groups, on average, I don't

208
00:12:41 --> 00:12:42
think there's a difference.
 

209
00:12:42 --> 00:12:44
Some people say, I think
women are better.

210
00:12:44 --> 00:12:48
Some people think men are
overestimaters or whatever.

211
00:12:48 --> 00:12:53
But, there's no
systematic bias here.

212
00:12:53 --> 00:12:55
Nothing systematic
happens here.

213
00:12:55 --> 00:12:57
Here something systematic
happens, in this

214
00:12:57 --> 00:12:59
particular version of it.
 

215
00:12:59 --> 00:13:06
 
 

216
00:13:06 --> 00:13:11
You would declare that males
as a group are overestimaters

217
00:13:11 --> 00:13:14
and females has a group
are underestimaters.

218
00:13:14 --> 00:13:16
What's the evidence for that?
 

219
00:13:16 --> 00:13:19
One of each.
 

220
00:13:19 --> 00:13:22
Clearly not meaningful.
 

221
00:13:22 --> 00:13:25
There's no statistical
reliability that you

222
00:13:25 --> 00:13:26
could glean from this.
 

223
00:13:26 --> 00:13:34
It could be that brown haired
people or blond haired people

224
00:13:34 --> 00:13:36
or something are different.
 

225
00:13:36 --> 00:13:39
But you know that there's a
group difference here, because

226
00:13:39 --> 00:13:43
males and females are
definitely different groups.

227
00:13:43 --> 00:13:45
As soon as you've got evidence
that there's a difference

228
00:13:45 --> 00:13:48
across this group, you are
willing to start making

229
00:13:48 --> 00:13:51
assumptions that
difference applies to the

230
00:13:51 --> 00:13:54
group as a whole.
 

231
00:13:54 --> 00:13:57
You see how that works?
 

232
00:13:57 --> 00:13:58
More or less?
 

233
00:13:58 --> 00:13:59
Somebody's nodding their head.
 

234
00:13:59 --> 00:14:00
That's encouraging.
 

235
00:14:00 --> 00:14:02
Ok.
 

236
00:14:02 --> 00:14:12
So that's factor one-- that
you're inclined to see

237
00:14:12 --> 00:14:14
your group as number one.
 

238
00:14:14 --> 00:14:18
And this is a point we'll come
back to, which is that you're

239
00:14:18 --> 00:14:21
inclined to see groups as
having properties that

240
00:14:21 --> 00:14:23
pertain to that group.
 

241
00:14:23 --> 00:14:26
And, you'll jump
to that quickly.

242
00:14:26 --> 00:14:35
Now the tendency to think that
group identification tells you

243
00:14:35 --> 00:14:39
something about properties of
the group, that's known

244
00:14:39 --> 00:14:40
as stereotyping.
 

245
00:14:40 --> 00:14:43
If I know that you are part
of this group, I believe I

246
00:14:43 --> 00:14:46
know something about you.
 

247
00:14:46 --> 00:14:47
Period.
 

248
00:14:47 --> 00:14:52
 
 

249
00:14:52 --> 00:14:54
When we talk about
stereotyping, we tend to think

250
00:14:54 --> 00:14:56
of it in negative terms.
 

251
00:14:56 --> 00:15:02
That is a fairly self evident
consequence of factor one

252
00:15:02 --> 00:15:04
mixed with factor two.
 

253
00:15:04 --> 00:15:09
If you are inclined to think
that your group is number one,

254
00:15:09 --> 00:15:14
and you're inclined to think
that group identity tells you

255
00:15:14 --> 00:15:19
something, it follows that
being a member of another

256
00:15:19 --> 00:15:23
group, means you're of a group
that is, at best number two.

257
00:15:23 --> 00:15:25
Ain't number one,
that's my group.

258
00:15:25 --> 00:15:30
So you in this group that I've
identified, are in the some

259
00:15:30 --> 00:15:34
other lesser group
on this scale.

260
00:15:34 --> 00:15:35
That is fairly obvious.
 

261
00:15:35 --> 00:15:38
What's a little less obvious,
and is, at least worth

262
00:15:38 --> 00:15:45
mentioning, is that stereotypes
are not just descriptions of

263
00:15:45 --> 00:15:49
things that are common
that to that population.

264
00:15:49 --> 00:15:50
Here's the silly example.
 

265
00:15:50 --> 00:15:57
Let us consider the
Asian women stereotype.

266
00:15:57 --> 00:16:02
Is it part of the Asian woman,
is bipedality the Asian

267
00:16:02 --> 00:16:06
woman stereotype?
 

268
00:16:06 --> 00:16:07
No.
 

269
00:16:07 --> 00:16:09
Nobody sits around and
says all those Asian

270
00:16:09 --> 00:16:12
women have two feet.
 

271
00:16:12 --> 00:16:13
That's stupid.
 

272
00:16:13 --> 00:16:15
Nobody's going to say
anything like that, because

273
00:16:15 --> 00:16:17
everybody's got two feet.
 

274
00:16:17 --> 00:16:22
What's important in a
stereotype is, stereotypes are

275
00:16:22 --> 00:16:24
different scores in the sense.
 

276
00:16:24 --> 00:16:27
It's not necessarily accurate
ones you understand.

277
00:16:27 --> 00:16:28
They can be completely bogus.
 

278
00:16:28 --> 00:16:33
But what defines a stereotype,
is what somebody thinks

279
00:16:33 --> 00:16:40
differentiates one group from
the population as a whole.

280
00:16:40 --> 00:16:45
So I put some data down
here from one big

281
00:16:45 --> 00:16:51
study of stereotypes.
 

282
00:16:51 --> 00:16:54
Please note that
these are not facts.

283
00:16:54 --> 00:16:56
I mean they are facts in the
sense that they are data.

284
00:16:56 --> 00:16:58
They're not true facts
about, in this case,

285
00:16:58 --> 00:16:59
the German population.
 

286
00:16:59 --> 00:17:05
What they are is what this
particular group of subjects

287
00:17:05 --> 00:17:08
reported believing
about different groups.

288
00:17:08 --> 00:17:10
In this case, the Germans.
 

289
00:17:10 --> 00:17:16
I excerpted it from
a huge study.

290
00:17:16 --> 00:17:18
The factors, efficient,
extremely nationalistic,

291
00:17:18 --> 00:17:21
scientific- minded,
and pleasure- loving.

292
00:17:21 --> 00:17:28
You will note that the largest
single category, the highest

293
00:17:28 --> 00:17:32
value for the German
population, in this collection

294
00:17:32 --> 00:17:38
of data points, is pleasure-
loving, but that's not part of

295
00:17:38 --> 00:17:41
what would be considered the
stereotypes here, because

296
00:17:41 --> 00:17:45
it's not higher then the
population as a whole.

297
00:17:45 --> 00:17:49
This particular group of
subjects asserted that 82% of

298
00:17:49 --> 00:17:53
people in the world would be
pleasure- loving, and a mere

299
00:17:53 --> 00:17:55
72% of the Germans would be.
 

300
00:17:55 --> 00:17:59
Therefore, pleasure- lovingness
would not be considered part of

301
00:17:59 --> 00:18:03
that stereotype because it it's
not something that

302
00:18:03 --> 00:18:11
distinguishes this view of
Germans from the view of

303
00:18:11 --> 00:18:14
people as a whole.
 

304
00:18:14 --> 00:18:17
And, so, efficient, yes.
 

305
00:18:17 --> 00:18:18
Extremely nationalistic yes.
 

306
00:18:18 --> 00:18:22
And interesting is something
like scientifically- minded

307
00:18:22 --> 00:18:25
would be considered part of the
stereotype, even though it's

308
00:18:25 --> 00:18:27
not even held to be a majority.
 

309
00:18:27 --> 00:18:29
 
 

310
00:18:29 --> 00:18:32
This perception doesn't say
that a majority of Germans are

311
00:18:32 --> 00:18:34
scientifically- minded,
but more Germans are

312
00:18:34 --> 00:18:40
scientifically- minded
according to this view then the

313
00:18:40 --> 00:18:41
population as a whole.
 

314
00:18:41 --> 00:18:44
Again, I have no idea what
the true data would be for

315
00:18:44 --> 00:18:45
the German population.
 

316
00:18:45 --> 00:18:48
I don't know if they think
about science at all.

317
00:18:48 --> 00:18:54
But, the perception is that the
stereotype would include

318
00:18:54 --> 00:18:57
efficient, nationalistic, and
scientific, and not pleasure-

319
00:18:57 --> 00:19:03
minded, because of this issue
of a differential relationship

320
00:19:03 --> 00:19:06
to the perceived baseline.
 

321
00:19:06 --> 00:19:11
To the population as a whole.
 

322
00:19:11 --> 00:19:17
One of the factors contributing
to the power of stereotyping

323
00:19:17 --> 00:19:21
is, what's known as the out
group homogeneaity effect.

324
00:19:21 --> 00:19:24
So, the in group is you.
 

325
00:19:24 --> 00:19:26
You're in some group.
 

326
00:19:26 --> 00:19:29
The out group are other people.
 

327
00:19:29 --> 00:19:33
The out group homogeneaity
effect is the tendency to

328
00:19:33 --> 00:19:40
think that all of them
are kind of alike.

329
00:19:40 --> 00:19:43
You don't think that about
you in the same way.

330
00:19:43 --> 00:19:46
 
 

331
00:19:46 --> 00:19:50
From the last lecture,
the nerdy high school

332
00:19:50 --> 00:19:53
freshman group.
 

333
00:19:53 --> 00:19:57
If that was my in group, I
would know that they aren't all

334
00:19:57 --> 00:20:01
alike, because I'm one of them,
and I know that I'm different

335
00:20:01 --> 00:20:05
from all those other nerdy
high school freshman.

336
00:20:05 --> 00:20:10
But, the jocks, man,
they're all alike.

337
00:20:10 --> 00:20:14
It's an effect, an
ignorance effect.

338
00:20:14 --> 00:20:19
Now, how does this
play into bias?

339
00:20:19 --> 00:20:24
How does this ignorance factor
play into not just thinking

340
00:20:24 --> 00:20:27
that all those people are
alike, but the thinking

341
00:20:27 --> 00:20:28
less of those people?
 

342
00:20:28 --> 00:20:34
If you ask about what you know
about groups that you don't

343
00:20:34 --> 00:20:39
interact with much, where do
you got your information

344
00:20:39 --> 00:20:41
about them?
 

345
00:20:41 --> 00:20:43
You get your information
from the news.

346
00:20:43 --> 00:20:46
What gets you on to the news?
 

347
00:20:46 --> 00:20:49
The fact that you're a
good student, and you

348
00:20:49 --> 00:20:51
love your mother?
 

349
00:20:51 --> 00:20:53
No.
 

350
00:20:53 --> 00:20:58
But, if you decide to go off
and commit an armed robbery,

351
00:20:58 --> 00:21:01
or something like that, that
might get you on the news.

352
00:21:01 --> 00:21:06
And, if you are a member of a
distinctive group, of some

353
00:21:06 --> 00:21:10
sort, that's not my group, I'm
going to say, hey, look, I've

354
00:21:10 --> 00:21:14
got a data point about, -- we
can keep picking on the Asian

355
00:21:14 --> 00:21:18
women -- I've got a data
point about Asian women.

356
00:21:18 --> 00:21:21
That one committed
an armed robbery.

357
00:21:21 --> 00:21:22
I know about Asian women now.
 

358
00:21:22 --> 00:21:24
They all commit armed robbery.
 

359
00:21:24 --> 00:21:27
Well, you don't do anything
quite that bald and stupid.

360
00:21:27 --> 00:21:31
But that's the sense in which
you are willing to color the

361
00:21:31 --> 00:21:33
entire group on the basis of
whatever information you have

362
00:21:33 --> 00:21:35
about one member of it.
 

363
00:21:35 --> 00:21:39
That's another version
of this effect.

364
00:21:39 --> 00:21:44
Is likely to lead to negative
assessments of the out group,

365
00:21:44 --> 00:21:46
because the information you get
about groups that you don't

366
00:21:46 --> 00:21:50
interact with is skewed
towards the negative.

367
00:21:50 --> 00:21:53
The stuff that's going to make
the news about a group is going

368
00:21:53 --> 00:21:57
to be typically the
negative effect.

369
00:21:57 --> 00:22:02
 
 

370
00:22:02 --> 00:22:06
So where are the strongest
stereotypes in the American

371
00:22:06 --> 00:22:11
population -- well actually
this is a somewhat old study at

372
00:22:11 --> 00:22:17
this point -- but when you
assess, you can use various

373
00:22:17 --> 00:22:23
questionnaire assessments
to assess how strongly a

374
00:22:23 --> 00:22:28
population holds stereotypic
views of another population.

375
00:22:28 --> 00:22:33
The heavily stereotyped views
held by an American population

376
00:22:33 --> 00:22:38
-- this is now about 15 years
ago-- were held of Turks, Arabs

377
00:22:38 --> 00:22:44
to a lesser degree of the
Japanese, groups that were not

378
00:22:44 --> 00:22:50
heavily represented in the
general Americans mix.

379
00:22:50 --> 00:22:54
Less heavy stereotypes for
groups with large immigrant

380
00:22:54 --> 00:22:58
populations in this country,
because you were more likely to

381
00:22:58 --> 00:23:08
know some people in that group,
and that makes the stereotypes

382
00:23:08 --> 00:23:10
less firm, less strong.
 

383
00:23:10 --> 00:23:18
One of the reasons that these
stereotypes matter is because

384
00:23:18 --> 00:23:23
along with being willing to
build them quickly and easily,

385
00:23:23 --> 00:23:31
we are also inclined to think
that the attributes that we put

386
00:23:31 --> 00:23:36
on other groups are causal.
 

387
00:23:36 --> 00:23:37
At least in others.
 

388
00:23:37 --> 00:23:39
This is what's known as the
fundamental attribution error.

389
00:23:39 --> 00:23:41
Let me explain a little.
 

390
00:23:41 --> 00:23:48
But I sent a note to a couple
of my social psych colleagues

391
00:23:48 --> 00:23:51
yesterday, as I was thinking
about this lecture, asking

392
00:23:51 --> 00:23:55
who was it who named the
fundamental attribution error,

393
00:23:55 --> 00:23:58
because that's a great
thing to be able to do.

394
00:23:58 --> 00:24:03
To be able to call what
you work on fundamental.

395
00:24:03 --> 00:24:07
And, it turns out to be
Nisbett, N I S B E T T, from

396
00:24:07 --> 00:24:10
the University of Michigan if
you want to track that down.

397
00:24:10 --> 00:24:15
But, in any case, let me
explain what this is.

398
00:24:15 --> 00:24:22
There are two broad ways of
thinking about personality.

399
00:24:22 --> 00:24:26
We all have some notion that
we've got a personality.

400
00:24:26 --> 00:24:31
And, thinking about the
attributes that make up that

401
00:24:31 --> 00:24:35
personality can be divided into
two broad categories that map

402
00:24:35 --> 00:24:39
onto the usual nature/ nurture
kind of arguments in the field.

403
00:24:39 --> 00:24:48
There are trait theories
that are typically on the

404
00:24:48 --> 00:24:50
more nature side of it.
 

405
00:24:50 --> 00:24:56
There are fundamental
attributes of personality maybe

406
00:24:56 --> 00:24:59
coming from a genetic origin.
 

407
00:24:59 --> 00:25:03
 
 

408
00:25:03 --> 00:25:06
You are who you are because
you have these traits.

409
00:25:06 --> 00:25:11
The alternative from the more
nurtured side of things, the

410
00:25:11 --> 00:25:24
environmental side of it is, is
a situationalist account that

411
00:25:24 --> 00:25:29
says you are who you are
because of where you are.

412
00:25:29 --> 00:25:37
Trait theory, you are here now
because you where born smart,

413
00:25:37 --> 00:25:43
and hard working, and studious,
or you basically have

414
00:25:43 --> 00:25:47
consistent over time,
hardworking, studious

415
00:25:47 --> 00:25:49
attributes to you.
 

416
00:25:49 --> 00:25:52
The situationlist account says
you're sitting here right now

417
00:25:52 --> 00:25:57
emitting student behavior
because you're in a student

418
00:25:57 --> 00:25:59
kind of environment.
 

419
00:25:59 --> 00:26:06
If we put you on a farm, you
would not be sitting there in

420
00:26:06 --> 00:26:08
the middle of the field
with a notebook taking

421
00:26:08 --> 00:26:09
notes about the cow.
 

422
00:26:09 --> 00:26:11
That's not what you'd be doing.
 

423
00:26:11 --> 00:26:15
In that situation, you'd be
doing farm kind of stuff.

424
00:26:15 --> 00:26:19
Like all such debates in the
field, the truth is going to

425
00:26:19 --> 00:26:22
lie somewhere in between.
 

426
00:26:22 --> 00:26:23
There's going to
be bits of both.

427
00:26:23 --> 00:26:26
You're not going to get any
mileage out of arguing strictly

428
00:26:26 --> 00:26:28
one or strictly the other.
 

429
00:26:28 --> 00:26:31
What you think about this, what
you think about the balance

430
00:26:31 --> 00:26:36
here, is important for the
policy purposes, for instance.

431
00:26:36 --> 00:26:41
Why did this guy
commit a crime?

432
00:26:41 --> 00:26:44
We know he committed a crime,
because we just convicted him

433
00:26:44 --> 00:26:47
of committing this crime.
 

434
00:26:47 --> 00:26:49
Why did he commit a crime?
 

435
00:26:49 --> 00:26:55
Is it because he's a
criminal sort of person?

436
00:26:55 --> 00:26:58
Fundamentally, dishonest,
nasty, kind of person.

437
00:26:58 --> 00:27:03
Or, is it because he was in
a situation that promoted

438
00:27:03 --> 00:27:06
criminal behavior?
 

439
00:27:06 --> 00:27:07
Why does that matter?
 

440
00:27:07 --> 00:27:12
Well if you're in this mode,
you may send him off to

441
00:27:12 --> 00:27:13
prison in both cases.
 

442
00:27:13 --> 00:27:16
In this case, you're likely to
think of prison as a place

443
00:27:16 --> 00:27:20
where you put bad people to
keep them out of the way for a

444
00:27:20 --> 00:27:22
length of time that's
appropriate to whatever

445
00:27:22 --> 00:27:24
bad thing they did.
 

446
00:27:24 --> 00:27:27
But, it's basically
as a punishment.

447
00:27:27 --> 00:27:31
This is the personality theory,
that would lead you to call

448
00:27:31 --> 00:27:34
your prison a correctional
institution.

449
00:27:34 --> 00:27:38
Because you think that you
could correct this person.

450
00:27:38 --> 00:27:44
If you think he's fundamentally
a bad person, the trick is to

451
00:27:44 --> 00:27:46
make sure he can't
do that anymore.

452
00:27:46 --> 00:27:49
If you think it's because of
the situation, that somehow

453
00:27:49 --> 00:27:53
forced to him into or pushed
him into criminal behavior,

454
00:27:53 --> 00:27:54
you want to fix that.
 

455
00:27:54 --> 00:27:58
 
 

456
00:27:58 --> 00:28:03
Modern prison philosophy is
neither all the way here,

457
00:28:03 --> 00:28:05
or all the way there.
 

458
00:28:05 --> 00:28:10
But, the balance is really a
personality theory question.

459
00:28:10 --> 00:28:16
The fundamental attribution
error is a tendency to hold to

460
00:28:16 --> 00:28:21
a more trait theoretic position
when you're talking about other

461
00:28:21 --> 00:28:24
people than when you're
talking about yourself.

462
00:28:24 --> 00:28:25
Why is it in error?
 

463
00:28:25 --> 00:28:32
Well it logically can't be the
case that you are the product

464
00:28:32 --> 00:28:36
largely of the situation and
that they are a product of

465
00:28:36 --> 00:28:39
their invariant parts.
 

466
00:28:39 --> 00:28:42
That over the population
as a whole isn't going to

467
00:28:42 --> 00:28:43
isn't going to hold up.
 

468
00:28:43 --> 00:28:50
 
 

469
00:28:50 --> 00:28:54
Why did this guy rob the bank?
 

470
00:28:54 --> 00:28:57
He robbed the bank,
because he's a criminal.

471
00:28:57 --> 00:28:59
Why did I rob the bank?
 

472
00:28:59 --> 00:29:02
I robbed the bank, because I
was hungry, and the door was

473
00:29:02 --> 00:29:05
unlocked, and I didn't really
rob the bank, I just kind of

474
00:29:05 --> 00:29:08
picked up the money that was
lying on the floor, and

475
00:29:08 --> 00:29:11
anyway, it was other guy.
 

476
00:29:11 --> 00:29:16
Much more likely to give
us situational account.

477
00:29:16 --> 00:29:21
Why did you get a bad
grade on the midterms.

478
00:29:21 --> 00:29:23
Suppose you got a bad
grade on the midterm.

479
00:29:23 --> 00:29:24
Why did you get a bad
grade on the midterm.

480
00:29:24 --> 00:29:27
Well, the story was really
lame, and it distracted me.

481
00:29:27 --> 00:29:28
And I didn't get enough sleep.
 

482
00:29:28 --> 00:29:34
And the course wasn't a
big priority for me.

483
00:29:34 --> 00:29:38
That's why I got it bad grade.
 

484
00:29:38 --> 00:29:42
Your TA says-- well your TA is
a good person, and doesn't say

485
00:29:42 --> 00:29:46
this-- but your TA looks at the
exam and says, why did they

486
00:29:46 --> 00:29:47
get a bad grade on the exam?
 

487
00:29:47 --> 00:29:49
They're stupid!
 

488
00:29:49 --> 00:29:52
That would be the fundamental
attribution error.

489
00:29:52 --> 00:29:56
Why did your TA
get a bad grade?

490
00:29:56 --> 00:29:59
I didn't get enough sleep
and stuff like that.

491
00:29:59 --> 00:30:02
We're more inclined to give
situationalist accounts of our

492
00:30:02 --> 00:30:07
own behavior and more inclined
to get trait accounts of

493
00:30:07 --> 00:30:08
other people's behavior.
 

494
00:30:08 --> 00:30:12
All right, so let's see where
this has gotten us to.

495
00:30:12 --> 00:30:14
We're inclined to put
people into groups.

496
00:30:14 --> 00:30:20
We're inclined to assign
attributes to those groups.

497
00:30:20 --> 00:30:23
We're likely to assign more
negative attributes to groups

498
00:30:23 --> 00:30:26
than we ought to because our
information about groups that

499
00:30:26 --> 00:30:31
we don't know is being skewed
in that direction, and we're

500
00:30:31 --> 00:30:36
likely to attribute behavior to
what we now perceive correctly

501
00:30:36 --> 00:30:39
or incorrectly as the
traits of the group.

502
00:30:39 --> 00:30:43
And, so you can see how you're
going to end up with a story

503
00:30:43 --> 00:30:47
that is a pretty negative
account of some out

504
00:30:47 --> 00:30:49
group or other.
 

505
00:30:49 --> 00:30:53
Oh by the way, there's a very
interesting wrinkle on the

506
00:30:53 --> 00:30:56
fundamental attribution error,
or at least there's certainly

507
00:30:56 --> 00:30:58
used to be, I do not know
whether this is still the case

508
00:30:58 --> 00:31:02
in the population as a whole,
and I certainly don't know,

509
00:31:02 --> 00:31:04
though I would be very
interested to know,

510
00:31:04 --> 00:31:07
whether it's true at MIT.
 

511
00:31:07 --> 00:31:10
 
 

512
00:31:10 --> 00:31:13
Let's try this as an
experiment, and see how your

513
00:31:13 --> 00:31:17
intuition goes -- OK we can do
the trait versus situational

514
00:31:17 --> 00:31:29
thing, and we're going
to have a math test.

515
00:31:29 --> 00:31:36
 
 

516
00:31:36 --> 00:31:39
You're interpreting your
own score on a math test.

517
00:31:39 --> 00:31:49
That score can be good or
bad, as you may know.

518
00:31:49 --> 00:31:56
There are explanations of
why the test for was good.

519
00:31:56 --> 00:32:00
A trait theory would
be, I'm a genius.

520
00:32:00 --> 00:32:04
A situationalist theory
would be, I'm lucky.

521
00:32:04 --> 00:32:12
 
 

522
00:32:12 --> 00:32:20
And, if the test is bad, you
can have an assessment that it

523
00:32:20 --> 00:32:30
says I'm dumb, or you going to
have an assessment that said,

524
00:32:30 --> 00:32:33
that the test is unfair.
 

525
00:32:33 --> 00:32:38
The interesting bit is
that one gender is more

526
00:32:38 --> 00:32:39
likely to say this.
 

527
00:32:39 --> 00:32:42
And the other gender is
more likely to say this.

528
00:32:42 --> 00:32:45
One gender is more
likely to say this.

529
00:32:45 --> 00:32:48
And the other gender's
more likely to say that.

530
00:32:48 --> 00:32:51
So each of these cells can be
associated with a gender.

531
00:32:51 --> 00:32:53
So I got a good
grade on the test.

532
00:32:53 --> 00:32:54
I'm a genius.
 

533
00:32:54 --> 00:32:56
Who are we talking about?
 

534
00:32:56 --> 00:32:57
AUDIENCE: Men.
 

535
00:32:57 --> 00:33:00
PROFESSOR: All right.
 

536
00:33:00 --> 00:33:06
And, it follows that this
must be the female cell.

537
00:33:06 --> 00:33:10
I got a bad grade on the test.
 

538
00:33:10 --> 00:33:11
I'm dumb.
 

539
00:33:11 --> 00:33:13
AUDIENCE: Female.
 

540
00:33:13 --> 00:33:16
PROFESSOR: And, that is
the historical finding.

541
00:33:16 --> 00:33:19
These are studies that came
out in the early days of

542
00:33:19 --> 00:33:22
the women's movement.
 

543
00:33:22 --> 00:33:27
And, I don't know about whether
or not it's still pertains.

544
00:33:27 --> 00:33:35
But the disturbing finding at
the time was that males were

545
00:33:35 --> 00:33:39
inclined to give trait
theoretic answers for the

546
00:33:39 --> 00:33:46
good stuff, I'm good, I'm
bright, I'm gorgeous.

547
00:33:46 --> 00:33:52
And, females were likely to say
I'm lucky and it's all makeup,

548
00:33:52 --> 00:33:53
or something like that.
 

549
00:33:53 --> 00:33:57
And on the other side, on the
bad side, the females were

550
00:33:57 --> 00:34:00
likely to give trait
theoretic answers.

551
00:34:00 --> 00:34:04
I'm dumb and ugly and
depressed and its terrible.

552
00:34:04 --> 00:34:10
And the guys were likely to say
I'm brilliant, I'm gorgeous, et

553
00:34:10 --> 00:34:14
cetera, and the test was really
kind unfair and my teacher

554
00:34:14 --> 00:34:19
hated me and stuff.
 

555
00:34:19 --> 00:34:24
It would be interesting to know
whether that was still true.

556
00:34:24 --> 00:34:25
We might as well take a poll.
 

557
00:34:25 --> 00:34:28
How many people think that if
we actually collected data,

558
00:34:28 --> 00:34:32
we'd find that something
like that was still true?

559
00:34:32 --> 00:34:35
How many think that we would
find it has gone away?

560
00:34:35 --> 00:34:39
 
 

561
00:34:39 --> 00:34:45
I have no idea if there's
new data on that.

562
00:34:45 --> 00:34:52
The basic point is that with
that modulation possibly we

563
00:34:52 --> 00:34:54
tend to see situational
explanations of our own

564
00:34:54 --> 00:34:57
behavior, and trait
explanations of other

565
00:34:57 --> 00:34:59
people's behavior.
 

566
00:34:59 --> 00:35:05
Now let's take a look at this
last factor, which I'm calling

567
00:35:05 --> 00:35:14
the role of the ignorance
in person perception.

568
00:35:14 --> 00:35:26
And see how that can lead to
what looks like a biased

569
00:35:26 --> 00:35:30
outcome, perhaps even if you
didn't have these

570
00:35:30 --> 00:35:33
other factors.
 

571
00:35:33 --> 00:35:36
This ignorance factor's now
going to interact with these

572
00:35:36 --> 00:35:42
other factors to make
biased outcomes.

573
00:35:42 --> 00:35:45
It's quite easy to come by.
 

574
00:35:45 --> 00:35:51
So, let's do a version of the
classic physics joke, about

575
00:35:51 --> 00:35:53
assume the horse is a sphere.
 

576
00:35:53 --> 00:35:57
 
 

577
00:35:57 --> 00:36:00
We're going to over
simplify the situation.

578
00:36:00 --> 00:36:11
The issue here is who are you
going to be friends with?

579
00:36:11 --> 00:36:15
Well, first of all, we have to
go back to the good, ernest

580
00:36:15 --> 00:36:19
high school discussion about
making friends with people.

581
00:36:19 --> 00:36:22
 
 

582
00:36:22 --> 00:36:24
And, does it matter if
they're wearing the

583
00:36:24 --> 00:36:26
latest designer whatever?
 

584
00:36:26 --> 00:36:30
And, the answer, of course,
is no, because you shouldn't

585
00:36:30 --> 00:36:34
judge a book by its cover.
 

586
00:36:34 --> 00:36:34
Good.
 

587
00:36:34 --> 00:36:36
Nice cliche.
 

588
00:36:36 --> 00:36:38
And, it is, of course, true.
 

589
00:36:38 --> 00:36:41
 
 

590
00:36:41 --> 00:36:43
In the first assume the
horse is a sphere over

591
00:36:43 --> 00:36:48
simplification, the problem,
let us assume that the set of

592
00:36:48 --> 00:36:51
people with whom you might be
friends in the world, is the

593
00:36:51 --> 00:36:56
set of let's just make it
all MIT undergraduates.

594
00:36:56 --> 00:37:00
And we know we don't want to
judge books by covers, so,

595
00:37:00 --> 00:37:05
therefore, what you're going
to do is, set up in depth

596
00:37:05 --> 00:37:08
interviews with everybody.
 

597
00:37:08 --> 00:37:10
And decide who's going
to be your friend on

598
00:37:10 --> 00:37:12
the basis of that.
 

599
00:37:12 --> 00:37:13
No.
 

600
00:37:13 --> 00:37:15
That's not going to work.
 

601
00:37:15 --> 00:37:18
Well, all right, the other
alternative is, don't want to

602
00:37:18 --> 00:37:22
judge books by their cover,
therefore I won't talk

603
00:37:22 --> 00:37:23
to anybody ever again.
 

604
00:37:23 --> 00:37:24
That's not going
to work either.

605
00:37:24 --> 00:37:29
So it is self- evident again,
various bits of this are self-

606
00:37:29 --> 00:37:33
evident, you've gotta make snap
decisions on the basis of

607
00:37:33 --> 00:37:35
imperfect information.
 

608
00:37:35 --> 00:37:37
It doesn't mean that you have
to make your decisions on the

609
00:37:37 --> 00:37:43
basis of whether or not
they're wearing designer

610
00:37:43 --> 00:37:44
whatevers, of course.
 

611
00:37:44 --> 00:37:48
But you're necessarily going
to have to make a first cut

612
00:37:48 --> 00:37:52
through the population on
the basis of essentially

613
00:37:52 --> 00:37:54
superficial information.
 

614
00:37:54 --> 00:37:55
Well, what's that going to do?
 

615
00:37:55 --> 00:38:08
Let us assume that in the
world, in an act of massive,

616
00:38:08 --> 00:38:13
further over simplification,
there are bad people

617
00:38:13 --> 00:38:17
and are good people.
 

618
00:38:17 --> 00:38:21
And, your job is to divide
the world into those

619
00:38:21 --> 00:38:22
two categories.
 

620
00:38:22 --> 00:38:24
So you're going to
make an assessment.

621
00:38:24 --> 00:38:27
 
 

622
00:38:27 --> 00:38:31
You're going to perform an act
of person perception, and

623
00:38:31 --> 00:38:41
divide the world into bad
people, and good people.

624
00:38:41 --> 00:38:44
We've got a nice simple
two buy two design here.

625
00:38:44 --> 00:38:49
Ideally you want everybody to
be in those two populations,

626
00:38:49 --> 00:38:50
and in those two cells.
 

627
00:38:50 --> 00:38:55
Even if you could do in depth
interviews with everybody, it's

628
00:38:55 --> 00:38:59
not clear you'd never make a
mistake, but clearly, if you're

629
00:38:59 --> 00:39:01
just going to be basing your
decisions on relatively

630
00:39:01 --> 00:39:06
superficial information, there
are going to be errors.

631
00:39:06 --> 00:39:10
I labeled these Type 1 and Type
2, which is actually jargon

632
00:39:10 --> 00:39:14
from signal detection land,
but don't worry about that.

633
00:39:14 --> 00:39:22
It just, in this case, give us
a chance to ask the question.

634
00:39:22 --> 00:39:27
Which of these type
of errors is worse?

635
00:39:27 --> 00:39:35
If you had a choice about which
error to make, how many people,

636
00:39:35 --> 00:39:42
given the choice between, let's
be clear about this, you've got

637
00:39:42 --> 00:39:46
a good person, and you declare
that good person to be bad, or

638
00:39:46 --> 00:39:48
you've got a bad person, and
you declare him to be good.

639
00:39:48 --> 00:39:50
You got a choice between
which of those errors

640
00:39:50 --> 00:39:50
you're going to make.
 

641
00:39:50 --> 00:39:54
How many people
vote for Type 1?

642
00:39:54 --> 00:39:56
How many people
vote for Type 2?

643
00:39:56 --> 00:39:56
OK.
 

644
00:39:56 --> 00:39:58
So a Type 2 person.
 

645
00:39:58 --> 00:40:03
Why do you you prefer
the Type 2 error?

646
00:40:03 --> 00:40:07
 
 

647
00:40:07 --> 00:40:08
Any Type 2 person?
 

648
00:40:08 --> 00:40:12
AUDIENCE: [INAUDIBLE]
 

649
00:40:12 --> 00:40:14
PROFESSOR: You're missing
out on good people.

650
00:40:14 --> 00:40:17
That's the nice person answer.
 

651
00:40:17 --> 00:40:20
You don't want to inadvertently
tar some nice, good person

652
00:40:20 --> 00:40:25
with the label of being bad.
 

653
00:40:25 --> 00:40:28
How about a Type 1 person?
 

654
00:40:28 --> 00:40:34
AUDIENCE: [INAUDIBLE]
 

655
00:40:34 --> 00:40:36
PROFESSOR: There's a lot of
people who can be your friends.

656
00:40:36 --> 00:40:37
You don't need all of them.
 

657
00:40:37 --> 00:40:39
And you want to get
to those bad people.

658
00:40:39 --> 00:40:39
How come?
 

659
00:40:39 --> 00:40:40
Anybody else?
 

660
00:40:40 --> 00:40:44
AUDIENCE: It could be harmful.
 

661
00:40:44 --> 00:40:45
PROFESSOR: Yeah.
 

662
00:40:45 --> 00:40:47
it could be dangerous.
 

663
00:40:47 --> 00:40:52
If we dichotomise this into
good and really bad, nasty,

664
00:40:52 --> 00:40:56
dangerous people, then that
intuition becomes a little

665
00:40:56 --> 00:40:59
clearer that you whatever you
else you do, you want to

666
00:40:59 --> 00:41:00
keep these people away.
 

667
00:41:00 --> 00:41:04
 
 

668
00:41:04 --> 00:41:07
This is applied signal
detection theory.

669
00:41:07 --> 00:41:13
Usually you do signal detection
theory in visual perception

670
00:41:13 --> 00:41:14
land or something.
 

671
00:41:14 --> 00:41:21
But this is what the next page
of the handout has on it.

672
00:41:21 --> 00:41:24
But here's where this
is coming from.

673
00:41:24 --> 00:41:27
Let us suppose, again, for
the sake of vast over-

674
00:41:27 --> 00:41:42
simplification, that there are
on a scale of goodness, that

675
00:41:42 --> 00:41:44
the only two types of
people in the world.

676
00:41:44 --> 00:41:53
There are good people
and bad people.

677
00:41:53 --> 00:41:59
And, you want to pick the
good people and reject

678
00:41:59 --> 00:42:00
the bad people.
 

679
00:42:00 --> 00:42:06
But the difficulty is that
you can't successfully

680
00:42:06 --> 00:42:09
see this, because your
information is lousy.

681
00:42:09 --> 00:42:13
The effect of your information
being lousy is that what you

682
00:42:13 --> 00:42:23
see is a distribution of
goodness and badness,

683
00:42:23 --> 00:42:24
something like that.
 

684
00:42:24 --> 00:42:27
 
 

685
00:42:27 --> 00:42:30
By the way, if you were doing
this in vision land, this would

686
00:42:30 --> 00:42:34
be one light and another light.
 

687
00:42:34 --> 00:42:35
Can you tell the difference
between a dim light

688
00:42:35 --> 00:42:38
and a bright light or
something like that.

689
00:42:38 --> 00:42:40
By the time it goes through
your nervous system, rather

690
00:42:40 --> 00:42:43
than being always looking
exactly like this, or always

691
00:42:43 --> 00:42:45
looking exactly like this,
sometimes the bright light

692
00:42:45 --> 00:42:48
looks a little dim, sometimes
the dim light looks

693
00:42:48 --> 00:42:49
a little bright.
 

694
00:42:49 --> 00:42:51
And how do you decide
which one you've seen?

695
00:42:51 --> 00:42:55
So, you got a person
in front of you.

696
00:42:55 --> 00:42:57
How can you decide whether
they're good or bad?

697
00:42:57 --> 00:43:00
Well, the best you can
do, is put a criterion

698
00:43:00 --> 00:43:01
in there somewhere.
 

699
00:43:01 --> 00:43:06
So let's just divide it.
 

700
00:43:06 --> 00:43:09
If I do that, that's OK.
 

701
00:43:09 --> 00:43:14
That means I'm going to declare
everybody on this side to be

702
00:43:14 --> 00:43:21
good, and everybody on
this side to be bad.

703
00:43:21 --> 00:43:25
And, OK, so I'm declaring all
of these people, who are,

704
00:43:25 --> 00:43:29
in fact, good to be good.
 

705
00:43:29 --> 00:43:37
The difficulty, the sad thing,
is the good people who I'm

706
00:43:37 --> 00:43:41
declaring to be bad, so the
Type 1 errors are here.

707
00:43:41 --> 00:43:45
 
 

708
00:43:45 --> 00:43:47
These are good people who
I declared to be bad.

709
00:43:47 --> 00:43:49
That's too bad.
 

710
00:43:49 --> 00:43:51
OK.
 

711
00:43:51 --> 00:43:55
Here, on this side are
all the bad people who

712
00:43:55 --> 00:43:56
I declared to be bad.
 

713
00:43:56 --> 00:43:57
That's exactly what
I wanted to do.

714
00:43:57 --> 00:44:06
But these guys, these
are the Type 2 errors.

715
00:44:06 --> 00:44:10
These are bad people who beat
my criterion level, and

716
00:44:10 --> 00:44:12
I said they're good.
 

717
00:44:12 --> 00:44:16
Now, you should be able to
figure out, looking at a

718
00:44:16 --> 00:44:20
picture like this, there's
no way to eliminate error.

719
00:44:20 --> 00:44:25
If this is the situation of the
stimuli I have to deal with,

720
00:44:25 --> 00:44:26
there's no way to
eliminate error.

721
00:44:26 --> 00:44:29
All I can do is
apportion error.

722
00:44:29 --> 00:44:34
So, if I decide that the Type 2
errors are the dangerous errors

723
00:44:34 --> 00:44:38
that I need to avoid, then I'm
going to move my

724
00:44:38 --> 00:44:40
criterion over.
 

725
00:44:40 --> 00:44:45
This is the second
picture on the handout.

726
00:44:45 --> 00:44:53
If I move my criterion over so
that I reduce my Type 2 errors

727
00:44:53 --> 00:44:59
to just these few, let's say,
now the result is that I've

728
00:44:59 --> 00:45:03
massively increased
my Type 1 errors.

729
00:45:03 --> 00:45:07
I'm now declaring all these
lovely people to be people I

730
00:45:07 --> 00:45:09
don't want to make
friends with.

731
00:45:09 --> 00:45:13
It's sad, but that's
the way it go.

732
00:45:13 --> 00:45:16
Because, as the gentleman
back there said, yeah.

733
00:45:16 --> 00:45:18
There are a lot of people
here, so I've got plenty of

734
00:45:18 --> 00:45:19
people to be friends with.
 

735
00:45:19 --> 00:45:22
And these guys will just have
to deal with the fact that

736
00:45:22 --> 00:45:23
they're not my friend.
 

737
00:45:23 --> 00:45:24
That's just the way it is.
 

738
00:45:24 --> 00:45:28
But, I'm not letting any of
these mean, nasty, rotten

739
00:45:28 --> 00:45:32
people in, except
for these guys.

740
00:45:32 --> 00:45:34
Most of these guys,
I'm going to avoid.

741
00:45:34 --> 00:45:34
OK.
 

742
00:45:34 --> 00:45:40
Now, look what happens when
you deal with an out group.

743
00:45:40 --> 00:45:43
When you deal with a group
other than your own.

744
00:45:43 --> 00:45:51
If the argument is that part of
what makes an out group the out

745
00:45:51 --> 00:45:56
group is the fact that you
know less about them.

746
00:45:56 --> 00:46:01
The way to express that
in these sort of signal

747
00:46:01 --> 00:46:04
detection terms, is as an
increase in the noise.

748
00:46:04 --> 00:46:07
An increase in the spread
of these distributions.

749
00:46:07 --> 00:46:10
So what that's going to end up
looking like is you still have

750
00:46:10 --> 00:46:12
the good people and
the bad people.

751
00:46:12 --> 00:46:23
But now your perception
is less accurate.

752
00:46:23 --> 00:46:44
 
 

753
00:46:44 --> 00:46:44
OK.
 

754
00:46:44 --> 00:46:46
That'll do.
 

755
00:46:46 --> 00:46:50
So, they now just overlap more,
because we just don't know

756
00:46:50 --> 00:46:51
as much about these people.
 

757
00:46:51 --> 00:46:56
You're not as good at -- you
want a trivial example of this?

758
00:46:56 --> 00:46:59
 
 

759
00:46:59 --> 00:47:09
Let's take wolves, there's an
out group you know, the ones

760
00:47:09 --> 00:47:11
with the big sharp, you know,
grandma what big teeth

761
00:47:11 --> 00:47:14
you've got, kind of wolves.
 

762
00:47:14 --> 00:47:16
Maybe there's a good wolf
out there somewhere.

763
00:47:16 --> 00:47:17
A nice wolf.
 

764
00:47:17 --> 00:47:19
You know the kind of wolf that
we're supposed to have adopted

765
00:47:19 --> 00:47:27
back in antiquity to make
into dogs eventually.

766
00:47:27 --> 00:47:31
But you meet a wolf on the
street, and you don't

767
00:47:31 --> 00:47:34
know much about him.
 

768
00:47:34 --> 00:47:37
Where should you draw your
threshold before bringing

769
00:47:37 --> 00:47:40
him home to play with
your six year old?

770
00:47:40 --> 00:47:42
You're going to draw
your threshold out

771
00:47:42 --> 00:47:43
here somewhere, right?
 

772
00:47:43 --> 00:47:47
I don't care if I reject
the one nice wolf.

773
00:47:47 --> 00:47:52
It's just really risky
to bring wolves home.

774
00:47:52 --> 00:47:54
And, that's because you're just
really, really ignorant about

775
00:47:54 --> 00:47:55
wolves, you just don't
know much about them.

776
00:47:55 --> 00:47:57
Maybe if you knew wolves
better, you'd know who

777
00:47:57 --> 00:47:58
the nice ones were.
 

778
00:47:58 --> 00:47:59
All right.
 

779
00:47:59 --> 00:48:01
With human populations
it's obviously much less

780
00:48:01 --> 00:48:02
dramatic than that.
 

781
00:48:02 --> 00:48:06
But you don't know much about
these other people, so the

782
00:48:06 --> 00:48:10
distributions theoretically
overlap more.

783
00:48:10 --> 00:48:13
You still only want to make
very few errors where you

784
00:48:13 --> 00:48:15
let bad people in next you.
 

785
00:48:15 --> 00:48:18
So that's going to cause you
to move that threshold still

786
00:48:18 --> 00:48:20
further over in this direction.
 

787
00:48:20 --> 00:48:23
Not because you don't
like these people.

788
00:48:23 --> 00:48:27
Understand that there's no
explicit bias going on here.

789
00:48:27 --> 00:48:32
You're just being cautious
in this in this story.

790
00:48:32 --> 00:48:33
You can get explicit
bias out of those the

791
00:48:33 --> 00:48:34
first three factors.
 

792
00:48:34 --> 00:48:37
But here, this factor has no
explicit bias in it at all.

793
00:48:37 --> 00:48:38
Just ignorance.
 

794
00:48:38 --> 00:48:42
So now you say oh good I'm only
letting this percentage of

795
00:48:42 --> 00:48:46
really bad people and
now obviously there's a

796
00:48:46 --> 00:48:48
little problem here.
 

797
00:48:48 --> 00:48:53
Your Type 1 where you
reject good people.

798
00:48:53 --> 00:48:57
 
 

799
00:48:57 --> 00:49:02
Now you have rejected almost
the entire population

800
00:49:02 --> 00:49:04
of this other group.
 

801
00:49:04 --> 00:49:08
You know that you're not biased
in your heart of hearts,

802
00:49:08 --> 00:49:12
because you can still say, as
it says on the handouts, this

803
00:49:12 --> 00:49:20
little tale of the distribution
some of my best friends are X.

804
00:49:20 --> 00:49:22
Whatever that out group is.
 

805
00:49:22 --> 00:49:27
Some of my best friends are
white, black, Christian,

806
00:49:27 --> 00:49:30
Jewish, whatever the other out
group is you're dealing with.

807
00:49:30 --> 00:49:34
This signal detection story
will get you there with some

808
00:49:34 --> 00:49:37
people in the group who are
fine, because they beat your

809
00:49:37 --> 00:49:41
threshold, and the vast bulk of
the rest of it, who disappear

810
00:49:41 --> 00:49:48
because you're applying the
same caution to an out group,

811
00:49:48 --> 00:49:51
that you were applying to the
group that you knew

812
00:49:51 --> 00:49:51
something about.
 

813
00:49:51 --> 00:50:00
So, that's how ignorance
can end up being a factor

814
00:50:00 --> 00:50:04
in producing what looks
like biased behavior.

815
00:50:04 --> 00:50:07
If you compare these two,
you'd have to say I'm

816
00:50:07 --> 00:50:08
biased against this group.
 

817
00:50:08 --> 00:50:12
Because 100 of these people,
I'm only letting five

818
00:50:12 --> 00:50:13
of them be my friend.
 

819
00:50:13 --> 00:50:16
100 of these people I'm letting
60 of them be my friend.

820
00:50:16 --> 00:50:21
That's a biased outcome
from no explicit bias.

821
00:50:21 --> 00:50:27
Now this question of explicit
versus either no bias or

822
00:50:27 --> 00:50:31
implicit bias is an
interesting one.

823
00:50:31 --> 00:50:39
You don't necessarily
have a clear idea of the

824
00:50:39 --> 00:50:41
biases that you may have.
 

825
00:50:41 --> 00:50:51
One of the more interesting and
more disturbing bits -- there's

826
00:50:51 --> 00:50:58
a thing called an implicit
attitude test, the IAT.

827
00:50:58 --> 00:51:03
If you want to try this
out on yourself, go to

828
00:51:03 --> 00:51:07
www.prejudice.com I think
is the right site.

829
00:51:07 --> 00:51:09
That is one site.
 

830
00:51:09 --> 00:51:18
But if that fails go and find
your way to the website

831
00:51:18 --> 00:51:19
mosuronbanashi at Harvard.
 

832
00:51:19 --> 00:51:23
 
 

833
00:51:23 --> 00:51:25
She is one of the leading
practitioners of this, and her

834
00:51:25 --> 00:51:28
website will link you to a
place where you can try

835
00:51:28 --> 00:51:30
this out on yourself.
 

836
00:51:30 --> 00:51:34
As the website will tell
you, before forewarned.

837
00:51:34 --> 00:51:36
You may find the results
of this experiment to

838
00:51:36 --> 00:51:39
be disturbing to you.
 

839
00:51:39 --> 00:51:42
But, it's well worth
trying out yourself.

840
00:51:42 --> 00:51:44
Now what is this
experiment about?

841
00:51:44 --> 00:51:49
This experiment is, in effect,
a version of a Stroop

842
00:51:49 --> 00:51:51
interference test.
 

843
00:51:51 --> 00:51:56
The classic stroop interference
experiment is an experiment

844
00:51:56 --> 00:52:00
where what you do is you see a
collection of words and your

845
00:52:00 --> 00:52:04
job is, whatever, the word
says, it's just to tell me

846
00:52:04 --> 00:52:08
what color the ink is that
the word is written in.

847
00:52:08 --> 00:52:15
So if I write cat in
red ink, you say red.

848
00:52:15 --> 00:52:21
And, if I write dog, in
blue ink, you say, dog.

849
00:52:21 --> 00:52:21
No.
 

850
00:52:21 --> 00:52:23
You say blue.
 

851
00:52:23 --> 00:52:24
That's a different
interference.

852
00:52:24 --> 00:52:33
The problem is, that if I write
red in blue ink, some people

853
00:52:33 --> 00:52:38
will simply make the mistake of
saying red, and everybody on

854
00:52:38 --> 00:52:43
average, will be substantially
slowed down, because of an

855
00:52:43 --> 00:52:48
inability to suppress
that response.

856
00:52:48 --> 00:52:52
And if I do red in red ink
they'll be speeded up.

857
00:52:52 --> 00:52:55
So, if the two sources conflict
with each other, you're slowed.

858
00:52:55 --> 00:52:59
If the two sources agree with
each other you're speeded.

859
00:52:59 --> 00:52:59
OK.
 

860
00:52:59 --> 00:53:07
So, here's what you do
in an IAT experiment.

861
00:53:07 --> 00:53:13
What you do, is you tell
people, I'm going to

862
00:53:13 --> 00:53:15
show you some words.
 

863
00:53:15 --> 00:53:19
And, if they're good words,
they're nice words,

864
00:53:19 --> 00:53:21
you push this button.
 

865
00:53:21 --> 00:53:25
 
 

866
00:53:25 --> 00:53:32
And, if they're nasty words,
you push this button.

867
00:53:32 --> 00:53:33
OK.
 

868
00:53:33 --> 00:53:34
No problem.
 

869
00:53:34 --> 00:53:38
So nice boink.
 

870
00:53:38 --> 00:53:39
Evil.
 

871
00:53:39 --> 00:53:40
Boink.
 

872
00:53:40 --> 00:53:40
Pain.
 

873
00:53:40 --> 00:53:41
Boink.
 

874
00:53:41 --> 00:53:42
And so on.
 

875
00:53:42 --> 00:53:43
Not very tough.
 

876
00:53:43 --> 00:53:44
OK.
 

877
00:53:44 --> 00:53:46
Second task.
 

878
00:53:46 --> 00:53:53
I'm going to show you some
pictures of people, if it's an

879
00:53:53 --> 00:53:59
old person, I want you to push
one button, if it's a young

880
00:53:59 --> 00:54:05
person, I want you to
push another button.

881
00:54:05 --> 00:54:09
So we put me up there, boink.
 

882
00:54:09 --> 00:54:12
Put you up there, boink.
 

883
00:54:12 --> 00:54:17
Now what we do, is
we do mixed blocks.

884
00:54:17 --> 00:54:20
Where you're going to see
words and pictures together.

885
00:54:20 --> 00:54:29
And, I'm going to tell you, OK,
now if you see a nice word, or

886
00:54:29 --> 00:54:34
an old face, push this button,
if you see a nasty word, or

887
00:54:34 --> 00:54:37
young face, push this button.
 

888
00:54:37 --> 00:54:40
It's just the tasks on
top of each other.

889
00:54:40 --> 00:54:45
Whatever we do in this regard,
you'll be slower, because now

890
00:54:45 --> 00:54:47
you have to keep two rules in
mind at once, but what's

891
00:54:47 --> 00:54:53
striking, is it if you do nice
and old, you're significantly

892
00:54:53 --> 00:54:57
slower than if you
do nice and young.

893
00:54:57 --> 00:55:02
Its as if nice and old
maps the same response.

894
00:55:02 --> 00:55:06
It causes a conflict that
doesn't work for you.

895
00:55:06 --> 00:55:08
And nice and young does.
 

896
00:55:08 --> 00:55:14
As if you've got a biased in
favor of young over old.

897
00:55:14 --> 00:55:18
 
 

898
00:55:18 --> 00:55:21
If we ask you to why is this an
implicit attitude test if we

899
00:55:21 --> 00:55:25
give you an explicit attitude
test that says what's your

900
00:55:25 --> 00:55:27
attitude towards young
and old people.

901
00:55:27 --> 00:55:30
You may perfectly well
report I love old people.

902
00:55:30 --> 00:55:30
I love young people.
 

903
00:55:30 --> 00:55:32
I love everybody.
 

904
00:55:32 --> 00:55:36
But, you'll still come
up with this result.

905
00:55:36 --> 00:55:38
I deliberately did this one,
because it doesn't tend to

906
00:55:38 --> 00:55:43
carry an awful lot emotional
loading for people.

907
00:55:43 --> 00:55:54
But the reason this may be a
disturbing test for people to

908
00:55:54 --> 00:56:00
take -- you should still go off
and do it-- is that if I do

909
00:56:00 --> 00:56:07
nice and black, African
American pictures and nasty and

910
00:56:07 --> 00:56:15
white, regardless of your
explicit report, of what you

911
00:56:15 --> 00:56:19
consider your bias to be, the
white population in this

912
00:56:19 --> 00:56:23
country, will, on average, have
slower reaction time to nice,

913
00:56:23 --> 00:56:29
black pairings then to a
pairing of nice and white.

914
00:56:29 --> 00:56:31
Actually, one of the more
interesting and depressing

915
00:56:31 --> 00:56:35
findings, is that doesn't
even completely reverse

916
00:56:35 --> 00:56:39
with an African American
population of subjects.

917
00:56:39 --> 00:56:41
In the African American
population, the last time I

918
00:56:41 --> 00:56:45
checked on the data, the
pairings were roughly equal.

919
00:56:45 --> 00:56:49
So, the African American
population has presumably some

920
00:56:49 --> 00:56:55
ethnocentric biased in its own
favor, an implicit bias in its

921
00:56:55 --> 00:57:00
own favor, but that's counter
balanced, by some incorporation

922
00:57:00 --> 00:57:06
of overall bias against, and so
they come out is roughly equal.

923
00:57:06 --> 00:57:11
The debates and the literature
about this line of work is this

924
00:57:11 --> 00:57:16
talking about implicit
attitudes which is a notion

925
00:57:16 --> 00:57:18
that regardless of what we
think, we're all racists

926
00:57:18 --> 00:57:19
or something like that.
 

927
00:57:19 --> 00:57:26
Or, is this just say that we
have somehow incorporated that

928
00:57:26 --> 00:57:29
we know, at some level, the
biases of the culture, as a

929
00:57:29 --> 00:57:32
whole, even though, we,
ourselves, may not be biased.

930
00:57:32 --> 00:57:35
It is an interesting question
beyond the scope of what I can

931
00:57:35 --> 00:57:38
talk about today, whether there
is a serious difference

932
00:57:38 --> 00:57:40
between those two.
 

933
00:57:40 --> 00:57:45
But, the disturbing finding is,
-- and you can do this -- it's

934
00:57:45 --> 00:57:48
not black, white, old,
young, by no means is

935
00:57:48 --> 00:57:49
the limit on this game.
 

936
00:57:49 --> 00:57:51
Once you've got the
methodology, you can

937
00:57:51 --> 00:57:53
do it on anything.
 

938
00:57:53 --> 00:58:00
So, shortly after September 11,
they started doing these tests

939
00:58:00 --> 00:58:06
on opinions about nice -- and
you don't need to do with faces

940
00:58:06 --> 00:58:10
either, so if you want to do
Arab versus non- Arab, you

941
00:58:10 --> 00:58:13
can just do it with names.
 

942
00:58:13 --> 00:58:17
So, you do Abdul, and Mohammed
and so on, and then you

943
00:58:17 --> 00:58:20
do Chris, and Jane.
 

944
00:58:20 --> 00:58:22
And, you find that in the
American population, at the

945
00:58:22 --> 00:58:28
moment, nice and Mohammed is
slower then nice and Robert.

946
00:58:28 --> 00:58:34
It's not surprising, but, it
is, nevertheless, disturbing

947
00:58:34 --> 00:58:37
how easy it is to
show these effects.

948
00:58:37 --> 00:58:38
They're robust.
 

949
00:58:38 --> 00:58:41
They show up across
populations, and they show

950
00:58:41 --> 00:58:48
up fairly independent of
what people report.

951
00:58:48 --> 00:58:54
It doesn't matter if you
explicitly, and let's

952
00:58:54 --> 00:58:56
assume that people are
reporting honestly.

953
00:58:56 --> 00:59:01
It doesn't matter if you
explicitly have the bias.

954
00:59:01 --> 00:59:07
You can show something that
looks like a bias with

955
00:59:07 --> 00:59:07
a test of this sort.
 

956
00:59:07 --> 00:59:09
Anyway, give it a try.
 

957
00:59:09 --> 00:59:12
It's interesting
and disturbing.

958
00:59:12 --> 00:59:16
And the interesting and
disturbing department, I will

959
00:59:16 --> 00:59:20
continue in a minute or two
talking about the link between

960
00:59:20 --> 00:59:24
attitudes and actual behavior.
 

961
00:59:24 --> 00:59:26
So, this is a good place
for a short break.

962
00:59:26 --> 01:01:31
 
 

963
01:01:31 --> 01:01:41
So, look.
 

964
01:01:41 --> 01:01:42
Bias.
 

965
01:01:42 --> 01:01:49
We can presumably agree that
bias isn't a good thing, but if

966
01:01:49 --> 01:01:53
it stays in the realm of
private opinion, or if it stays

967
01:01:53 --> 01:01:56
in the realm of implicit
opinion that you don't even

968
01:01:56 --> 01:02:02
know what you have yourself,
it's not exactly front page

969
01:02:02 --> 01:02:07
news, but we also all know from
reading the front page, that

970
01:02:07 --> 01:02:13
there are regrettably frequent
occcasions where one group is

971
01:02:13 --> 01:02:19
willing to slaughter another
group based on very little

972
01:02:19 --> 01:02:24
more, if anything more,
than group identity.

973
01:02:24 --> 01:02:34
So, an absolutely critical
question, is how can people be

974
01:02:34 --> 01:02:39
moved from attitude to action?
 

975
01:02:39 --> 01:02:44
And, the disturbing aspect of
what we know from experimental

976
01:02:44 --> 01:02:51
psych about this, is that it is
surprisingly easy to have your

977
01:02:51 --> 01:02:56
behavior controlled
by outside forces.

978
01:02:56 --> 01:03:00
The experimental work, of
course, cannot get people to go

979
01:03:00 --> 01:03:04
out and slaughter each other.
 

980
01:03:04 --> 01:03:07
Nothing like that would be even
faintly moral, but rather like

981
01:03:07 --> 01:03:10
the Clay and Kandinsky
experiments.

982
01:03:10 --> 01:03:14
You can do experiments that
show that it's surprisingly

983
01:03:14 --> 01:03:21
easy to manipulate the
situation in ways that changes

984
01:03:21 --> 01:03:25
behavior in ways, that changes
behavior in ways that look at

985
01:03:25 --> 01:03:26
least a little bit disturbing.
 

986
01:03:26 --> 01:03:31
One of the classics, back in
the '50's, that gets this

987
01:03:31 --> 01:03:36
literature going, was done by
Ash at Columbia at the time.

988
01:03:36 --> 01:03:43
I think that the picture
is still in the book.

989
01:03:43 --> 01:03:49
A marvelous collection of male,
Columbian nerds from the '50's.

990
01:03:49 --> 01:03:51
Anybody read the chapter
yet, and happen to

991
01:03:51 --> 01:03:51
know if it's there?
 

992
01:03:51 --> 01:03:55
 
 

993
01:03:55 --> 01:03:58
Here's the basic experiment,
here you come into this

994
01:03:58 --> 01:04:04
experiment, and you're
doing an experiment

995
01:04:04 --> 01:04:07
on visual perception.
 

996
01:04:07 --> 01:04:14
Ash shows you a card
with three lines on it.

997
01:04:14 --> 01:04:18
 
 

998
01:04:18 --> 01:04:23
Your job is to say
which line is longer.

999
01:04:23 --> 01:04:26
 
 

1000
01:04:26 --> 01:04:28
Now, the odd thing about
this, well, you're not

1001
01:04:28 --> 01:04:30
an experimentalist, so
why should you care?

1002
01:04:30 --> 01:04:32
But it's a little odd,
as an experiment to be

1003
01:04:32 --> 01:04:34
doing this in a group.
 

1004
01:04:34 --> 01:04:36
But it turns out, you're
doing this in a group.

1005
01:04:36 --> 01:04:43
And, so Ash holds up the card,
and you say B, and you say B,

1006
01:04:43 --> 01:04:46
and you say B, and
everybody said B.

1007
01:04:46 --> 01:04:47
Next card.
 

1008
01:04:47 --> 01:04:49
I'm not going to bother
changing my cards, but

1009
01:04:49 --> 01:04:50
you get the basic idea.
 

1010
01:04:50 --> 01:04:58
On the critical trial,
up comes this card.

1011
01:04:58 --> 01:05:02
He says, C.
 

1012
01:05:02 --> 01:05:04
He says C.
 

1013
01:05:04 --> 01:05:06
He says C.
 

1014
01:05:06 --> 01:05:09
She says C.
 

1015
01:05:09 --> 01:05:10
C.
 

1016
01:05:10 --> 01:05:11
C.
 

1017
01:05:11 --> 01:05:16
Now, we're up to -- I
stuck with her because,

1018
01:05:16 --> 01:05:17
she's got glasses.
 

1019
01:05:17 --> 01:05:22
Because the nerdy Columbia
guy has glasses on too.

1020
01:05:22 --> 01:05:24
What does she do?
 

1021
01:05:24 --> 01:05:26
Well, why did all
these guys say C?

1022
01:05:26 --> 01:05:28
Are some kind of morons?
 

1023
01:05:28 --> 01:05:30
They're all confederates
of the experimenter.

1024
01:05:30 --> 01:05:34
The only real subject
is this person.

1025
01:05:34 --> 01:05:41
And, the question
is, does she say C?

1026
01:05:41 --> 01:05:50
The answer is about a third of
the time in the original Ash

1027
01:05:50 --> 01:05:54
experiment, the answer's
yes, she says C.

1028
01:05:54 --> 01:05:57
It is completely clear that
even when she doesn't say

1029
01:05:57 --> 01:06:02
C, she's uncomfortable.
 

1030
01:06:02 --> 01:06:06
This is an experiment
on peer pressure.

1031
01:06:06 --> 01:06:11
And, it's perfectly clear that
when everybody else is saying

1032
01:06:11 --> 01:06:15
C, she's busy taking off her
glasses, and checking them, and

1033
01:06:15 --> 01:06:18
stuff like that, to see
what's going on here.

1034
01:06:18 --> 01:06:20
There's something wrong.
 

1035
01:06:20 --> 01:06:25
What do you think manipulates
-- the standard result is you

1036
01:06:25 --> 01:06:27
got about a third of the people
complying with the pressure.

1037
01:06:27 --> 01:06:30
What reduces that compliance?
 

1038
01:06:30 --> 01:06:36
 
 

1039
01:06:36 --> 01:06:37
AUDIENCE: How [INAUDIBLE]
 

1040
01:06:37 --> 01:06:38
PROFESSOR: Yeah, sure.
 

1041
01:06:38 --> 01:06:42
 
 

1042
01:06:42 --> 01:06:44
Presumably it's hard
to get the result.

1043
01:06:44 --> 01:06:46
 
 

1044
01:06:46 --> 01:06:49
But, OK if we keep the
physical stimuli the same.

1045
01:06:49 --> 01:06:49
Yes.
 

1046
01:06:49 --> 01:06:52
 
 

1047
01:06:52 --> 01:06:53
AUDIENCE: [INAUDIBLE]
 

1048
01:06:53 --> 01:06:58
PROFESSOR: If somebody picks
A, and it just looks noisy.

1049
01:06:58 --> 01:07:01
I don't know if they ever did
that particular manipulation.

1050
01:07:01 --> 01:07:03
That's an interesting question.
 

1051
01:07:03 --> 01:07:05
That might change things.
 

1052
01:07:05 --> 01:07:10
AUDIENCE: If they have six or
seven people, [INAUDIBLE]

1053
01:07:10 --> 01:07:14
PROFESSOR: It doesn't
take any support.

1054
01:07:14 --> 01:07:17
You probably know this from
arguments with groups

1055
01:07:17 --> 01:07:19
of people or something.
 

1056
01:07:19 --> 01:07:21
It's hard to be the
first person to voice

1057
01:07:21 --> 01:07:22
the minority view.
 

1058
01:07:22 --> 01:07:24
It's much easier to be
the second person.

1059
01:07:24 --> 01:07:25
I think I actually have
the data from that.

1060
01:07:25 --> 01:07:26
Yeah.
 

1061
01:07:26 --> 01:07:27
One supporter.
 

1062
01:07:27 --> 01:07:31
One supporter in the group
drops compliance from

1063
01:07:31 --> 01:07:34
one third to 1/12.
 

1064
01:07:34 --> 01:07:40
So, big drop in the
amount of compliance.

1065
01:07:40 --> 01:07:45
The more people who say C, the
more likely you are to comply.

1066
01:07:45 --> 01:07:48
The smaller the group, the less
likely you are to comply.

1067
01:07:48 --> 01:07:55
But, the point is, that even in
a matter as seemingly straight

1068
01:07:55 --> 01:08:01
forward as which line is
longer, you can feel that

1069
01:08:01 --> 01:08:04
pressure from others.
 

1070
01:08:04 --> 01:08:08
The most famous experiment in
this canon is an experiment

1071
01:08:08 --> 01:08:11
done by Stanley Milgram.
 

1072
01:08:11 --> 01:08:15
And, in that experiment,
here's the set up.

1073
01:08:15 --> 01:08:23
You come into the lab for
a study on learning.

1074
01:08:23 --> 01:08:26
The effects of
punishment on learning.

1075
01:08:26 --> 01:08:28
And, there are two of you.
 

1076
01:08:28 --> 01:08:33
And, one of you is going to be
the learner and one of you

1077
01:08:33 --> 01:08:36
is going to be the teacher.
 

1078
01:08:36 --> 01:08:37
OK.
 

1079
01:08:37 --> 01:08:39
And we're going to
decide this randomly.

1080
01:08:39 --> 01:08:41
Flip a coin.
 

1081
01:08:41 --> 01:08:44
You're going to be
the teacher today.

1082
01:08:44 --> 01:08:47
Now, in fact, this
is not random.

1083
01:08:47 --> 01:08:48
The subject is
always the teacher.

1084
01:08:48 --> 01:08:53
The learner is always a stooge
of the experiementer and

1085
01:08:53 --> 01:08:55
here's what happens.
 

1086
01:08:55 --> 01:08:58
You're told you going to do
some sort of a task, and your

1087
01:08:58 --> 01:09:05
job, as the teacher, is to give
the learner a shock every

1088
01:09:05 --> 01:09:07
time they make a mistake.
 

1089
01:09:07 --> 01:09:11
This was done back in the '60's
with this great big hunk of

1090
01:09:11 --> 01:09:18
electrical equipment, with a
gazillion switches on there,

1091
01:09:18 --> 01:09:25
running from 15 volts to, I
think 450 volts, in 15 volt

1092
01:09:25 --> 01:09:32
increments, with instructive
little labels like mild, and

1093
01:09:32 --> 01:09:36
then, up here somewhere is
severe, and by the time, you

1094
01:09:36 --> 01:09:41
get up here, it's labeled
something like XXX.

1095
01:09:41 --> 01:09:44
 
 

1096
01:09:44 --> 01:09:46
The rule is every time
you make a mistake, you

1097
01:09:46 --> 01:09:48
increase the voltage.
 

1098
01:09:48 --> 01:09:53
Now, we will give you
a 45 volt shock.

1099
01:09:53 --> 01:09:57
You, the teacher, gets a
45 volt shock, just to

1100
01:09:57 --> 01:09:58
see what it's like.
 

1101
01:09:58 --> 01:10:03
And, a 45 volt shock from this
apparatus is mildly unpleasant.

1102
01:10:03 --> 01:10:05
It's nothing you'd
want to sign up for.

1103
01:10:05 --> 01:10:07
It's not going to kill ya,
though the suggestion

1104
01:10:07 --> 01:10:10
is that this might.
 

1105
01:10:10 --> 01:10:16
And, that's the
rule of the game.

1106
01:10:16 --> 01:10:22
Now, before doing the
experiment Milgram went and

1107
01:10:22 --> 01:10:26
asked everybody under the sun,
what the result would be.

1108
01:10:26 --> 01:10:32
 
 

1109
01:10:32 --> 01:10:34
Well, the answer is that
everybody under the sun will

1110
01:10:34 --> 01:10:37
bail out pretty early here.
 

1111
01:10:37 --> 01:10:39
Nobody's going to get them
very massive shocks.

1112
01:10:39 --> 01:10:47
 
 

1113
01:10:47 --> 01:10:52
He asked theologians, he asked
psychologists, he ask people

1114
01:10:52 --> 01:10:57
off the street, and everybody
agreed this is not going to

1115
01:10:57 --> 01:11:01
lead to much in the
way of shocks.

1116
01:11:01 --> 01:11:05
And, this made the result
of the first experimental

1117
01:11:05 --> 01:11:07
a little surprising.
 

1118
01:11:07 --> 01:11:11
Everybody went all the way
through to 450 volts.

1119
01:11:11 --> 01:11:15
The entire population of
subjects in the first study

1120
01:11:15 --> 01:11:19
went through to 450 volts.
 

1121
01:11:19 --> 01:11:23
Now, were they with a
thrilled about this?

1122
01:11:23 --> 01:11:24
No.
 

1123
01:11:24 --> 01:11:26
It was also absolutely clear
that the subjects were very

1124
01:11:26 --> 01:11:28
uncomfortable about this.
 

1125
01:11:28 --> 01:11:32
And, that they questioned
whether they should do this.

1126
01:11:32 --> 01:11:36
And, Milgram had an absolutely
stereotypical, he'd prepared,

1127
01:11:36 --> 01:11:39
in advance, the response.
 

1128
01:11:39 --> 01:11:42
Please continue, the
experiment must go on.

1129
01:11:42 --> 01:11:43
And, that was it.
 

1130
01:11:43 --> 01:11:45
You were free to leave.
 

1131
01:11:45 --> 01:11:47
Though he didn't explain this
to you in great detail.

1132
01:11:47 --> 01:11:50
But if you said,
should I do this?

1133
01:11:50 --> 01:11:52
He said, please continue.
 

1134
01:11:52 --> 01:11:53
The experiment must go on.
 

1135
01:11:53 --> 01:11:55
And people did.
 

1136
01:11:55 --> 01:12:00
Now, he was a little surprised.
 

1137
01:12:00 --> 01:12:05
In the original version, the
alleged learner had been

1138
01:12:05 --> 01:12:07
taken out and put in
a different room.

1139
01:12:07 --> 01:12:09
And Milgram figured,
well, look.

1140
01:12:09 --> 01:12:13
Maybe these people just didn't
believe the set up story here.

1141
01:12:13 --> 01:12:19
So, they moved the alleged
learner to a position where

1142
01:12:19 --> 01:12:26
he was visible and making
noises about this.

1143
01:12:26 --> 01:12:29
 
 

1144
01:12:29 --> 01:12:34
He's vigorously protesting
as the voltage gets larger.

1145
01:12:34 --> 01:12:40
At some point, up here,
he says he's not going

1146
01:12:40 --> 01:12:41
to respond anymore.
 

1147
01:12:41 --> 01:12:48
 
 

1148
01:12:48 --> 01:12:50
So, the experiment's rigged,
so that he keeps making

1149
01:12:50 --> 01:12:51
mistakes as a result.
 

1150
01:12:51 --> 01:12:52
No response is
considered an error.

1151
01:12:52 --> 01:12:54
And Milgram is there saying
please continue, the

1152
01:12:54 --> 01:12:56
experiment must go on.
 

1153
01:12:56 --> 01:12:56
Oh.
 

1154
01:12:56 --> 01:13:00
He also makes useful comments
like, I have a heart condition,

1155
01:13:00 --> 01:13:01
and stuff like that.
 

1156
01:13:01 --> 01:13:04
It's pretty vivid stuff.
 

1157
01:13:04 --> 01:13:05
So, what happens in that case?
 

1158
01:13:05 --> 01:13:05
Well.
 

1159
01:13:05 --> 01:13:06
Great.
 

1160
01:13:06 --> 01:13:08
No longer do 100% of the
subjects go all the way

1161
01:13:08 --> 01:13:10
through to the end.
 

1162
01:13:10 --> 01:13:14
Only 2/3 of them go all the way
through with a subject who has

1163
01:13:14 --> 01:13:20
stopped responding, and has
announced for all you know

1164
01:13:20 --> 01:13:24
you're killing this
guy, perhaps.

1165
01:13:24 --> 01:13:29
This is pretty disturbing
kind of stuff.

1166
01:13:29 --> 01:13:34
It was very disturbing
to the the subjects.

1167
01:13:34 --> 01:13:38
Actually, this is one of the
experiments that produces the

1168
01:13:38 --> 01:13:43
need for informed consent
in experimental psychology.

1169
01:13:43 --> 01:13:46
You didn't just go and hijack
people off the street and say,

1170
01:13:46 --> 01:13:50
you have to be in
my experiment.

1171
01:13:50 --> 01:13:52
These people were volunteers.
 

1172
01:13:52 --> 01:13:55
But, there was no sort of
consent process the way

1173
01:13:55 --> 01:13:56
we would have today.
 

1174
01:13:56 --> 01:14:00
And, the level of distress
produced in these subjects was

1175
01:14:00 --> 01:14:08
part of what drove the field
to require informed consent.

1176
01:14:08 --> 01:14:10
Why was the experiment
done at all?

1177
01:14:10 --> 01:14:13
This is an experiment done
in the '60's, less than a

1178
01:14:13 --> 01:14:17
generation after World War II.
 

1179
01:14:17 --> 01:14:19
And, a question that had
obsessed social psychology,

1180
01:14:19 --> 01:14:24
since World War II, was how
could the Nazi atrocities

1181
01:14:24 --> 01:14:25
have happened?
 

1182
01:14:25 --> 01:14:29
Who were the people who
went and killed millions

1183
01:14:29 --> 01:14:31
of other people?
 

1184
01:14:31 --> 01:14:34
Not the soldiers, but the
people who killed six million

1185
01:14:34 --> 01:14:38
Jews, and I don't remember, a
million and a half gypsies,

1186
01:14:38 --> 01:14:41
and some large number
of gays and so on.

1187
01:14:41 --> 01:14:43
Who were these people?
 

1188
01:14:43 --> 01:14:49
There was a theory out there
that encapsulated in a book

1189
01:14:49 --> 01:14:51
called The Authoritarian
Personality which among other

1190
01:14:51 --> 01:14:56
things, fed into stereotypes
about the Germans which was

1191
01:14:56 --> 01:15:00
that there was a certain type
of person, who was willing --

1192
01:15:00 --> 01:15:04
the Nuremberg trials, the war
crime trials after the war, had

1193
01:15:04 --> 01:15:07
produced over and over again,
the line, "I was just

1194
01:15:07 --> 01:15:10
following orders".
 

1195
01:15:10 --> 01:15:12
And, the notion was, well
there are some people who

1196
01:15:12 --> 01:15:13
are just good it that.
 

1197
01:15:13 --> 01:15:16
They just follow orders.
 

1198
01:15:16 --> 01:15:19
And, the rest of us
were not like that.

1199
01:15:19 --> 01:15:23
Milgram suspected that
was not the case.

1200
01:15:23 --> 01:15:26
Milgram suspected that the
answer was that under the right

1201
01:15:26 --> 01:15:32
situation, many people could be
pushed into acts, that they

1202
01:15:32 --> 01:15:36
would objectively think
were impermissible.

1203
01:15:36 --> 01:15:43
This is his effort to
get at that question.

1204
01:15:43 --> 01:15:47
 
 

1205
01:15:47 --> 01:15:51
If this sounds like current
events to you, every social

1206
01:15:51 --> 01:15:54
psychologist with half a
credential was on the news

1207
01:15:54 --> 01:16:01
after the Abu Ghraib Prison
Scandals earlier in the year,

1208
01:16:01 --> 01:16:04
because it just sounded so
much like this sort

1209
01:16:04 --> 01:16:05
of problem again.
 

1210
01:16:05 --> 01:16:11
The people who were charged,
many of them responded with the

1211
01:16:11 --> 01:16:17
response, that they were simply
if not following orders,

1212
01:16:17 --> 01:16:19
following the implicit
instructions that they

1213
01:16:19 --> 01:16:22
felt around them.
 

1214
01:16:22 --> 01:16:25
You got to endless articles in
the papers, saying, Oh, you

1215
01:16:25 --> 01:16:28
know, so and so was just like
everybody else back home.

1216
01:16:28 --> 01:16:31
I don't understand how he, I
don't understand how she, could

1217
01:16:31 --> 01:16:36
end up in these pictures doing
things that any reasonable

1218
01:16:36 --> 01:16:39
person would say are
unacceptable in a in a

1219
01:16:39 --> 01:16:42
military prison situation.
 

1220
01:16:42 --> 01:16:45
It's the same kind of question,
different scale of magnitude,

1221
01:16:45 --> 01:16:47
of course, to the
Nazi atrocities.

1222
01:16:47 --> 01:16:49
But it's the same question.
 

1223
01:16:49 --> 01:16:56
How do people end up
doing things like this?

1224
01:16:56 --> 01:16:59
Before attempting to answer,
which I can see is going to run

1225
01:16:59 --> 01:17:02
into my next lecture, let me
tell you about a different

1226
01:17:02 --> 01:17:05
experiment designed to get
at the same question.

1227
01:17:05 --> 01:17:14
This is an experiment where you
think you're in a study, sort

1228
01:17:14 --> 01:17:17
of a consumer relations kind
of study, that you might run

1229
01:17:17 --> 01:17:19
into it the shopping mall.
 

1230
01:17:19 --> 01:17:23
They've set up the experiment
at a shopping mall.

1231
01:17:23 --> 01:17:26
And, the cover story is this.
 

1232
01:17:26 --> 01:17:30
We're doing some research
on community values.

1233
01:17:30 --> 01:17:33
Because we're basically
doing investigations

1234
01:17:33 --> 01:17:35
for a legal case.
 

1235
01:17:35 --> 01:17:38
Here's the situation.
 

1236
01:17:38 --> 01:17:42
This guy was living with a
woman to whom he's not married.

1237
01:17:42 --> 01:17:48
His employer found out
about it, and fired him.

1238
01:17:48 --> 01:17:55
This has gone to court,
and the court case hinges

1239
01:17:55 --> 01:17:57
on community standards.
 

1240
01:17:57 --> 01:18:01
If community standards are that
living in sin, out of wedlock,

1241
01:18:01 --> 01:18:04
is a bad, bad thing, then
it's OK to fire him.

1242
01:18:04 --> 01:18:05
Otherwise not.
 

1243
01:18:05 --> 01:18:07
So we've gotta find out what
the community standards are.

1244
01:18:07 --> 01:18:09
Let's all have a discussion.
 

1245
01:18:09 --> 01:18:11
So, I've told you the story.
 

1246
01:18:11 --> 01:18:13
I've got the cameras
rolling here.

1247
01:18:13 --> 01:18:16
I, the experimenter, I'm
going to step out and

1248
01:18:16 --> 01:18:17
you guys discuss.
 

1249
01:18:17 --> 01:18:17
OK.
 

1250
01:18:17 --> 01:18:18
You guys discuss.
 

1251
01:18:18 --> 01:18:19
That's great.
 

1252
01:18:19 --> 01:18:22
Now, I come back and there's
a group of ten of you or

1253
01:18:22 --> 01:18:24
something discussing this.
 

1254
01:18:24 --> 01:18:26
I come back in, and I
say, that was great.

1255
01:18:26 --> 01:18:29
Thank you very much.
 

1256
01:18:29 --> 01:18:31
I know what you believe,
because I've been

1257
01:18:31 --> 01:18:32
watching this.
 

1258
01:18:32 --> 01:18:37
But, I want you, you and you,
please to argue from the

1259
01:18:37 --> 01:18:39
point of view, that the
guy should be fired.

1260
01:18:39 --> 01:18:41
I know it's not what
you really believe.

1261
01:18:41 --> 01:18:42
But just argue for that.
 

1262
01:18:42 --> 01:18:43
OK.
 

1263
01:18:43 --> 01:18:46
Comes back in.
 

1264
01:18:46 --> 01:18:46
OK.
 

1265
01:18:46 --> 01:18:49
Now I want you, you, and you
to join that argument arguing

1266
01:18:49 --> 01:18:51
that the guy should be fired.
 

1267
01:18:51 --> 01:18:52
Fine.
 

1268
01:18:52 --> 01:18:56
And then in the penultimate
step, is I want everybody to

1269
01:18:56 --> 01:19:02
have a chance to look into
the camera, and say why

1270
01:19:02 --> 01:19:03
the guy should be fired.
 

1271
01:19:03 --> 01:19:03
I know you don't believe.
 

1272
01:19:03 --> 01:19:07
But, just give me a little
speech as if you believed it.

1273
01:19:07 --> 01:19:08
OK.
 

1274
01:19:08 --> 01:19:09
Cool.
 

1275
01:19:09 --> 01:19:09
OK.
 

1276
01:19:09 --> 01:19:10
Last step.
 

1277
01:19:10 --> 01:19:17
Here's this statement that says
that I can use my videotape in

1278
01:19:17 --> 01:19:21
any fashion I wish
including submitting it as

1279
01:19:21 --> 01:19:22
evidence in court.
 

1280
01:19:22 --> 01:19:24
Will you please sign.
 

1281
01:19:24 --> 01:19:27
 
 

1282
01:19:27 --> 01:19:28
I'll be back in a minute.
 

1283
01:19:28 --> 01:19:29
I gotta go rinse a
few things out.

1284
01:19:29 --> 01:19:31
But you guys decide.
 

1285
01:19:31 --> 01:19:37
The question is do people sign?
 

1286
01:19:37 --> 01:19:41
I mean this is obviously
basically asking you to perjure

1287
01:19:41 --> 01:19:47
yourself if that wasn't
what you believed.

1288
01:19:47 --> 01:19:50
Now this was a huge
experimental design originally.

1289
01:19:50 --> 01:19:55
They were crossing a number of
people with gender and they

1290
01:19:55 --> 01:19:58
originally had a plan for, well
I can't remember, a

1291
01:19:58 --> 01:19:59
gazillion groups.
 

1292
01:19:59 --> 01:20:04
They called off the experiment
early, because like the Ash

1293
01:20:04 --> 01:20:07
experiment and like the Milgram
experiment, this experiment

1294
01:20:07 --> 01:20:12
produced extremely strong
feelings in their subjects.

1295
01:20:12 --> 01:20:15
They had gotten a form of
informed consent, which I can't

1296
01:20:15 --> 01:20:21
go into now, but even with
that, it felt unethical

1297
01:20:21 --> 01:20:22
to them to keep going.
 

1298
01:20:22 --> 01:20:24
So, they had 33 groups.
 

1299
01:20:24 --> 01:20:26
It was a busted design.
 

1300
01:20:26 --> 01:20:27
But they had 33 groups.
 

1301
01:20:27 --> 01:20:32
Of those 33 groups, of those
33 groups, how many did the

1302
01:20:32 --> 01:20:35
Milgram thing, went all the
way and everybody sign?

1303
01:20:35 --> 01:20:35
Do you think?
 

1304
01:20:35 --> 01:20:38
 
 

1305
01:20:38 --> 01:20:40
The answer is one.
 

1306
01:20:40 --> 01:20:40
I think.
 

1307
01:20:40 --> 01:20:42
One group got total obedience.
 

1308
01:20:42 --> 01:20:46
16 of the groups unanimous
refusal to sign.

1309
01:20:46 --> 01:20:52
Nine groups got majority
refusal to sign.

1310
01:20:52 --> 01:20:56
So, in this experiment,
compliance was

1311
01:20:56 --> 01:20:58
much, much lower.
 

1312
01:20:58 --> 01:21:03
And, the question that I'll
take up next time, for the

1313
01:21:03 --> 01:21:06
start of the next lecture, is
what's the difference between

1314
01:21:06 --> 01:21:09
the Milgram experiment -- let
me say one last thing about

1315
01:21:09 --> 01:21:10
the Milgram experiment.
 

1316
01:21:10 --> 01:21:15
This is not an isolated
result -- Milgram's at Yale.

1317
01:21:15 --> 01:21:17
It's not just an isolated
experiment that happens

1318
01:21:17 --> 01:21:19
in New Haven in 1960.
 

1319
01:21:19 --> 01:21:22
Replicated all over the place.
 

1320
01:21:22 --> 01:21:23
Doesn't matter in America.
 

1321
01:21:23 --> 01:21:25
Doesn't matter age,
sex of the subjects.

1322
01:21:25 --> 01:21:27
It replicates beautifully.
 

1323
01:21:27 --> 01:21:29
Why does this work, and the
other experiment didn't?

1324
01:21:29 --> 01:21:32
 
 

