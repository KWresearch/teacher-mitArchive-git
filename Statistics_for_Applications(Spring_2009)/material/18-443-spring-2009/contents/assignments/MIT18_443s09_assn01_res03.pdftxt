MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

CONVOLUTION,  ADDING  GAMMA  VARIABLES,  AND  CHI-SQUARES 

18.443 

This  will  be  a  review  of  some  facts  from  probability  that  might  or  might  not  be 
familiar. 
If  f  and  g  are  two  real-valued  functions  of  a  real  variable,  having  in  mind 
probability  density functions, then their  convolution  is deﬁned by 
  ∞ 
  ∞ 

(f  ∗ g )(t) =  �
−∞ 

f (t − y )g (y )dy  =  �

f (x)g (t − x)dx, 

−∞ 

where   if  either  integral  exists  for  a  given  t,  so  does  the  other  one  with  the  same  value,  by 
the  substitutions  x  =  t  − y  or  y  =  t  − x.  Convolution  gives  us  the  density  of  the  sum  of 
two  independent  random  variables  having  densities: 

Theorem  1. 
If  X  and  Y  are  independent  random  variables  having  densities  f  and  g 
respectively, then  X  +  Y  has density  f  ∗ g . 
Proof.  By  independence, (X, Y ) has bivariate density  f (x)g (y ).  Thus  for  any  t, 
 

P (X  + Y  ≤ t) =  � �

f (x)g (y ) dy dx. 

x+y≤t 

 

∞ 

 

t

 

t 

 

∞ 

 ∞ 
 t−x
�
�
Since  x  + y  ≤ t  is  equivalent  to  y  ≤ t − x,  we  get  =  −∞  f (x) −∞  g (y ) dy dx.  Making  the 
substitution  u  =  y +  x  in  the  inner  integral  for  each  ﬁxed  value  of  x,  so  that  du  =  dy ,  we 
get 

f (x)

g (u  − x) du dx  = 

f (x)g (u  − x) dx du 

�
�
�
�
−∞  −∞ 
−∞ 
−∞ 
where  the  integrals  were  interchanged  (justiﬁably  since  f  ≥ 0  and  g  ≥ 0).  To  ﬁnd  the 
density  of  X + Y  we just have to diﬀerentiate  with  respect to  t,  which  by  the  fundamental 
theorem  of  calculus gives (f  ∗ g )(t) as  stated, Q.E.D. 
Next,  convolution  will  be  applied  to  gamma  densities.  Recall  that  for  any  α >  0 
and  λ >  0  a Γ(α, λ) density is given by  fα,λ(x)  = 0 for  x  ≤ 0  and  for  x >  0  it  equals 
λαxα−1 e−λx /Γ(α)  where  Γ(α)  is  deﬁned  as  ∫  ∞  xα−1 e−xdx.  The  next  fact  can  be  called 
0 
the  “Addition  theorem”  for  gamma  variables.  Also  recall  that  X  ∼ D  means that  X  has 
distribution  or  density  D. 
Theorem  2.  If  X  and  Y  are independent,  X  ∼ Γ(α, λ) and  Y  ∼ Γ(β , λ) (with  the  same 
λ), then X  +  Y  ∼ Γ(α + β , λ). 
Proof.  Applying  Theorem  1  with  f  the Γ(α, λ) density  and  g  the Γ(β , λ) density,  noting 
that  f (u) = 0 for  u  ≤ 0  and  g (y ) = 0 for  y  ≤ 0,  we have (f  ∗ g )(x) = 0 for  x  ≤ 0  while  for 
x >  0  we  have 
 
 

(f  ∗ g )(x) =  �

∞ 

−∞ 

f (x  − y )g (y )dy  =  �
0 

x 

f (x  − y )g (y )dy 

1 

because  for  the  integrand  to  be  non-zero  we  need  y >  0  and  x  − y >  0  so  y < x. 
Then  plugging  in  the  deﬁnitions  of  the  gamma  densities,  we  have  for  the  constant  c  = 
λα+β /(Γ(α)Γ(β )), 

 

x 

(f  ∗ g )(x) =  c �
0 

(x  − y )α−1 e 
−λ(x−y) β−1  −λy dy 
y
e 
 

x 

=  ce −λx  �
(x  − y )α−1 y β−1 dy . 
Making  the substitution  u  =  y/x, noting  that  y  and  u  are variables of integration and  that 
integrals  are  evaluated  for  each  ﬁxed  value  of  x >  0,  we  get 
  1 

0 

u β−1 (1 − u)α−1 du. 

(f  ∗ g )(x) =  ce −λx  x α+β−1 �
0 
1 uβ−1 (1 − u)α−1 du,  and  called  the  beta  function  of  β  and 
Now,  B (β , α),  deﬁned  as  ∫0
α,  doesn’t  depend  on  x.  So,  since  f  ∗  g  is  a  probability  density  by  Theorem  1,  with 
∫  ∞ (f  ∗ g )(x)dx  = 1,  it  must  be  the  Γ(α +  β , λ) density,  as  stated, Q.E.D. 
0 
Moreover,  matching  up  the  constants  at  the  end  of  the  last  proof,  we  can  express  the 
beta  function  in  terms  of  gamma  functions: 

B (β , α)  = Γ(α)Γ(β )/Γ(α +  β ). 

From  this  it  follows  that  B (α, β )  ≡  B (β , α).  A  family  of  probability  densities  called 
beta densities  on  the interval (0, 1)  is deﬁned by  bα,β (x) =  xα−1 (1 − x)β−1 /B (α, β ) for 
0  < x <  1 and  bα,β (x) = 0 otherwise for any  α  and  β  such  that 0  < α  <  ∞ and 0  < β <  ∞. 
For  any  positive  integer  k , a  χ2 
k  variable,  or  a  chi-squared  variable  with  k  degrees  of 
freedom,  is  deﬁned  as  a  variable  given  by 

χ2 
2  +
k  =  Z1

2 
+  Zk 

· · · 
where  Z1 , ..., Zk  are i.i.d.  N (0, 1)  variables.  This  is  the  deﬁnition  given  in  Rice,  section 
6.2,  ﬁrst  for  k  =  1  and  then  for  general  k  = 2, 3, ....  As  Rice  mentions,  a  χ2  variable has 
1 | ≤ √x) and 
k 
2  ≤ x) =  P (|Z
a Γ(k/2, 1/2)  distribution.  To  show  this,  for  any  x >  0,  P (Z1
2  does  have  a 
by  the  fundamental  theorem  of  calculus  and  the  chain  rule  we  see  that  Z1
Γ(1/2, 1/2)  distribution.  Then,  the  fact  for  all  k  = 2, 3, ...,  follows  by  induction,  applying 
the  addition theorem for gamma  variables (Theorem 2)  at  each  step.  We have  λ  = 1/2 for 
all  these distributions. 

2


