MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.443


THE DELTA-METHOD, MULTINOMIAL DISTRIBUTIONS,

AND AN EXAMPLE: STANDARD ERROR OF LOG ODDS RATIOS


The  delta-method  gives  a  way  that  asymptotic  normality  can  be  preserved  under 
nonlinear,  but  diﬀerentiable,  transformations.  The  method  is  well  known;  one  version 
of it is given in J.  Rice,  Mathematical  Statistics  and  Data  Analysis,  3d.  ed.,  2007,  §4.6, 
including  second derivatives.  Here,  ﬁrst  a simple form of it  using  only  a ﬁrst derivative, for 
functions  of  one  variable,  will  be  given.  A  multidimensional  version  is  used  in  Section  3.7 
of  Mathematical  Statistics,  18.466  course  notes  by  R.  Dudley,  on  the  MIT  OCW  website. 
For multinomial distributions, their asymptotic behavior for large  n  will be  mentioned  and 
applied  to  chi-squared  statistics  and  odds  ratios. 

→ +∞, 
→
0  either  as  x
Notations  with  O  and   o:  if  g >  0 then  f  =  o(g ) means  that  f /g 
→namely  lim sup |f |/g <  +∞ under  a  given  limit  condition.  The  same  notations  also  apply 
0,  or  whatever  condition  is  speciﬁed,  while  f  =  O(g ) means  that  f /g  stays bounded, 
x 
to  sequences  indexed  by  an  integer  n  → ∞,  e.g.  an  =  o(bn ) is  used  for  bn  >  0  and  means 
0. →There  are  corresponding  notions  “in  probability:” 
an/bn 
if  Un  is  a  sequence  of  random 
variables and  an  a  sequence  of  constants  >  0 then  Un  =  Op (an) means that for every   ε >  0 
0  in probability  means 
there   is  an  M  such  that  Pr( Un /an  > M )  < ε  for  all  n.  Un 
|
|
→
→
→ as  n  → ∞. 
|
|
that  for  every  ε >  0,  Pr( Un > ε)
0 
Un  =  op (an ) means  that  Un/an 
0
in probability. 

Theorem. Let  Yn  be  a  sequence  of  real-valued  random  variables  such  that  for  some  µ and 
σ ,  √n(Yn  − µ) converges  in  distribution  as  n  → ∞ to  N (0, σ 2).  Let  f  be  a  function  from 
R  into  R  having  a  derivative  f ′ (µ) at  µ.  Then  √n[f (Yn) − f (µ)] converges  in  distribution 
as  n  → ∞ to  N (0, f ′(µ)2σ 2 ). 
Remarks.  In  statistics,  where  µ  is  an  unknown  parameter,   one  will  want  f  to be dif­
ferentiable  at  all  possible  µ  (and preferably, for f ′  to  be  continuous,  although  that  is  not 
needed  in  the  proof ).  An  example  of  Yn  satisfying  the  conditions  is:  let  X1 , ..., Xn, ...  be 
i.i.d.  random  variables  with  ﬁnite  mean  µ  and  variance  σ 2 ,  and  let  Yn  be  the  sample  mean 
Yn  = (X1  +
· · · 
+  Xn)/n.
Proof. We have Yn −µ =  Op (1/√n) as  n  → ∞. Also, f (y ) =  f (µ)+f ′  (µ)(y −µ)+o(|y −µ|) 
as  y
→
µ  by  deﬁnition  of  derivative.  Thus 

so 

f (Yn ) =  f (µ) + f  ′  (µ)(Yn  − µ) + op (|Yn  − µ|), 
√n[f (Yn) − f (µ)] =  f  ′  (µ)√n(Yn  − µ) + √nop (1/√n). 
The last term is  op (1),  so  the  conclusion  follows. 

� 

Multinomial distributions.  First let  n  = 1.  For  any  set (event)  A  let 1A  be its indicator 
function,  so  that  1A (x)  = 1 if  x  is  in  A  and  0  otherwise.  For  a  given  probability  P , 

1


the  covariance  of  two  indicator  functions  is  clearly  given  by  Cov(1A , 1B ) =  P (A  ∩ B ) −
P (A)P (B ).  In  two  special  cases,  for  A  =  B  we  get  var(1A ) =  P (A) − P (A)2  =  P (A)[1 −
P (A)],  the  known  variance  of a  Bernoulli  variable.  If  A  and  B  are disjoint, i.e.  A ∩ B  is 
empty, then Cov(1A , 1B ) =  −P (A)P (B ). 
Suppose  on  n  =  1  trial  there  are  k  distinct  possible  outcomes  A1 , ..., Ak  with proba­
bilities  P (Ai) =  pi  for  i  = 1, ..., k .  Deﬁne  a  k-dimensional  random  vector  X  = (x1 , ..., xk) 
such  that  xj  = 1 if  Aj  occurs  and  xj  =  0  otherwise,  in  other  words  xj  = 1Aj .  Now  sup­
pose X1 , ..., Xn  are  n  i.i.d. (independent  and identically distributed)  k-dimensional  random 
n
�
vectors  each  having  the  same  distribution  as  X .  Let  Sn  = 
i=1 Xi  = (n1 , ..., nk ).  Then, 
clearly,  n1 , ..., nk  have  a  multinomial  distribution  for  n  trials with probabilities (p1 , ..., pk ). 
When  two  independent  real  variables  are  added,  their  means  and  variances  add.  Sim­
ilarly,  when  independent  vector-valued  variables  (U1 , ..., Uk )  and  (V1 , ..., Vk )  are  added, 
their  mean  vectors  are  added  and  so  are  their  covariance  matrices,  in  other  words  for  any 
r, s  = 1, ..., k , 

Cov(Ur  + Vr , Us  + Vs ) = Cov(Ur , Us ) + Cov(Vr , Vs ) 

because the covariances of independent  variables are 0.  So, if  we add  n  i.i.d. vector random 
variables, speciﬁcally  the  X1 , ..., Xn  mentioned previously, the mean vector and  covariance 
matrix  of  their  sum  Sn  are just  n  times  the  corresponding  quantities  for  X1 .  For  multi­
nomial  variables,  this  gives  the  known  mean  vector  E (n1 , ..., nk) =  n(p1 , ..., pk)  and  the 
variance  var(nj ) =  npj (1 − pj ) for  j = 1, ..., k ,  which  we  know  since  nj  is binomial (n, pj ). 
For  r  =6
s,  we  get  the  covariance  Cov(nr , ns ) =  −npr ps . 
Let  Yr  = (nr  − npr )/√npr  for  r  = 1, ..., k .  Then  each  Yr  has  mean  0  and  variance 
1 − pr .  For  r  =  s, Cov(Yr , Ys ) =  −√pr ps .  Thus  the  covariance  matrix  of  Y  = (Y1 , ..., Yk) 
6 =  δrs  − √pr ps  where  δrs  = 1 for  r  =  s  and 0  otherwise (Kronecker delta). 
is given by  Crs 
The  asymptotic  chi-squared distribution  of  the  X 2  statistic.  Let  U1  = (√p1 , ...,  √pk ).  Then 
U1  is  a  unit  vector,  it  has  length  1,  since  p1  +
+  pk  =  1.  Now  consider  the  change  of 
· · · 
coordinates in  k-dimensional  space  where  U1  becomes  the  ﬁrst  basis  vector  and  U2 , ..., Uk 
are other unit  vectors perpendicular to each  other and  to  U1 . The pro jection  of  any  vector 
V  onto  U1 ,  i.e.  the  ﬁrst  component  of  V 
in  the  new  coordinates,  is  the  inner  product  of 
V  with  U1 , times  U1 .  For  V  =  Y  = (Y1 , ..., Yk),  the  inner  product  of Y  with  U1  is  0  since 
�k 
r=1 nr  − npr  =  n  − n  = 0. 
When  the  covariance  matrix  Crs  is  transformed  into  the  new  coordinates,  the  identity 
matrix  term  is  unchanged,  whereas  √pr ps  is  transformed  into  the  matrix  having  a  1  in 
the  upper  left  corner  and  0  elsewhere,  since  {√pr }
k
in the  old  coordinates is just the 
r=1 
ﬁrst  basis  vector  U1  = (1, 0, ...,  0)  in  the  new  coordinates,  and  it  gets  premultiplied  by  its 
transpose  to  form  the  simple  matrix  described.  Therefore,  the  covariance  matrix  C  for  Y 
is  transformed  into  the  matrix  with  1’s  on  the  diagonal  except  0  in  the  upper  left  corner, 
and  0’s  everywhere  else. 
As  n  becomes  large,  the  components  of  Yr  become  asymptotically  normal  with  mean 
0  and  the  same  variances  and  covariances.  The  chi-squared  statistic  for  testing  the  simple 
k 
hypothesis (p1 , ..., pk ) is  X 2  =  �r
2 .  Via  the  change  of  variables,  we  see  that  if  the 
=1 Yr 
hypothesis  is  true,  then  as  n  → ∞,  the  distribution  of  X 2  approaches  that  of  the  sum  of 
squares  of  k  − 1  normal  variables  with  mean  0  and  variance  1  which  have  covariances  0 
2


and  so  are  independent  —  in  other  words,  a  χ2  variable  with  k  − 1  degrees  of  freedom 
(the  distribution  is  well  known,  but  proofs  of  it  are  not  given  in  most  beginning statistics 
books). 

that 

ni 
n 

Zi 
pi√n

Conﬁdence  intervals  for  odds  ratios.  Here  we  have  a  multinomial  distribution  with  k  = 4 
categories,  written  in  terms  of  a  2  × 2 table,  with probabilities (p00 , p01 , p10 , p11 ) and  ob­
served  numbers (n00 , n01 , n10 , n11 ).  The  odds  ratio  is  deﬁned  as  Δ  =  p00p11 /(p01p10 ) and 
the  usual  estimate  of  it,  which  is  the  maximum  likelihood  estimate  under  the  full  multi­
nomial  model,  is  Δˆ =  n00n11 /(n01n10 ).  According  to  the  independence  or  homogeneity 
hypothesis  H0 :  pij  ≡ pi· p·j ,  we  have  Δ  =  1.  But  supposing  H0  is  rejected,  then  we’d  like 
to  get  not  only  the  estimate  ˆΔ  but  a  conﬁdence  interval  for  Δ. 
To  reduce  indices,  let’s  replace  indices  00  by  1,  10  by  2,  01  by  3,  and  11  by  4,  so  that 
Δ becomes  p1 p4 /(p2p3 ) and  Δˆ =  n1n4/(n2n3 ).  Let  Zi  = (ni  − npi )/√n  for  i  = 1, ...,  4,  or 
Zi  =  √piYi  in  terms  of  the  Yi  previously  deﬁned.  We  have  Cov(Zr , Zs ) =  pr δrs  − pr ps  for 
any  r, s  = 1, ...,  4. As  n  becomes large, (Z1 , ..., Z4) has approximately a normal distribution 
with  mean  0  and  the  same  covariance.  We  have  ni  =  npi  +  √nZi  for  i  = 1, ...,  4.  Then 
 
 
�
�
=  pi  1 + 
. 
→
0 (with  an error of 
Taking logs of both  sides,  and  using  the fact that log(1 + x) ∼ x  as  x 
order  x2 , by  a Taylor series with  remainder)  we get that log(ni/n) = log(pi )+Zi/(pi√n)+εi 
where  each  εi  =  Op (1/n) as  n  → ∞. 
If  in  the  deﬁnition  of  Δˆ we  replace  each  ni  by  ni/n  then  it  is  unchanged.  It  follows 
 
 
�
�
Z
1 
Z
3 
1 
log(Δ) ˆ = log(Δ) +  √
+  ε 
p3
p1
n 
(1/n).  Thus,  log( ˆ
with  ε  =  Op
Δ)  is  asymptotically  normal  with  mean  log(Δ). 
For  its 
variance,  we  have  a  sum  of  four  terms  (plus  a  constant  with  0  variance  and  terms  of 
smaller  order,  by  the  delta-method  theorem;  note  that  the  derivative  of  the  log  function 
at 1 is 1,  so the  f ′ (µ)2  factor  equals  1).  We  need  to  add  the  variances  of  these  four  terms, 
which  gives 
 
 
4 
4 
�
�
1 
�
�
n  −4 + 
r=1 
r=1 
We  also  have  to  add  covariance  terms,  each  multiplied  by  2.  For  each  r  =  s  we  have 
Cov(Zr , Zs ) =  −pr ps  and  so  Cov(Zr /pr , Zs/ps ) =  −1.  In  the  six  covariances  of  the  four 
terms  we have two  coming from terms of  the  same  sign, (1,4)  and (2,3), and  the  other four 
from  terms  of  opposite  sign.  So  the  covariances  contribute  2(2 − 4)(−1/n) = +4/n to the 
total  variance,  and  the  asymptotic  variance  of log( ˆΔ)  is 
 
 
  1 
�
�
4
1  �
r=1  pr 
n 
3


1 − pr 
npr 

 

1 
pr 

= 

+ 

Z
4 
p4

− 

Z
2 
p2

− 

 

. 

. 

6
Here  pr  are  the  unknown  probabilities,  and  we  estimate  each  term  npr  by  its  MLE  which 
is  the  observed  nr .  Then taking the square root, we get that log( ˆΔ)  is  asymptotically 
normal  with  mean log(Δ)  and  standard deviation (standard  error in this  case)  estimated 
by 
 
  4 
�
�
 
��
�
r=1 
Based  on  the  normal  distribution,  this  gives  us  conﬁdence  intervals  for  log(Δ)  and  then 
exponentiating, for Δ itself. 
If  any  nij  is  small,  for  example  less  than  5,  the  normal  approximation  is  questionable 
and  the  standard  error  is  large,  so  the  estimate  is  uncertain.  If  all  four  nij  are  large,  as 
in  the  data  for  hospitalized  Medicare  patients,  then  the  normal  approximation  should  be 
quite good. 

  1 
nr 

. 

Acknow ledgment.  Marcelo  Alvisio  pointed  out  the  method  of  ﬁnding  conﬁdence  intervals 
for  odds  ratios  via their logarithms (found  on the Web)  during  the  spring  of 2006. 

4


