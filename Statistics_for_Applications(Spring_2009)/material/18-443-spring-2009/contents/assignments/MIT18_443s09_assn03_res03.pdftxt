MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

R. Dudley

The turning point test


This  is  a  test  of  the  hypothesis  H0  that  X1 , ..., Xn  are  i.i.d.  with  a  continuous  distri­
bution.  So,  it’s  a  nonparametric  test,  in  other  words  it  doesn’t  assume  Xj  have  a  normal 
or  any  other  special  distribution.  For  j = 2, ..., n  − 1, let  Ij  = 1 if  Xj  <  min(Xj−1 , Xj+1) 
or  Xj  >  max(Xj−1 , Xj+1),  otherwise Ij  = 0,  also  if  j = 1  or  n.  Say  a  turning  point occurs 
at  j if  and  only if  Ij  = 1,  in  other  words,  the  sequence  turns  from  increasing  to  decreasing, 
or  from  decreasing  to  increasing,  at  j .  Another  way  to  say  it  is  that  the  sequence  Xi , 
i  = 1, ..., n,  has  a  local  minimum  or  local  maximum  at  i  =  j . 
Random  variables  Y1 , Y2 , ...,  are  said  to  be  m-dependent  if  for  each  k  = 1, 2, ...,  the 
set  Y1 , ..., Yk  of  random  variables  is  independent  of  the  set  of  all  Yi  for  i > k  +  m. 
It 
follows  from  this  that  for  any  j  =�
i,  Yj  and  Yi  are  independent if  |i  − j | > m  but  may 
be dependent if  |j − i| ≤ m,  hence  the  name  m-dependent.  The  Ij  are  2-dependent  since 
for  any  k  ≥ 1,  I1 , ..., Ik  depend  only  on  X1 , . . . , Xk+1 ,  and  {Ik+3 , Ik+4 , ...} depend  only 
on  Xj  for  j  ≥ k  + 2.  Thus  Ii  and  Ij  are  independent if  |j − i| ≥ 3.  There  are  central 
limit  theorems  for  m-dependent  random  variables  which  will  apply  to  the  Ij  under  H0 
since  they  are  uniformly  bounded  and  for  2  ≤ j  ≤ n  − 1  they  are  identically  distributed. 
One  reference is Berk (1973).  It  will follow that  Tn  :=  �n−1  Ij ,  the  number  of  turning 
j=2 
points,  has  an  asymptotically  normal  distribution  as  n  → ∞.  After  we  ﬁnd  its  mean  and 
variance  under  H0 ,  we  can  can  thus  use  Tn  as  a  test  statistic,  rejecting  H0  if  Tn  is  too 
many  standard  deviations  away  from  its  mean. 
There   is  code  in  R  for  the  turning  point  test,  assuming  the  normal  approximation  is 
valid, turning.point.test.R. 
If  Xj  actually behave not  as i.i.d. variables but have a pattern such  as  f (j ) + Vj  where 
Vj  are  small  random  variables  in  relation  to  diﬀerences  f (j ) − f (j − 1)  of  the  non-random 
smooth function  f , then there  will  tend  to be too few turning points in the  Xj .  If  there  are 
too  many  turning points then  Xj  change  direction  even  more  often  than  do  i.i.d.  variables, 
which  we  will  see  have  turning  points  at  about  2/3  of  all  values  of  j .  These  alternative 
kinds  of  behavior  are  qualitatively  diﬀerent.  Most  often  it  seems   to  make  sense  to  use  a 
1-sided  test,  rejecting  the  model  when  there  are  too  few  turning  points. 
It has been  shown (Stuart, 1954)  that the turning point test is  not  actually  very  eﬃ­
cient  as  a  test  of  the  hypothesis  that  X1 , ..., Xn  are  exactly  i.i.d.  Speciﬁcally,  against  the 
alternative  regression  model hypothesis that  Xj  =  α + βXj + εj  where  εj  are i.i.d.  N (0, σ 2) 
for  some  σ ,  with  β  = 0,  the  usual  estimator  β� of  the  slope  β  provides  a  test  statistic  com­
pared  to which  the turning point statistic has asymptotically  0  relative eﬃciency (Stuart, 
1954, pp. 153-154). 
But,  if  the  regression  model  does  hold  with  i.i.d.  errors   εj  (not  necessarily  normal, 
but  having  mean  0  and  ﬁnite  variance)  then  the  residuals  ε�j  in  the  regression  will  be 
approximately i.i.d., for  n  large  enough.  For  the  residuals,  the  estimated  slope  β� will be 
exactly  0.  Kendall, Stuart and Ord (1983, pp. 430-436)  are considering  time series  which 
beside  a  trend  might  have  seasonal  variations,  although  here  we’re  concerned  just  with 
trend.  They  say (p. 430):  “When seasonal variation and trend have been  removed from 
the data we are left with  a series [of  residuals] which will present, in general, ﬂuctuations of 

1


�
a more or less regular kind.”  They  consider tests for whether these  ﬂuctations are random, 
and ﬁrst of  all  among  “most suitable”  tests, the turning point test (pp. 431-436). 
First,  the  properties  of  the  turning  point  test  for  actual  i.i.d.  Xj  will be developed, 
then,  we’ll look  at properties for  residuals.  The following fact gives the  mean  and  variance 
of  Tn  under  H0 . (For n  = 3  the  variance  is  2/9.)  After its proof, probabilities that  Tn  ≤ 1 
will  be  given  for  all  n. 
.
If  Z  is  a  standard  normal  variable,  P (Z  ≤ −1.645)  =  Φ(−1.645)  = 0.05.  So,  the 
following  theorem  will  imply  that  if  n  is  large  enough  for  the  normal  approximation  to 
apply,  Tn , we  can reject the i.i.d. hypothesis in a 1-sided  test (signiﬁcantly  too few turning 
points)  if 
� 16n  − 29 
− 1.645 
Tn  ≤ 
90 
Theorem  1.  Under  the  hypothesis  H0 :  X1 , ..., Xn  are  i.i.d.  with  a  continuous  distribution, 
and  n  ≥ 3,  we  have  E Tn  = 2(n  − 2)/3,  and  for  n  ≥ 4,  Var(Tn) = (16n  − 29)/90. 
Proof.  Clearly,  under  H0 ,  for  each  j = 2, ..., n  − 1,  Pr(Xj−1  > Xj  < Xj+1 ) =  Pr(Xj−1  < 
Xj  > Xj+1 ) = 1/3,  so  E (I 2) =  E (Ij ) =  Pr(Ij  = 1)  = 2/3  and  E Tn  = 2(n  − 2)/3. 
j 
To  ﬁnd  E (T 2 ) we  will  ﬁnd  E (Ij Ik ) for  each  j  and  k .  For  2  ≤ j < j  + 3  ≤ k  ≤ n  − 1, 
n 
Ij  and  Ik  are independent, thus  E (Ij Ik ) = 4/9. 
For  n  ≥ 4  and  2  ≤ j ≤ n  − 2,  to  ﬁnd  E (Ij Ij+1 ),  note  that Xj−1 , Xj , Xj+1 ,  and  Xj+2 
have  24  possible  orderings  and  thus  rankings,  each  equally  likely,  with  ranks  1  to  4.  We 
have  Xj−1  > Xj  < Xj+1  > Xj+2  if  the  ranks  of  Xj  and  Xj+1  are  1  and  4  respectively,  in 
2  of  the  24  cases;  or  if  the  ranks  are  1  and  3,  or  2  and  4,  or  2  and  3,  in  1  each  of  the  24 
cases,  and  in  no  other  cases.  Reversing  all  the  orderings  of  the  four  we  have  a  symmetric 
event  with  the  same  total  probability.  Thus 

2(n  − 2) 
3 

. 

E (Ij Ij+1 )  =  2(5/24)  = 5/12. 

For  n  ≥ 5  and  2  ≤ j ≤ n  − 3,  to  ﬁnd  E (Ij Ij+2 ), there  are 120 possible  rankings  of Xi 
for  i  =  j − 1, j, ..., j  + 3,  each  equally  likely.  We  have  Xj−1  > Xj  < Xj+1  > Xj+2  < Xj+3 
if  the  ranks  of  Xj  and  Xj+2  are  1  and  2  respectively  in  6  of  the  120  cases,  and  likewise  if 
they  are  2  and  1;  or  if  they  are  1  and  3,  or  3  and  1,  in  a  total  of  4  of  the  120  cases,  and  not 
otherwise;  thus  altogether with probability 16/120  = 2/15,  which is  also the probability  of 
the  symmetric  event  Xj−1  < Xj  > Xj+1  < Xj+2  > Xj+3 . 
We have  Xj−1  > Xj  < Xj+1  < Xj+2  > Xj+3  if  the  ranks  of  Xj  and  Xj+2  are  1  and 
5  respectively  in  6  of  the  120  cases;  if  the  ranks  are  1  and  4,  or  2  and  5,  in  two  cases  each, 
totaling 4  cases;  or if  they  are 2  and 4, in one case, for  a grand  total probability  of 11/120. 
Considering  also  the  symmetric  case  that  all  the  inequalities  are  reversed,  it  follows  that 

E (Ij Ij+2 ) =

4
15 
Collecting  terms  we  get  for  n  ≥ 6 that 
5
2 
(n 
3 − 2) + 2(n  − 3)  + 2(n 
12  

11 
60 

2 )
E (Tn 

=

+

2


= 

27 
60 

= 

9 
20 

. 

9
− 4)  + 2 
20  

n−4
·  9 �  (n  − j − 3) 
4 
j=2 

=  n

9 �
� 4
18 �
� 2
5
5
4
2 
+ +
+ [n − 9n + 20],
+ +
−
6
2
9
5 
3
10 
3
9 (n2  − 4n  + 4)  which  equals 
n ) − 4
2
and  Var(Tn) =  E (T
1
90 	

[120 + 225 + 324 − 800 + 160]   = 

[60 + 75 + 81

n 
90

− 360 + 160]

− 

16n 
− 29  
90  

,

≥
the  proof  for   n 
6.  
as  stated,  completing
If  n  = 5,  then  since  the  term  from  the  summation  sign  is  4(n − 4)(n − 5)/9  it  equals  0 
as  it  should,  so  the  variance  is  still  as  stated.  The  summation  term  also  vanishes  if  n  = 4, 
as does the term coming from  E Ij Ij+2  as it  should, so the conclusion remains true.  Q.E.D. 

For  n  = 3,  T3  is  a  Bernoulli  variable  with  mean  2/3  and  variance  2/9,  which  diﬀers 
from  the  given  formula  in  that  case. 
Via  the  following  exact  probabilities,  we  can  reject  H0  for  n  not  large  enough  for  Tn 
to  be  approximately  normal,  if  Tn  has  a  value  ≤ 1.  Deﬁne  Tn  = 0 for  n  = 2. 
Proposition  2.  For  any  n  ≥ 2, (a)  Pr(Tn  = 0)  = 2/n!  and (b)  Pr(Tn  = 1)  = (2n  − 4)/n!. 
Thus  Pr(Tn  ≤ 1)  = (2n  − 2)/n!. 
Proof.  There  are  n! possible  orderings of  the  observations, each  equally probable,  and  only 
one  each  where  X1  > 
· · ·	
> Xn  and  X1  < 
· · · 
< Xn ,  so (a)  follows,  and (b)  also holds for 
n  = 2. 
For  n  ≥ 3  and  k  = 2, ..., n  − 1,  the  probability  that  X1  < 
· · · 
· · · 
> Xn
< Xk  > 
equals the probability  that  Xk  is  the  maximum  X(n)  of  X1 , ..., Xn,  which  is  1/n, times the 
probability  that  X1  < 
· · · 
< Xk−1 ,  which  is  1/(k  − 1)!,  also  conditional  on  Xk  =  X(n) ,
· · · 
> Xn ,  which  is  1/(n  − k)!,  also  not  depending  on 
times  the  probability  that  Xk+1  > 
X1 , ..., Xk . The probability  that there is just one turning point,  at  k ,  is  twice  the  above 
probability.  Thus the probability  that  Tn  = 1 is 
2  � �n  − 1�  =
2  � �n  − 1�  =
n−1	
n−2
n−1
2n  − 4 
2 �	 1
1 
1
n  ·  (k − 1)!  ·  (n  − k)! 
n! 
n! 
k − 1 
j
n! 
j=1 
k=2	
k=2 
because  �m  �m�  = 2m  for  any  integer 
0  and  �m� =  �m� = 1.  Thus (b)  follows, 
m  ≥
j=0	
0 
m 
j 
Q.E.D. 
The sma
llest  n  for which  H0  can  be  rejected   at  the  0.05 level if 
Tn  = 1 is  n  = 7, where 
0.025.  This  probability  is  in   a  sense  one-sided,  bu
Pr(T7 
≤	
t E T7  =  10/3  by  the 
1)  =
10  − 1 =  7
,  whereas  Pr(T7  ≥ 17/3)  = 0 since  T7 
≤
5.  Thus  the  two-sided 
Theorem,  and
 
3 
3
r(|T7  − E T7 | ≥ 7/3)  = 0.025  <  0.05. 
probability  P
given  a   model  in  which   Yj  =  g (Xj |θ) + εj ,  where (Xj
 , Yj )  are  observed  for 
Suppose 
gression  function  with 
j  = 1, ..., n,  X
j  are  non-random   design   points,  and   g (x θ)  is  a   re
|
a parameter  θ to be estimated from the data, with an estimate θ� =  θ�n . 
In  a  classical 
model,  εj  are  assumed  to  be  i.i.d.  N (0, σ 2) for  some  unknown  σ . The  εj  are  called  errors, 
although  they  may  result  from  random  variation  rather  than  measurement  errors  per  se. 
�
The  observable  quantities  ε�j  =  Yj  − g (Xj |θ),  which are  estimates  of  the  unobserved  εj , 
3 

=

	
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1) 

(3) 

, 

are  called  residuals.  For  consistent  estimation,  by  deﬁnition,  if  a  true  θ  =  θ0  exists,  θ�n  will 
approach  it  in  probability  as  n  becomes  large.  Supposing  that  g (x θ) is  continuous  with 
|
respect  to  θ ,  g (Xj |θ�n)  will  approximately  equal  g (Xj |θ0 ),  so  ε�j  will  approximately  equal 
εj .  Thus,  approximately,  we  can  apply  a  turning  point  test  to  the  ε�j  to  see  if  the  model 
assumption  of  εj  i.i.d.  is  valid. 
To be  more precise let’s  consider the  case  of  simple linear  regression  Yj  =  a + bxj + εj 
where  xj  are  non-random  and  εj  are i.i.d.  N (0, σ 2).  In  order  for  turning  points,  or  any 
patterns  in  residuals  ε�j ,  to  be  meaningful,  we  can  and  do  assume  that  x1  ≤ x2  ≤ · · · ≤  xn . 
Here  θ  = (a, b).  We  know  that  the  least-squares  and  maximum  likelihood  estimates  of  a 
and  b  are given by 
�n
j=1 (xj  − x)(Yj  − Y ) 
�a  =  Y  − �bx, 
�b  = 
S 2 
x 
2  :=  �n
j=1 (xj  − x)2  >  0 if  the  xj  are  not  all  equal,  as  will  be  assumed.  The  ﬁtted 
where  Sx 
value  at  x  =  xj  is then  Y�j  =  �a  + �bxj  =  Y  + �b(xj  − x).  Thus the  j th  residual 
ε�j  =  Yj  − Y�j  =  Yj  − Y  − �b(xj  − x). 
(2) 
Then 
n
n
� 
� 
xj �
εj
εj  = 0  = 
�
j=1 
j=1 
In  the  second  equation  xj  can  be  replaced 
where  the  ﬁrst  equation  follows  from  (2). 
equivalently  by  xj  − x, by  the  ﬁrst equation, and  the second  then follows easily  from (1). 
When  n  =  3  there  is  exactly  one  turning  point  in  the  residuals  as  otherwise  there 
trend in them,   which is impossible by (3). 
would be  a
e  Y  =  a  +  bx  + ε.  It   follows  that 
We hav
+ (b − �b)(xj  − x). 
ε�j  =  εj  − ε
has  a  N (0, σ 2/n)  distribution.  Also,   as  it  doesn’t  depen
the  model,   ε 
Here,  under
d on 
t  aﬀect   whether   εj  has  a   turning  point  at  a  given   j .  From   (1)  and  sinc
j ,  it  doesn’
e xj 
�
small,   �of  order  1/√n, if  |xj  − x| is  of  typical  order 
dom  it  follows   that  b  − b  has  a  N (0, σ 2/S 2
are  non-ran
x )  distribution.  Thus  if  n  is l
arge, 
mall   if  |xj − x| is  larger,  meaning  that  in   a  sense  xj  is  an   outlier  among �th
|(b − �b)(xj  −
2 /n, 
S
x)| is  probably 
x
but  not  so  s
e xi .
s  can  occur  for  relatively  few  values  of  j .  If  xj  is  not  such  an   outlier,  th
Such  outlier
en  εi 
and  ε�i  will  either both  or neither have a turning point  at  i  =  j unless  one  of  the  diﬀerences 
|εj  − εj−1 | or  |εj  − εj+1 | is  small  of  order  1/√n,  which  will happen  with  small probability. 
In  autoregressive  time  series  models,  there  is  a  classic  test  for  whether  residuals  are 
i.i.d.  N (0, σ 2), the Ljung-Box test (Ljung  and Box, 1976).  That diﬀers from the  case  of 
simple  linear  regression  not  only  in  the  autoregressive  property  of  the  model  but  in  that 
time  points  are  equally  spaced,  whereas  in  linear  regression  the  Xj  need  not  be  equally 

(4) 

4


 
 
 
 
spaced,  so  that  the  correlations  of  Yj ,  if  they  are  not  zero  under  some  alternative,  have  a 
diﬀerent  structure  than  in  the  time  series  case. 

Notes. On m-dependent  random variables and  central limit theorems for them, Berk (1973) 
gives  an  extended  theorem  for  triangular  arrays  of  random  variables.  He  gives  the  earlier 
references  on the same topic by Hoeﬀding  and Robbins (1948), Diananda (1955), and Orey 
(1958). 

REFERENCES 

Berk, K. N. (1973), A  central limit theorem for  m-dependent  random  variables  with 
unbounded  m,  Ann.  Probability  1, 352-354. 
Diananda, P. H. (1955), The  central limit theorem for  m-dependent  variables,  Proc. 
Camb.  Philos.  Soc.  51, 92-95. 
Dudley, R. M. (1999),  Uniform Central  Limit Theorems, Cambridge University Press. 
Hoeﬀding,  W.,  and  Robbins,  H.  (1948),  The  central  limit  theorem  for  dependent 
random  variables,  Duke  Math.  J.  15, 773-780. 
Kendall, M., Stuart, A.,  and Ord, J. K. (1983).  The  Advanced  Theory  of  Statistics, 
vol. 3,  Design  and  Analysis,  and  Time-Series,  4th  ed.,  MacMillan,  New  York. 
Ljung,  G.  M.,  and  Box,  G.  E.  P.  (1978),  On  a  measure  of  lack  of  ﬁt  in  time  series 
models,  Biometrika  65, 297-303. 
Orey, S.  A. (1958), Central limit  theorem for  m-dependent  random  variables,  Duke 
Math.  J.  25, 543-546. 
Stuart, A. (1954).  Asymptotic relative eﬃciencies of distribution-free tests of random­
ness  against  normal  alternatives.  J.  Amer.  Statist.  Assoc.  49, 147-157. 

5


