MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.443 
THE  LIMIT  DISTRIBUTION  OF  THE  X 2  STATISTIC 

Suppose  we have  a  multinomial (n, p1 , ..., pk ) distribution,  where  pj  is the probability 
of  the  j th  of  k  possible  outcomes  on  each  of  n  independent  trials.  Thus  pj 
≥
0  and 
�k 
pj  =  1.  The  probability  that  the  j th  outcome  occurs   nj  times  for  each  j  in  the  n
j=1 
trials (so that  necessarily  n1  +
· · · 
+ nk  =  n) is  the  multinomial  probability 
� 
� 
n 
nk 
n1  n2 
p1  p2  · · ·  pk  . 
n1 , ..., nk 
If values  n1 , n2 , ..., nk  are observed, and  a simple hypothesis  H0  speciﬁes values of  p1 , ..., pk , 
then the  X 2  statistic for testing  H0  is 

X 2  = 

(nj  − npj )2 
. 
npj

k 
� 
j=1 
Theorem.  If  the  hypothesis  H0  is  true,  then  as  n  → ∞,  the  distribution  of  X 2  converges 
to  that  of  χ2 (k − 1), i.e.  χ2  with  k − 1  degrees  of  freedom.  
Proof.  Under  H0 , the  random  vector (n1 , ..., nk) has  a  multinomial (n, p1 , ..., pk ) distribu­
tion.  Let’s  ﬁnd  the  covariance  of  ni  and  nj  for  i  =  j .  If  we  can  do  that  for  i  = 1  and  j = 2 
we  can  extend  the  result  to  any  i  and  j . 
For  each  j ,  the  (marginal)  distribution  of  nj  is binomial (n, pj ).  Let  q1  :=  1  − p1 . 
Given  n1 ,  the  conditional  distribution  of  n2  is binomial (n  − n1 , p2/q1 ).  Thus  E (n2 |n1 ) = 
(n  − n1 )p2/q1  and 

E (n1n2 ) =  E (n1E (n2 |n1 )) =  n 2 p1 p2 /q1  − p2 q −1E n1
2 .
1 
2  = Var(n1 ) + (E n1)2  =  np1q1  +  n2 p1
2  we get 
Since  E n1

E (n1n2 ) = 

2 2 
n p1 p2 

2
n p1 p2 

−
q
1 

− np1 p2  = (n 2  − n)p1p2 , 

which  is  symmetric  in  p1  and  p2  as  it  should  be.  It  follows   that  Cov(n1 , n2 ) =  −np1 p2 . 
It’s  natural  that  this  covariance  should  be  negative  since  for  larger  n1 ,  n2  will tend to be 
smaller.  For  1  ≤ i < j  ≤ n  we  have  likewise  Cov(ni , nj ) =  −npi pj . 
Let  Yj  :=  (nj  − npj )/√npj  for  j = 1, ..., k .  Then  X 2  =  Y +
2
· · · 
1 
have  E Yj  = 0  and  E Y 2  =  qj  :=  1 − pj .  We  also  have  for  i  =6
j
j 
E YiYj  =  Cov(Yi , Yj ) =  Cov(ni , nj )/(n√pi pj ) =  −√pipj . 
Recall  that  δij  = 1 for  i  =  j and 0 for  i  =6
j .  As  a  matrix,  Iij  =  δij  gives the  k × k  identity 
matrix.  We have 
Cij  :=  E YiYj  =  Cov(Yi , Yj ) =  δij  − √pipj 
1 

+  Y 2  For  each  j  we 
k  .

6
for  all  i, j  = 1, ..., k . 
Let  up  be the  column  vector (√p1 , ...,  √pk )′  .  This  vector  has  length  1.  We  can  then 
′
write  C  =  I  − upup  as  a  matrix.  Let’s  make  a  change  of  basis  in  which  up  becomes  one 
of  the  basis  vectors,  say  the  ﬁrst,  e1 ,  and  e2 , ..., ek  are  any  other  vectors  of  unit  length 
′
perpendicular  to  each  other  and  to  e1 .  In  this  basis  C  becomes  D  =  I  − e1 e1  which is 
a  diagonal  matrix,  in  other  words  Dij  = 0 for  i  =  j .  Also  D11  =  0,  and  Dj j  = 1 for 
j = 2, ..., k . 
Let  the  vector  Y  = (Y1 , ..., Yk) in  the  new  coordinates  become  Z  = (Z1 , ..., Zk),  where 
EZj  = 0  for  all  j  and the  Zj  have  covariance  matrix  D. 
As  n  → ∞, by  the  multidimensional  central limit theorem (proved in 18.175,  e.g. in 
Professor Panchenko’s OCW  version of the course, Spring 2007), (Z1 , Z2 , ..., Zk )  asymp­
totically  have  a  multivariate  normal  distribution  with  mean  0  and  covariance  matrix  D, 
in  other  words  Z1  ≡ 0  and  Z2 , ..., Zk  are  asymptotically  i.i.d.  N (0, 1).  Thus  X 2  =  Y 2  = 
|Z |2  =  Z 2  +  · · ·  +  Z 2  has  asymptotically  a  χ2 (k − 1)  distribution  as  n  → ∞, Q.E.D. |
|
2 
k 
Chi-squared  tests  of  composite  hypotheses. 
In  doing  a  chi-squared  test  of  a  composite 
hypothesis  indexed  by  an  m-dimensional parameter  θ ,  two  kinds  of  adjustment  may  be 
made.  One  is  to  estimate  θ  by  some  θˆ and  compute  the  chi-squared  statistic 
k 
Xˆ 2  =  �  (nj  − npj (θˆ))2 
. 
npj (θˆ)
j=1 
The usual  rule is that for  n  large enough, this should have approximately  a  χ2  distribution 
with  k  − 1 − m  degrees  of  freedom.  For  that  to  be  valid,  θˆ should  be  a  suitable  function 
of  the  statistics  n1 , ..., nk .  Two  suitable  estimators  for  this  are  the  minimum  chi-squared 
estimate,  where  θˆ is  chosen  to  minimize  Xˆ 2 ,  or  the  maximum  likelihood  estimate  θˆM LE 
based  on  the  given  data  n1 , ..., nk . 
Another  adjustment  that’s  made  is  that  if  the  expected  numbers  npj (θˆ) in  some  cate­
gories  are  less  than  5,  we  can  combine  categories  until  all  the  expectations  are  larger  than 
5. 

Suppose  we  combine  categories,  which  certainly  will happen if  we  start  with inﬁnitely 
many  possible  outcomes,  as  in  a  Poisson  or  geometric  distribution  where  the  outcome  can 
be  any  nonnegative  integer.  Then  when  we  come  to  do  the  test,  the  nj  will  no  longer  be 
the  original  data,  which  may  be  called  the  ungrouped  data,  but  they’ll  be  what  are  called 
grouped data. 
Another  way  data  can  be  grouped  is  that  the  original  observations  X1 , ..., Xn  might 
be  real  numbers,  for  example,  and  we  want  to  test  by  χ2  whether  they  have  a  normal 
N (µ, σ 2) distribution,  so  we  have  an  m  =  2  dimensional  parameter.  One  can  decompose 
the  real  line  into  k  intervals (the leftmost  and  rightmost being  half-lines)  and do a  χ2  test. 
Here  the  numbers  ni  of  observations  in  each  interval  are  already  grouped  data. 
It  tends  to  be  very  convenient  to  estimate  the  parameters  based  on  ungrouped  data, 
for  all  the  cases  mentioned (normal, Poisson, geometric)  and hard  to  estimate them  using 
grouped  data.  Unfortunately  though,  using  estimates  based  on  ungrouped  data,  but  doing 
a  chi-squared  test  on  grouped  data,  violates  the  conditions  for  the  X 2  statistic  to  have 

2


6
a  χ2  distribution  with  k  − 1  − m  degrees  of  freedom,  as  many  textbooks  have  claimed 
it  does,  although  Rice,  third  ed.,  p.  359,  correctly  points  out  the  issue.  The  problem  is 
that  the  ungrouped  data  have  more  information  in  them  than  the  grouped  data  do,  and 
consequently,  if  the  hypothesis  H0  is  true,  an  estimate  θ˜ based  on  the  ungrouped  data 
tends  to  be  closer  to  the  true  value  θ0  of  the  parameter  than  the  estimate  θˆ based  on 
the  grouped  data  would  be,  and  consequently  farther  from  the  observations,  in  the  sense 
measured by  the  X 2  statistic. 
Let  θ˜ be  an  estimate  of  θ  based  on  ungrouped  data  and  let 
k 
X˜ 2  =  �  (nj  − npj (θ˜))2 
. 
npj (θ˜)
j=1 
The distribution of  the  X˜ 2  statistic for  n  large is somewhere between those of  χ2 (k − 1 −m) 
and  χ2 (k  − 1).  To  take  account  of  this,  if  one  uses  estimates  based  on  ungrouped  data, 
here  are  conclusions  one  can  draw,  for  a  given  signiﬁcance  level  α: 
If  X˜ 2  is  so  large  that  H0  would  be  rejected  based  on  χ2 (k  − 1),  then  it  should  be 
rejected.  This  makes  sense  because  even  if  we  had  tested  the  simple  hypothesis  θ  =  θ˜
(which we couldn’t legally do because we didn’t know  θ˜ in advance)  we would have rejected 
it. 

If  X˜ 2  is  small  enough  so  that  H0  would  not  be  rejected  based  on  χ2 (k − 1 − m), then 
it  should  not  be  rejected.  This  is  correct  because,  by  deﬁnition  of  minimum  chi-squared 
estimate  θˆ based  on  the  grouped  data,  we  know  that  Xˆ 2  X˜ 2 ,  so  using  Xˆ 2  we  wouldn’t 
≤
reject  H0 . 
If  X˜ 2  is in an intermediate range where neither of the previous conditions applies, then 
one  is  uncertain  whether  the  hypothesis  should  be  rejected  and  either  more  computation, 
to  evaluate  the  MLEs  or  minimum  chi-squared  estimates  based  on  the  grouped  data,  or 
more  data,  are  needed. 

3


