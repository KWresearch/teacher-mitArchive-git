MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.443


FACTS ABOUT NORMAL DISTRIBUTIONS AND SAMPLE STATISTICS 

First,  here  is  a  known  fact  about  normal  distributions. 
Theorem  1.  If  X  and  Y  are  independent  random  variables  with  normal  distributions, 
X  ∼ N (µ, σ 2) and  Y  ∼ N (ν, τ 2 ) then  X +Y  is  also  normal,  with X +Y  ∼ N (µ+ ν, σ 2 + τ 2 ). 
This is proved in the  “addnormals.pdf ”  handout posted  on the course website.  Paper 
copies aren’t being distributed in class because we assume many  of you know this fact from 
a  probability  course. 
The  next  fact  is  stated  early  in  Section  6.3  of  Rice,  p.  195.  For  any  X1 , ..., Xn,  X  is 
· · · 
deﬁned  as  the  sample  mean  X  :=  (X1  +
+  Xn )/n.
Theorem  2.  Let  X1 , . . . , Xn  be i.i.d.  N (µ, σ 2).  Then  X  ∼ N (µ, σ 2/n). 
Proof.  For  any  distribution  F  having  ﬁnite  mean  µ  and  variance  σ 2 ,  if  X1 , . . . , Xn 
are   i.i.d.  (F ),  then  X  has  mean  µ  and  variance  σ 2/n.  So  the  only  problem  is  to  show 
that  X  has  a  normal  distribution  in  this  case.  Now,  Sn  deﬁned  as  X1  +
+  Xn  has  a 
· · · 
normal  distribution,  speciﬁcally  N (nµ, nσ 2), by  Theorem 1 and induction.  Multiplying by 
a  constant  1/n gives  X  which then has the  stated distribution, Q.E.D. 
In  statistics,  the  mean  µ  and  variance  σ 2  of  a  distribution  may  be  unknown  and  can 
be  estimated  from  the  data  by  the  sample  mean  X  and  sample  variance 
n 
1  �(Xj  − X )2 ,
n  − 1 
j=1 
deﬁned for  n  ≥ 2,  respectively.  Here  S 2  is Rice’s notation and  s2 
X  is my preferred  notation. 
Scientiﬁc calculators often use sx  (“sample standard deviation”) whose square is the sample 
variance  s2 
X . 
The next fact includes Corollary A  and Theorem B in Section 6.3  of Rice.  It gives the 
distribution  of  s2  (depending on  σ 2 ) and  its  independence  of  X  in  the  normal  case.  Rice 
X 
uses  the  notation  S 2  instead  of  sX 
2  . 
 
2
orem 3.  If  X1 , ..., Xn  are i.i.d.  N (µ, σ ), n  ≥ 2, then 
The
(a) X  and  s2 
X  are  independent   random  variables; 
(b) (n  − 1)s2  /σ 2  ha a   χ2
s 
(n  − 1)  distribution. 
X 
of.  Let  Yj  =  Xj  − µ  for  j  = 1, . . . , n.  Then   Y  =  X  − µ  and  s2  =  s2 
X .  So  we  can  
Pro
Y
assu
me  µ = 0. 
It’s   convenient  to   make  a   rotation  of  coordinates  in   n-space.  Let   the  standard  basis 
n
vect
i  =  {δij }j=1  where  δij  = 1 for   i  =  j  and 0 for  i  = j .  The  ﬁrst  element  of  the 
ors be  δ
basis  will  be  e1  = (1/√n, . . . ,  1/√n).  This  does   have  length  1.  Then  we   can  always 
new
further orthonormal basis vectors  e2 , ..., en, for example  e2  = (1/√2, −1/√2
ﬁnd
, 0, . . . ,  0), 
√
√
√
e3  =
(1/ 6, 1/ 6, −2/ 6, 0, . . . ,  0),  etc. 
For   any  two   vectors  x  = (x1 , . . . , xn )  and  y  = (y1 , . . . , yn )  (with  respect  to   the 
n 
stan
dard  basis)  we  have   the  usual   dot  product  x ·  y  =  �j=1 xj yj ,  with  the   squared   length 
given by  |x|2  =  x  ·  x.  
of  x

S 2  =  s 2  =
X 

1


 
 
6
Now,  for  the  random  vector  X  = (X1 , . . . , Xn)  we  have  X  =  X e1 /√n,  and 
· 
(X , . . . ,  X )  = (X e1 )e1 ,  which  is  the  pro jection  of  X  to  the  e1  axis.  The  lengths  of 
· 
vectors  and  their  dot  products  are  preserved  by  rotations  of  coordinates,  so 

n
�(Xj  − X )2  = 
j=1 

n 
|X  − (X  ·  e1 )e1 | 2  =  �(X  ·  ei )2 . 
i=2 

Recall  that  exp(y )  is  a  notation  for  e .  Since  X1 , . . . , Xn  are i.i.d.  N (0, σ 2),  their  joint 
y
density is 

j=1  exp(−xj /(2σ 2))  = (σ√2π)−n exp(−|x| 2 /(2σ 2)). 
(σ√2π)−nΠn
2
This  distribution  is  invariant  under  any  rotation  of  coordinates  (change  of  orthonormal 
basis),  speciﬁcally  |x|2  = (x ·  e1 )2 + (x ·  e2 )2 + · · ·  (x ·  en )2 . Thus  X ·  e1 ,  . . . , X  ·  en  are i.i.d. 
N (0, σ 2) and  X ei /σ  are i.i.d.  N (0, 1).  It  follows  that  X  =  X e1 /√n  is  independent  of 
· 
·  n 
�n 
2 
s = (n  − 1)−1 � (X ei )2 , proving (a).  Also, (n  − 1)s /σ 2  =
(X ei )2/σ 2  has  a 
2 
·
·
X
X
i=2
i=2
χ2 (n
− 1)  distribution, proving (b), Q.E.D. 
Here  is   another   way  of  looking  at  chi-squared   distributions.  As  noted  in   th
e above 
f,  if   X1 , ..., Xd  are  i.i.d.  N (0, 1),  their  joint  density  is  (2π ) d/2 
exp( x 2 /2
proo
−|
|
) on  d-
−
nsional  space.  Let   Y  =  X 2 
1 + ·  +  Xd ,  so  that  Y  has  a  χ2 (d) distribution.  
2
dime
· ·
We  have 
≤ t) = 0 for  t  ≤ 0.  For   t >  0,  P ( Y ≤ t) is  the  integral  of  the  density   over  the 
| √|
P (Y 
region 
e  |x|2  ≤ t,  or  equivalently  |x
t.  Suppose  d  ≥ 2.   Using  spherical  coor
|
dinates, 
wher
√≤
 
t
ntegral becomes   Ad (2π ) d/2 
1 
rd− exp(−r2 /2)dr   where  A
�
pending 
d  is a cons
tant de
the i
−
0 
,  the (d  − 1)-dimensional  surface  area  of  the  unit  sphere  |x| = 1 in  d-space. 
on  d
By  the 
titution  x  =  r2 ,  r  =  √x,   dr  =  dx/(2√x), the integral becomes 
subs
  t 
A (2π ) d/2 � x(d 2)/2 
−
−
d
 
0

−x/2)dx/2. 

exp(

Since (d − 2)/d = (d/2) − 1, and a probability density has a unique normalizing  con­
stant,  this  gives  another  proof  that  the  χ2 (d) distribution is the Γ(d/2, 1/2)  distribution. 
Moreover,  since  we know that the  normalizing  constant is (1/2)d/2/Γ(d/2),  we  can  evalu­
ate  Ad  = 2π d/2/Γ(d/2).  For  example,  if  d  =  2,  since  Γ(1)  =  0!  =  1,  we  get  A2  = 2π , 
the  circumference  of  the  unit  circle  as  desired. 
If  d  =  3,  then  by  the  recursion  for­
mula,  Γ(3/2)  = Γ(1/2)/2 =  √π/2,  so  A3  = 4π ,  which  is  in  fact  the  area  of  the  unit 
sphere  in  3  dimensions.  Also,  the  volume  of  the  unit  ball  { x
|
1} in  d  dimensions is 
| ≤
 1 
Vd  =  Ad  0  rd−1 dr  =  Ad /d =  π d/2/Γ((d/2) + 1), giving  V2  =  π  and  V3  = 4π/3  as  desired. 
�

2


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
