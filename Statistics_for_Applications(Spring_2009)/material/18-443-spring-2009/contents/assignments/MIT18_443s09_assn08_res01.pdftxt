MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.440  and  18.443 

Gamma  and  beta  probabilities 

This handout is based  on section 1.5  of  a book  manuscript,  Handbook  and  “Tables”  of 
Classic Probabilities, by Robert J. Holt, R. M. Dudley, David Yang Gao, and Lewis Pakula. 
The  numbering  from  that  section  is  preserved,  but  some  revisions  have  been  made. 

1.5  Gamma  and  beta  functions  and  probabilities.  The  gamma  function  is deﬁned 
for  any  a >  0 by 

(1.5.1) 

Γ(a) 

:= 

∞ 

 

�
0 

x a−1 e −x dx. 

1 
The integral is  ﬁnite if (and  only if )  a >  0, because  �0
for  x  large  enough. 
Integration  by  parts  shows  that  Γ(a  + 1)  =  aΓ(a) for  any  a >  0.  We  have  Γ(1)  =  1. 
It follows by  induction that Γ(k + 1)  =  k !  for  any  nonnegative  integer  k . 
For  any  a >  0 the function deﬁned by 

xa−1 dx  = 1/a < ∞, and  xa−1  < ex/2 

(1.5.2) 

γa (x) 

:=  x a−1 e −x/Γ(a) 

for  x >  0,  and 0 for  x  ≤  0,  is  a  probability  density.  The  corresponding  distribution  is 
called  a  gamma  distribution  with  parameter  a. 
If  the  random  variable  X  has  a  gamma  distribution  with  parameter  a  then  EX  =  a 
since  EX  = Γ(a  + 1)/Γ(a).  Likewise  EX 2  = Γ(a  + 2)/Γ(a) = (a  + 1)a  so Var(X ) =  a  and 
σX  =  a1/2 . 
Recall  that for any  random variable  X  with density  f  and  any  c >  0,  cX  has a density 
c−1 f (x/c).  Applying  that to  c  = 1/λ for  any  λ >  0, if  X  has density  γa  then  X/λ  has the 
density  γa,λ  deﬁned by 

γa,λ (x) =  λa  x x−1 e −λx /Γ(a) 

for 0  < x <  +∞  and  0  otherwise.  A  random  variable  Y  with  this  density  evidently  has 
E Y  =  a/λ  and  Var(Y ) =  a/λ2 . 
The  Beta  function  is  deﬁned  for  any  a >  0  and  b >  0 by 
  1 

(1.5.3) 

B (a, b) 

:= 

x a−1 (1 − x)b−1 dx. 

�
0 

:=  1  −  x  shows  that 
Clearly,  0  < B (a, b)  <  ∞  for  any  a >  0  and  b >  0.  Letting  y 
B (b, a) ≡  B (a, b).  Let  βa,b (x) :=  xa−1 (1 −  x)b−1 /B (a, b) for 0  < x <  1  and  0  for  x  ≤  0  or 
x  ≥  1.  Then  βa,b  is a probability  density.  The probability  distribution with  this density  is 
called  a  beta  distribution  with  parameters   a, b.  Its  distribution  function  is  then  deﬁned  as 
 

(1.5.4) 

Ix (a, b) 

:= 

x 

�

βa,b (t)dt,  0  ≤  x  ≤  1. 

0 

1 

The  following  fact  relates   gamma  distributions  with   diﬀerent  parameters  with  each 
other  and  relates   gamma   and  beta   functions. 

1.5.5  Theorem.  For  any  a >  0  and  b >  0,  
(a) B (a, b) ≡  B (b, a) ≡  Γ(a)Γ(b)/Γ(a  +  b). 
(b)  If   X  and  Y  are  independent  random   variables  having  gamma   distributions  with  pa­
rameters  a  and  b  respectively, then   U  :=  X + Y  has  a gamma distribution with parameter 
a 
+ b.

Proof.  First  consider (b).  U  has  a  density  u  given  by  a  convolution  of  those  of  X  and  Y , 
namely,  for  any  x >  0, 
 

u(x) = 

γa (x  −  y )γb(y )dy 

x 

�
0 

(x  − y )a−1 e −(x−y) y  b−1 e −y dy/(Γ(a)Γ(b)) 

 

x 

�
0 

=

�
The  substitution  y  =  tx,  0  ≤  t  ≤  1 gives 

−x 
=  e 

0 

 

x 

(x  −  y )a−1 y  b−1dy/(Γ(a)Γ(b)). 

−x 
=  e x a+b−1B (b, a)/(Γ(a)Γ(b)). 

Since  u  must be a probability density, it  must be the gamma density  with parameter  a + b, 
� 
and the  normalizing  constants  must  agree,  so both (a)  and (b)  follow. 

Iterating Theorem 1.5.5, it follows that if  Xi  are independent identically distributed 
variables,  each  having  the  standard  exponential  distribution  with  density  e−x  for  x  ≥  0 
and 0 for  x <  0,  so  that  the  Xi  have gamma distributions  with parameter 1, then for  each 
n  = 1, 2, ..., Sn  =  X1 + ...  + Xn  has  a  γn  density.  If  each  Xi  has  a  γa,λ  density  then  Sn  has 
a  γna,λ  density. 
It  is  now  easy  to  ﬁnd  the  means  and  variances  of  beta  distributions.  If  X  has  a  beta 
distribution  with  parameters  a, b,  in  other  words  has  distribution  function  (1.5.4),  then 
EX  =  B (a+1, b)/B (a, b). Similarly EX 2  =  B (a+2, b)/B (a, b) =  a(a+1)/[(a+b)(a+b+1)]. 
Thus 

(1.5.6) 

EX  =  a/(a  + b),  Var(X ) = 

ab 
(a  +  b)2 (a  + b + 1) 

. 

Note that 1 − X  has  a beta distribution  with parameters  b, a. Thus  E (1 − X ) =  b/(a  + b) 
which  equals  1  −  a/(a  + b) as  it  should.  Also,  1 − X  has  the  same  variance  as  X ,  and  so 
the  expression  for  Var(X ) is preserved by  interchanging  a  and  b  as  it  should  be. 
Let 0  < λ  <  ∞  and let  Y  be a Poisson random variable with parameter  λ.  Then  some 
notations  are 

k 
P (k , λ)  =  Pr(Y  ≤  k) =  e −λ  � λj /j !, 
j=0 

2


 
∞ 
�j k=
There  are  identities  relating  the  Poisson  and  gamma  distributions: 

−λ 
Q(k , λ)  =  Pr(Y  ≥  k) =  e 

λj /j !.

1.5.7  Theorem. For  any positive integer  k , if  X  has  a  γk  density,  we  have  for  any  x  ≥  0, 

(1.5.8) 

and 

(1.5.9) 

Q(k , x) =  P (X  ≤  x) 

P (k −  1, x) =  P (X >  x). 

For 0  < λ  <  ∞, if  Y  has  a  γk,λ  density  and  0  < t <  ∞, then 

(1.5.10) 

and 

(1.5.11) 

P (Y  ≤  t) =  Q(k , λt) 

P (Y > t)P (k

−

1, λt)

. 

Proof.  Equation  (1.5.9)  follows  by  diﬀerentiating  with  respect  to  x  and  noting  that 
the  derivative  of  P (k  −  1, x) gives  a (ﬁnite)  telescoping  sum.  Equation follows by  taking 
complements. 
Then letting  Y  =  X/λ,  Y  has the given density,  and (1.5.11) follows from (1.5.9), and 
� 
(1.5.10) follows by  taking  complements  or from (1.5.8). 

A  similar  identity  relates  beta  and  binomial  probabilities.  Let  0  < p  <  1,  q  = 1 −  p, 
let  X  be  a binomial (n, p) random  variable  and 

B (k , n, p)  =  Pr(X  ≤  k)  =


E (k , n, p)  =  Pr(X  ≥  k)  =


k 
�
j=0 

n 
�
j=k 

b(j, n, p), 

b(j, n, p). 

1.5.12  Theorem. If 0  < p  <  1,  and 0  ≤  k  ≤  n  are integers, then 

E (k , n, p) =  Ip (k , n −  k + 1), 

if  k  ≥  1;  B (k , n, p) =  I1−p (n  −  k , k + 1), 

if  k < n. 

Proof.  The  ﬁrst  equality  again  follows  from  diﬀerentiating  a  ﬁnite  sum  with  respect  to  p 
which gives a telescoping  sum.  The second  then follows from  B (k , n, p) ≡  E (n − k , n, 1 − p). 
� 

3 

Quotients  of  independent  variables  with   the  densities  just  given  have  distributions 
that  can  be  expressed   in   terms  of  beta  probabilities: 

1.5.13  Theorem.  Let  X  and  Y  be independent gamma  variables  with parameters (a, λ) 
and (b, µ) respectively.  Then  for  0  < t <   ∞,  P (Y /X  ≤  t) =  P (V  ≥  λ/(λ + µt)) where  V 
has  a  beta  distribution   with  parameters  a, b. 

Proof.  We have 

P (Y  ≤  tX ) = 

∞ 

 

�
0

λa  x a−1 e −λxΓ(a)−1 

tx 

 

�
0 

µ b  y  b−1 e −µy Γ(b)−1dydx. 

For  each  ﬁxed  x,  make  the  substitution  y  =  sx  in the inner integral.  Then 

P (Y  ≤  tX ) = 

∞ 

 

0

�

λa x a−1 e −λx Γ(a)−1 

 

t 

�
0 

µ b (sx)b−1 e −µsxΓ(b)−1xdsdx. 

The  integral  is  absolutely  convergent,  so  the  integrals  with  respect  to  s  and  x  can  be 
interchanged.  We  have 
 

∞ 

�
0 

x a+b−1 e −(λ+µs)x dx  =  Γ(a  +  b)/(λ +  µs)a+b  . 

Thus 

 

t 

P (Y  ≤  tX ) =  λa µ b 

�
Make  the  substitution  v  :=  λ/(λ +  µs).  Then 
  1 

0 

P (Y  ≤  tX ) = 

. 

s b−1 (λ +  µs)−a−bds/B (a, b). 

�
λ/(λ+µt) 

v a−1 (1 − v)b−1 dv/B (a, b). 

� 

4


