MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

Line-ﬁtting  by  distance:  errors-in-variables  regression. 

18.443, R. Dudley 

Regression  of  y  on  x  is  based  on  the  idea  that  the  points  xi  are  not  random  variables 
but  some  ﬁxed  points  called  “design  points,”  measured  with  little  or  no  error,   while  the 
yi  are   random  variables.  Thus  y -on-x  regression  minimizes  the  sum  of  squared  vertical 
deviations.  One  can  also  do  x-on-y  regression  which  assumes  that  the  points  yi  are  some 
ﬁxed  points  while  xi  are  random  variables  and/or  are  measured  with  errors.  So  x-on-y 
regression  minimizes the sum of  squares of horizontal deviations of  the data points from  a 
line. 

sx  = 

. 

, sy  = 

For given (X1 , Y1 ), . . .  ,  (Xn, Yn ),  with n  ≥ 2, let  sx  be  the  sample  standard   deviation 
of  the  Xi ,  and  sy  of the  Yi , 
�1/2 
�1/2
 
 
n
n
�
�
1 
1
(Yi  − Y )2 
(Xi  − X )2 
�
�
n  − 1
n  − 1
i=1 
i=1 
If  sx  =  0  then  the  y -on-x  line is not uniquely determined.  Any line through (X , Y )  will 
minimize  the  sum  of  squares  of  vertical  deviations  of  the  points  from  the  line.  Likewise  if 
sy  =  0  the  x-on-y  line  is  not  unique.  In  all  other  cases  these  regression  lines  are  deﬁned 
and  unique. 
If all the points are on a line, then that line  will  clearly be the best-ﬁtting line either for 
vertical deviations (y -on-x) or horizontal deviations (x-on-y ) because  these  deviations  will 
be  0  in  that  case.  It  may  be  surprising  that  these  are  the  only  times  these  two  regressions 
agree: 
Theorem  1.  For  given  observations  in  the  plane,  (X1 , Y1 ), . . .  ,  (Xn , Yn ),  where  n  ≥ 2, 
s2 
x  >  0  and  s2 
y  >  0, the lines given by  y -on-x  and  x-on-y  regression  only  agree  when  all  the 
points (Xi , Yi) are  on  a  line. 

Proof.  Both  regression lines pass through  the point (X , Y ).  The  slope  of  the  y -on-x  line 
·  sy 
is  r 
/sx  (Rice, §14.2.3 p. 561)  where  r  is  the  correlation  coeﬃcient  of  the  observations. 
The  slope  of  the  x-on-y  line,  if  we  take  the  y  axis  as  horizontal  and  the  x  axis  as 
· 
vertical, is then  r sx/sy .  In  the  original  orientation  where  the  x  axis  is  horizontal  and  the 
y  axis is vertical, the slope is  replaced by its  reciprocal, which is (1/r)sy /sx .  So,  the  two 
lines  are  only  the  same  if  r  = 1/r so  r2  = 1, r  =  ±1.  Then the points (Xi , Yi ) are  all  on  a 
line (with positive  slope if  r  =  1  or  negative  slope  if  r  =  −1), Q.E.D. 
About  notation:  Rice  on  p.  561  uses  deﬁnitions  of  sample  covariances  and  variances 
with  a factor of  1  rather than 
1  . Note that in his next three displays after the deﬁnitions, 
n−1 
n
such  factors  will  appear  both  in  the  numerator  and  denominator,  so  they  will  divide  out. 
One just needs to be consistent in using  one factor or the other.  Also note that  what Rice 
calls  sxx  and  syy  are  estimators  of  sample  variance (as  opposed to  standard deviation). 
Theorem  1  implies  that  the  two  regression  lines  will  in  nearly  all  cases  be  diﬀerent 
(if  n  ≥ 3).  If  the  y -on-x  regression  line  has  a  positive  slope,  but  the  correlation  r <  1, 
then the  x-on-y  line  always  has  a  larger  slope,  by  a  factor  of  1/r2 .  In  many  situations,  the 
assumptions for  y -on-x  and  x-on-y  regression  may  not  hold.  We  need  something  better. 

1


 
A third  way  of  ﬁtting  a line to a set of points (x1 , y1 ), . . .  ,  (xn , yn) is  to  minimize  the 
sum  of  squared  distances  of  the  points  to  the  line.  This  corresponds  to  what  is  sometimes 
called  “errors-in-variables”  regression.  The  idea  is  that  both  xi  and  yi  are  measured  with 
error,  so  that  both  are  random  variables. 
For any point  p  and line  L  in the plane, let  d(p, L) be the distance from p  to  L. Given 
a  joint  distribution  of  (X, Y )  in  the  plane,  where  E (X 2  +  Y 2 )  <  ∞,  a  line  Lo  will be 
called  a  bfsd line (best-ﬁtting by  squared distance line)  if  E [d((X, Y ), L)2 ] is  minimized  at 
L  =  Lo . This  will  apply  to data  sets (xi , yi), i  = 1, . . . , n,  as  will  be  explained  at  the  end. 
Let  Cov(X, Y ) =  E (X Y ) − EX E Y ,  the  covariance  of  X  and  Y ,  for  any  random 
variables (X, Y ).  If  the  standard  deviations  σX  >  0  and  σY  >  0  then  the  correlation  of  X 
and  Y  is deﬁned by  ρ  =  ρX,Y  = Cov(X, Y )/(σX σY  ).  We  have  −1  ≤ ρ  ≤ 1. 
Let  La,b  be  the  line  y  =  a  +  bx  for  any  real  numbers  a, b.  Let  L∞;c  be  the  vertical 
line  x  ≡ c,  −∞ < y  <  ∞.  So  every  line  in  the  plane  is  either  a  line  La,b  or  a  line  L∞;c  for 
some  a, b  or  c.  Then  bfsd  lines  are  characterized  as  follows. 
Theorem 2.  For  any  random  vector (X, Y ) in  the  plane  with  E (X 2  +  Y 2 ) <  ∞ there is 
at least  one bfsd line.  All  such lines go through the point (EX, E Y ).  Let  σ  =  σX  and 
τ  =  σY  .  If  σ  =  τ  = 0,  or  σ  =  τ >  0  and  ρ  =  ρX,Y  = 0, then  every line through (EX, E Y ) 
is  a  bfsd  line. 
In  all  other  cases  the  bfsd  line  L  is  unique. 
If  σ >  0 =  τ  then  L  =  LEY ,0 ,  or  if  σ  = 0  < τ  then  L  =  L∞;EX . 
If  σ >  0  and  τ >  0  then:  if  ρ  =  0  and  σ 2  > τ 2  then  L  =  LEY ,0 ,  or  if  σ 2  < τ 2  then 
L  =  L∞;EX . 
If  σ >  0, τ  >  0 and  ρ  = 0 (the general case)  then  Lo  =  La,b  has slope  b  = tan θ  (which, 
given  θ ,  uniquely  determines the line  as (y − E Y ) =  b(x  − EX )) and  θ  is  as  follows: 
If  σ > τ  then  θ  =  θI  where 
 
 
2Cov(X, Y )
. 
X  − σ 2 
σ 2 
Y 

tan−1 

�

(1) 

θI  = 

1 
2 

�

If  σ < τ  then  θ  =  θI I ,  deﬁned  as  θI  +  π/2.

If  σ  =  τ  then  since  ρ  = 0, Cov(X, Y ) =  0  and:

if Cov(X, Y ) >  0,  θ  =  π/4,  b  = 1; 
if Cov(X, Y ) <  0,  θ  =  −π/4,  b  =  −1. 
Proof.  In  each  of  the following  cases,  the probability  that (X, Y )  is  on  the  given  line  L 
is 1,  so  E (d((X, Y ), L)2) = 0  and  L  is  a  bfsd  line:  σ >  0 =  τ ,  so  Y  ≡ E Y  is  constant  and 
L  =  LEY ,0  is  horizontal;  or  X  ≡ EX  is  constant,  σ  = 0  < τ  and  L  =  L∞;EX ,  the  vertical 
line  x 
EX ;  or  the  distribution  of  (X, Y )  is  concentrated  at  one  point  (EX, E Y ), i.e. 
≡
σ  =  τ  =  0,  and  L  is  any line  through (EX, E Y ),  either  a  line  y  − E Y  =  b(x  − EX ) for 
any  ﬁnite  slope  b,  or  the  vertical  line  L∞;EX . 
To  ﬁnd  the  distance  d((X, Y ), L)  from  a  point  (X, Y )  to  a  line  L, if  L  =  L∞;c  it’s 
| 
| 
| 
X  − c  .  If  L  =  La,0  it’s  Y  − a  .  So  suppose  L  =  La,b  with  b  =  0.  Here  are  two  ways  of 
| 
evaluating  the distance.  First, here’s  a geometric-trigonometric way.  The  vertical distance 
from (X, Y )  to  La,b  is  clearly  |Y  − a  − bX |.  A  line  M  through (X, Y )  perpendicular  to 
La,b  forms  an  angle  θ  at (X, Y ) with  a  vertical line.  Then the perpendicular distance from 

2


6
 
6
 
6
 
6
 
 
 
 
(X, Y ) to  La,b  is  |Y  − a − bX | cos θ .  On  the  other  hand,  one  can  see  by  drawing  a  diagram 
or  otherwise  that  the  line  La,b  forms  the  same  angle  θ  with  a  horizontal  line.  Thus  the 
slope  of  La,b ,  namely  b,  equals  tan  θ .  From the trigonometric identity 1 + tan2  θ  ≡ sec2 θ 
we  get  cos θ  = 1/√1 + b2 ,  and  the  squared  distance 

(2) 

d((X, Y ), La,b)2  =

(Y  − a  − bX )2 
. 
1 + b2 

Here  two  diﬀerent  although  symmetric  diagrams  could  be  needed  depending  on  whe­
ther  b >  0  or  b <  0.  (By  the  way  the  French  group  of  mathematical  authors  with  the 
pseudonym Bourbaki decided that diagrams couldn’t be part  of proofs and decided  to have 
no diagrams in their books.)  Anyhow, here’s another way  to evaluate  d((X, Y ), La,b )2 . We 
ﬁrst  ﬁnd  the  line  M  through (X, Y ) perpendicular to  La,b ,  which  has  slope  −1/b,  so  M  is 
y − Y  =  −(x  − X )/b.  The  intersection  of  M  with  La,b  gives  a  +  bx  =  Y  − (x  − X )/b, 
x  =  ξ  = (Y  − a  + X/b)/(b +  b−1 ) = (bY  − ab  + X )/(b2  + 1), 
y  =  η  =  ξ  +  bξ  = (b2Y  + a  +  bX )/(b2  + 1). 

So the  square  of  the distance from (X, Y ) to  La,b  is 

(X  − ξ )2  + (Y  − η)2  = [(b2X  − bY  +  ab)2  + (Y  − bX  − a)2 ]/(b2  + 1)2 
= [b2 (Y  − bX  − a)2  + (Y  − bX  − a)2 ]/(b2  + 1)2  = (Y  − bX  − a)2/(b2  + 1), 
agreeing  with  the  squared distance found in (2). 
So  E (d((X, Y ), La,b)2 ) =  E ((Y  − bX  − a)2 )/(b2  + 1).  For  ﬁxed  b,  the  minimization 
to  ﬁnd  a  in  terms  of  the  other  quantities  is  exactly  as  in  y -on-x  regression  and  gives  the 
same  result.  Namely,  we  have  a  quadratic  function  of  a,  which  goes  to  +∞ as  |a| does. 
So  it  will  be  minimized  at  the  unique  point  where  the  partial  derivative  with  respect  to 
a  is  0,  which  gives  −2E (Y  − bX ) + 2a  =  0,  or  a  =  E Y  − bEX .  This  says  that  the  point 
E (X, Y ) = (EX, E Y ) is  on  the  line  La,b ,  again, just  as for  y -on-x  regression. 
The line  La,b  through (EX, E Y ) and  the horizontal line  LEY ,0  form some angles  θ . As 
already indicated, we will take  a  θ  such  that the slope  b equals tan θ . Recall that for any real 
number  x, tan−1 x  is  an  angle  φ  such that tan φ  =  x  and  −π/2  <  φ  < π/2.  Then  tan−1 x 
is  uniquely  deﬁned  since  the  tangent  function  is  strictly  increasing  for  −π/2  <  θ  < π/2 
and  takes  all  real  values  there.  The  tangent  function  is  periodic  of  period  π .  Thus,  all 
angles  φ  such  that tan φ  =  x  are  of  the  form  tan−1 x + mπ  where  m  is  an integer, positive, 
negative or 0.  On any interval  of length  π , containing just  one of its endpoints, the tangent 
function takes  all  real  values  once  each,  and  also goes to  ±∞ at  one point.  It’s  convenient 
for present purposes to choose  θ  such  that  −π/4  ≤ θ <  3π/4, which is an interval  of length 
π  containing  only its lower  endpoint.  For  any  real  number (slope)  b  this  gives  a  unique  θ 
such that tan θ  =  b. 
The  squared  distance  from  La,b  to (X, Y ) is 

[Y  − E Y  − (tan θ)(X  − EX )]2/(1 + tan2 θ) 
3 

which  since 1 + tan2 θ  = 1/ cos2 θ  equals 
[(Y  − E Y ) cos θ − (X  − EX ) sin θ ]2 . 
We  want  to  ﬁnd  θ  to  minimize  the  expectation  of  this,  which  is 

X  sin2 θ . 
2  cos 2 θ − 2Cov(X, Y ) sin θ cos θ +  σ 2 
f (θ)  ≡  σY 
Since  f  is  smooth  and  periodic  of  period  2π  (actually,  of  period π  because  of  the  product 
and  squaring),  setting  f ′  (θ) = 0 we can expect to ﬁnd  at least one minimum and  at least 
one  maximum.  They  will  turn  out  to  be  in  perpendicular  directions.  We  get 
f  ′  (θ) = 2 sin θ cos θ(σ 2 
X  − σ 2 
Y  ) − 2 cos(2θ)Cov(X, Y ).
If  σ 2  =  σ 2  this  gives  tan(2θ) =  2Cov(X, Y )/(σ 2
2  ).  There  are  two  solutions  for  θ ,
X  6
X  − σY 
Y
namely  θI  given by (1)  and  θI I  =  θI  + (π/2),  since  tan(φ +  π ) =  tan φ  for  any  φ.  Then 
−π/4  < θI  < π/4  < θI I  <  3π/4,  so  both  θI  and  θI I  are  in  the  chosen  interval  for  θ . A 
point  where  f ′  (θ) = 0 will be a relative minimum if  f ′′  (θ) >  0.  We  have 
X  − σ 2 
f  ′′  (θ) =  2 cos(2θ)(σ 2 
Y  ) + 2 sin(2θ) 2Cov(X, Y ).
· 
2  )/ cos(2θ).  If  σ 2  > σ 2  we 
At  a  point  where  f ′  (θ)  =  0  this  becomes  f ′′  (θ)  =  2(σ 2
X  − σY 
Y 
X
since  cos(2θI ) >  0  for  a  minimum,  and  θI I  will  give  a  maximum.  If  σ 2  < σY 
2
want  θ  = 
θI
X 
we  want  θ  =  θI I  so that  π/2  <  2θ <  3π/2  and  cos(2θI I ) <  0  for  a  minimum,  while  θI  then 
gives  a  maximum. 
Now,  what  if  σ 2  =  σ 2  ? In that  case  f ′  (θ) =  −2 cos(2θ)Cov(X, Y ).  If Cov(X, Y ) = 0, 
Y 
X
f  is  a  constant  and  all  θ , in  other  words  all lines through (EX, E Y ),  are  equally  good. 
(Consider for example a bivariate normal distribution with mean  0  whose covariance matrix 
is  σ 2  times  the  identity  matrix.  This  distribution  is  rotationally  invariant,  so  clearly  all 
lines  through  the  origin  ﬁt  it  equally  well.) 
If Cov(X, Y ) =6
0  then  we  need  cos(2θ) = 0, so we can take  θ  =  ±π/4.  Again  we  need 
to  consider  the  second  derivative,  which  is  f ′′  (θ) = 4 sin(2θ)Cov(X, Y ).  To have  f ′′  (θ) >  0 
for  a  minimum  of  f , if Cov(X, Y )  >  0  we  want  θ  =  π/4,  giving  a  line  with  slope  1.  If 
Cov(X, Y ) <  0  we  want  θ  =  −π/4,  giving  a  line  with  slope  −1.  This  completes  the  proof 
� 
of Theorem 2. 

cov(X, Y ) = 

When  ﬁtting  a line to  a  ﬁnite sample (x1 , y1 ), . . .  ,  (xn , yn), EX  is  replaced  by  x,  E Y 
by  y ,  σ 2  =  σ 2  by  the  sample  variance 
2  ,  τ 2  =  σ 2  by  the  sample  variance  s2  ,  and  the 
sX
X 
Y 
Y 
covariance Cov(X, Y ) by  the  sample  covariance  deﬁned  as 
n 
 
1 
�
(Xj  − X )(Yj  − Y ) 
n  − 1 
j=1 
in  this  case,  using  1/(n  − 1)  to  ﬁt  with  the  usual  deﬁnition  of  sample  variances  s2  and 
X 
2  . (If 1/(n  − 1)  is  replaced  by  1/n in  both  sample  variances  and  the  sample  covariance, 
sY 
the  result  is  the  same  since  these  factors  cancel  out,  appearing  both  in  the  numerator  and 
denominator  of (1),  as long  as it’s done  consistently.) 
Acknow ledgment.  Daniel  Kane  suggested  the  trigonometric  formulation  and  result  in 
February, 2005. 

4 

 
 
