MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.443


MEAN-SQUARE ERRORS OF ESTIMATORS:

BIAS,  VARIANCE,  AND  INFORMATION  INEQUALITIES


Suppose  we  have  a  parametric  family  of  probability  distributions  with  a  likelihood 
function  f (x, θ)  for  one  observation,  where  f (x, θ)  is  a  probability  mass  function  for  a 
discrete  distribution  or  a  probability  density  function  for  a  continuous  distribution.  Let 
Eθ  denote  expectation,  and  Pθ  probability,  when  θ  is  the  true  value  of  the  parameter. 
Let  X  = (X1 , ..., Xn) be  a  vector  of  i.i.d.  observations  with  distribution  Pθ .  Suppose 
g  =  g (θ)  is  a  real-valued  function  of  the  parameter  θ .  One  criterion  for  choosing  an 
estimator  T  =  T (X )  of  g (θ) is  to  minimize  the  mean-squared  error (MSE)  Eθ ((T (X ) − 
g (θ))2).  Recall  that  T  is  called  an  unbiased  estimator  of  g (θ) if  Eθ T (X ) =  g (θ) for  all  θ . 
More generally, the  bias  of  T  as  an  estimator  of  g (θ) is deﬁned by 

bT (θ) 

:=  bT ,g (θ) 

:=  Eθ T  − g (θ) 
for  all  θ .  Thus  T  is  unbiased  as  an  estimator  of  g (θ) if  and  only  if  bT (θ) = 0 for all  θ .  If 
Eθ (T 2 )  <  +∞ for  all  θ ,  let  Varθ (T )  be  the  variance  of  T  for  the  given  θ ,  which  equals 
Eθ (T 2 ) − (Eθ T )2 .  The  MSE  equals  the  variance  plus  the  bias  squared,  as  follows: 
Theorem.  For  any  statistic  T (X )  such  that  Eθ (T 2 )  <  ∞ for  all  θ  and  any  real-valued 
function  g (θ),  the  mean-square  error  of T  as  an  estimator  of  g  is given by 

Eθ ((T (X ) − g (θ))2)  =  Varθ (T ) + bT (θ)2 . 

Proof. Let  h(θ) :=  Eθ T .  Then  we  have 

Eθ ((T (X ) − g (θ))2) =  Eθ ((T (X ) − h(θ) + h(θ) − g (θ))2) 
=  Varθ (T ) + 2Cov((T (X ) − g (θ), bT (θ)) + bT (θ)2  =  Varθ (T ) + bT (θ)2 
where the  covariance is 0 because for given  θ ,  bT (θ) is a  constant,  so the proof is complete. 
Q.E.D. 

In  a  classical  approach,  say  in  research  from  the  1930’s  through  the  mid-1950’s  and 
still  in  many  textbooks,  one  looked  at  unbiased  estimators,  so  that  b(θ)  ≡ 0,  and  then 
tried  to  minimize  the  variance.  A  lower  bound  for  the  variance  of  unbiased  estimators,  the 
so-called information inequality, or  Cram´er-Rao inequality (Rice, Section 8.7,  Theorem A, 
and  later  in  this  handout),  proved  in  the  1940’s,  was  considered  one  of  the  main  theorems 
of  mathematical  statistics. 
An  estimator  T (X )  is  called  inadmissible  as  an  estimator  of  g (θ),  for  mean-squared 
error, if there is another estimator  U (X ) such  that  Eθ [(U (X )−g (θ))2] ≤ Eθ [(T (X )−g (θ))2] 
for  all  θ  where  the  inequality  becomes  strict,  with  ≤ replaced by  <,  for  some  θ .  If  there  is 
no  such  U  then  T  is  called  admissible. 
Let’s  call  T (X ) strongly  inadmissible  if  we  add  to  the  the  deﬁnition  that  Eθ [(U (X ) − 
g (θ))2 ] < Eθ [(T (X ) − g (θ))2 ] for  all  θ  in  a  non-empty  open  set  V ,  namely,  a  set  such  that 
1


for  some  θ0  in  V  and  r >  0,  also  θ  is in  V  for  all  θ  such  that  |θ − θ0 | < r .  In  one  dimension 
this  would just  say  that  V  includes  a  non-degenerate  interval. 
If  π  is  a  prior  density  with  π (θ) >  0  for  all  θ ,  and  T  is  a  Bayes  estimator  for  g (θ), 
namely  the  integral  of  g (θ) times the posterior density  πX (θ), then T  cannot  be  strongly 
inadmissible,  or there  would be  an  estimator  with  smaller  overall  risk (integrating  mean­
square error times  π (θ)), contradicting the Bayes property  of  T , as shown in lecture Monday 
3/10 (I looked but  so far have  not found this fact in Rice). 
Suppose  we  have  a  normal  distribution  on  d-dimensional  space  where  the  coordinates 
xj  are  normal  and  independent  with  means  µj  and  variance  1.  The  analogue  of  squared 
diﬀerence  is  squared  Euclidean  distance  |x  − y |2  =  �d 
(xj  − yj )2 ,  so  that  for  the  mean 
j=1
vector  µ = (µ1 , ..., µd),  and an  estimator  T (X ) of  it,  also  with  d-dimensional  values,  we’re 
aiming  to  minimize  Eµ (|T (X ) − µ|2 ).  A  surprising  discovery  by  Charles  Stein  in  1956  was 
that  although  the  sample  mean  X  is  an  admissible  estimator  of  the  mean  vector  µ  for 
d  = 1  or 2, it is  not for  d  = 3  or  larger;  biased  estimators  can  do  better.  Details  are  given 
in  the  18.466  OCW  notes,  Section  2.7. 
Yatracos (2005) considered  the sample variance for 1-dimensional data.  Let the sample 
variance  be  deﬁned  as 

n 
(Xj  − X )2 , 
� 
j=1 
where we know that  cn  = 1/(n − 1) gives an unbiased  estimator of  the variance whenever it 
is  ﬁnite,  whereas  cn  = 1/n gives the maximum likelihood estimate for normal distributions 
and  the  statistic  used  in  method-of-moments  estimation.  Yatracos  proved  the  following 
let  X1 , ..., Xn  be  i.i.d.  with  any  distribution  such  that  E (X 4)  <  ∞,  Xj  are  not 
fact: 
1 
constant,  and  in  a  family  such  that  for  any  c  with 0  < c <  ∞,  the  distribution  of  cX1  is 
also  in  the  family.  Then  the  classical  sample  variance  with  cn  = 1/(n  − 1)  is inadmissible 
as  an  estimator  of  the  true  variance.  An  estimator  with  smaller  mean-squared  error  is 
obtained  by  taking 

cn 

cn  = 

n  +  2 
n(n  + 1) 

. 

Of  course,  the  resulting  estimator  has  a  non-zero  bias,  but  the  bias  becomes  very  small  as 
n  becomes  large  and  the  reduction  in  variance  is  enough  to  make  the  total  MSE  smaller. 
The Stein and Yatracos examples are part of the reason that the information inequality 
is  not  emphasized in this  course.  Still, the  rest  of  this handout  will present it,  ending  with 
a  form  that  applies  when  there  is  a  bias. 
The  Cauchy-Schwarz  inequality  as  applied  to  random  variables  is  as  follows.  It  was 
given  in  the  course  in  showing  that  correlations  have  absolute  value  at  most  1. 
Fact.  Let  X  and  Y  be  two  random  variables  with  E (X 2)  <  ∞ and  E (Y 2 )  <  ∞.  Then 
(E (X Y ))2  ≤ E (X 2)E (Y 2 ).  Equality  holds  if  and  only  if  X  and  Y  are linearly  dependent 
(one  is  a  constant  times  the  other). 
Proof. For any  real  t,  E ((X +tY )2 ) ≥ 0.  Expanding gives E (X 2)+2tE (X Y )+t2E (Y 2 ) ≥ 0. 
If  E (Y 2 )  =  0  then  P (Y  = 0)  = 1,  E (X Y )  =  0,  the  inequality  holds,  and  X  and  Y  are 
linearly  dependent  since  Y  ≡ 0 X .  So  assume  E (Y 2 ) >  0.  Then the quadratic function  of 
· 
2


t,  at2 + bt + c  ≥ 0 where  a  =  E (Y 2 ), b  = 2E (X Y ) and  c  =  E (X 2).  So the quadratic cannot 
have  two  distinct  real  roots,  or  it  would  become  negative  somewhere.  So  b2  − 4ac  ≤ 0, 
which  gives  the  inequality.  If  it  becomes  an  equality,  then  b2  − 4ac  = 0  and  the  quadratic 
has  one  real  root  t  such  that  E ((X  +  tY  )2 ) = 0,  so  X  +  tY  =  0  with  probability  1,  and 
X, Y  are linearly  dependent.  Q.E.D. 
Replacing  X  and  Y  by  X − EX  and  Y  − E Y ,  we get (Cov(X, Y ))2  ≤Var(X )Var(Y ). 
If both  variances  are positive,  we  can take  square  roots  and divide by  standard deviations 
to obtain the known fact that for the correlation  ρX,Y  we have  |ρX,Y  | ≤ 1,  and  ρX,Y  =  ±1 
if  and  only  if  X  and  Y  are  linearly  dependent,  speciﬁcally: 
Corol lary.  For  two  random  variables  X, Y  with positive,  ﬁnite variances,  ρX,Y  =  ±1 if  and 
only if  Y  =  cX  for  some  constant  c  where  c >  0  if  and  only  if  ρX,Y  =  1  and  c <  0  if  and 
only  if  ρX,Y  =  −1. 
The Fisher  information.  Given a parametric family  with likelihood function  f (θ , x), where 
θ  varies  over  a  one-dimensional  interval  a < θ < b, provided that  f  is twice diﬀerentiable 
with  respect  to  θ  and  the  expectation  exists,  the  Fisher  information  at  θ  is  deﬁned  by 
I (θ) =  −Eθ ∂ 2  log f (θ , x)/∂ θ2 . Let  X1 , X2 , ...,  be i.i.d. with the likelihood function  f (θ0 , x). 
Then  under  some  conditions,  I (θ)  exists  for  a < θ < b, for  n  large  enough,  with  proba­
bility  converging  to  1,  a  unique  MLE  θˆn  exists,  and  is  asymptotically  normal,  speciﬁcally, 
√n(θˆ
n  − θ0 )  converges  in  distribution  to  N (0, 1/I (θ0)) (the reasoning,  without a precise 
formulation  of  the  conditions  which  justify  it,  is  given  in  Rice,  subsection  8.5.2;  precise 
details  are  given  on  the  18.466  OCW  lecture  notes,  Section  3.6. 
The  Fisher  information  can  be  written  in  an  alternate  form,  namely, 

I (θ) =  Eθ ((∂ log f (θ , x)/∂ θ)2). 

This  is  shown  in  Rice,  Lemma  A  of  subsection  8.5.2,  provided  derivatives  with  respect  to 
θ  can  be  interchanged  with  integrals  with  respect  to  x  (or  sums  in  the  case  of  discrete 
distributions). 
The  information  inequality.  Some extended forms of  the information inequality, also called 
the  Cram´er-Rao,  Fr´echet-Cramer-Rao,  or  Rao-Cram´er  inequality  will  be  given. 

Theorem.  Let  Y  =  u(X1 , ..., Xn)  be  an  unbiased  estimator  of  a  function  g (θ)  and  have 
a  ﬁnite  variance.  Suppose  also  that  the  Fisher  information  I (θ)  (for  one  observation) 
exists and is positive, and  that  suﬃcient  conditions hold for interchanging  derivatives with 
respect to  θ  and  integrals  with  respect  to  x.  Then  Varθ (Y ) ≥ g  ′ (θ)2/(nI (θ)). 
Proof.  The Fisher information for  n  i.i.d.  observations  is  n  times  the  Fisher  information 
for  one observation.  Thus we  can assume there is one (vector)  observation and  take  n  = 1. 
Unbiasedness  of  Y  means  that,  if  the  distribution  of  x  is  continuous,  for  all  θ , 
� 
If  x  has  a  discrete  distribution  then 
to  θ  gives 

� 
x  . Diﬀerentiating  with  respect 
Y (x)(∂ f (θ , x)/∂ θ)dx. 

Y (x)f (θ , x)dx  =  g (θ). 

� 
g  ′ (θ) = 

...dx  is replaced by 
� 

3 

� 
Since 
(∂ f (θ , x)/∂ θ)dx  =  0,  and  we  can  insert  a  factor  g (θ) not  depending  on  x  in the 
latter  integral,  we  get


dx 

f (θ , x)dx 
� 

. 

=

∂ f (θ , x)
(Y (x) − g (θ)) 
∂ θ 
∂ log f (θ , x) 
∂ θ 

�

g  ′ (θ)  =
� 
(Y (x) − g (θ)) 
� 
=  Eθ  (Y (x) − g (θ)) 
It follows then by  the Cauchy-Schwarz inequality  that 
g  ′ (θ)2  ≤  Eθ ((Y (x) − g (θ))2)Eθ ((∂ log f (θ , x)/∂ θ)2) 
= Varθ (Y )I (θ).  Dividing  both  sides  by  I (θ) gives the  conclusion, Q.E.D. 

∂ log f (θ , x) 
∂ θ 

Now  let’s  consider  when  equality  holds  in  the  information  inequality.  When  is  an 
unbiased  estimator  Y  optimal in the sense that its variance, for each  θ , is as small  as it  can 
be (attains the lower bound in the information inequality)? Looking  at the  conditions for 
equality  in  the  Cauchy-Schwarz  inequality,  we  ﬁnd  that  ∂ log f (θ , x)/∂ θ and  Y (x) − g (θ) 
must  be  linearly  dependent  for  each  θ .  If  even  for  one  θ ,  Y (x) − g (θ) = 0 for almost all  x, 
that  means  the  statistic  Y ,  which  cannot  depend  on  θ ,  must  be  a  constant  with  respect  to 
x, so it  cannot be an unbiased  estimator except for a constant function  g , which is a trivial 
case.  So  suppose  ∂ log f (θ , x)∂ θ  =  h(θ)(Y (x) − g (θ)) where  the  proportionality  constant 
h(θ) in  the  linear  dependence  can  depend  on  θ .  Solving  the  ﬁrst-order  partial  diﬀerential 
equation in  θ  gives log f (θ , x) =  u(θ)Y (x) + v(θ) + j (x) where  u  is the indeﬁnite integral 
of  h  with  respect  to  θ ,  v  is  the  indeﬁnite  integral  of  −(hg )(θ) and  j (x) is  a  function  of  x, 
whose partial derivative with  respect to  θ  is 0 (like a constant  of integration).  Then, taking 
the  exponential  of  both  sides,  we  see  that  our  parametric  family  of  distributions  must  be 
an  exponential  family,  and  the  only  estimators  which  attain  the  information  inequality 
lower  bound  are  constants  times  Y (x),  so  that  the  only  functions  of  θ  having  unbiased 
estimators  attaining  the  lower  bound  are  constants  times  g (θ).  So,  for  example,  if  θ  itself 
has  an  unbiased  estimator  attaining  the  lower  bound,  then  any  nonlinear  function  such  as 
θ2  cannot. 
General  estimators  which  may  have  bias.  To  make  the  information  inequality  more  useful 
it  will  be  extended  to  estimators  that  are  not  necessarily  unbiased. 
In  fact,  unbiased 
estimators  don’t  always  exist,  and  even  when  a  unique  unbiased  estimator  exists,  it  is  not 
necessarily  a good  estimator, as we have seen in the example of  estimating  p2  in a binomial 
distribution  with  2  trials. 
Let  Y  be  any  statistic.  Viewed  as  an  estimator  of  a  function  g (θ),  deﬁne  the  bias  of 
Y  as  bY  (θ) =  Eθ Y  − g (θ). 
Theorem  (extended  information  inequality).  For  any  statistic  Y  and function  g (θ)  such 
that  g  is  diﬀerentiable  and  the  bias  bY  (θ) of  Y  as  an  estimator  of  g  is  also  diﬀerentiable, 
and  under  the  conditions  of  the  previous  theorem, 

Eθ ((Y  − g (θ))2)  ≥ 

′ 
′ 
)(θ))2 
bY 
((g  + 
(θ 
nI

+  bY  (θ)2 . 

4 

Proof.  Y  is  an  unbiased  estimator  of  g (θ) + bY  (θ),  and 

Eθ ((Y  − g (θ))2) =  Eθ (Y  − Eθ Y  +  Eθ Y  − g (θ))2) 
= Varθ Y  +  bY  (θ)2 .  Applying  the previous theorem gives the  result, Q.E.D. 

REFERENCES 

Dudley, R. M. (2003).  Mathematical  Statistics, 18.466 lecture notes, Spring 2003.  On MIT 
OCW (OpenCourseWare)  website, 2004. 

Yatracos, Y. (2005).  Artiﬁcially  augmented  samples,  shrinkage,  and  mean  squared  error 
reduction.  J.  Amer.  Statist.  Assoc.  100, 1168-1175. 

5


