MIT OpenCourseWare 
http://ocw.mit.edu 

18.443 Statistics for Applications 
Spring 2009 

For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 

18.443  problem  set  4 

1.  The  following  values: 

0.312, 0.238, 0.446, 0.968, 0.576, 0.471, 0.596 

were   generated  using  R  by  the  command 
x  <- rgamma(7,alpha,lambda) 
to  give  7  i.i.d.  variables  with  a  gamma(α, λ)  distribution  for  some  numerical  values  of  α 
and  λ  that  I  gave.  Estimate  what  those  values  were  by  method  of  moments. 

2.  Suppose  in  two  independent  trials  with  probability  p  of  success  on  each  we  observe  X

successes.

(a)  Find  an  unbiased  estimator  T (X ) of  the  function  g (p) =  p2 .  Hint:  X  has just  three

possible values 0, 1, 2, so the estimator is given by  the three numbers   T (0), T (1), and T (2).

For  each  p,  each  value  of  X  has  a  certain  binomial  probability.  So  the  condition  for  T (X )

to  be  unbiased  gives  an  equation  that  has  to  be  satisﬁed  for  all  p  with 0  < p  <  1.

(b)  Does  the  equation  in  the  hint  give  unique  solutions  for  T (j ),  j  = 0, 1, 2,  and  what

solution(s)  do  you  ﬁnd?

(c) What is  most surprising  about the  results of part (b)?


3.  A  beta(a, b) distribution  has  a  density


fa,b (x) =  x a−1 (1 − x)b−1/B (a, b) 
for  0  < x <  1  and  0  elsewhere,  for  0  < a <  +∞ and  0  < b <  +∞,  and  B (a, b)  is 
the  beta  function,  which  normalizes  the  density  to  be  a  probability  density  and  satisﬁes 
B (a, b) = Γ(a)Γ(b)/Γ(a  +  b)  where  Γ  is  the  gamma  function.  If  X  has  this  density  then 
EX  =  a/(a  +  b).  The  simplest  example  of  such  a  distribution  is  the  uniform   U [0, 1] 
distribution  with  a  =  b  = 1. 
If  p  is  the  probability  of  success  in  each  of  n  independent  trials,  suppose  p  has  a 
prior distribution beta(a, b).  Then  if  we  observe  X  successes in the  n  trials, the posterior 
distribution  will  be  beta(a  + X, b + n  − X ).  The  Bayes  estimate  T (X, n) of  p,  minimizing 
the  risk,  i.e.  minimizing  the  integral  from  0  to  1  of  (T (X, n) − p)2  with  respect  to  the 
posterior distribution,  will just be the integral  of  p  times the posterior density. 
(a) For  the  beta(1/2, 1/2)  prior,  what  is  the  Bayes  estimator  of  p  as  a  function  of  n  and 
X ? 
(b)  Compare the estimator from part (a)  to the classical  estimator ˆp  =  X/n  in  terms  of

their  squared-error  losses  Ep (( ˆp − p)2 ) and  Ep ((T (X, n) − p)2 ).  For  which  values  of  p  does

each  estimator  perform  better  in  the  sense  of  having  smaller  expected  loss?


4.  Let  X1 , ..., Xn  be  i.i.d.  having  a  geometric  distribution  with  for  some  p  such  that

0  < p  ≤ 1,  namely  P (X1  =  k) = (1 − p)k−1 p  for  k  = 1, 2, . . .  .

(a) What is the  maximum likelihood  estimate (MLE)  of  p  based  on  X1 , ..., Xn?

(b) What  is  the  method-of-moments  estimate  of  p?

(c) Suppose we view the situation as follows.  We have done Sn  =  X1 + +Xn  independent

· · ·
trials  with  probability  p  of  success  on  each  and  observed  exactly  n  successes.  Then  what 

is  the  binomial  MLE  of  p? (It isn’t obvious that this should be equivalent to (a),  as we 
did  a  random  number  of  trials  and  stopped  after  the  nth  success.) 

exp 

exp 

f (x, θ) = 

5.  Consider  the  family  of  mixtures  of  two  normal  distributions,  having  densities  of  the 
form 
 
 
 
 
(x  − µ2 )2 �
(x  − µ1 )2 �
�
�
1 − λ 
λ 
+
√2πσ2 
√2πσ1 
− 
− 
2σ 2 
2σ 2
1 
2 
where  θ  = (λ, µ1 , σ1 , µ2 , σ2 ) is a 5-dimensional parameter with µ1  and  µ2  any  real  numbers, 
0  < σj  <  ∞ for  j  = 1, 2,  and 0  < λ  ≤ 1/2.  Suppose  given  n  observations  X1 , ..., Xn,  not 
all  equal,  assumed  to  be  i.i.d.  from  such  a  distribution.  If  a  value  θ ′  of  a parameter is  such 
that  as  θ  approaches  θ ′  (possibly under  some  restrictions),  the  likelihood  approaches  +∞, 
then  we  may  consider  θ ′  as  a maximum likelihood  estimate (MLE)  of  θ ,  or  the MLE if it’s 
unique. 
(a)  For  the  given  family  of  normal  mixture  densities,  do  there  exist  such  θ ′  ?  Are  they 
unique?  Hint:  the  exponential  of  a  nonpositive  number  is  at  most  1,  so  the  likelihood  can 
only  approach  +∞ if  at  least  one  of  the  σj  approaches 0.  But if  say  σ1  approaches 0, then 
exp(−(Xj  − µ1 )2 /(2σ 2))  will  approach  0  very  fast  if  µ1  is  ﬁxed  and  unequal  to  Xj .  In 
1
the  likelihood  function  the  Xj  are  ﬁxed  and  the  parameters  are  free  to  vary,  so  for  what 
value(s)  of  µ1  would  we  get  large  likelihood  as  σ1  ↓ 0? 
(b)  Suppose  the  observations  are  really  i.i.d.  with  a  density  of  the  given  form  having 
0  < λ  ≤  1/2,  0  < σ1  <  ∞, 0  < σ2  <  ∞.  How  successful  will  choosing  parameters 
for  which  the  likelihood  is  very  large,  as  in  part  (a),  be  in  approximating  the  actual 
parameters, supposing we can take  n  as large as we want? Speciﬁcally, supposing  λ  = 1/2 is 
ﬁxed, how well  will  the distribution function of  the distribution with  estimated parameters 
approximate  the  one  for  the  true  parameters? 

