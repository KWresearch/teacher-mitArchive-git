Pascal  Matrices

Alan  Edelman  and  Gilbert  Strang

Department  of  Mathematics,  Massachusetts  Institute  of

Technology 

Every  polynomial  of  degree  n  has  n  roots;  every  continuous  function 
on  [0, 1]  attains  its  maximum;  every  real  symmetric  matrix  has  a  complete 
set  of  orthonormal  eigenvectors.  “General  theorems”  are  a  big  part  of  the 
mathematics  we  know.  We  can  hardly  resist  the  urge  to  generalize  further! 
Remove  hypotheses,  make  the  theorem  tighter  and  more  diﬃcult,  include 
more  functions, move  into  Hilbert  space,. . .  It’s  in  our  nature. 
The other  extreme  in mathematics might be  called  the “particular  case”. 
One speciﬁc function or group or matrix becomes special.  It obeys the general 
rules,  like  everyone  else.  At  the  same  time  it  has  some  little  twist  that 
connects  familiar  ob jects  in  a  neat  way.  This  paper  is  about  an  extremely 
particular  case.  The  familiar  ob ject  is  Pascal’s  triangle. 
The  little  twist  begins  by  putting  that  triangle  of  binomial  coeﬃcients 
into  a  matrix.  Three  diﬀerent  matrices—symmetric,  lower  triangular,  and 
upper  triangular —can hold Pascal’s triangle in a convenient way.  Truncation 
⎡ 
⎤
⎡
⎤
⎡
⎤
produces n by n matrices Sn  and Ln  and Un—the pattern is visible for n = 4: 
⎥⎥⎦
 .

⎥⎥⎦
 U4  =
 ⎢⎢⎣

⎥⎥⎦
 L4  =
 ⎢⎢⎣

S4  =
 ⎢⎢⎣

We  mention  ﬁrst  a  very  speciﬁc  fact:  The  determinant  of  every  Sn  is  1. 
(If we emphasized det Ln  = 1 and det Un  = 1, you would write to the Editor. 
Too special !)  Determinants are often a surface reﬂection of a deeper property 
within  the  matrix.  That  is  true  here,  and  the  connection  between  the  three 
matrices  is  quickly  revealed.  It  holds  for  every  n: 

1 
1
1 1
4

3 
1 2
1 3  6  10 

1 4 10  20 

1 
1 1

1 2 1

1 3 3 1 

1 1 1 1

1 2 3

1 3

1 

S  equals  L  times  U 
and  then  (det S ) = (det L)(det U ) = 1 . 

This  identity  S  =  LU  is  an  instance  of  one  of  the  four  great  matrix 
factorizations  of  linear  algebra  [10]: 

1 

1.	 Triangular  times  triangular:  A = LU  from Gaussian  elimination 

2.	 Orthogonal  times  triangular:  A = QR  from Gram-Schmidt 

3.	 Orthogonal  times  diagonal  times  orthogonal:  A = U ΣV T  with 
the  singular  values  in  Σ 
4.	 Diagonalization:  A = SΛS −1  with eigenvalues in Λ and eigenvectors 
in  S .  Symmetric  matrices  allow  S −1  =  S T—orthonormal  eigenvectors 
and  real  eigenvalues  in  the  spectral  theorem. 

In A = LU ,  the  triangular  U  is  the  goal  of  elimination.  The  pivots  lie  on  its 
diagonal  (those  are  ratios  det An/ det An − 1 ,  so  the  pivots  for  Pascal  are  all 
1’s).  We  reach  U  by  row  operations  that  are  recorded  in  L.  Then  Ax  =  b 
is  solved  by  forward  elimination  and  back  substitution.  In  principle  this  is 
straightforward, but the cost adds up:  billions a year  for the most  frequently 
used  algorithm  in  scientiﬁc  computing. 
For  a  symmetric  positive  deﬁnite  matrix,  we  can  symmetrize  A  =  LU 
to  S  =  LLT  (sometimes  named  after  Cholesky).  That  is  Pascal’s  case  with 
U  = LT ,  as  we  want  to  prove. 
This  article  will  oﬀer  four  proofs  of  S  = LU .  The  ﬁrst  three  are  known, 
the  fourth  might  be  partly  new.  They  come  from  thinking  about  diﬀerent 
ways  to  approach  Pascal’s  triangle: 

First  proof :  The  binomial  coeﬃcients  satisfy  the  right  identity 

Second  proof :  S, L,  and  U  count  paths  on  a  directed  graph 

Third  proof :  Pascal’s  recursion  generates  all  three matrices 
� 
Fourth  proof :  The  coeﬃcients  of  (1 + x)n  have  a  functional meaning. 
The binomial identity that equates Sij  with  Lik Ukj  naturally comes ﬁrst— 
but  it  gives  no  hint  of  the  “source”  of  S  =  LU .  The  path-counting  proof 
(which  multiplies  matrices  by  gluing  graphs!)  is  more  appealing.  The  re­
cursive  proof  uses  elimination  and  induction.  The  functional  proof  is  the 
shortest:  Verify  S v  = LU v  for  the  family  of  vectors  v  = (1, x, x2 , . . .).  This 
allows  the  “meaning”  of  Pascal’s  triangle  to  come  through. 
The  reader  can  guess  that  the  last  proof  is  our  favorite.  It  leads  toward 
→
1/(1 − x) are particular 
→
larger  ideas;  transformations  like x 
1 + x and x 
→ 
cases of x 
(ax + b)/(cx + d).  We are close  to matrix  representations of  the 

2 

� 
M¨obius group.  At the same time S, L, and U  arise in the multipole method — 
one of  the “top  ten algorithms of  the 20th century,” which has  tremendously 
ak /(x − rk ). 
speeded  up  the  evaluation  of  sums 
You see that the urge to generalize is truly irresistible!  We hereby promise 
not  to  let  it overwhelm  this  short paper.  Our purpose  is only  to  look at Pas­
cal’s  triangle  from  four  diﬀerent  directions—identities,  graphs,  recursions, 
and  functions.  Pascal  matrices  led  to  several  Worked  Examples  in  the  new 
textbook [10], and this paper is on the course web page web.mit.edu/18.06/. 
� � 
Proof  1:  Matrix  Multiplication 
The  direct  proof  multiplies  LU  to  reach  S .  All  three  matrices  start  with 
i 
row  i = 0  and  column  j  = 0.  Then  the  i, k  entry  of L  is  k  = “i  choose  k”. 
n � �� � � 
�
�
� 
Multiplying  row  i  of  L  times  column  j  of  U  = LT ,  the  goal  is  to  verify  that 
i + j
j 
i
=
k 
i 
k
k=0 
� � 
�  �  � � 
Separate  i + j  ob jects  into  two  groups,  containing  i  ob jects  and  j  ob jects. 
If  we  select  i − k  ob jects  from  the  ﬁrst  group  and  k  from  the  second  group, 
we  have  chosen  i  ob jects  out  of  i + j .  The  ﬁrst  selection  can  be  made  in 
i 
j 
i
i−k  =  k  ways and the second selection in  k  ways.  Any number k  from 
� 
min(�i,j ) � �� � � 
0  to min(i, j )  is  admissible,  so  the  total  count  agrees  with  equation  (1): 
j 
i + j 
i
= 
. 
i 
k 
k
k=0 
In this form the sum accounts for the triangularity of L and U .  The binomial 
coeﬃcients  are  zero  for  k > i  and  k > j . 
A  shorter  proof  is  hard  to  imagine  (though  Proof  4  comes  close).  But 
the  discovery  of LU  = S  would  be  unlikely  this way.  Binomial  people would 
know the  identity (1),  the rest of us are conditioned to take their word  for  it. 
David  Ingerman  showed  us  how  to  multiply  matrices  by  “gluing  graphs”— 
and  this  gives  a  visual  explanation  [3,  7]  of  LU  = S . 

Lik Ukj  = 

(2) 

= Sij  . 

(1) 

Proof  2:  Gluing  Graphs 

The  ﬁrst  step  is  to  identify  Sij  as  the  number  of  paths  from  ai  to  bj  on  the 
up-and-left  directed  graph  in  Figure  1. 

3 

Only  one  path  goes  directly  up  from  a0  to  bj ,  agreeing  with  S0j  =  1  in 
the  top  row  of  S .  One  path  goes  directly  across  from  ai  to  b0 ,  agreeing with 
Si0  =  1.  From  that  row  and  column  the  rest  of  S  is  built  recursively,  based 
on  Pascal’s  rule  Si − 1, j  + Si, j − 1  = Sij .  We  show  that  path-counting  gives 
the  same  rule  (and  thus  the  same matrix  S ). 

��
b3  ��

b 2 

b1 

b0 

��
��

��
��

��
a0 

��
��

��
��

��
��

���
a1 

��
��

��
��

��
���

��
a 2 

� �

� �

� �

a3 

Figure  1:  The  directed  graph  for  the  path-counting matrix  S . 

A  typical  entry  is  S22  =  “4  choose  2”  =  6.  There  are  6  paths  from  a2 
to  b2  (3  that  start  across  and  3  that  start  upwards).  The  paths  that  start 
across  then  go  from  ai − 1  to  bj ;  by  induction  those  are  counted  by  Si − 1, j . 
The  paths  that  start  upward  go  to  level  1  and  from  there  to  bj .  Those  are 
counted  by  Si, j − 1  and  Pascal’s  rule  is  conﬁrmed.  (For  this  we  imagine  the 
whole graph shifted down one level, so we are actually going from ai  to bj − 1 
in  Si, j − 1  ways.)  We  do  not  know  who  ﬁrst  connected  the  matrix  S  with 
this  graph. 
Now  cut  the  graph  along  the  45◦  line  in  Figure  2.  We want  to  show  that 
Lik  counts  the  paths  from  ai  to the  (k , k)  point  on  that  diagonal  line.  Then 
Ukj  counts  paths  from  the  45◦  line  to  bj . 
The  reasoning  is  again  by  induction.  Start  from  Li0  =  1  for  the  single 
path  across  from  ai  to  (0, 0).  Also  Lii  =  1  for  the  single  path  up  to  (i, i). 
Pascal’s  recursion  is  Lik  = Li − 1, k  + Li − 1, k − 1  when  his  triangle  is  placed 
into  L. 
By  induction,  Li − 1, k  counts  the  paths  that  start  to  the  left  from  ai , 
and  go  from  ai − 1  to  (k , k).  The  other  paths  to  (k , k)  start  upward  from  ai . 
By  shifting  the  graph  down  and  left  (along  the  45◦  line)  we  imagine  these 

4 

��
b3  ��

b 2 

b1 

b0 

��
��

��
��

��
a0 

��
��

��
��

��
��

��
a1 

��
��

��
��

��
��

��
a 2 

� �

� �

� �

a3 

Figure  2:  L  counts  paths  to  the  45◦  gluing  line.  U  counts  paths  above. 

paths  going  from  ai − 1  to  the  point  (k − 1, k − 1).  Those  continuations  of 
the  upward  start  are  counted  by  Li − 1, k − 1 .  The  path  counts  agree  with 
Pascal’s  recursion,  so  they  are  the  entries  of  L.  Similarly  Ukj  counts  the 
paths  from  (k , k)  to  bj . 
It  only  remains  to  recognize  that  gluing  the  graphs  is  equivalent  to  mul­
tiplying  L  times  U !  The  term  Lik Ukj  counts  paths  from  ai  to  bj  through 
(k , k).  Then  the  sum  over  k  counts  all  paths  (and  agrees  with  Sij ).  The 6 
·
·
· 
paths  from  a2  to  b2  come  from  1  1 + 2  2 + 1  1.  This  completes  the  second
proof. 
One  generalization  of  this  proof  (to  be  strongly  resisted)  comes  from 
removing  edges  from  the  graph.  We  might  remove  the  edge  from  a1  to  a0 . 
That cancels all paths that go across to a0  before going up.  The zeroth row of 
1’s  is  subtracted  from  all  other  rows  of  S , which  is  the  ﬁrst  step  of Gaussian 
elimination. 
Those row operations (edge removals) are at the heart of Proof 3.  S  = LU 
is  the  fundamental matrix  factorization  produced  by  elimination. 

Proof  3:  Gaussian  Elimination 

The  steps  of  elimination  produce  zeros  below  each  pivot,  one  column  at  a 
time.  The  ﬁrst  pivot  in  S  (and  also  L)  is  its  upper  left  entry  1.  Normally 
we  subtract multiples  of  the  ﬁrst  equation  from  those  below.  For  the  Pascal 
matrices  Brawer  and  Pirovino  [1]  noticed  that  we  could  subtract  each  row 

5 

from  the  row  beneath. 
The  elimination  matrix  E  has  entries  Eii  =  1  and  Ei, i − 1  =  −1.  For 4 
by  4 matrices  you  can  see  how  the  next  smaller  L  appears: ⎤
⎡ 
⎡⎤
⎡⎤
�

�

EL4  =
 ⎢⎢⎣

⎢⎢⎣ 
⎥⎥⎦
 =
 ⎢⎢⎣

⎥⎥⎦ 
⎥⎥⎦

1

1 
1
−1 1

0 1

1 1

−1 1

= 
1 2 1

0 1 1

−1 1 
1 3 3 1 
0 1 2 1

E  times L gives  the Pascal  recursion Lik − Li − 1, k  = Li − 1, k − 1 , producing 
the  smaller matrix  Ln − 1—shifted  down  as  in  (3). 
This  suggests  a  proof  by  induction.  Assume  that  Ln − 1Un − 1  =  Sn − 1 . 
� 
� 
� 
�

� �

Then  equation  (3)  and  its  transpose  give 
. 

(ELn )(UnE T ) = 

0

1
0  L3

.


(3)


(4) 

0 
1
0  Ln − 1 

0 
1
0  Un − 1 

=

0 
1
0  Sn − 1 

We hope  that  the  last matrix  agrees with ESnE T .  Then we  can premultiply 
by  E −1  and  postmultiply  by  (E T )−1 ,  to  conclude  that  LnUn  = Sn . 
Look  at  the  i, j  entry  of  ESnE T : 
(ESn )ij  =  Sij  − Si − 1, j  and 
(ESnE T )ij  = (Sij  − Si − 1, j ) − (Si, j − 1 − Si − 1, j − 1 ) . 
In  that  last  expression,  the ﬁrst  three  terms  cancel  to  leave Si − 1, j − 1 .  This 
is  the  (i, j )  entry  for  the  smaller  matrix  Sn − 1 ,  shifted  down  as  in  (4).  The 
induction  is  complete. 
This  “algorithmic”  approach  could  have  led  to LU  = S  without  knowing 
that  result  in  advance.  On  the  graph,  multiplying  by  E  is  like  removing  all 
horizontal  edges  that  reach  the  45◦  line  from  the  right.  Then  all  paths must 
go upward  to that line.  In counting, we may take their last step for granted— 
leaving  a  triangular  graph  one  size  smaller  (corresponding  to  Ln − 1 !). 
The  complete  elimination  from  S  to  U  corresponds  to  removing  al l  hor­
izontal  edges  below  the  45◦  line.  Then  L  =  I  since  every  path  to  that  line 
goes  straight  up.  Elimination  usually  clears  out  columns  of  S  (and  columns 
of edges) but this does not leave a smaller Sn − 1 .  The good elimination order 
multiplies  by  E  to  remove  horizontal  edges  a  diagonal  at  a  time.  This  gave 
the  induction  in  Proof  3. 

6 

Powers,  Inverse,  and  Logarithm  of  L 

Lv
 =


(5) 

(6)


In  preparing  for  Proof  4,  consider  the  “functional”  meaning  of  L.  Every 
Taylor  series  around  zero  is  the  inner  product  of  a  coeﬃcient  vector  a  = 
(a0 , a1 , a2 , . . .)  with  the  moment  vector  v  =  (1, x, x2 , . . .).  The  Taylor  series 
�

represents  a  function  f (x): 
ak x k  = a T v  = a TL−1Lv  . 
Here  L  becomes  an  inﬁnite  triangular  matrix,  containing  all  of  the  Pascal 
⎡ 
⎡⎤
⎡⎤
⎤
triangle.  Multiplying  Lv  shows  that  (5)  ends  with  a  series  in  powers  of 
⎢⎢⎣

⎥⎥⎦ 
⎥⎥⎦
 =
 ⎢⎢⎣

⎥⎥⎦

⎢⎢⎣ 
(1 + x): 

1 
1

1
1 1

x

1 + x

(1 + x)2 
2x

1 2 1

·
·
·
·
· 
· 
� � 
The  simple  multiplication  (6)  is  very  useful.  A  second  multiplication  by 
Multiplication  by  Lp 
L  would  give  powers  of  2 + x.

gives  powers  of  p + x.

The  i, j  entry  of Lp  must  be  pi−j 
i 
,  as  earlier  authors  have  observed  (the  4

⎡ ⎢⎢⎣

⎤
j 
by  4  case  is  displayed):

⎥⎥⎦

1 
p 
1 
p2  2p 
1 
p3  3p2  3p  1 
For all matrix sizes n = 1, 2, . . . , ∞ the powers Lp  are a representation of the 
groups Z and R  (integer p and  real p).  The  inverse matrix L−1  has  the  same 
⎡ 
⎡⎤
⎡⎤
⎡⎤ 
form  with  p = −1.  Call  and  Velleman  [2]  found  L−1 
which  is DLD−1 : 
⎥⎥⎦ 
⎢⎢⎣ 
⎢⎢⎣

⎥⎥⎦ 
⎢⎢⎣

⎢⎢⎣ 
⎥⎥⎦
1 
1 
1
1
−1

−1

−1

1 1

=

1

1 2 1

−1
1 3 3 1 
⎡ 
⎤
Lp  has  the  exponential  form  eAp  and  we  can  compute  A = log L: 
⎥⎥⎦
 .

⎢⎢⎣


1 
−2
1 
3  −3

and  LpLq  = Lp+q  .


− I 
eAp 
p 

A =  lim

p→0 

Lp

− I
p 

=


=  lim 
p→0 

L−1  = 

1 

1

−1 

Lp  =


⎤
⎥⎥⎦


.


(7)


1

−1 
(8) 

(9)


0

1 0

0 2 0

0 0 3 0 

7 

· · · 
The  series  L = eA  = I + A + A2/2! + 
has  only  n  terms.  It  produces  the 
binomial  coeﬃcients  in  L.  This  matrix  A  has  no  negative  subdeterminants. 
Then  its  exponential  L  is  also  total ly  positive  [8,  page  115]  and  so  is  the 
product  S  = LU . 

Pascal  Eigenvalues 

A  brief  comment  about  eigenvalues  can  come  before  Proof  4  of  S  =  LU . 
The  eigenvalues  of  L  and  U  are  their  diagonal  entries,  all  1’s.  Transposing 
L−1  =  DLD−1  in  equation  (8)  leads  to  U −1  =  DU D−1 .  So  L  and  U  are 
similar to their inverses (and matrices are always similar to their transposes). 
It  is  more  remarkable  that  S −1  is  similar  to  S .  The  eigenvalues  of  S 
must come in reciprocal pairs  λ and 1/λ, since similar matrices have the same 
eigenvalues: 

S −1	 =  U −1L−1  = DU D−1DLD−1 
= (DU )(LU )(U −1D−1 ) = (DU )S (DU )−1  . 

(10) 
√

and  λ2  = 4 − √
The  eigenvalues  of  the  3  by  3  symmetric  Pascal  matrix  are  λ1  =  4 + 
15 
15  and  λ3  =  1.  Then  λ1λ2  =  1  gives  a  reciprocal  pair,  and 
λ3  = 1  is  self-reciprocal.  The  references  in  Higham’s  excellent  book  [5],  and 
help  pascal  in MATLAB,  lead  to  other  properties  of  S  = pascal(n). 

Proof  4:  Equality  of  Functions 

If  S v  = LU v  is  veriﬁed  for  enough  vectors  v ,  we  are  justiﬁed  in  concluding 
that  S  =  LU .  Our  fourth  and  favorite  proof  chooses  the  inﬁnite  vectors 
v  =  (1, x, x2 , . . .).  The  top  row  of  S v  displays  the  geometric  series  1 + x + 
· · · 
= 1/(1 − x).  Multiply  each  row  of  S v  by  that  top  row  to  see  the 
x2  + 
next  row.  The  functional  meaning  of  S  is  in  the  binomial  theorem. 
⎡ 
⎡⎤
⎡⎤
⎤
We  need  |x| < 1  for  convergence  (x  could  be  a  complex  number): 
⎥⎥⎥⎥⎦

⎥⎥⎥⎥⎦ 
⎢⎢⎢⎢⎣ 
⎢⎢⎢⎢⎣

⎢⎢⎢⎢⎣

⎥⎥⎥⎥⎦

·

1/(1 − x) 

·

1/(1 − x)2 
·

1/(1 − x)3 
=

·

1/(1 − x)4 
· 
· 

1 
1
1 1
4

3 
1 2
1 3  6  10

1  4  10  20

·
·
·
·

1

x

2 
x

3 
x

· 

S v
 =


.


(11)


8 

.


(12)


The  same  result  should come  from LU v .  The ﬁrst  step U v  has extra powers 
⎡⎤
⎤
⎡ 
⎡⎤
of  x  because  the  rows  have  been  shifted: 
⎢⎢⎢⎢⎣

⎢⎢⎢⎢⎣ 
⎥⎥⎥⎥⎦

⎢⎢⎢⎢⎣

⎥⎥⎥⎥⎦

⎥⎥⎥⎥⎦ 
·

1/(1 − x) 
1 1 1 1 
1

·

x/(1 − x)2 
0 1 2 3

x

·

2/(1 − x)3 
2 
0 0 1 3

x

x

=

·

3/(1 − x)4 
3 
0 0 0 1

x

x

· 
·
·
·
·
· 
· 
Factoring out 1/(1−x), the components of U v  are the powers of a = x/(1−x). 
Now  multiply  by  L,  with  no  problem  of  convergence  because  all  sums  are 
ﬁnite.  The  nth  row  of  L  contains  the  binomial  coeﬃcients  for  (1 + a)n  = 
⎡ 
⎡⎤
⎡⎤
⎤
x )n  = ( 1−1 
(1 +  1−x
x )n : 
⎥⎥⎥⎥⎦

⎢⎢⎢⎢⎣

⎢⎢⎢⎢⎣ 
⎥⎥⎥⎥⎦ 
⎢⎢⎢⎢⎣

⎥⎥⎥⎥⎦

=

Thus  S v  =  LU v  for  the  vectors  v  =  (1, x, x2 , . . .).  Does  it  follow  that 
S  =  LU ?  The  choice  x  =  0  gives  the  coordinate  vector  v 0  =  (1, 0, 0, . . .). 
Then  S v 0  = LU v 0  gives  agreement  between  the  ﬁrst  columns  of  S  and  LU 
(which  are  all  ones).  If  we  can  construct  the  other  coordinate  vectors  from 
the  v ’s,  then  all  the  columns  of  S  and  LU  must  agree. 
The  quickest  way  to  reach  (0, 1, 0, . . .)  is  to  diﬀerentiate  v  at  x  =  0. 
� 
� 
� 
� 
Introduce  v Δ  = (1, Δ, Δ2 , . . .)  and  form  a  linear  combination  of  v Δ  and  v 0 : 
v Δ  − v 0  = LU 
v Δ  − v 0 
S 
. 
Δ 
Δ

1/(1 − x) 

1/(1 − x)2 
1/(1 − x)3 
1/(1 − x)4 
· 

1 0 0 0 
1 1 0 0

1 2 1 0

1 3 3 1

·
·
·
·

1

a

2 
a

3 
a

· 

·

·

·

·

· 

L(U v ) =


1

1 − x


.


(13)


U v
 =


(14) 

→
Let Δ 
0.  Every  series  is  uniformly  convergent,  every  function  is  analytic, 
every  derivative  is  legitimate.  Higher  derivatives  give  the  other  coordinate 
vectors, and the columns of S  and LU  are identical.  By working with inﬁnite 
matrices,  S  = LU  is  conﬁrmed  for  all  orders  n  at  the  same  time. 
An  alternative  is  to  see  the  coordinate  vectors  as  linear  combinations  of 
(a  continuum  of )  v ’s,  using  Cauchy’s  integral  theorem  around  x = z = 0. 
These  functional  proofs  need  an  analyst  somewhere,  since  an  algebraist 
working  alone  might  apply  S  to  S v .  The  powers  of  this  positive  matrix  are 

9 

�
∞
=  −1/x.  Even  worse  if  you  multiply

(1 − x)−n 
suddenly  negative  from

⎡
⎤
⎤
⎡ 
again  by  S  to  discover  S 3v  = −v :

1
⎥⎥⎦

and  S 3 v  =
 ⎢⎢⎣

⎥⎥⎦

⎢⎢⎣

−1

−1/x

−x 
−(x − 1)/x2 
−x

−(x − 1)2/x3 
2
· 
·
We  seem  to  have  proved  that  S 3  =  −I .  There  may  be  some  slight  issue  of 
convergence.  This didn’t bother Cauchy  (on his good days), and we must be 
seeing  a matrix  generalization  of  his  geometric  series  for  1/(1 − 2): 
1 + 2 + 4 + 8 +  = −1 . 
· · · 

= −v  . 

S 2 v  =


(16) 

(15)


M¨obius  Matrices 

A  true algebraist would  look  for matrices of Pascal  type  in a group  represen­
tation.  Suppose  the  inﬁnite  matrices  S  and  U  and  L  represent  the  M¨obius 
1/(1 − x)  and  x/(1 − x)  and  x + 1  that  we  met  in 
→
transformations  x 
Proof  4.  Then  LU  =  S  would  have  an  even  shorter  Proof  5,  by  composing 
y = x/(1 − x)  and  z = y + 1  from  L  and  U : 
1 
x 
1 − x 
1 − x 
We  hope  to  study  a  larger  class  of  “M¨obius matrices”  for  (ax + b)/(cx + d). 
A  ﬁnite-dimensional  representation  leads  to  M 3  =  I  for  the  rotated  matrix 
⎤
⎡
with alternating signs known to MATLAB as M  = pascal(n, 2).  Here is n = 3: 
⎦

M 3  =
 ⎣

3 

= I  because


+ 1 = 

= x . 

z = 

. 

1 
1 1

−2  −1 0

0

0

1

1

1 −  1− 
1 
1
1−x 

Waterhouse  [11]  applied  that  idea  ( mod  p)  to  prove  a  theorem  of  Strauss:  If 
n  is  a  power  of  p,  then  S 3  = I  (mod  p).  It  seems  quite  possible  that  digital 
transforms  based  on  Pascal  matrices  might  be  waiting  for  discovery.  That 
would  be  ironic  and  wonderful,  if  Pascal’s  triangle  turned  out  to  be  applied 
mathematics. 

10 

Conclusion:  Two  Opinions  of  Pascal’s  Triangle 

Pascal  was  not  the  ﬁrst  to  create  his  triangle!  Edwards  [4]  describes  the 
gradual  discovery  of  its  entries,  in  Persia  (Omar  Khayyam  himself )  and  in 
China and Europe and  India.  The proofs  were Pascal’s (including a proof by 
induction  that  became  a  model  for  future  mathematicians).  We  very  much 
appreciated the sentiments of James Bernoulli, who completed the connection 
· · · 
with  powers  by  computing  1p  +
+ N p :

“This  Table  has  truly  exceptional  and  admirable  properties;  for 
besides  concealing  within  itself  the  mysteries  of  Combinatorics, 
it  is  known  by  those  expert  in  the  higher  parts  of  Mathematics 
also  to  hold  the  foremost  secrets  of  the  whole  of  the  rest  of  the 
sub ject.” 

No  one  could  say  better  than  that.  But  a  genius  of  our  own  day  expressed  a 
diﬀerent  thought,  which  our  friendly  readers  would  surely  never  share  [9]: 

“There  are  so many  relations  present  that  when  someone  ﬁnds  a 
new  identity,  there  aren’t  many  people  who  get  excited  about  it 
any more,  except  the  discoverer!” 

References 

[1]  Robert Brawer  and Magnus Pirovino, The Linear Algebra  of  the Pascal 
Matrix,  Linear  Algebra  and  Its  Applications  174  (1992)  13–23. 

[2]  Gregory  Call  and  Daniel  Velleman,  Pascal’s  Matrices,  American  Math. 
Monthly  100  (1993). 

[3]  E.B.  Curtis,  David  Ingerman  and  J.A.  Morrow,  Circular  planar  graphs 
and  resistor  networks,  Linear  Algebra  and  Its  Applications  283  (1998) 
115–150. 

[4]  A.W.F. Edwards, Pascal’s Arithmetical Triangle:  The Story of a Mathe­
matical Idea, Charles Griﬃn, 1987 and Johns Hopkins University Press, 
2002. 

[5]  N.J.  Higham,  Accuracy  and  Stability  in  Numerical  Algorithms,	 SIAM 
(1996). 

11 

[6]  Peter  Hilton  and  Jean  Pederson,  Looking  into  Pascal’s  Triangle:  Com­
binatorics,  Arithmetic,  and  Geometry, Math. Magazine  60  (1987)  305– 
316. 

[7]  David Ingerman, Discrete and continuous inverse boundary problems on 
a  disc,  Ph.D.  Thesis,  University  of Washington,  1997. 

[8]  Samuel Karlin, Total Positivity, Vol. 1, Stanford University Press, 1968. 

[9]  Donald  Knuth,  Fundamental  Algorithms:  Vol.  I,  The  Art  of  Computer 
Programming,  Addison-Wesley,  1973. 

[10]  Gilbert  Strang,  Introduction  to  Linear  Algebra,  3rd  edition,  Wellesley-
Cambridge  Press,  2003. 

[11]  W.C.  Waterhouse,  The  map  behind  a  binomial  coeﬃcient  matrix  over 
Z/pZ,  Linear  Algebra  and  Its  Applications  105  (1988)  195–198. 

12 

MIT OpenCourseWare
http://ocw.mit.edu 

18.06 Linear Algebra 
Spring 2010 

For information about citing these materials or our Terms of Use, visit:  http://ocw.mit.edu/terms. 

