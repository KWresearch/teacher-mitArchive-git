Urban   Operations  Research  

Prepared  by  Michael  D.   Metzger 

Fall  2004 

Problem  Set  1   Solutions 

9/27/04 

1.  LO   Problem  2.3  (Ingolfsson,  1993;  Kang,  2001) 

Anyone  who  arrives  at  the   transfer  station   observes  two  independent  Poisson  processes,  A  and  B . 
λA
Each  arrival   of  the  combined   Poisson  process  comes  from  process  A  (B )  with   probability  λA+λB 
λB 
),  independently of  all  other  arrivals. Thus  the  combined  Poisson   process  has  an  embedded 
( 
λA+λB 
Bernoulli  process.  To   solve  this   problem,  you   need   a  good   grasp   of  the  fundamental  properties  of 

Poisson  and  Bernoulli   processes.   If  you  feel  uncomfortable  with  the  answers  below,  now   is  a  good 

time   to  review   Poisson  and  Bernoulli  processes,  for  example,  by reading   Chapter   4   of  “Fundamentals 

of  Applied   Probability  Theory”  by  Alvin  Drake. 

(a)	

(i) The  times   between  successive   A  train   arrivals  are  independent  exponential  random  vari(cid:173)

ables   with  parameter  λA  = 3/hour.  By  the  memoryless  property,  the  fact  that  Bart 
arrives  at   a  random   time   has  no  relevance.   The  time  he  has  to  wait  until  the  next   A 

train  is  still  an  exponential  random   variable  with   parameter  λA . So  if we  call  his  waiting 
time  X ,  then  the  PDF   of   X  is  given  by 

fX (x) =   λAe −λA x  = 3e 
−3x

, x  ≥  0. 

This  is  in  fact  a   random   incidence  question.  So  we  can  also  solve  this  question  by  using 

formula  (2.65)   in  the  textbook.  Let  Y  be  the  interarrival  time  of  A  trains,  then 

fX (x) =  

1  −  FY  (x)
E [Y  ] 

. 

1 
E [Y  ] =  λ
A 

1
=  3 .  FY  (x) =  P {Y  ≤  x} = 

λAe−λA y dy   = 1  −  e−λA x  = 1  −  e−3x  .  Hence 

x
0 

� 
−3x )
1  −  (1  −  e
1 
3 

fX (x) = 

= 3e −3x  , x  ≥  0. 

(ii)  An  easy  way  to  answer  this   question  is  to  ﬁrst  translate  it   into  a  Bernoulli  process  that 

one  is  familiar  with,   for  example,  a  sequence  of  coin   tosses.  An  A  train   then  becomes  a 

“head”   and  a  B  train  becomes  a  “tail”.  The  problem   now  reads:  What  is  the  probability 

that  one  obtains  at  least  3   tails  before  a  head   is  obtained?  This  probability   is  the  same 

as  the  probability  that  the  next  three  tosses  result  in   tails  (i.e.  the  next  three   trains 

are   B  trains).  The  outcomes  of  the  fourth  and  subsequent  tosses  are  irrelevant  for  the  

1


purposes  of  answering  this   question.   Thus, 

P {at  least   3  B  trains  arrive} =  P {next  3  trains  are  B  trains} 
�3  �
�3  � �3
�
2  
=
= 
3 
Another  way  to  answer  this  question  is  to  consider  the  complementary  event.  Let   NB 
be  a   random   variable  denoting the  number  of  B  trains  that  arrive  while   Bart  is  waiting. 

λB 
λA  +  λB 

6
3  +   6 

= 

. 

Then 

P {NB  ≥ 3} = 1  − P {NB  = 0} − P {NB  = 1} − P {NB  = 2} 
�2 
� �3
�
2 
3 
(iii)  In  order  for  exactly  3  B  trains  to  arrive  while  Bart   is  waiting,   the  next  3  trains  should 

λB 
λA 
·
λA  +  λB  λA  +  λB 

λB 
λA  +  λB 

λA 
λA  +  λB 

λA 
λA  +  λB 

= 1  −

−  

− 

= 

. 

be  B  trains  and  the  fourth  one  should  be  an   A  train. 

P {exactly 3   B  trains} =  P {next  3  trains  are  B  trains,  the  4th train   is  an  A  train} 
� �3  � � 
�3 
�
2
1 
3
3 
(b)  The  combined  Poisson  process  of  train   arrivals  has  an  arrival  rate  of  λ  =  λA  +  λB  = 9/hour. 
The  probability  of  having exactly  9   arrivals  during  any  hour  (t  = 1)  in   this  process  is  obtained 

λB 
λA  +  λB 

λA 
λA  +  λB 

=

= 

. 

by  using   the  Poisson   PMF  formula.  

PK (9) = 

(λt)9e
−λt 
9! 

= 

99 −9
e
9! 

� 0.1318. 

(c)  To  answer  this   question,   one  needs  to  consider  another  Bernoulli  process  associated  with  the  

combined  Poisson  process.  In  this  Bernoulli  process,  each   hour  is  an   independent  trial  and 

an  hour  is   a  success  if  exactly  9  trains  arrive  during  that  hour.  From  (b),  a  success  occurs 

with   probability  PK (9)  � 0.1318.  The  question  asks  what  the  expected   number  of  trials  until 
the  ﬁrst   success  is.  The   answer  is  the  expected   value  of  a  geometric  random  variable   with 

parameter  PK (9),  i.e. 

E [number  of  hours  until exactly 9   trains  per  hour] = 

1
PK (9)  

� 

1  
0.1318 

= 7.59. 

(d) 

(i)  An  A  train  will   be  delayed  if the  time,  denoted   by  Z ,  from  the  arrival  of  the  A  train  to 

the  moment  of  the  arrival  of  the  next  B  train   is  less  than  30   seconds. By the  memoryless 

property,  this  time   has   an  exponential  PDF  with  parameter  λB .  Thus  the  probability 

2


PD  =  P {Z   ≤  30  seconds} = 

that   an  A  train  is  delayed  is  obtained  by 
�  (30   secs)(1   hr/3600  secs) 
0 
�  1/120  
−1/20 
6e −6t  dt  = 1  −  e
0 
Since  probabilities  equal   long-run  frequencies,  this  is  also  approximately the  fraction  of 

λB e −λB t  dt 

=

. 

A  trains  that  are   delayed  over  some   reasonably  long  period,  say  a  month. 

(ii)  A  B  train  passenger  that  beneﬁts  from  the  delay  policy does  not  have  to  wait   at  all  for 

an  A  train,   so   her  expected  waiting  time  under  the  policy  is  zero.  Without  the   policy, 
1
1 
=  =  20  minutes.  Therefore  such   a  passenger’s  
her  mean  waiting  time   would  be   λ
3 
A 
mean  waiting   time   reduction   is  20   minutes. 

Next,   consider  a   passenger  in  an  A  train.  With  probability  PD  computed   in  (i),  the 
passenger’s  travel  time   will   increase  by the  amount  of   time  that  the  A  train   waits  for  a 

B  train.   Note  that  the  mean   increase  in   travel  time  for  a  passenger  in  an  A  train   that 

is  held  for  a   B  train  is  equal  to  E [Z  |  Z  ≤  1/120].  To  compute  this  quantity,  we  invoke 

the   total   expectation  theorem:  

E [Z ] =  E [Z   |  Z  ≤  1/120]P {Z  ≤  1/120} +  E [Z  | Z  >  1/120]P {Z  >   1/120}. 

1 
Since  E [Z ] =  λ
B 

1
and  E [Z |Z  >  1/120]  =   120  + 

1
λB 

,  we  have 

1/λB  =  E [Z   | Z  ≤  1/120]PD  +  (1/120   +  1/λB )(1   −  PD ). 

Rearranging terms,  

E [Z   | Z  ≤  1/120]  =  

1/λB  −  (1/120  +  1/λB )(1   −  PD )
PD 

�  0.00413  hours  =   14.9  seconds. 

Therefore  the  mean   increase  in  travel time  for  an   A  train   passenger  is 

E [Z   | Z  ≤  1/120]  ×  PD  =  14.9  seconds  ×  (1  −  e −1/20 ) = 0.73  seconds. 

One  might   attempt  to  obtain   the  mean  increase  in   travel time  (i.e.  the  expected  waiting 

time,  denoted  by  E [W ]) for  an   A  train   passenger   as  follows: 

3 

E [W ] =  E [W | Z ≤  1/120]PD  +  E [W | Z > 1/120](1  −  PD ) 
?  1
1 
· 
= 
2  120  

×  (1  −  e −1/20 ) +   0 . 

This  is  not   correct   because   E [W  |  Z ≤  1/120] 

1
1 
120 .  The  restriction,  Z ≤  1/120,
2 
cuts  oﬀ  the  right   tail  of  the  exponential  density  curve,  fW (w),  but   what  is  left  is  NOT 
uniform. 

= 

·

Let  us  assume  that  it  never  happens  that  two  or  more  A trains  are  delayed  waiting  for  

the  same  B  train,   i.e.  we  ignore  the  possibility  that  two  A trains  may  arrive  within 

30  seconds  of   each  other.   Under  this  assumption,  one  A train   is  held   for  every  B train 

receiving the  beneﬁts. Therefore  the  policy will  lead   to  a  net  global  travel time  reduction 

if 

⇒  E [NBA ] × 

E [total  time  reduction] > E [total  time  increase] 
1 
3 
⇒  E [NBA ] > 0.012  ×  E[NA ], 

hours  > E [NA ] ×  0.00413  hours 

where  NBA  is  the  number  of  people   on   a  B train   who  wish to   transfer  to  an  A train  and 
NA  is  the  number  of   people  on  an  A train   being  held.  In   words,   the  policy  is  favored  if  
the  average  number  of   people  on  a  B train   who  wish   to  transfer  is  at  least   1.2%  of   the 

average  number   of  people   on  an  A train. 

This   is  a  condition  that  one  would expect  to   hold true  for  most  subway transfer   stations. 

You  might  want   to  think  about  the  eﬀect  of  ignoring   the  possibility  of  two  A trains 

arriving   within  30  seconds  of   each  other.  How   would  one  assess  the  reasonableness  of 

this  simpliﬁcation?  How  would   the  analysis  change  if  one  did  not  make  this  simplifying 

assumption? 

3.1  (Pinker,  1995) 

(a)  Let  Ji , i = 1, 2  be  the  event  that  Jones  patrols  beat  i,  let   Ei , i = 1, 2  be  the  event  that  Elmer 
burglarizes  beat   i,  and  let   S be  the  event  that  Elmer  and   Jones  work  on   the  same  beat. Then 

Pr(S )  =  Pr(S |J1 ) Pr(J1 ) +   Pr(S |J2 ) Pr(J2 ) 

=  Pr(E1 |J1 ) Pr(J1 ) +   Pr(E2 |J2 ) Pr(J2 ) 

= 

(1/3)(1/2)   +  (1/3)(1/2)  = 1/3 

4 

�
(b)  Let  XE  and  XJ  be  Elmer  and  Jones’s  positions  during  the  burglary.  We  are  told  that  these 
two  random   variables  are  independent  and  both  have  a   uniform  distribution  over   [0,  l].  Let 

W  =  |XE  − XJ |. We  are  asked to   compute  Pr{W  >  l/2|W  >  l/4}. The  two  events  of  interest  
are  shown  in  the  sample   space  for  (XE , XJ )  below.  Since  (XE , XJ ) is  uniformly  distributed 
over  [0, l]2 ,  the  desired  probability  is  the  ratio  of  the  areas  of   the  events  {W  >   l/2}∩{W  >  l/4} 

and  {W  >  l/4}.   From   the  picture   of  the  sample  space, 

Pr{W  >  l/2|W  >  l/4} = 

Pr({W  >   l/2} ∩   {W  >  l/4}) 
Pr{W  >  l/4} 

=

l2/4 
9l2/16  

= 

4 
9 

Figure  1:  Sample  Space 

(c)  In   (b)  we  computed   the   areas  corresponding  to   Pr{W  >  l/2}  and  Pr{W  >  l/4}.  By  general(cid:173)

izing the   argument,   we  can  compute 

Pr{W  > w} = (1/l2 )(2)(1/2)(l  −  w)2  = (1   −  w/l)2  for  0  ≤  w  ≤  l 

The  cdf   is  FW  (w) = Pr{W   ≤  w} = 1  −  Pr{W  > w} = 1  −  (1  −  w/l)2  and  the  pdf  is 

fW  (w) = 

d
dw 

FW (w) = 

d 
dw 

(1  −  (1  −  w/l)2 ) = (2/l)(1   −  w/l) for  0  ≤  w  ≤  l 

(d)  The  conditional   expectation  rule  applies  to  probabilities  of  events  also.  This  is  easily  seen  by  

PA  = 

= 

= 

Pr(A|w)fW  (w)dw


Pr(A|w)fW  (w)dw  + 

letting   IA  be  an  indicator  random  variable  which  takes  on   the  value  one  if  event  A  occurs  
and  zero  otherwise.   Then  E[IA ] is  seen   to  equal  PA .  As  a  result, 
�  l

0 
�  d 
0 
�  d 
0 
(2/l) 

�  l 
Pr(A|w)fW  (w)dw 
d 
�  l 
(1   −  w/d)(2/l)(1   −  w/l)dw  + 
d 
�  d 
(1  −  (1/l  +  1/d)w  +  (1/dl)w 2 )dw 
0 
(2/l)  d  −  (1/l  +  1/d)d2/2  +   (1/dl)d3/3 
� 
� 
(2/l)  d  −  d2/(2l) −  d/2  +  d2/(3l) 
� 
� 
d/l  −  (1/3)d2/l2
� 
� 
= (d/l)(1  −  (1/3)(d/l)) 

(0)(2/l)(l  −  w/l)dw 

= 

= 

= 

= 

When   d  =  0,   then  Jones  will  apprehend  Elmer  only  if  they  are  at   exactly  the  same  location, 

5 

which  is  an  event   of  probability  zero.  Accordingly,  PA  = 0  when  zero  is  substituted   for  d. 
When   d  =  l  is  substituted,   PA  = 2/3   is  obtained.  Note  that  this  is  equal  to  Pr(A|E[W ])  = 
Pr(A|l/3)  = 1  −  (l/3)/l  = 2/3.  This  happens  because  when  d  =  l,  then  Pr(A|w) is  a  linear 

function  of  w,  say  aw  +  b.  Then 

PA  = 

=

Pr(A|w)fW  (w)dw 

�  l 
0 
�  l 
(aw  +  b)fW  (w)dw 
0 
�  l 
�  l 
=  a 
0
0 
=  aE[W ] +   b 
=  Pr(A|E[W ]) 

wfW (w)dw  +  b

fW  (w)dw 

When   d <  l,  then  Pr(A|w) is  piece-wise  linear,  i.e.,  non-linear  over  [0,  l]. 

(e)   Let  S  be  the  event  that  Elmer  and   Jones  are  on  the  same  beat  and  A  be  the   event  that  Jones 

apprehends  Elmer.  Then  

Pr(A) = Pr(A|S ) Pr(S ) +  Pr(A|  not  S )(1   −  Pr(S )) =  PA (1/3)  +  0   = 

� 
� 
Furthermore,  event  A  is  independent  of  whether  Elmer  has been   apprehended  on previous 

d 
3l 

d
3l 

≡  p

1  − 

nights,  i.e.,  the  process  of  Jones  apprehending   (success)  or  not  apprehending  (failure)  Elmer 

on  successive  nights  is  a  Bernoulli  process.  In  Bernoulli   process  terminology,  we  are  asked 

to  compute  the  probability  that  the  ﬁrst  success  occurs  on  the  third   trial.   This  equals  the  

probability  of  two  failures  followed   by  a  success,  i.e., 

(1   −  p)2 p  = (1   −  PA/3)2 (PA/3)  

(f )  Let  Ai  and  Si  be  the  events  that  Elmer  is  apprehended  on  the  i-th   night  and   that  Elmer  and 
Jones  are  on  the  same   beat   on  the  i-th   night.  Let  IAi  and  ISi  be  indicator  variables  for  those 

6


events.  Then  we  want

Pr  �  10 
� 1 

ISi  =  3| 

10 
� 1 

IAi  =  0 �  = 
= 

= 

Pr{ �10  
1  IAi  =  0|  �10  
1  ISi  =  3} Pr{ �10  
1  ISi  =  3} 
Pr{ �10  
1  IAi  =  0} 
(1  −  PA )3 �10  
3  � (1  −  Pr(S ))7  Pr(S )3 
(1  −  PA/3)10  
(1  −  PA )3 (120)(2/3)7 (1/3)3 
(1  −  PA/3)10  
(1  −  PA )3 
(3  −  PA )10  

=  15,   360  

(g)   As  before,  Pr(A) = Pr(A|S )(1/3),   and  

�  l 
0 
Given  that  Elmer  is   to   Jones’s  left,  W  will  be  uniformly  distributed  over  [0,  l/2]. The  same  is 

Pr(A|w)fW (w)dw 

Pr(A|S ) =  

true  if  Elmer  is   to   the  right  of  Jones.   Therefore,  unconditionally,  W  is  uniformly distributed 

over  [0,  l/2].  First,  suppose  that  d <  l/2.  Then  
�  d 
0 

Pr(A|S ) =  

(1   −  w/d)(2/l)dw  =  d/l 

Second,  suppose  that  d  ≥  l/2: 

Pr(A|S ) = 

�  l/2  
0 

(1   −  w/d)(2/l)dw  = 1  −  (1/4)(l/d) 

Therefore, 

Pr(A) =  

if  d <   l/2 

� 
d 
3l  
1  1  − 
�
� 
3
As  in   part  (d),   Pr(A)  = 0  if   d  =  0.  If  d  =  l,  Pr(A)  = (1/3)(1 − 1/4)  = 1/4,  which   agrees  with  
� 
� 

1
Pr(A|E[W ])  =   Pr(A|l/4)   = 
3

if  d  ≥  l/2

1  − 

1 
3 

1 
3 

l/4
l 

= 

1 
4 

l 
4d 

7 

Problem 3 (Metzger 2004) 

PART A: 

Let V=Number of left to right pedestrians crossing on a green light.


We can divide the quantity into two parts: W and Z


Let V=W+X+Z


Where:


W=The probability the  first pedestrian to arrive after a green light is a left to right


X= The probability the second pedestrian to arrive after a green light is a left to right


Z=The number of left to right pedestrians to arrive during the Q min between the second

arrival(X) and the green light.


Thus we can take the expectation of each side:


E[V]=E[W+X+Z]


We can split the above up using the additive property of expectations as follows:


E[V]=E[W]+E[X]+E[Z]


As shown in class for case 3, we can write W as follows:


W  =


À 
λL 
p w 
.. 
. 
.. 1 
ŒŒ
λ  + λR
Ã

L 
λ 
Œ
R 
.. 0 
.. 
. 
p w 
ŒÕ 
λ  + λR
L 

¤ 
ŒŒ
‹ 
Œ
Œ› 

What the above is saying is the probability the first arriving pedestrian is a left to right is the 
proportion of arrivals that are left to right. Thus the expected value for W: 

W E 
[ 

] = 

1 *  + 

λ
λ 
λ 
L 
R 
L
λ + λ 
λ + λ 
λ + λ
L 
R
L 
R
L
R
Since arrivals are independent, based on the properties of a Poisson process(see chapter 2 for 
more information)  we can write an expression for X in the same manor 

0 *  = 

X  =


À

λL 
p w 
.. 
. 
.. 1 
ŒŒ
λ  + λR
Ã

L 
λ 
Œ
R 
.. 0 
.. 
p w 
. 
ŒÕ 
λ  + λR
L 

¤ 
ŒŒ
‹ 
Œ
Œ› 

8


We can similarly write the expectation out in the same fashion. 

X E 
[ 

] = 

λ
L 
λ + λ 
R
L

1 *  + 

λ 
R 
λ + λ 
R
L 

0 *  = 

λ 
L
λ + λ
R
L 

We know from class that Z is has a Poisson PDF, with time parameter Q and can be written as 
follows. 

Z P  
( 

=  z ) = 

(λ Q) z 
z! 

−λ Q

e

Thus the expected value of Z is: 

Z E 
[ 

] = λ Q

Thus we can now sum these three quantities to find E[V] 

[ 
V E 

] = 

λ
λ 
L
L 
+ 
λ + λ  λ + λ 
R
L
R
L 

+ λ Q  = 

2λ 
L 
λ + λ
R
L 

+ λ Q 

Part B: 

In this part we need to find the prob V=0. Based on our above equation V=W+X+Z, the 
probability V=0 is the probability that each of V’s components are zero. Thus we can write this 
mathematically as: 

( 
V P 

= 

)0  =  W P 
(

= 

(
* )0 
X P 

= 

(
* )0 
Z P 

= 

)0 

The P(W=0) is simply the probability that the first pedestrian to arrive after a green light is a right 
to left pedestrian. Thus: 

W P 
( 

= 

)0  = 

λ 
R
λ  + λR
L 

Since arrivals are independent the probability that X=0 is equal to the probability that W=0. Thus: 

( 
X P 

= 

)0  = 

λ 
R
λ  + λR
L 

Lastly the probability that ZX=0, is the probability that we have no arrives in the Q time units 
after ‘X’ arrives. Thus we can plug z=0 into the above formula for the distribution of Z to find 
this quantity. 

Z P 
( 

= 

)0  =  e  −λQ

Thus we can now find the probability that V=0 

9


V P 
( 

= 

)0  =

2

≈
 λR 
’

ΔΔ
÷÷

«
 λ  + λR
L ◊


−λQ

e


Part C: 

WE next need to find the probability density for the time between green lights. Clearly this time 
we can again call A is composed of three intervals. The time until the first pedestrian to 
arrive(call this time B), plus the time between the first and the second pedestrian to arrive(call 
this time C), plus Q time unites after the second pedestrian arrives. 

Thus: 

A=B+C+Q. 

We know that the arrivals are a merged Poisson process, and thus from basic prob we know that 
the inter-arrival times of a Poisson process are exponentially distributed. Thus we can write the 
distributions of B and C as follows: 

− 
(  = λe  λ 
t
) 
t 

f B 

(  = λe  −λt 
t
) 

f 

C 

Thus in order to calculate the time distribution of V we need to sum two exponentially random 
variables plus a constant. From probability we know the sum of two exponential random 
variables is an earlang random variable of order two. Two see this once can convolve the two 
distributions. Lastly we need to account for the Q time units after the second pedestrian arrives 
thus we add  a shift factor of Q by replacing every t with a t-Q in the earlang distribution. Thus 
we get: 

(  = λ (t  − Q)e  −λ( t −Q ) 
2 
t
) 

f 

A 

Part D: 
We need to calculate the expected amount of time a random pedestrian waits for a green light. 
There are three types of pedestrians. 

Type 1: Those who are the first to arrive after a green light 

Type 2: Those who are the second to arrive after a green light 

Type 3: Those who arrive during the Q time units between the second arrival and the green light. 

Based on part A we can calculate the probability a random pedestrian is of each type. 

During each interval we have one type one, one type to, and we expect on average λQ Type 3’s. 

Thus using the total probability theorem: 

10


P(Type  )1  = 

P(Type  )2  = 

P(Type  )3  = 

1 
2 + λ Q


1

2 + λ Q


λ Q

2 + λ Q 

Thus now we need to calculate the expected wait time for a random arriver in each group.  Let us 
start with type 3. Pedestrians who are in type 3 are uniformly distributed over the last Q time 
Q 

units between each green light. Thus on average a pedestrian in this class waits, 

. A pedestrian 

2 
in type two waits always exactly Q time units till the light turns green. A pedestrian in type one 
waits the time between the 1st  and second arrival plus Q time units thus on average the time 
1 
λ 

, plus the Q time units after the second pedestrian 

between arrivals in a passion process is 

arrives until the light turns green, thus his average waiting time is 

1 
λ 

+  Q . 

Thus the expected waiting time of a random pedestrian is simply the weighted average of these. 

E[RandomPedestrian ] =


≈
1 
Δ 
«

λ 

’

+  Q ÷
◊


1
+
 λ Q 
2 

+ 

( Q )

1

2 +  λ Q 

+


≈
Q
Δ 
«


’
 λ Q

÷
◊

+  λ Q
2  2 

Part E: 

In order to calculate the expected wait time for a random observer we need to use the following 
formula derived in the book  (eq. 2,66) and in class. 

Let Z be the expected wait time of a random observer: 

[ 
Z E 

] =

2 
σ x 
2µ 
x 

+

µ 
x
2 

Let E[A] be the time till the first pedestrian after a green.


Let E[B} be the time between the first and second pedestrians after a green.


Since expectations are additive we know:


µ = 
x 

] + 
[
A E 

[
B E 

] +  Q  = 

1
1 
+ 
λ λ 

+  Q  = 

2 
λ 

+  Q 

11 

2 . 
We can similarly find σ x 

Let  var( A) = 

1

λ2


(Poisson Property) 

Thus  var( B) =


1

λ2

Since the time between the second arrival and the next green is a constant it has a variance of

zero.


Thus since we can add variances:


var( A) = 

1

λ2


σ2  =  var( A) +  var( B ) = 
x 

1
λ2 

+ 

1 
λ2


= 

2

λ2


Thus if we plug these two equations into the equation for E[Z] we obtain the desired quantity. 

12 

